Drosophila melanogaster, or the common fruit-fly, offers the geneticist a near perfect organism to explore the models of inheritance via simple, straight forward, cross breeding experiments. Short living -- with a fast generation time -- easy to handle and maintain in cultures and female only cross-over events, make D. melanogaster most desirable in the lab. As result much is known about the fruit flies genome; however, on occasion new mutations are discovered and experimentation is undertaken to determine the exact genes involved in these new mutants.
A true breeding culture of Drosophila, dubbed U-5343, is an example of such a scenario. The flies exhibit three genetically controlled novel phenotypic traits: black bodies, terminating pre-margin longitudinal veins LII and LIV, and white eyes. Ascertaining the nature of these mutations is an interesting problem and useful to the study of genetics in general. To answer questions, such as: what are modes of inheritance (dominant/recessive/sex-linked/autosomal); how many genes are involved; and, where are the genes located -- cross breeding experimentation was undertaken. Specifically, five initial crosses and five secondary crosses were preformed in hopes of answering these questions.
Two of these initial crosses were reciprocal crosses. Cross A (unknown virgin female x +type male) and Cross B (+type virgin female x unknown male) were done to determine if the genes are recessive or dominate and sex-linked or autosomal. The secondary crosses, F1xF1 of Cross A and F1xF1 of Cross B, were done to determine: the number of genes controlling the trait; these genes interaction, if multiple present; the segregation ratios for each gene; independent assortment between genes, by a χ2 test; and, the map distances between linked genes. Answering these questions was accomplished by scoring flies of both F1 and F2 generations of the two crosses.
Cross I (unknown virgin females x M-I males) and F1xF1 of Cross I were done to determine the relationship between the genes and the X-chromosome. The specifics of the marker type can be found in the MATERIALS AND METHODS section, however, these crosses were potentially helpful to determine if the genes are on the X-chromosome and, if yes, where they are located. Again, scoring F1 and F2 flies could be used to determine the loci of the genes on X-chromosome -- it was important here to record gender in this scoring. However, if no genes are found on the X-chromosome (based on the results of Crosses A and B), then simply scoring F2 flies would be helpful to later determine the map distance between linked genes not on the X-chromosome. This is possible because if the genes are not sex-linked, then ignoring the secondary marker characteristics of the F2 would make them functionally the same as the F2's of cross A and B -- since then they become equivalent to the wild-type flies.
Finally, the Crosses II and III were done to determine where the genes were located in regards to the second and third autosome, with autosomal specific traits. Specifics of these marker types and their phenotypic expression can be found in the MATERIALS AND METHODS. In brief, however, these initial crosses, when scored would help show only if the mutant genes were dominant or recessive. A secondary backcross of the male progeny of these crosses with unknown virgin females was necessary to determine which chromosome the genes were on. Notably absent for all the crosses described here is an emphasis on autosome IV; since this autosome is so small it is not likely that these mutations' loci are there.
The crosses and scoring were done over a series of seven weeks and all flies for all crosses were grown at 24° C. All flies were anesthetized using CO2 and inspected under a dissecting or stereoscopic microscope while on CO2 pads. The average time sedated was minimized to prevent sterility. Flies were handled with small brushes and prodding needles; for specifics on fly handling please see Jeyabalan (2005).
In total five different cultures of flies were used: Oregon-R (Ore-R) type wilds, unknown culture U-5343, and marker stocks labeled M-I, M-II, M-III. Two Ore-R stocks and eight mutant stocks were initially provided. Two new cultures of U-5343 were made during the experiment to maintain the stock for later backcrosses. There was continuous access to all marker stocks. Wild-type flies were phenotypically described as: tan bodied, red eyed, full wing venation with straight wings, and normal straight bristles. U-5343 flies were observed to be black bodied with white eyes and lacked a complete LII and LIV longitudinal veins (failing to reach margin); they were wild-type for all marker genes. M-I flies were wild-type expect for the following traits: crossvienless (cv-X-chromosome @ 13.7) and forked, shortened bristles (f-X-chromosome @ 56.7). M-II flies were wild-type expect for the following traits: short-thin bristles (Bl-autosome II @ 54.8), small 'lobed' eyes (L-autosome II @ 72.0), and with curled wings (Cy-autosome II @ 6.1). M-II was a balanced-lethal culture so all flies showed a Bl L/Cy genotype. M-III flies were wild-type expect for the following traits: small 'smooth' eyes (Gl-autosome III @ 41.4) and short blunt bristles (Sb-autosome III @ 58.2) with the inversion known as LVM present (autosome III lethal balancing inversion), a non-phenotypically expressed trait. Again, M-III was a balanced -- lethal culture so all flies showed a Gl Sb/LVM genotype. All cultures were true breeding. For more details about mark stocks and traits see Jeyabalan (2005)
Concerning the crosses in general, when virgin females were needed all flies were removed from source bottle and, then, 6-8 hours later virgin females were harvested. For all crosses parent flies were removed from bottles 8-10 days after the cross and all flies were scored 14-20 days after the cross. All crosses were done in duplicate unless otherwise indicated. When scoring, body color, eye color, wing venation, sex, and relevant marker traits were all recorded. See appendix (skeletal report) for scoring data table structure.
Cross A: unknown virgin females were crossed with wild-type males, F1 flies were scored. From the F1 generation, flies were collected and placed into a new bottle for the F1xF1 of Cross A. F2 flies were scored.
Cross B: unknown virgin females were crossed with wild-type males, F1 flies were scored. From the F1 generation, flies were collected and placed into a new bottle for the F1xF1 of Cross B. F2 flies were scored.
Marker Cross I: unknown virgin females were crossed with M-I males, F1 flies were scored. From the F1 generation, flies were collected and placed into a new bottle for the F1xF1 of Cross I. F2 flies were scored.
Marker Cross II: unknown virgin females were crossed with M-II type males, F1 flies were scored. From the F1 generation, Bl L/Cy+ male flies were collected and crossed with unknown virgin females -- a male backcross. Cross was not done in duplicate. Additionally, Bl+ L+/Cy male flies were collected and crossed with unknown virgin females -- not done in duplicate. The first cross yielded offspring which were scored. The second cross yielded no offspring, so no flies were scored.
Marker Cross III: unknown virgin females were crossed with M-III type males, F1 flies were scored. From the F1 generation, Gl Sb/LVM+ male flies were collected and crossed with unknown virgin females -- a male backcross. The male backcross progeny were scored.
After all the data was collected, the unneeded flies were euthanized and discarded in a humane fashion.
In general, all but one of the crosses went as expected and there were no major problems with determining the nature of the genes involved in the U-5343 mutant culture. In the analysis below, to find any χ2 calculations please see appendix (calculations); also, please see appendix (skeletal report) for raw data tables.
These two tables are very revealing: first, since these crosses were reciprocal and there is no significant differences between male and female expressed phenotype, it is most likely that all mutations are autosomal in nature. Secondly, since no mutant types are present in any flies, it should be that the mutations are all recessive. These conclusions can only be drawn, however, because it was given that the unknowns were true-breeding homozygous mutants. Further analysis of the F2 generation helps confirm these initial observations.
Qualitatively, in the F2 generation, mutant wing venation and normal wing venation were both observed. Additionally, mutant body color and wild-type body color were observed. However, four eye colors were observed: red, white, orange and brown -- suggesting that more then one mutant gene is affecting eye color.
Observations of the male parent backcross help to reveal the autosome associated with the mutant genes. As aforementioned, the two marker crosses represent a balanced lethal system, where heterozygosity is maintained, even in the face of crossing over. With such marker systems, one can easily determine the autosome upon which an unknown mutation lies. Backcrossing the males (hybrid M-II, or M-III, and unknown flies) with the unknown female culture produces male backcross progeny. If the recessive mutation(s) is linked to the markers, then it will not show-up with the marker phenotype in the male back cross progeny -- since crossing over does not occur in the male Drosophila. Alternatively, if the recessive mutation(s) does show up with the marker type, then it must be independently assorting and, therefore, on a different autosome than the marker.
For the male backcross progeny observed here, compelling results allow for autosomal assignment of all mutant genes. M-II backcross progeny were observed phenotypically as bristle, lobed, and black bodied -- showing that the body color is not on the second autosome, but on the third. Also, neither the wing mutation nor the eye color mutation were present with the markers in the M-II backcross progeny, implying their presence on the second autosome. However, the curly-type backcross did not produce any viable offspring; most likely because balanced lethal systems often create very fragile males, easily sterilized by overexposure to CO2. Nevertheless, the M-III backcross showed glued, stubbled, white-eyed, wing mutant flies, confirming the presence of these three mutant genes (a, b, and tv) on the second autosome and supporting the results of the successful M-II backcross. Additionally, the mutant body color did not show up with the marker phenotypes, also corroborating the assignment of this gene to the third autosome.
In summary then, the mutant genes a, b, and tv are on the second autosome and the mutant gene bb is on the third autosome. Thus, using a three point cross and the combined data of the F2 progeny from crosses A, B, and M-I, one can complete2 the map of autosome two, by determining the mutant positions. However, from these experiments, the map of autosome three cannot be completed; though, a female parent back cross of M-III would allow for the missing mapping data to be scored.
As all other crosses have indicated, no unknown mutations are present on the X-chromosome. This, however, does not mean that the M-I cross was done in vain -- by ignoring the marker phenotypes, the M-I cross effectively becomes the same as the A/B crosses. Thus, by combining all of these crosses (table 7), a robust data set is created where a three point cross can determine the map distances between a, b, and tv. From the table then:
From these map points, we see that the proper gene order is tv-b-a and we can now map the autosome. The mutation tv is was given at 3.8 mu. Therefore, b is 3.8+42.0 = 45.8 mu and a is 3.8+81 = 84.8. The map, with all marker genes included, is represented by image 1:
Through well thought-out cross breeding experiments, the previously unknown genetic components of a true breeding Drosophila m. culture (U-5342) was determined. In all, four new genes were discovered and described, in brief, that control body color, wing venation, and eye color. The gene bb ("black body") is autosomal recessive and located on the third autosome; further tests are needed to establish the exact location. The genes a and b epistatically controlling eye-color, they are linkage with a distance of about 38.6 mu. They express themselves in four ways phenotypically, red (a+_bb), orange (a+_bb), brown (aab+_) and white (aabb). And are mapped at the locations in the figure above. Finally, the gene tv (pre-terminating LII and LIV longitudinal veins) is autosomal recessive and on the second chromosome, also mapped above with a and b.

The ability of a species to compete for limited resources is often central to its ability to survive in a given environment. Different species possess differential competitive abilities, which are often dependent upon environmental conditions. For all organisms, the effects of competition become more pronounced with an increase in density because more organisms are searching for resources in the same areas, depleting the resources to continually lower levels.
One way that plants gain a competitive edge is by forming mutualistic relationships with other organisms, thereby working together to promote mutual survival. Rhizobium bacteria have a symbiotic relationship with the roots of legumes in which they fix atmospheric nitrogen and provide it to the plant in exchange for resources such as carbon (Begon, et al., 2003).
In this experiment we examined how the presence of Rhizobium affected the competitive ability of red clover (Trifolium pretense) which is a legume, when grown with red fescue (Festuca rubra) which is not leguminous. We hypothesized that Rhizobium form a mutualistic relationship with legumes and provide it with nitrogen, and from this we predicted that red clover would compete better against red fescue (a grass) in environments with Rhizobium present.
Our results did not indicate a difference in competitive ability of red fescue in the presence or absence of Rhizobium. This is illustrated by the fact that the confidence intervals in the linear regression graphs are broadly overlapping (Figure 1). The R squared values for the Rhizobium and no Rhizobium treatments of .810 and .778 respectively indicate that the best fit lines in the graph are accurate representations of our data. The p-values of .038 and .048 for the R square data indicate that it is statistically significant, which means there is a significant relationship between target weight and neighbor density and hence that competition is occurring.
Our results did not support our prediction that red clover would compete better in the presence of Rhizobium. One explanation for this is the possibility that we made some incorrect assumptions as part of our experimental model. One important assumption we made was that nitrogen was a limiting resource which both plant species were competing for, which in reality may or may not have been the case. The nitrogen concentration may have been high enough in the soil that the additional nitrogen provided by the Rhizobium did not provide any appreciable advantage to the red clover. If this were the case, then the Rhizobium may have actually hurt the clover's competitive ability by taking its resources without providing necessary nutrients in exchange. If we were able to do the experiment again, it would be helpful to test the nitrogen content of the soil to ensure that it is in fact a limiting resource.
Another important assumption we made was that red clover and red fescue shared the same traits apart from clover being a legume and fescue a grass. However, there is one other difference in that red clover is a dicot while red fescue is a monocot. Physiological differences, particularly in the root system, suggest that dicots are better at absorbing nutrients from the soil than monocots. Although this would imply that clover is still a better competitor, it is possible that there are other confounding factors which gave the fescue a similar competitive effect in the presence and absence of Rhizobium.
A more practical aspect of our experimental design that could be problematic is that we looked at the biomass of the part of the plant above the ground only, by clipping it at ground level and then weighing it. If the effects of competition led one plant species or the other to divert more resources to the roots and away from stems and leaves, our data could be misleading.
One other potential problem exists with our model in that it does not account for the proximity of plants to each other. While it is a logical assumption that with increasing density, proximity of plants increases, it is possible that in low density plots, seeds of competitors by chance happened to fall very close to the target seeds. The closer the plants are physically, the more their resource utilization areas overlap, and the stronger the effects of competition between them.
Aside from the aforementioned problems with the assumptions of our model, the additive design of our experiment seems to be the best way to examine competition between species. An additional experiment we could have performed is a replacement series in which the proportion of the two species is varied while keeping the total plant density the same. The problem with this approach is that it is impossible to distinguish between intra-and interspecific competition (Park et al., 2003). Since for this experiment we are looking specifically at the interspecific competitive effects, the additive design is the better approach.
Examining the competitive effects and responses of various plant species to interactions with other species has particular relevance to ecologists or conservation biologists trying to manipulate species composition. Over the long term, it would seem that competitive response is more important to a plant's ability to persist at a site. Unless the plant is the top competitor in an ecosystem, it is likely that with time other species which are good competitors will begin to grow at the same site and deplete the resource base to low levels. In order to persist, the plant will have to be tolerant of low resource levels; otherwise it will be outcompeted by plant species which are better able to absorb resources from the environment.
Although some plants that have a strong competitive effect also have a strong competitive response, the two do not necessarily go together. It is possible that a plant which has a strong competitive effect is good at hoarding resources at a particular site and simply survives by preventing other species from establishing there.
This experimental model also has relevance to the Lotka-Volterra model for competition. In order to test whether stable coexistence can occur (the effects of intraspecific competition are greater than interspecific), we would have to do an additional experiment in which the target and neighbor species were switched, because the current set-up only determines the effect of fescue on clover and not vice-versa. With data from both density experiments there are equations which make it possible to sort out the effects of intra-vs. interspecific competition (Park et al., 2003).
Establishing the effects of plant species interactions has a lot of implications for farming (weed-crop competition), plant conservation and the management of invasive species. By understanding how different environmental conditions affect these interactions, ecologists will be better able to use this knowledge to their advantage in creating environments which allow for a particular target species to have the greatest competitive advantage.

Pollution of aquatic environments by petroleum products is an issue of concern to many ecologists and environmentalists. While it receives a lot of attention after major commercial oil spills, this is not in fact the primary source of oil pollution. According to a report by the National Academies' National Research council, 29 million gallons of petroleum products enter North American waters annually from anthropogenic sources, 85% of which is from land runoff, oil-contaminated rivers, airplanes and small watercraft. All this is in addition to the 47 million gallons that naturally seep into waters through the ocean floor (Coleman, et al., 2003).
Petroleum products can have several negative consequences for aquatic ecosystems. The major constituents of petroleum products are complex hydrocarbons, comprising 80-90% of unused motor oil (Irwin, et al., 1997). Of all these hydrocarbon groups, polycylic aromatic hydrocarbons (PAHs) are thought have the most toxic effects, as they attach easily to sediment surfaces and are relatively resistant to degradation (Vo, et al., 2004). Oil slicks formed at the surface can potentially limit oxygen exchange at the water-air boundary, decreasing the levels of dissolved oxygen in the water. Oil at the surface also has the potential to coat the gills of aquatic organisms, adversely affecting respiration (Bhattacharyya, et al., 2003).
Not many studies have been done to investigate the effects of oil pollution on freshwater systems, as most large-scale oil spills occur in marine environments. However, studies on the effects of the more general category of freshwater pollutants, which include oils/grease, but also heavy metals and other toxic pollutants, have been done. In fact, aquatic insects are often used as an indicator of water quality, a concept known as biomonitoring. Benthic invertebrates are the most frequently used organisms in biomonitoring, as they are a diverse group of organisms which exhibit varying degrees of tolerance to environmental conditions (Merritt and Cummings, 1996). Insects of the order Odonata, which includes damselflies and dragonflies, are known to be moderately sensitive to environmental pollutants, and their absence from a particular habitat is frequently indicative of poor water quality. In an experiment investigating the effects of urban runoff on stream water quality, benthic organisms were surveyed upstream and downstream of an urban development. The prevalence of dragonflies was found to be significantly higher at upstream sites (22%) than at downstream sites (4%). It was concluded that this was a result of pollution and eutrophication decreasing the levels of dissolved oxygen in the stream sites downstream of the development (DeBarruel and West, 2003).
Odonates are particularly sensitive to dissolved oxygen levels. According to Merritt and Cummings (1996) "a major challenge for any aquatic insect is to obtain sufficient quantities of oxygen for its metabolic needs" (36). For dragonflies and damselflies this is particularly true due to their closed tracheal systems; rather than obtaining oxygen through direct contact with air, their tracheal gills uptake oxygen dissolved in the water (Merritt and Cummings, 1996). Therefore any circumstance which decreases dissolved oxygen levels has negative repercussions for dragonfly and damselfly respiration.
For this experiment, I set up a laboratory system to investigate the effects of different concentrations of motor oil on dragonfly nymphs and their predatory interactions with damselfly nymphs. The dependence of dragonflies and damselflies on relatively high levels of dissolved oxygen, and oil's effects on dissolved oxygen concentration led me to hypothesize that adding motor oil to simulated pond environments would affect activity levels in dragonflies. I predicted that an increase in concentration of motor oil would lead to lower activity levels in dragonflies.
The activity levels of both dragonflies and damselflies have implications for predation. Both dragonflies and damselflies are predatory insects, but presumably due to size advantage, dragonflies prey on damselflies and not vice-versa. Dragonflies are generally ambush predators, waiting for prey to approach before attacking, but will occasionally switch to a more active foraging strategy if food availability is low (Gullan and Cranston, 2005). They rely primarily on tactile cues in searching for prey but also take advantage of visual cues (Resh and Rosenberg, 1984).
As a corollary to my first hypothesis, I hypothesized that the addition of oil to the environments would have an effect on predation rates. A decrease in activity of damselflies (prey) would result in fewer visual cues for dragonflies and a decrease in activity level of dragonflies would lead to less active foraging behavior. Therefore, I predicted a negative correlation between oil concentration and number of prey eaten.
I set up five clear plastic containers and filled each with 2 liters of pond water in an attempt to most effectively simulate a natural freshwater habitat. The first container was a control with no oil added, and the four successive containers were contaminated with increasing concentrations of unused motor oil (.1%, .5%, 1% and 2%). I collected dragonfly and damselfly nymphs from Willow Pond at Matthaei Botanical Gardens, and added one dragonfly predator to each container along with multiple damselfly prey. I performed two replicates of the experiment; two damselflies were placed with each dragonfly in the first replicate and six damselflies in the second. [Note: I misidentified several of the prey items I added to the containers in the 2nd replicate, meaning that some containers contained a mixture of mayflies and damselflies. However, I don't believe this should have an effect on the experiment as both are preyed upon by dragonfly nymphs].
After placing the insects in the containers, I collected data on their activity level by tracking their movement at ten minute intervals. Every ten minutes I marked the approximate location of the dragonfly and then noted whether it moved from that spot in the following time interval. I collected activity data a second time after the insects had been exposed to the oil for five days. During each collection I recorded data for five time intervals.
In order to determine the predation rate, I counted the number of surviving damselflies after five days and then determined the number of prey eaten. I divided this by the initial number of prey items in order to determine the percentage of prey eaten. I used percentage of prey eaten rather than number of prey eaten to standardize the measurements for the two replicates, since they had different initial numbers of prey. Because I wanted to determine how a range of concentrations affected percentage of prey eaten, I performed a linear regression on the data.
The graph of the linear regression examining the relationship between oil concentration and dragonfly activity showed a very weak negative correlation between the two variables (Figure 2). The very low R squared value (.034) reveals that the best fit line is not an accurate reflection of the data at all. Furthermore, the p-value is well over .05 (.612) meaning that the slope is not significantly different from zero.
The graph of the linear regression relating percentage of prey eaten to oil concentration showed an overall negative correlation (slope =-.378) (Figure 1). The standard error measurement for the slope is relatively high as there are many data points which are quite far from the line. The R squared value of .527 indicates that the best fit line is a satisfactory representation of the data but is not exceptionally accurate. The low p-value (.017) denotes a significant relationship between the two variables.
The results do not support my first prediction of a negative correlation between oil concentration and dragonfly activity. The high p-value suggests that the activity data are a result of random chance and hence no significant relationship exists between the two variables. There are several possible explanations for this. First of all, more data would be necessary to make any solid conclusions about the activity level of dragonfly nymphs. With such a small sample size (four data points in two replicates) results are unlikely to show concrete trends, as any outliers have a significant effect. Secondly, dragonfly nymphs may not be particularly active organisms in general, as they are "sit and wait" predators and do not resort to active foraging unless it becomes necessary for survival (Gullan and Cranston, 2005). Lastly, activity measurements were taken during the day, while dragonflies tend to be more active at night as an avoidance mechanism from predation by fish and other aquatic organisms (Merritt and Cummins, 1996). Taking more activity data over a longer period of time, with some collections being done at night would likely provide a clearer picture of the relationship between oil pollution and activity levels.
It would have been helpful to measure the dissolved oxygen levels in the different environments, as the whole notion of the decreased activity levels was predicated on the notion that surface oil inhibits diffusion of oxygen through the water. Some studies suggest that it takes several months of exposure to oil before a measurable difference in oxygen concentration is detectable. Harrel (1985) studied the effects of an oil spill in a Texas stream, comparing the water quality of the contaminated stream with that of a nearby control stream. Four days after the initial oil spill, oil was visible on the water surface and there was a strong smell of hydrocarbons, but measurements of water quality did not differ much between the two streams. Not until six months later were decreases in dissolved oxygen concentration apparent (Harrel, 1985). If this were the case in our system, then no difference in activity levels would be expected after only five days.
Though prediction 1 was not supported, prediction 2, regarding the relationship between oil concentration and predation rate, was supported. The regression line showed a statistically significant negative correlation between the two variables. The reasons for these results are not entirely clear, however, nor are the results free of statistical flaws. One of the biggest statistical problems I encountered was due to the fact that I had different numbers of initial prey items in the two replications. While I tried to standardize the predation measurements by examining the percentage of prey eaten as opposed to the number of prey eaten, this did not completely solve the problem, because there was not a continuous range of possible values for the percentage. For the first replicate, because there were only two prey, there were only three incremental possibilities for predation percentage, 0%, 50% and 100% while in the 2nd replicate there was a broader yet still incremental range (0, 1/6, 1/3, ½, etc). Statistically it would have been far better to have six initial prey items in both replicates.
While my initial hypothesis that decreased oxygen levels would lead to less active foraging behavior from the dragonflies and fewer movements (i.e. visual cues) by the damselfly may still hold, it is called into question by the failure of prediction 1. An alternative possibility is that the thick, opaque film formed on the top of the water blocked a large portion of the incoming light and made it more difficult for the dragonflies to clearly see prey. It is also possible that ingestion of toxic water-soluble components of oil led to changes in the predator's ability to search for or attack prey.
There is a compelling possibility that the number of prey eaten was more strongly correlated to predator size than to concentration of oil. I did not take quantitative data on predator size, but based on my observations, those predators that ate the largest percentage of prey also tended to be the largest of the predator group. This makes logical sense as large predators have higher energy needs and need to consume more prey in order to satisfy those needs. The size of predators and prey was not a factor that was controlled for and may have had a significant impact on the results.
A second factor which was not controlled for but could have affected the results was the presence of secondary food sources. It was assumed that in this environment dragonfly nymphs would prey exclusively upon damselfly nymphs; however, dragonflies are generalists and will consume anything small enough for them to handle. Since pond water was used in the experiment, small prey items such as Daphnia and midges were present in unknown quantities, because their small size made them impossible to filter out. Smaller dragonflies may have preferred to go after these smaller prey items in lieu of damselflies, violating the underlying assumption of the experiment.
Future versions of this experiment could be improved by using larger sample sizes and taking more measurements over a longer time period. It would be interesting to include the size of the predator as one experimental factor by measuring either the weight or length of the dragonflies, and examine the interactions between size and oil concentration, and how that effects predation rate, determining which factor seems to have the greatest effect. Also, future experimenters would ideally find a way to ensure that damselflies were the only food resource available to dragonflies.
The results of this experiment have implications for those trying to maintain a species balance within aquatic ecosystems. If oil concentration does directly affect predation (in addition to other factors like size) it is important to put forth greater efforts to reduce oil pollution coming into ponds and rivers from everyday sources such as road run-off. Dragonfly and damselfly nymphs both play key roles in aquatic food webs both as predators and as prey. By changing predatory behavior and species composition at one trophic level, oil pollution has the potential to radiate throughout the food web, causing indirect effects up to the human level.

The development of a testable hypothesis is an important part of the scientific method and is a key characteristic of good science. Hypotheses help to define the focus of research and create experiments that can provide answers to meaningful questions. Without well-defined hypotheses researchers run the risk of making erroneous conclusions from data that was not collected in an experiment appropriately designed to test for those conclusions.
In this study I examined the difference in frequency of hypothesis testing between two journals. I hypothesized that the best quality journals would be most selective in accepting submissions and therefore would be most likely to publish articles which strongly adhere to the scientific method. I predicted that the highest quality journals would have more articles with a clearly defined hypothesis than lower quality journals.
As a quantitative measure of journal quality I used the journal impact factor which is defined by ISI as, "a measure of the frequency with which the 'average article' in a journal has been cited in a particular year or period" (Thomson Scientific website). The rating is calculated by dividing the number of times articles from the previous two years in the journal were cited in the current year by the total number of articles published in the previous two years. The use of the journal impact factor as an indicator of journal quality is controversial in the scientific community, as a number of concerns have been raised about it; however, for the purposes of this study it provides the best objective and quantifiable measure of quality.
In order to test my prediction that articles from higher-impact journals would have clearer a priori hypotheses, I examined five articles from each of two different journals. For the high-impact journal I selected Ecology, which is in the top ten of ecology journals with an impact factor of 3.7. As a comparatively lower-impact journal I selected Plant Ecology, with an impact factor of 1.28. Articles were selected based on time of publication (2000-2005 only) and subject matter (invasive plant species).
In evaluating each article based on the clarity of the hypothesis, I assigned each article a rating from 1 to 3. A rating of three is indicative of an indisputably clear hypothesis stated in the article abstract or introduction. A two signifies that some background theory and research questions were addressed, but no specific hypotheses were provided. A rating of one designates those articles that did not have any indication of a hypothesis being tested.
Table 1 shows the ratings I assigned to each article as a quantification of the clarity of the hypothesis being tested in the experiment. Articles are numbered according to the order that they are presented in the literature cited section at the end. All five articles in the journal Ecology received the highest rating, indicative of clear hypotheses unambiguously stated in the article's introduction. Hypothesis testing in Plant Ecology varied from 1-3.
The data appear to support the hypothesis and prediction, as all articles from the high-impact journal had clearly stated hypotheses compared with only two from the lower-impact journal. Admittedly, this is not a conclusive finding as the sample size used was quite small and the ranking system was not completely objective or scientific.
For the articles given a rating of 3, hypotheses were explicitly stated, generally in the form "we hypothesized that..." or the typical "if...then" format. All of these articles addressed their hypothesis in the discussion, stating whether it was supported or contradicted. In all cases where the results were inconsistent with the hypothesis, alternatives hypotheses were proposed along with ideas for future investigation into the a posteriori hypothesis.
The article that received a two (article 7) listed a series of questions the study was designed to address but did not suggest an answer to any of these questions a priori. In the discussion at the end, the authors addressed each question individually and proposed several a posteriori hypotheses based on the data they collected. Interestingly, this illustrated some of the problems associated with this style of experimentation. The authors acknowledged that they were unable to provide an answer to their first question because their experimental design was unsuitable for doing so. Similarly, few concrete answers were found for the subsequent questions as the authors were primarily only able to provide speculation based on data and observations. Had they designed experiments to test specific a priori hypotheses, perhaps they would have been able to make more definitive conclusions.
The two articles that received a rating of one broadly defined the aims of their experiment but did not narrow the scope enough by providing a specific hypothesis. The introductions of article 10 and article 8 explained what they hoped to determine as a result of the experiment but did not provide any predictions or indicate any expected results. Both papers were more exploratory in nature but both achieved results which seemed to satisfy the objective of their study.
All papers seemed to be logical and well-organized, partly due to the fact that all scientific papers follow a similar format: abstract followed by introduction, materials and methods, and so on. This essentially forces the author(s) to structure the paper in a way that is logical for the reader. The only way in which the presentation of papers might be improved is by making a more explicit connection between a hypothesis and the experiment designed to test that hypothesis. When papers have multiple hypotheses in the introduction and multiple tests described in the methods section, it would be helpful to know which test was done specifically to address a particular hypothesis.
Hypothesis testing has always been presented as a cornerstone of science so it is surprising that a significant number of science articles are being published without clear hypotheses. The results of this study suggests that there is a possible positive correlation between a journal's impact factor and hypothesis testing within its articles, though more study needs to be done. In order to test this hypothesis more thoroughly it would be important to use larger sample sizes and select articles from additional publications with both high and low impact factors.

Drosophila melanogaster is an ideal genetic model organism in several respects. Because of its small size it is easily manipulated, its short life cycle allows for genetic analysis over several generations in a short period of time, its few chromosomes simplify genetic analysis, and previous researchers have already identified and mapped many mutations. The overall purpose for this experiment was to map unknown heritable mutations in Drosophila to their chromosomal locations. The main hypothesis was that the three mutant traits, dark body color, white eye, and short longitudinal veins, were being inherited in a normal autosomal recessive pattern. Specifically, our aims were to cross unknown Drosophila stock to Oregon-R and to Marker flies, with previously mapped chromosomal mutations, to determine for each mutant trait the method of inheritance and the chromosomal location. Specific hypotheses are: that genes for body color are segregating normally, genes for wing venation are segregating normally, body color and wing venation genes are independently assorting, two genes determine eye color, the two eye color genes are independently assorting, and the eye color genes are linked to the body color gene.
F1 were wild type for all three mutant traits, with no difference between male and female phenotype.
F2 progeny had four distinct eye colors, with red the most frequent, but white, orange, and brown appearing with similar frequencies. Light body occurred more frequently than dark body. 
F1 were wild type for all three mutant traits, with no difference between male and female phenotype.
F2 progeny had four distinct eye colors, with red the most frequent, but white, orange, and brown appearing with similar frequencies. Light body occurred more frequently than dark body.
More wild type phenotypes appear for both body color and wing venation.
Red eye color is the most frequent, and white, orange, and brown appear with similar frequencies.
Dark body color shows up with Bristle Lobed; mutant wing venation never shows up with Bristle Lobed.
With one exception, dark body color does not show up with Glued Stubble. Mutant wing venation does show up with Glued Stubble.
Red eye color appears most frequently, and white, orange, and brown appear with similar frequencies. Light body color is more frequent than dark body color. Wild type wing venation is more frequent than mutant wing venation.
Crosses A and B were set up as reciprocal crosses in order to determine whether the unknown mutations were autosomal or sex-linked and dominant or recessive. From the Cross A-F1 results (Table 1), we conclude that the pattern of inheritance for all three mutations is recessive, because they do not appear in the F1 offspring, and autosomal, because the F1 males appear wild type. If any of the mutations had been sex-linked, F1 males would have received a mutant gene from the P female and a Y chromosome from the P male and would have presented a mutant phenotype. From the Cross B-F1 results (Table 3), we also conclude that the pattern of inheritance for all three mutations is recessive because they do not appear in the F1 offspring, but we cannot determine whether the mutations are autosomal. If all the mutations were autosomal, F1 progeny would have one mutant and one wild type copy of each gene and a wild type phenotype. If any of the mutations were sex-linked, they still would not show up in the F1 progeny because the females would have one X chromosome with a wild type copy of the genes, giving it a wild type phenotype, and the males would only have a wild type copy of the mutant genes on their X chromosome from the P female. However, since the reciprocal crosses produced the same results, we can conclude that the mutations are autosomal.
If genes for body color are segregating normally as hypothesized, we expect a F2 ratio of wild type body color to mutant body color of 3:1. Based on the combined F2 results of A and B, we observed a ratio of 3.4:1 (Table 10). The X2 value for these results is 1.39, which corresponds to a p value greater than 0.05, and we can deduce that the mutant body color gene is segregating normally. A 3:1 phenotypic ratio corresponds to an autosomal recessive trait so we conclude that the body color gene is autosomal recessive. With the null hypothesis that genes for wing venation are segregating normally, we expect a F2 ratio of wild type wing venation to mutant wing venation of 3:1. From the combined F2 results of A and B (Table 11), we observed a ratio of 3.5:1. The X2 value for these results is 2.25, which corresponds to a p value greater than 0.05, and by the same reasoning as above, we deduce that the wing venation gene is autosomal recessive. We next hypothesized that body color gene and wing venation gene were assorting independently. With this hypothesis we expect a ratio for wild type body color and wild type wing venation: mutant body color and wild type wing venation: wild type body color and mutant wing venation: mutant body color and mutant wing venation of 11.9:3.5:3.4:1 (Table 12). We observed a ratio of 13.2:4.0:3.9:1.0; the X2 value for this set of data is 0.24, corresponding to a p value greater than 0.05, and we can deduce with that body color and wing venation are assorting independently.
We observed red, brown, orange, and white eyes in a ratio close to 6:1:1:1 in the F2 progeny (Table 6). Based on this we propose that two genes (orange eyes, a, and brown eyes, b) are involved in the expression of eye color, because with the simplifying assumption that each mutant gene has two alleles, at least two genes are needed to produce four phenotypes. Our hypothesis of interaction is a parallel pathway where at least one wild type copy of each gene is needed to produce the wild type red eye (Fig. 1). Our null hypothesis for their association is that they are independently assorting. This gives a X2 value of 61, which corresponds to a p value less than 0.05 and a strong rejection of the hypothesis. Therefore the two genes for eye color are linked. F1 progeny were all a+b+/ab (ab received from the unknown parent and a+b+ received from the wild type parent). Because crossing-over during meiosis only occurs in female gametes, all male gametes were either ab or a+b+, and recombinant female gametes were a+b or ab+, producing the following recombinant genotypes and phenotypes: a+b/ab (brown), a+b+/ a+b+ (red), ab+/ab (orange), and ab+/a+b+ (red). Based on the frequency of these recombinant genotypes within the F2 progeny, the two eye color genes are located 45.4 map units apart.
Overall the data from F1 and F2 of Crosses A and B did not present any problems and from it we concluded that the three mutant traits have a high probability of being autosomal, body color and wing venation have a high probability of assorting independently, and it is highly probable that two linked genes control eye color.
Crosses II and III were set up with unknown flies crossed with Marker II and Marker III stocks respectively. Marker II and Marker III stocks serve as chromosomal markers because they carry dominant homozygous lethal mutations. Marker II has mutant alleles of the Bristle and Lobed genes and a wild type allele of the Curly gene on one copy of chromosome II and wild type alleles of the Bristle and Lobed genes and a mutant allele of the Curly gene on the other copy. Because these three mutations are dominant, these flies should present short bristles, curly wings, and small lobed eyes but the bristle phenotype does not show up in the Marker II flies we have. Marker III has mutant alleles of the Glued and Stubble genes and a wild type allele of the chromosomal inversion LVM on one copy of chromosome III and wild type alleles of the Glued and Stubble genes and a mutant allele of the chromosomal inversion LVM on the other copy. As these three mutations are dominant, Marker III flies have small, smooth eyes and short blunt stubbles (LVM has no mutant phenotype). Both marker stocks are true-breeding because any progeny that do not have the same genotype as the parents lethally receive two copies of at least one mutation (Fig. 2).
In order to map body color and wing venation genes to chromosomes, we set up F1 male backcrosses for Crosses II and III. In a F1 male backcross, F1 males showing a marker phenotype are crossed to unknown females. In the backcross progeny, if an unknown mutant trait shows up with a marker, then the mutant trait and marker are on different chromosomes; if they do not show up together, they are on the same chromosome. For Cross II-F1 male backcross progeny, dark mutant body color shows up with both Bristle Lobed and Curly markers, indicating that the body color gene is not on chromosome II with them (Table 7). Mutant wing venation never shows up with Bristle Lobed or Curly, indicating the wing venation gene is on the chromosome II with those markers. For Cross III F1 male backcross progeny, dark mutant body color does not show up with Glued Stubble markers, indicating the body color gene is also on chromosome III (Table 8). Mutant wing venation does show up with the Glued Stubble markers, indicating that the wing venation gene is not on chromosome III. There is one exception of a dark bodied, Glued Stubble fly, but as it is only one fly, this is probably due to human scoring error or the presence of a random fly from the lab during scoring.
Overall the Cross II and III F1 and male F1 backcross data strongly support the conclusion that body color gene is on the chromosome III and wing venation gene is on chromosome II.
We set up Cross I between unknown virgin females and crossveinless forked Marker I males and carried the cross through the F2 generation. In the F1 generation we expected to see all phenotypically wild type females and all crossveinless forked males (Fig. 3). However, we saw wild type males instead of crossveinless forked males and Glued Stubble males and females. Possibly some Marker III males were among the parents, which would account for the presence of Glued Stubble. Since every F1 progeny should have received a mutant gene from the unknown parental female for crossveinless and for forked, all males should show both traits. We do not believe the absence of crossveinless forked males is from scoring error, as we checked many flies repeatedly. An alternate explanation is that the unknown females were not virgin when introduced and produced phenotypically wild type progeny, or that the parental females were not from unknown stock.
As we determined from Crosses A and B, the eye color genes are autosomal. By the combined F2 data from Crosses A, B, and I (Table 9), if we hypothesize that the two eye color genes are independently assorting with each other, we get a X2 value of 98, which corresponds to a p value less than 0.05. So, we must reject the hypothesis and conclude that the two eye color genes are linked. The F1 progeny for all three crosses are all a+b+/ab (ab received from the unknown parent and a+b+ received from the wild type parent). Because crossing-over during meiosis only occurs in female gametes, all male gametes were either ab or a+b+, and recombinant female gametes were a+b or ab+, producing the following recombinant genotypes and phenotypes: a+b/ab (brown), a+b+/ a+b+ (red), ab+/ab (orange), and ab+/a+b+ (red). Based on the frequency of these recombinant genotypes within the combined F2 progeny, the two eye color genes are located 44.2 map units apart.
In Table 9, mutant eye color and mutant body color appear together less frequently than mutant eye color and mutant wing venation (54 versus 126). In our skeletal report we interpreted this to mean that the wing venation gene and eye color genes were more independent than the body color gene and the eye color genes, and we assigned eye color genes to the chromosome body color is on, chromosome III. We used the body color and eye color data in Table 13 to calculate map distances of 65.9 map units between d (dark body color gene) and a, 67.3 map units between d and b, and 44.2 map units between a and b. This produced the final results seen in Table 15 (p. 15 of Skeletal Report) and Fig. 4 (p. 15 of Skeletal Report). The map in Fig. 4 for chromosome III does not make sense (we have the distance between d and b calculated directly as 67.3 map units, and indirectly by using the distances between d and a and a and b as 110.1 map units) because the eye color genes should really be on chromosome II (as clarified by Jessica Lehoczky). For this report we realized that we misinterpreted our data for the skeletal report. Since in the P generation, mutant eye color, body color, and wing venation all appear together, then linked genes will continue to appear together and unlinked genes will tend to appear separately. Since mutant eye color and mutant wing venation appear together more frequently than mutant eye color and mutant body color, eye color genes are linked to the wing venation gene on chromosome II. With this assignation in Table 14 we calculated map distances of 42.2 map units between sv (short longitudinal vein) and a, 86.4 map units between sv and b, and 44.2 map units between a and b. Table 16 and Fig. 5 show the final determined genetic map for our unknown Drosophila. Although the calculated map distances for sv, b, and a with respect to each other on chromosome II differ from the given values of sv (II:3.8), a (II:57.0), and b (II:104.5) (as clarified by Jessica Lehoczky), they are more logical than the distances found when body color and eye color genes are linked. The difference between our experimentally determined map distances and the actual numbers could be due to scoring error, crossing error, or an insufficiently sized pool.
Looking at Table 16, we can see that in U-4033 Drosophila the unknown genes orange (a), brown (b), dark body (d), and short veins (sv) are all autosomal recessive, with genes a, b, and sv on chromosome II and gene d on chromosome III. The linked genes a, b, and sv are mapped on chromosome II with respect to each other as shown in Fig 5. To help further clarify our data,

Bacteria are unicellular, haploid organisms that are suited for experiments because they reproduce rapidly by binary fission, produce genetically identical progeny, and are genetically simple. Normal bacteria can grow on minimal media, media containing only water, a carbon source, and inorganic salts, but some mutant auxotrophic bacteria have specific nutritional requirements. These requirements can be used to select for or against specific strains. Some bacteria can transfer a copy of accessory DNA to another by conjugation. During conjugation, cells containing a sex factor, F+, unidirectionally transfer a copy of this sex factor to cells lacking the sex factor, F-, through a conjugation canal. Because the conjugation canal is easily broken, the entire F factor rarely transfers, and so genes closer to the origin of the F factor (the first point to enter the F-cells) will appear with greater frequency among the resulting progeny. Once the F factor has been transferred the new genes will be expressed if they are dominant, and recombination can occur between the new genes and the DNA of the recipient cell. This phenomenon can be used to map bacterial genes. The frequency with which a particular gene appears in exconjugant progeny depends on its distance from the origin, and the number of recombinant progeny can be used to determine relative map distance between genes. In this experiment we conjugated F+ and F-cells with different genotypes and plated the resulting exconjugants onto different selective media plates. Our purpose was to determine the order of the genes involved with the selected traits relative to the point of origin, and to determine relative map distances between genes if possible.
From the media 1 replica plates, we counted a total of 144 colonies, with the most on media 1, followed by media 5, media 2, media 3, and media 4 (with zero colonies). From the media 2 replica plates, we counted a total of 137 colonies, with the most on media 2, followed by media 5, media 3, media 1, and media 4 (with zero colonies). The tabulated results are in Tables 1 and 2.
Plate 1 lacked threonine-leucine and selected for TL+, plate 2 lacked proline and selected for pro+, plate 3 lacked glucose and contained lactose and selected for lac+, plate 4 lacked glucose and contained galactose and selected for gal+, and plate lacked methionine and selected for met+. Plate 3 selected for lac+ and plate 4 selected for gal+ because each of those plates selected for bacteria that could utilize lactose and galactose as the sole carbon source, respectively. Among the media 1 replica plates, colonies 1, 2, 4, 10, 20, 23, and 36 did not grow on the media 1 plate, and therefore could not be taken into account for any of the other plates since we could not tell if these colonies on other plates did not grow because of the specific media they were on or because they were plated improperly (Table 3). Among the media 2 replica plates, colonies 1, 6, 34, and 35 did not grow on the media 2 plate, and therefore could not be taken into account for the other plates (Table 4). With these adjustments, the total number of colonies on media 2 replicate plates was 134 and the total number of colonies on media 1 replicate plates was 130. More colonies grew overall on media 2 replica plates than media 1 replica plates, indicating that more colonies received the pro+ gene to survive on the pro-media than received the TL+ gene to survive on the TL-media, and that the pro gene must then come before the TL gene on the F factor. To determine the order of the other genes with respect to each of these markers, for each set we ranked the plates in order of most colonies grown due to received DNA. For plates 1, 2, 3, and 4, this was the number of colonies grown, but for plate 5 this was actually the number of colonies that did not grow since the original F-bacteria were met+ (Tables 3 and 4). So, F-bacteria grew on plate 5, but exconjugant bacteria that received the met-gene did not. For the media 1 replica plates, the colonies grew best on media 1, followed by media 2, media 3, media 5, and media 4. Based on these results, the gene map is: TL, pro, lac, met, and gal (Fig. 1). For the media 2 replica plates, the colonies grew best on media 2, followed by media 3, media 1, media 5, and media 4. Based on these results, the gene map is: pro, lac, TL, met, and gal (Fig. 2). The map based on media 2 plates is better because we know from above that the pro gene is closer to the origin than the TL gene. Since the two maps are inconsistent, we have to modify map 2 to take into account map 1. Based on the example in lecture notes, the modified map is lac, pro, TL, met, and gal (Fig. 3). On this map, the pro and lac genes are closer to the origin than the TL gene, so we can use the TL+ data to determine the distances between pro-TL and TL-lac (but not with gal or met since you can only determine map distances between markers and genes that enter the F-cell before them). Based on the TL+ data, pro and TL are 37.2 map units apart and lac and TL are 60.5 map units apart. From this we infer that lac and pro are 23.3 map units apart. However, looking at colonies selected for lac+ out of the pro+ colonies, the calculated map distance is 23.9 map units. Since calculating small distances is more accurate, we conclude that the map distance between lac and TL is 61.1 map units (Fig. 3). Once again, we cannot calculate any distances involving met or gal since they are farther from the origin than both of the markers we selected for. The slight discrepancy in the results could be due to scoring error, plating error, or other random error we have no control over (such as bacteria that does not grow as well as it should or bacteria that spontaneously mutate to grow when it should not). To compensate for the scoring error, we could dilute our bacteria by another factor of 10, but this would probably make the resulting data pool too small. A better alternative would be to repeat the experiment several times and then calculate the genetic distances using a large pool of data.
We studied conjugation in bacteria and the gradient of transmission method to map five genes on an F factor. We determined that with respect to the point of origin, the genes are in the order: lac, pro, TL, met, and gal, and that the relative map distance between lac and pro is 23.9 map units, between lac and TL is 61.1 map units, and between pro and TL is 37.2 map units.

Lactococcal strains have several important industrial applications. In particular the species Lactococcus lactis has very simple and well characterized biochemical pathways and is added to milk to start the cheese making process. Starter cultures ferment sugars to produce lactic acid which serves to acidify milk and give cheese its flavor and texture (McAuliffe 2001). Lactococcal enzymes that utilize lactose as an energy source (confer Lac+ phenotype) are plasmid encoded as are several other essential enzymes. Bacteriophage resistance, protease and peptidase production, bacteriocin production and resistance are also encoded by genes that are present on native lactococcal plasmids (Cotter 2003). Serine proteases (confer Prt+ phenotype) degrade casein and thus provide the cells with the amino acids they need to grow. Food spoilage caused by pathogenic bacteria is prevented by the bacteriocin nisin that is encoded on several plasmids native to Lactococcus lactis (Takala 2002). The cells must also be resistant to nisin if they produce it (NisR phenotype). Nisin production and resistance, preptidase production, and lactose utilization all give lactococcus advantages in certain environments and are transferred through conjugation from donor to recipient cells by way of plasmids. Conjugation is dependent on transfer (tra) genes and certain self-transmisible plasmids can mobilize other non-transmissible plasmids. Insertion sequences (IS) are present on several lactococcal plasmids and cause recombination events to occur. During conjugation or mobilization, plasmids can recombine so as to produce new plasmids of different sizes and with different genetic makeups. The aim of this investigation was to analyze phenotypic transfer in transconjugants of Lactococcus lactis ssp lactis by selection of individual markers conferred from donor plasmids.
were combined and placed onto a nutrient agar plate to allow for conjugation to occur. The donor strain contained several different sized plasmids. The phenotypes associates with these various sized plasmids are as follows:
The 60kb plasmid encodes tra genes and also has insertion sequences so it is therefore self-transmisible and the IS's allow for recombination events to take place. After conjugation, cells were spread onto three different selective media plates in three replicates. The selection plates used were M17L + Sm, FSDA + Sm, and M17G + Sm + Nis; see Table 1 for an explanation of the phenotypes these selection plates are used to identify. Donor and recipient cells were also separately spread on these selective plates as controls. The number of colonies on each plate was recorded. Ten transconjugants from each of the three selective media plates were selected and streaked for single colonies on their respective medias. These single colonies were then pick and patched onto each of the three selective media plates in order to identify the growth response for the particular transconjugant. The phenotype of the transconjugants was recorded in terms of lactose utilization, nisin resistance, and proteinase production. Within the class each of the different possible phenotypic combinations were chosen and inoculated into M17G broth. The plasmids of these transconjugants were extracted and run on an agarose gel in order to visualize the plasmids present in the transconjugant. See the detailed procedure in the Micro3021 Microbial Genetics Course Kit (Cavicchioli 2005, p. 52-55).
After the conjugation mix was plated onto the selective media plates the number of transconjugants was recorded, see Table 2. These transconjugant counts show the relative transfer frequency of various phenotypes. The most transconjugants, ~900 colonies, were found on the M17G + Sm + Nis plates followed next by the FSDA + Sm plates with ~500 colonies and the M17L + Sm plates had 15 transconjugants.
After single colonies were pick and patched onto the selective media plates many different phenotypic combinations were observed. The results in Table 3 show the class frequency of each phenotype as a percentage. There was a higher frequency of the NisR phenotype than any other phenotype.
The agarose gel phenotypic profile of our class showed that our results were not adequate to complete the report and so plasmid extraction data (Figure 1) and a table of the frequency of transconjugant plasmid profiles for given phenotypes collected throughout the years of this investigation (Figure 2) from Jeff Welch were used. In Figure 1, lanes two and three are the control donor and recipient strains. All of the plasmids (60, 56, 40, 24, and 3.7kb in size) can be visualized in the donor and there are no plasmids in the recipient. The lanes four through twenty-four show the size of plasmids that are present in cells that exhibit certain phenotypes.
Part of the initial transconjugant counts, Table 2, and transfer frequency data, Table 3, was expected. First, the most growth, ~9000 transconjugants/ml, was observed in the M17G + Nisin + Sm plates and the least growth, ~150 transconjugants/ml, was observed on the M17L + Sm plates. We would expect more nisin resistant colonies (growth on M17G + Nisin + Sm plates) than Lac+ colonies (growth on M17L + Sm plates) because nisin resistance is encoded on the 60kb plasmid that is self-transmissible and does not depend on any other plasmid to mobilize its transfer and therefore transfers at a greater frequency. Another reason there are more nisin resistant transconjugants than Lac+ is also because several of the plasmids in the donor strain encoded nisin resistence, both the 56kb and the 60kb plasmids, while only the 40kb plasmid conferred the Lac+ phenotype. There is an over-representation of Prt+ transconjugants. The number of colonies per milliliter would be expected to be similar to the Lac+ since both phenotypes are encoded on a single plasmid that is not self-transmissible. Table 3 depicting the transfer frequencies of various phenotypes further shows that the NisR phenotype is more common than the Lac+ phenotype and this is because of the same reasons mentioned previously.
Jeff Welch also kept records of phenotype frequencies over several years as shown in Table 4. This collection of data has an over-representation of the NisS phenotype, in particular the Lac+ Prt-NisS phenotype is the most frequent of all of the phenotypes. These skewed results can be explained when one considers that this investigation is set up so that each of the possible phenotypes is chosen for plasmid extraction and so the rarer phenotypes appear to be more common than they actually are.
Some of the various phenotypes in Figure 1 relate to the genotype or plasmid composition that would be expected. For example, lane thirteen has the phenotype Lac-Prt+ NisR and has two plasmids of sizes 56kb and 60kb. Proteinase is encoded on the 56kb plasmid and nisin resistance is encoded on both the 60kb and 56kb plasmids. The transconjugant did not have the 40kb plasmid that confers lactose utilization and the Lac-phenotype is fittingly observed. Another transconjugant where the phenotype and plasmid makeup match up is number twenty-three. The phenotype is Lac+ Prt-NisR and the 56kb plasmid encoding proteinase is missing.
The plasmid extraction gel in Figure 1 reveals the presence of plasmids that are of different sizes than the ones found in the donor strain. Different sized plasmids with varying genetic composition can be obtained through recombination events among the plasmids. The transconjugant in lane number nine is Lac+ Prt+ NisS but only has one plasmid that is greater than 60kb. It is likely that the 40kb (Lac+) plasmid and the part of the 56kb plasmid that confers the Prt+ phenotype recombined to form this novel sized plasmid that gives this particular phenotype. It is also possible that the Lac and/or Prt genes recombined with the chromosome of the transconjugant and confer the Lac+ Prt+ NisS phenotype in that way. Recombination events happen when insertion sequences are present flanking certain genes. These sequences are marked for excision from the plasmid and insertion into either another plasmid or chromosomal DNA. An example of a transconjugant where it is likely that a chromosomal recombination event occurred are lane numbers four and five of Figure 1. These transconjugants have 56kb and 60kb plasmids most likely giving them their Prt+ and NisR phenotypes, but there is no 40kb plasmid present that would confer the Lac+ phenotype that is observed. This data suggests that the Lac gene recombined into the chromosomal DNA.
In summary, the nisin resistant transconjugants were more frequent observed because NisR is carried on a self-transmissible plasmid and is also present on more than one plasmid while the other phenotypes, Prt+ and Lac+, were only carried on one plasmid each. Some phenotypes matched up with their expected genotypes, but there were many that did not. Transconjugants with plasmids of sizes different than those present in the donor strain were observed because recombination events occurred.

This experiment used the model organism Drosophila melanogaster, more commonly known as the fruit fly, which is one of the most valuable organisms in genetic and developmental research today. Drosophila has been used for many years in research and its entire genome has been sequenced. The life cycle for the fruit fly is short, ten to fourteen days, with a developmental time of nine days and because of this, experimental results can be generated relatively quickly. Drosophila flies are inexpensive to maintain in the laboratory setting and are easy to manipulate. All of these characteristics contribute to the wide use of this organism in genetic research today.
This experiment aimed to characterize an unknown mutant strain of flies through genetic analysis by answering several questions that this experiment was designed to answer. First, how many of the phenotypic differences in the unknown strain are inherited? And of the inherited traits, how many genes are involved in defining the trait? Moreover, how are the genes inherited, are they dominant, recessive, autosomal, sex-linked or do they have some other inheritance pattern? Finally, where are the genes for the identified traits located on the chromosomes? In order to answer these questions, scoring information from reciprocal crosses between the unknown flies and known wild type flies were coupled with scoring data from crosses with three marker strains that have mutations in genes of known chromosomal locations. Three marker strains were used corresponding to the three main pairs of chromosomes in D. melanogaster (Drosophila has a forth pair of chromosomes but they are very small and do not code for many genes). In order to completely characterize and map any unknown mutations F2 progeny were generated from the reciprocal crosses and the sex chromosome marker cross and male backcross progeny were generated from the two other marker cross progeny.
Another supplemental objective of this experiment was for the experimenters to learn how to work with Drosophila melanogaster. Skills in the handling and maintenance of this frequently used organism are valuable tools to add to a future molecular biologists repertoire.
Eight bottles of unknown mutant flies were obtained and their phenotypes were recorded. Two bottles of wild type flies were also obtained and all unknown and known bottles were cleared of adults. The wild type strain used was Oregon-R (Ore-R) characterized by a gray body, red eyes, and normal wing venation. Virgin females (virgin ♀) and males (♂) were collected from the unknown, wild type, and marker strains. The marker number represents the chromosome number that the known mutations in that strain are on. Refer to the MCDB 306 Genetics Laboratory Manual for further details about these strains.
Reciprocal crosses A and B, and marker crosses I, II, and III were set up in duplicate according to Table 2 Phase I Crosses. Approximately ten females and twenty males were added to each of the crosses described. One hundred flies were scored from each of the F1 crosses before the bottles were cleared and more virgin females and males were collected in order to set up the second round of crosses. In Phase II cross A, B, and marker cross I were used to create F2 progeny by crossing the F1 progeny. Phase II marker crosses II and III were male backcrosses between unknown females and mutant marker males. See Phase II Crosses in Table 2. The scoring results from phase I and II crosses were analyzed and the genetic inheritance pattern, linkage data, and map distances were determined.
The F1 progeny of cross A and B were tabulated, see Table 3. In both A and B crosses, all of the F1 progeny (♂ and ♀) had wild type body color and wild type wing venation which suggests that these mutant phenotypes are recessive. Furthermore, males and females were distributed approximately equally for the body color and wing venation trait in both crosses which suggests that the genes for body color and wing venation traits are autosomal.
Table 3 shows that all cross A females have wild type eye color and all males have mutant eye color. This pattern of inheritance is indicative of a sex-linked gene since males receive only the mutant allele they have the mutant phenotype but females have one wild type allele and one mutant allele so they have a wild type phenotype given that mutant eye color is a recessive trait. Cross A F1 females all inherit one copy of the wild type eye color gene which is enough to confer the wild type phenotype so eye color is a recessive trait. Phase II of the A and B crosses involved a cross between the F1 progeny, see Table 4 for body color and wing venation results.
If body color and wing venation are two allele, recessive, single gene traits with complete dominance then they would be expected to segregate 3:1 wild type:mutant. The body color trait segregated 3.56:1 wild type gray body:mutant black body. A χ2 test was conducted which did not reject the hypothesis that body color is a recessive single gene trait. The wing venation trait segregated at a ratio of 4.4:1 wild type:mutant wing venation and a χ2 test rejected the hypothesis that wing venation is a recessive single gene trait (Skeletal Report 5). This data conclusion is not consistent with the F1 progeny data that showed that wing venation was a recessive trait. However, since only a small number of F2 flies were scored there is a chance of random error which may explain the unexpected result of the χ2 test. Scoring errors or expressivity/penetrance problems of the wing venation mutation could also have been factors in this unexpected result.
Since the genes for body color and wing venation have been characterized as autosomal, another χ2 test was conducted to see whether the two genes were independently assorting. If two genes independently assort then the expected dihybrid ratio of F2 progeny is 9:3:3:1, wild type:single mutant:other single mutant:double mutant. A modified dihybrid ratio had to be created based on the observed F2 ratios in order to more accurately judge independent assortment based on the data generated from this experiment. See Table 5 for the modified expected ratios and expected number of flies for the dihybrid cross compared to the actual observed numbers for each of the four phenotypes. If the genes were segregating independently then a ratio of 15.7 : 4.4 : 3.56 : 1 would be expected; however, there are approximately three times more double mutants than would be expected if the genes were segregating independently which logically suggests that the genes are linked. And indeed, a χ2 test rejected the hypothesis that the genes for body color and wing venation are independently assorting so the alternative is that they are linked. The map distance between body color and wing venation was calculated to be 29.14 m.u. (Skeletal Report 6)
As Table 6 shows, a new orange eye color was observed in the F2 generation and there are only wild type females in cross B F2 progeny. Based on this data, the best hypothesis for the interaction that creates eye color is a two gene epistatic interaction where homozygous white eye mutations are epistatic over homozygous or heterozygous orange mutations. The proposed interaction is:
The expected results based on this hypothesis are that the two genes for eye color are linked on the X chromosome because the F1 data showed that eye color is sex-linked and the only explanation for having only wild type F2 females for cross B is if this is the case. See Page A of the attached Skeletal Report for the details of the crosses and how alleles segregate to give this pattern of inheritance. The map distance between the two eye color genes, w and n, was calculated from cross A F2 data to be 28.57 mu.
Marker crosses II and III were conducted in order to map autosomal genes, which in the case of this unknown included the genes for body color and wing venation. Each of the mutations in the M2 and M3 strains was dominant and homozygous lethal. The marker stocks were part of a balanced lethal system which consisted of a true breeding heterozygote combination of known marker mutations. Crossovers are suppressed in this type of genetic setup and the original Bl L / Cy and Gl Sb / LVM genotypes for marker II and marker III, respectively, were conserved. The initial M2 and M3 marker crosses produced F1 progeny that were expected from the conclusions based on A and B reciprocal cross progeny. All progeny were either Bristle Lobed or Curly in M2 or Glued Stubbled or wild type in M3 because those mutations were dominant in the marker stocks. All F1 progeny had wild type body color and wing venation because these two genes were determined to be autosomal and recessive.
In phase II crosses, F1 males were backcrossed to unknown mutant females and if the recessive mutation showed up with the male backcross progeny (Br L or Cy for marker II and Gl Sb for marker III) and four phenotypes were present in the F1 male backcross progeny, then the gene for the mutation is not linked to that marker's chromosome. If a recessive mutation is linked to the markers chromosome, then the mutant phenotype would not show up with male backcross progeny and only two phenotypes would be present in F1 male backcross progeny. The results of phase II male backcrosses are summarized in Table 7.
Green colored numbers were scored from F1 curly (M2) or wild type (M3) ♂s x unknown ♀s
Black colored numbers were scored from F1 Bristle Lobed (M2) or Glued Stubbled (M3) ♂s x unknown ♀s
Table 7 data from cross II shows that mutant body color and mutant wing venation segregated with the male backcross curly progeny. This means that the body color and wing venation genes are not on the second chromosome. The mutations did not segregate with the Bristle Lobed marker mutations in this experiment, but it would be expected that a larger population of flies would show mutant body color and mutant wing venation showing up together in the F1 male backcross progeny. Cross III shows that mutant body color and mutant wing venation do not segregate with F1 male backcross progeny suggesting that these genes are located on the third chromosome. Two flies were scored to be Glued Stubbled, dark body, white eye, wild type wing venation, marked by an asterisk in Cross III. This unexpected result is likely due to a scoring error.
The mutant wing venation gene was given to be at locus 47 m.u. and was said to have a smaller map distance than the gene for body color. The map distance between the genes for wing venation and body color was calculated to be 29.14 m.u. This information is enough to assign a location on chromosome three to each of these three genes, see Figure 2.
Marker cross I was conducted in order to map sex-linked genes, which in the case of this unknown included the genes for eye color. The mutant marker genes present on the X chromosome are recessive and produce a crossveinless (cv) forked (f) phenotype. The initial M1, M2, and M3 marker crosses produced the expected mutant white eye color male and wild type red eyed female F1 progeny. All M1 F1 progeny were wild type for marker 1 mutations (cv f) since all males received a normal copy of these genes from the unknown female parent.
Based on the analysis of F2 data of crosses A and B the two eye color genes were determined to be linked on the X chromosome. The possible genotypes and phenotypes for Cross I F2 progeny are depicted in Tables 8 and 9 as well as the number of male flies scored in each of the phenotypic categories. Only males were included in this table and in determining map distances because it is only males that carry information about crossover events that are needed to map genes. The female progeny had approximately equal numbers (~64 flies) of red eye and white eye progeny and half that number (34 flies) of orange progeny. Table 8 and 9 are set up as three point crosses with only one of the marker I mutations included in relationship to each eye color gene. Table 10 is a two point cross for the marker mutations (cv and f) that shows the phenotype and number of male progeny in each category.
These two and three point crosses were used to calculate the map distances between cv, f, w, and n on the X chromosome, see Table 11. For calculation details refer to page 11 of the Skeletal Report attached. After finding that the map distance between cv and f was slightly off from the actual known distance, the map distances between all other genes were corrected using the correction factor (43/41.7). The numerator in this factor is the actual distance between cv and f and 41.7 was the experimental map distance. Table 11 shows corrected map distances.
The map distances in Table 11 along with the relative number of flies in each category give enough information to determine gene order. The cv-w and cv-n distances add together to give the distance between w and n which tells that cv is between w and n on the chromosome. In the f-w-n three point cross the least number of flies were found in the orange forked phenotype which was assumed to be the double cross over event. In a double cross over the only gene that will be different from the parental genotype will be the gene in the middle of the two crossovers. The genotype of the F1 female parent was f w+ n+ and the genotype of the double cross over was f w+ n. The gene that is different from the parent is n which means that n is in between f and w.
The locus of each gene was finally determined using the gene order and map distance data explained above coupled with the given locus of gene w at 1.5 m.u.. See Figure 3 for a diagram of the X chromosome and the locus of the eye color and marker I genes.

The fly Drosophila Melanogaster is one of the most studied organisms used in biological research in genetics and is considered a model system for several reasons: they contain only 3 pairs of autosomal chromosomes and one pair of sex chromosomes; the entire genome has been sequenced; they are small and easy to grow in the laboratory; they have a short generation and high productivity; and the males do not show recombination.
The purpose of this experiment was to determine the number of genes involved in each mutant phenotype (white eye color, dark body color, short veined wings veins), the mode of inheritance for these mutations, and the genetic loci of each of these genes. The experiment was carried out in three main steps. Unknown flies with wild type flies underwent two reciprocal crosses to determine the mode of inheritance for each mutation; the analysis of the F1 and F2 generations revealed whether the mutant phenotypes were dominant or recessive and autosomal or sex linked. In the next step, U-5309 virgin females were separately crossed with males from M2 and M3 marker stocks that contained known mutations on autosomal chromosomes 2 and 3 respectively. The F1 males of these crosses were back crossed to U-5309 virgin female flies in order to ascertain the genetic loci of the autosomal mutations. Finally, a cross between U-5309 female flies with male flies of known marker mutations on the X chromosome (M1) was carried out to the F2 generation in hopes of establishing the genetic loci of possible sex linked mutations.
The flies were prepared for observation and handling by anesthetization using CO2. The flies were then transferred to CO2 pad and examined and counted under a microscope. (See pg. 1-6 of lab manual for more details).
Several different strains of flies were used for this experiment: unknown mutant strain U-5309, wild type Oregon R strain, and three marker mutation strains with known mutations. The genotypes and phenotypes for each strain are listed in Table 1.
In this experiment, 5 different crosses were used. In Cross A, Oregon-R males and U-5309 virgin females were crossed to produce an F1 generation. The males and females of the F1 generation were then crossed with each other to produce an F2 generation. Likewise, U-5309 males and Oregon R virgin females were crossed to produce an F1 and F2 generation in Cross B. In a separate cross, Cross I, U-5309 virgin females were crossed with M1 males also to produce an F1 and F2 generation. In Cross II and Cross III respectively, U-5309 virgin females and M2 and M3 males were carried out to the F1 generation, and then the F1 males were backcrossed with U-5309 virgin females. (See pp 19-25 of lab manual for more details)
The F1 data of crosses A and B show the wild type phenotype for body color in both males and females (Tables 2 and 3). This signifies that the wild type phenotype is dominant over the mutant dark body phenotype. The F2 results of crosses A and B show two phenotypes, light and dark body, in both males and females. The presence of both wild type and mutant body color in the F1 and F2 generations indicate that the gene for body color is autosomal. In the F2 generation, there is a sizable majority of light body color over dark body color, which further supports that dark body color is a recessive mutation and independently assorts (Table 4). Chi square analysis was used to determine whether body color is a single gene trait that independently assorts properly to produce a 3:1 phenotypic ratio in progeny. The p value was calculated to be 1.86 x 10-3, which indicates that our data body color is not segregating properly. This either implies that there may be another factor that influences the segregation of body color alleles so that they do not independently assort, or that a larger sample of F2 progeny should be examined to produce more accurate results.
The F1 data of Crosses A and B also show wild type phenotype for wing venation in both males and females. This suggests that the mutant wing venation is recessive and autosomal. Furthermore, the F2 generation shows males and females both containing both wild type and mutant longitudinal wing venation, and also resulted in significantly more wild type longitudinal wing venation (503) than the mutant phenotype (115). This suggests that the gene for wing venation undergoes independent assortment. However, chi square analysis to test if wing venation is a single gene trait that undergoes independent assortment produced a p value of 2.43x10-4, which indicates that wing venation is not undergoing independent segregation properly. This result, similar to body color, indicates that there also may be another factor influencing body color segregation or that a larger sample of F2 progeny should be examined to produce more accurate results.
Chi square analysis was also used to see if longitudinal wing venation and body color genes assorted independently from each other. Using the observed ratios obtained from body color data and wing venation data separately, chi square analysis produce a p value of 0, which led to the conclusion that body color and longitudinal wing venation did not assort independently from each other. Recombination analysis produced a genetic map distance of 42.1 map units between the two genes. (See Appendix for all chi square calculations)
The Cross A F1 results for eye color produced all females with red eyes and all males with white eye color. (Table 2) The females of the F1 generation must all be heterozygous, which shows that the mutant eye color is a recessive mutation. However, since the F1 generation only contained males with white eyes, the gene for eye color was determined to be sex linked; all the males in the F1 generation receive one X chromosome from the mutant female parent, and a Y chromosome from the wild type male parent which does not contain any corresponding wild type alleles. The Cross B F1 results for eye color show both male and female flies with
red eyes (Table 3) This is because the males inherit their single x chromosome from their wild type female parent.
The F2 generation of Cross B produced several different eye color phenotype: red, white, and orange (Table 5). This suggests that there are two genes involved in determining eye color. The appearance of specifically three different eye colors suggests that eye color is controlled by two genes showing epistatic interaction (Figure 2). The high number of mutant phenotype observed indicates that the gene coding for white eyes (a) is epistatic over the gene coding for orange eyes (b). Also in the B Cross F2 generation, only the red eye phenotype was found in females. If the two genes had undergone independently assortment, the females in F2 would contain more than one phenotype. However, females only showed wild type phenotype for eyes, indicating that these two genes are linked. Moreover, the unequal appearance of different eye colors between the male and female flies of the F2 generation in cross B further supports the previous conclusion that the genes for eye color located on the x chromosome. Using recombination analysis, the genetic map distance between the two genes controlling eye color was determined to be 30.0 map units for cross A and cross B combined.
In order to determine the genetic loci of the two linked autosomal recessive mutations of U-5309, body color and wing venation, males from the M2 and M3 balanced marker stocks were crossed with U-5309 virgin females in Crosses 2 and 3 respectively. The crosses produced an F1 generation that showed wild type phenotypes for both wing venation and body color, as expected. Balanced marker stocks are true breeding heterozygous strains that maintain the same genotype through a homozygous lethal system. 50 flies of each cross were scored to confirm that the marker mutations, body color, and wing venation were all assorting properly.
Continuing Cross 2, male F1 flies with lobed eyes were back crossed with virgin U-5309 females. There were not enough progeny from this cross for allow for statistical significance. However, from the few flies that were scored, the back cross progeny showed three distinct phenotypes: light body, normal wing venation, and bristle lobed; light body normal wing venation, and curly wing; dark body, mutant wing, and curly wing. Since dark body color and mutant longitudinal wing appeared with the marker mutations, it indicates that the body color gene and wing venation gene are not located on the same chromosome as the marker mutations, the 2nd chromosome (Figure 1B). The white eye phenotype also appeared with the marker 2 mutations in male backcross progeny, confirming that the genes for eye color are not located on the 2nd chromosome, as expected. The less stable homozygous lethal system of the marker genes along with the exposure of CO2 to the F1 males may account for the extremely low backcross progeny count.
Cross 3 was continued by back crossing male F1 flies with virgin U-5309 females to see if the body color gene and wing venation gene are located on the 3rd chromosome. There were only two different phenotypes observed in the progeny: light body, normal wing venation, glued eyes, and stubble bristles; dark body, mutant wing venation. Only wild type phenotypes for body color and wing venation appeared with the marker mutations glued eye and stubble bristles in the male backcross progeny, indicating that both wing venation and body color genes are both on the 3rd chromosome. It was known that the wing venation mutation is located at 0 on the chromosome
3. Therefore, according to the distance between the two genes previously determined through recombination analysis, the location of the body color gene is at 42.1 map units on the third chromosome, between the genes for glued eye and stubble bristles (Figure 1C).
To discover the genetic loci on the x chromosome of the two genes controlling eye color, virgin U-5301 females were crossed with marker 1 males (Cross I). The F1 generation contained red eyed females and white eyed males, as expected. Among the 50 female progeny of the F2 generation, the phenotype found in the least number was orange eyes. This further supports our conclusion of an epistatic relationship between the two genes. A total of 104 male progeny were scored and 6 phenotypes were observed with the possibility of 8 different genotypes between the three genes: cv, a, b. (Table 6). The phenotypes that occurred twice were corrected to the number of its corresponding recombinant allele in order to account for having two possible genotypes. Since the location of gene a already was already known to be at position 1.5, the distance between gene a and gene cv was used to determine the correction factor. The corrected distances showed that the distance between cv-b was the largest, thus placing gene a between cv and b (Table 7). However, this would place gene b in a location entirely off of the chromosome. Therefore it was predicted that the data collected was not representative of the actual genetic loci of gene b.
Another 3 point cross was calculated to determine the location of the two eye color genes in relation to the known forked bristle gene of the marker mutation. (Table 8). Similar to the cv-a-b cross, the phenotypes that occurred twice were corrected to the number of its corresponding recombinant allele in order to account for having two possible genotypes. The distance between f-b was calculated to be the largest, therefore supporting the placement of gene a between genes b and f. Since the locations of gene a and gene f were both given, the distance between them was used in the correction factor. (Table 9).
One possible explanation for the inconsistent results in the 3 point crosses could be attributed to the difficulty in distinguishing between orange and red eyes of the back cross progeny in Cross I. Also, if more Cross 1 F2 progeny had been scored, perhaps more accurate results could be obtained.
A 2-point cross was also determined between cv-f. (Table 10) Based on knowing the actual gene loci of genes cv and f, it was expected that would be more crossveinless and forked bristled progeny than actually observed. The calculated map distance between the two genes based on our observed results was 3.8, which is significantly smaller than the known 43 m.u. It is not reasonable to use the correction factor of 43/3.8 to correct the values derived from the 3-point crosses because the values calculated would be much larger than the actual chromosome. Scoring more than progeny may have resulted in relatively more recombinant progeny, therefore producing a more accurate distance between the two genes.
Based on previously knowing the locations of genes a, cv, and f, and only looking at the relative positions of the genes to each other, the gene order should be b-a-cv-f. However, the map distances between the four genes given in the point crosses are significantly inconsistent with each other. All three point crosses placed gene b in a location off of the actual x chromosome. Because of these unfeasible results, the genetic distance determined through recombination analysis of Crosses A and B was ultimately used to determine the location of gene b. Since the location of gene a at 1.5 is known, gene b must be located at position 31.5. This would place gene b between gene cv and gene f at positions 13.7 and 56.7 respectively (see Figure 1).
In our experiment, it was determined through crosses A and B that the genes for wing venation and body color are autosomal, recessive, and linked. From the F2 generation of both crosses, the genetic distance between the two genes was calculated to be 42.1. Crosses II and III located gene d and gene sv on chromosome 3. Although Cross II did not produce enough back cross progeny for significance, the data obtained still suggested that genes sv and d were both located on the 3rd chromosome. Therefore, given that the location of gene sv is at 0 on chromosome 3, it was determined that the location of gene d is at position 42.1 of the same chromosome.
Through Cross A and Cross B, it was revealed that our mutant eye color is controlled by two genes that exhibit epistatic interaction. The gene for white eyes is epistatic over the gene for orange eyes. Based on the phenotypes of the F1 and F2 generations, it was found that the genes for eye color are also recessive and sex linked. The two 3-point crosses and one 2-point cross analysis of Cross I resulted in a location for the orange eye gene that was unfeasible. Since the genetic location of gene a on the x chromosome was previously known, the location of gene b was determined to be at position 30.0 based on the recombinant frequencies calculated from crosses A and B. This would place gene b between the marker genes cv and f at 13.7 and 56.7 m.u. respectively.

We used bacterial conjugation to measure genetic distances and observe recombined phenotypes through the use of E. Coli, specifically, an Hfr strain that is F+. This categorization indicates that the bacteria contain a non-attached, transferable sex factor that can be donated to an F-strain, which we introduced through uninterrupted mating. This creates a donor pair, the male is known as a genetic donor, F+ and the female as a genetic recipient, F-. Bacterial conjugation is the formation of a very fragile conjugation tube that creates a canal for the F factor to be transferred. The canal is created through contact of pili from one bacterium to the cell wall of another, known as collision. Next, the bacteria must identify each other as a donor-acceptor pair. Once the conjugation tube has been formed, the transfer occurs. This is an energy dependent, sequential transfer that is unidirectional and proceeds at a slow but constant rate. The selecting gene must then be expressed phenotypically, if they are dominant. Finally there must be integration of the gene into the replicated genome of the accepting bacteria. This specific order of operation sets limitations to conjugation. The gene that is closest to the origin of transfer has the greatest chance of getting into the F-cell's chromosome. The origin of transfer is the point at which a break occurs in the integrated F factor. This is the leading point of the chromosome to be transferred. By selecting for a certain gene that we expect to be transferred and analyzing the amount of bacteria that grew in the specified media, we can determine genetic distances between these genes and are able to tell where recombination about these genes took place.
In addition to thiamine, the following supplements were supplied to the corresponding plates:
To begin the process of conjugation, we mixed 0.2 ml of Hfr culture with 1.8ml of F-culture. We then incubated the mixture at 37oC for 60-70 minutes, this is known as uninterrupted mating. We then took the mixture out of incubation and added 1ml of it to 9ml of nutrient broth for dilution. This created a concentration of 10-1. With a pipette, we transferred 0.1ml from the original solution to three plates, media 1, 2 and 4. We then transferred 0.1ml from the diluted solution to three new plates, media1, 2, and 4. These six plates were then incubated at 37oC for 48 hours. The following day, the plates were scored and 2 master plates were created from the viable colonies, one on media 1 the other on media 2. We chose to create our master plates from the original solution, and allow our dilution plates to be used by other students. These plates were then incubated at 37oC for 24 hours. Once the incubation period had passed, we replicated the master plates onto the above mentioned media plates. The media 1 master plate was replicated to media 2,then 3,then 4,then 5, and finally to media 1. The media 2 master plate was replicated to media 1, then 3, then 4, then 5, and finally to media 2. Once results were observed, the following day, the plates were placed into cold storage until they could be viewed by the GSI.
In this experiment we found that through uninterrupted mating of the Hfr and F-, we get the highest number of viable colonies from the media 2 plate, which selects for Pro+. We then see that the media 1 plate, which selects for TL+, exhibits the next highest number of colonies and finally that the least number of colonies grows on media 4, which selects for Gal+, as supported by the Table III of the appendix. We see from the media 1 master selection plate series that the most number of colonies grew on media 1, selecting as previously mentioned, and media 5, which selects for Met+, followed by media 2, also selects as previously mentioned, then media 3, which selects for Lac+. The least amount of colonies grew on media 4, the Gal+ selecting plate. The numerical data for the media 1 master plate selections can be found in Table IV of the appendix. The data from the media 2 master selection plate series shows the highest amount of growth on the media 2, as well as media 5. Media 3 plate had the next highest amount of colonies followed by media 1, to leave only media 4 as the plate having the fewest colonies. This information can be found with numerical values for colonies in Table V of the appendix.
Through the methods of selection we conclude that the first gene to enter the F-cell is Lac+ gene, which indicates its location as being the closest to origin of transfer. Following the Lac+ gene, in order, we see that Pro+, TL+, and Gal+ enter successively. We determine that the Met-gene from the Hfr was not expressed phenotypically in our recombinants. This is because any bacteria containing this gene, assuming it is dominant, would not have survived our selective media 5. The order of entry of these genes was determined through three criteria found through the analyses of the plates. The first criteria came from our initial uninterrupted data that told us through order of decreasing quantity that Pro+ must be closer to the origin of transfer Than TL+ and Gal+ because the media 2 plate had the most colonies. TL+ comes next followed by Gal+. The second criteria comes from our analyses of the TL+ selection plates which tells us that in this selection, Pro+ must be closer to TL+ than Lac+. The third criteria comes from the Pro+ selection plate series that tells us when selecting for Pro+, Lac+ must be closer to Pro+ than TL+. Through creation of organized tables (please see appendix) of the recombinant phenotypes, we are able to calculate map distances between the genes selected for and those that come between it and the origin of transfer. Table VI in the appendix has the data and calculations for the map distances carefully outlined. Below is a map of our found genetic order of entry as well as the calculated map distances of the selected genes. The discrepancy in the two calculated distances between the TL+ and Lac+ in Table VI is 10 mu. This could be due to the fact that we had more recombinants between crosses 1 and 4 of the media 1 selection than expected and less recombination between crosses 1 and 3 of the media 1 selection. In order to find a perfect correlation between the calculations, we would have needed more colonies that had recombined to give us a TL+ Pro+ Lac-phenotype. We could repeat the experiment to determine whether this was the case or whether there was a discrepancy in the media 2 master selection series. We chose to use the values for the map distances that were shorter because it is known that when we determine map units that the direct distance between genes is more accurate that when calculating a distance that passes over genes.
We can conclude from this experiment that the order of entry is Lac+ first, Pro+ second, TL+ third, Gal+ fourth and Met+ fifth. We can also conclude that the genetic map distance between Lac+ and Pro+ is 24 mu, the distance between TL+ and Pro+ is 36 mu, and consequently the distance between TL+ and Lac+ is 60 mu. We cannot conclude any distances from the origin for Gal+ or Met+ since they entered the F-bacteria after the genes that we selected for.

Drosophila melanogaster is a well-studied species that is easily anesthetized and handled. Because of this, as well as its relatively small genome of four chromosomes and short developmental duration of four days to imago, Drosophila is an ideal species for genetic analysis. The fact that virgin female flies, which are necessary in order to ensure proper genetic crossing, are easily extracted from a newly hatched population due to the innate inability of either sex to mate within the first ten or so hours of life, furthers this attraction to Drosophila as a controlled laboratory experiment. In this experiment we take advantage of these characteristics and use them to identify the genetic locus of specific mutations that cause deviations in the wild type phenotypic eye color, body color and wing venation. This can be achieved by performing five crosses. The parental reciprocal crosses, A and B, are performed to determine whether the mutant genes are sex-linked, autosomal, recessive, or dominate. The parental cross of the unknown virgin females with the males of marker stocks' II, III and I provide verification to the findings of the reciprocal crosses as well as confirmation of known information of the dominate marker II and marker III mutations, and the recessive mutations of the marker I stock. The F2 generations of these marker crosses become useful when determining gene interactions and chromosomal locus. By observing the expressed phenotypes in the F2 generation of the reciprocal crosses, we can determine the number of genes involved in a specific mutation, such as eye color, body color, or wing venation and calculate the map distance between them. We can also use chi-squared testing in order to prove or disprove hypotheses of our findings. If it is concluded that two mutations are linked through a chi-squared test, the map distances between them can be calculated through the well-known techniques as discussed in the MCDB 306 lecture.
The marker I cross can offer information about sex-linked genes and their locus on the first chromosome. With a population from these three crosses combined, we were able to perform more accurate statistical analysis through the chi-squared test and determine genetic linkage or independent assortment of multiple allele genes than with the reciprocal cross F2 generation data alone. The F2 generation of the marker II and III crosses tell us if the mutations are located on the respective chromosome by a lack of simultaneous expressivities of the phenotype. The map distances between the mutations found to be on those genes can be determined through analyzing the recombinant data from the F2 generation of the reciprocal crosses.
Once the flies were carefully removed from their feeding vials, and anesthetized with CO2, they were sexed. Virgin females could be obtained by clearing all flies from the vials and returning to collect and sex them within 10 hours of the clearing. Ten Crosses were then performed, each made in duplicate. Ore-R Drosophila was used as the wild type, U-2544 was used as the unknown mutant stock, and marker stocks I, II and III were used. As a parental cross, U-2544 virgin female were crossed with Ore-R males, this was called Cross A. A reciprocal parental cross was created between the Ore-R virgin females and the U-2544 males, Cross B. U-2544 virgin females were crossed with Marker I males, Cross I. U-2544 virgin females were crossed with Marker II males, Cross II. U-2544 virgin females were crossed with Marker III males, Cross III. Once the F1 generation was scored and analyzed, Crosses A, B, and I were allowed to continue through an F2 generation, with the removal of parents within 7days of set up. Cross II had F1 Bl+ L+ males removed and Cross III had F1 Gl+Sb+ males removed, each to be backcrossed with U-2544 virgin females in new vials. The complete and detailed procedure is described in the MCDB 306 lab manual.
The F1 data above indicates that the gene for body color is recessive. This is due to the lack of the mutant appearance in the F1 generation of both Crosses A and B. It also indicates that the mutation is autosomal due to the appearance of the wild type trait in the males. Had the trait been sex linked, the males of Cross A would have received their only X chromosome from the U-2544 mother and exhibited the dark body phenotype.
The F2 data for body color produced a 3:1 wild type to mutant phenotypic ratio. Since we can conclude from the F1 generation that the gene for body color is autosomal, it is not necessary to account a female to male ratio. For these results we hypothesize a normal segregation for the body color gene. The chi-squared value for this hypothesis was 0.2411. This result, under one degree of freedom, yields a p-value that is greater than 0.5 but less than 0.9. Therefore we accept our hypothesis of normal segregation for body color.
The F1 data for the wing venation is similar to that of the F1 data for mutant body color. The lack of appearance of the mutant trait in the F1 generation allows us to conclude that the mutation is recessive. The fact that the males of the F1 generation of Cross A forces us to conclude that the gene for wing venation is autosomal.
The data for the F2 generation yields a 4:1wild type to mutant phenotypic ratio. For this data we hypothesize a normal segregation of the gene for wing venation. The chi-squared value of 9.67 with one degree of freedom indicates a p-value that is much less than 0.005 and therefore we must reject the hypothesis and conclude that the wing venation gene does not segregate normally, and that it is linked.
The results of body color and wing venation combined produce a ratio that cannot be used to easily conclude whether the two traits are linked or whether they independently assort. Therefore we calculate a chi-squared test with a hypothesis that they do assort independently. With this assumption we calculate a value of 104.305. This result, under three degrees of freedom, yields a p-value that is much less that 0.005 and therefore the hypothesis must be rejected and we conclude that the mutant genes d and w are linked. Knowing this, we can calculate the map distance using the amount of recombinants in the F2 generation. Here we calculated that map distance between d and w to be 44.2 mu.
The F1 generation here provides us with the same information as found for the body color as well as wing venation mutations. We can conclude through the lack of mutant findings in the F1, we can conclude that the mutation is recessive. We can also conclude that it is autosomal due to the fact that the F1 males of Cross A do not exhibit this trait.
The F2 generation provides us with two new phenotypes, according to eye color. The red eye color is wild type and as such a parental phenotype. The same is true for the mutant parental white-eyed F2 flies. From this data we hypothesized that there were two genes interacting with each other in order to produce orange and brown eyes. The two genes o and b are epistatic to each other. This meaning that a wild type Ore-r would have o+_b+_ genes, homozygous or heterozygous, as long as one of each dominate gene were present to create the red eye. This would also indicate that our U-2544 would have oobb genes, homozygous recessive alleles for both genes involved in eye color. From this we state that the orange-eyed phenotype will be oob+_, homozygous recessive for the o gene expressing that it is epistatic to the b+ wild type gene. Similarly, we state that the brown-eyed phenotype will be o+_bb, homozygous recessive for the b gene expressing that the presence of two mutant alleles results in epistasis. Through Mendel's experiments with his peas, it is known that two genes that are independently assorting form a 9:3:3:1 ratio, given that each trait yields a 3:1 ratio. From the high amount of double recombinant phenotype in the F2 generation we can conclude that the genes for eye color are in fact not independently assorting but linked. This can be seen even without doing a chi-squared test by comparing the 9:3:3:1 expected ratio to the 10:1:2:3 found ratio. We calculated the map units between the two genes to be 43.6 mu.
The marker stocks are used in order to verify chromosomal locus. The known mutations are dominant Cy, Bl, and L in marker II stock, and Gl and Sb in the marker III stock. These mutations are known to be homozygous lethal, which form what we call a balanced lethal system. The F1 generation of these marker crosses alone does not tell us anything more than that which we have learned from the reciprocal crosses. When males of the F1 are separated and crossed with U-2544 virgin females, called a male backcross, we can use the resultant phenotypes to determine whether or not the selected trait is on chromosome II or III. Because of the lack of recombination in male Drosophila, we determine that the mutant genes appearing with the marker mutations reveal that they are not on the same chromosome. For example, we found the gene d, dark body, to be located on chromosome III. This was discovered through a male backcross of the marker III F1 progeny. The genotype of the F1 male is Gl+ Sb+ d+/ Gl Sb d, and it is crossed with a U-2544 female whose genotype is Gl+ Sb+ d+/ Gl+ Sb+ d+. Since there is no recombination in male Drosophila, and any recombination in the female would not change the genotype, we can conclude that any progeny would have the wild type allele d+ in its genotype and be dominant over our mutant gene. If the genes were not linked, the fact that there is no recombination between chromosomes in males would be irrelevant. This backcross theory is relevant to any backcross. The results from our marker III male backcross indicates that neither the body color gene, d, nor the wing venation gene, w, appear with the marker III mutations. From understanding of what the male backcross can tell us, we know that these genes must be located on the chromosome III. It is seen that the eye color mutation does appear with the chromosome III mutations and therefore cannot be located on the same chromosome. We then conclude that the genes must be located on chromosome II. This information would be indicated by a backcross of the marker II stock, had it thrived. The cross failure was probably due to over exposure of the sensitive marker II stock to the CO2 anesthetizing agent. Because of this mating failure between any male progeny and the U-2544 virgin females, we must deduce the chromosomal locus through a process of elimination using other experimental and known information. Since we know that o and b are not sex linked, they cannot be located on chromosome I. It is known that o and b are not located on chromosome four. Now that is experimentally determined through male backcross that o and b are not on chromosome III, we can only conclude that they must be located on chromosome II.
Combining the found information of crosses A, B and I provide us with a larger population and more accurate data for eye color analysis. By performing a chi-squared test we can test a hypothesis of independent assortment between the two genes. The result had a value of 277.46 yielding a p-value that was much less than 0.005 and so disproving our hypothesis. From this we can conclude that the o and b genes are linked. We use the recombinant data to calculate a map distance of 45.3 mu between o and b.
From this experiment it is concluded that the eye color mutations are linked and located on chromosome II. The gene o has a genetic locus of 59.2 mu while b has a genetic locus of 104.5 mu. It was also found that wing venation and body color mutations are linked and located on chromosome III. The w gene has a genetic locus of 0mu and d can be found at 44.2 mu. Crosses A and B's F1 generations provided us with the knowledge that the U-2544 mutations were autosomal recessive. The resulting F2 generations from these reciprocal crosses allowed us to perform chi-squared statistical analysis proving that the body color and wing venation genes are linked. The F2 generation data also allowed us to determine that two genes are involved in eye color, that they are epistatic to each other. We also used this data to determine an estimate of map distance between them. The male backcross of the marker crosses was necessary to determine chromosomal locus. In the case of the U-2544, it turns out, only the marker III cross was necessary in determining chromosomal locus. Through this cross, we determined that w and d were located on the third chromosome and that o and b were not. The marker cross I gave us additional information in calculation of the map distance between the two eye color genes. Had our mutant genes been sex linked, it would have been necessary to find the chromosomal locus. The loss of the marker II cross was regrettable but not detrimental to our conclusion that the eye color genes had a chromosomal locus here, due to the fact that Drosophila has a limited genome, one of the main reasons for using Drosophila in genetic analysis. The following maps indicate the chromosomal and genetic locus of the unknown mutations as well as the loci of the marker mutations.

Drosophila Melanogaster is useful as a model organism because it has a short life cycle of 10-14 days. Their short lifespan and small size make it easy to culture and relatively inexpensive to store large numbers of flies. Additionally, numerous mutant phenotypes are easily seen with or without the aid of microscopes so observing and separating the flies is not a painstaking task. Because Drosophilahas a very long history in biological research, there is a tremendous amount known about its genome. The knowledge about wild-type and marker strains will help in identifying where the mutant genes are and how they are inherited.
Ultimately, the purpose of this experiment was to characterize unknown mutations in a mutant strain of Drosophila Melanogaster. The goal was to determine the inheritance patterns of these mutations as well as to locate the specific position of the genes on the chromosomes. By comparing a pure line wild-type strain, known as the Ore-R strain, with the mutant strain, known as U-7812, distinct differences were found in the expression of body color, eye color, and wing venation. To find out how these three different phenotypes were created, various crosses were performed. In order to find out how the mutations were inherited (sex-linked verses autosomal; dominant verses recessive), reciprocal crosses between the Ore-R strain and mutant strain were performed. The progeny of the F1 generation underwent further reciprocal crossing to produce an F2 generation which was used to determine how the mutations are inherited. In addition to these reciprocal crosses, a series of marker crosses were performed using M1 and M3 marker strains (Note: Marker 2 was not available). The marker strains contain mutations with known positions on their respective chromosomes (M1 is the X-chromosome, M3 is chromosome 3), so they could be used to decide whether the marker mutations were linked with the unknown mutations. Female unknowns were crossed with males from the marker strains. An F1 cross was completed for the M1 group, while a male back-cross was performed for the M3 group. These marker crosses provided information to establish linkage relationships of the unknown genes and to assign each gene to a specific locus.
Four different strains of Drosophila melanogaster were used. All of the flies in the stocks are true breeding. The Oregon-R (Ore-R) stock contains wild-type flies have red eyes, complete wing veination, and tan-colored bodies. U-7812, the unknown stock with white eyes, dark body, and incomplete longitudinal veination, was utilized in every cross. The Marker 1 stock had crossveinless (cv) and forked (f) mutations, which are located on the X-chromosome. These mutations were eventually found to be sex-linked recessive. The Marker 3 stock had glued eyes (Gl), stubble (Sb), and LVM mutations, which are located on chromosome 3. These mutations were eventually found to be autosomal dominant. LVM flies express the same phenotype as a wild-type fly would; the actual difference is that an LVM fly has an inversion on chromosome 3. LVM is homozygous lethal and therefore serves as a balancer lethal system in the M3 crosses.
All crosses were performed twice and all protocol in performing the crosses as well as the handling of all drosophila strains can be found in the MCDB 306 lab manual.
All flies were put to sleep using carbon dioxide gas. After all F1 progeny were hatched, 100 flies were scored from crosses A, B, I, and III. After F2 and backcross progeny had hatched, 200 flies from each F2 generation were then scored with phenotype data recorded for all crosses.
The F1 results as shown above support that there is an autosomal recessive pattern of inheritance for the mutant alleles for wing venation, body color, and eye color. If the mutations in the U-7812 strain had been dominant, all of the F1 progeny would have expressed the mutated phenotypes. This is because all of the F1 progeny are heterozygous for the mutant gene. Also, if the mutations had been sex-linked, then all of the male F1 progeny would be mutated. This is because males have one X-chromosome from their U-7812 female parent, meaning any mutations would be expressed. Neither of these patterns was supported by the data gathered, but autosomal recessive inheritance of the alleles was supported. Therefore, it can be concluded that all three mutant alleles show an autosomal recessive inheritance pattern.
The F1 results as shown above support that there is a recessive pattern of inheritance for the mutant alleles for wing venation, body color and eye color. Like in Cross A, if the mutations in the U-7812 strain had been dominant, all of the F1 progeny would have expressed the mutated phenotypes. Distinguishing between sex-linked and autosomal cannot be done, because it is impossible for either sex to express the mutant phenotype. All F1 male progeny will receive their X-chromosome from their wild-type mother, so males will always express the wild-type phenotype. Females will be heterozygous for the mutant gene. Therefore, from this data, it is only possible to conclude that the genes are recessive. However, with the data from Cross A and Cross B together, it is possible to conclude that the genes are definitely autosomal recessive.
It is already known that crossveinless and forked mutations are sex-linked recessive. For the Marker 1 Cross, all of the progeny were wild-type phenotypically. The females were heterozygous for the mutant genes, and the males were all wild-type because they received the wild-type genes from the wild-type mothers. Therefore, the observed data makes sense.
It is already known that Glued-eye, Stubble, and LVM are dominant mutations on chromosome 3. However, since Gl, Sb, and LVM are homozygous lethal, mutant flies in the stock must be heterozygous at those loci (Refer to skeletal report to see the actual arrangement of the marker mutant genes as their arrangement is crucial for successful backcrosses). This means that the father has the potential to give one of two different genotypes during mating. Since the unknown female is wild-type for the three genes, the resulting F1 progeny should have two different phenotypes in equal ratios. The data gathered reflects all of these facts, because the F1 progeny are about half Glued-Stubble and half LVM expressing. The observed data makes sense.
The data above helped show three major things. First, it showed that the body color gene segregated properly and is controlled by one gene. The data corresponded to an observed ratio of 3.1:1 (wild-type:mutant), which was extremely close to the expected 3:1 ratio hypothesized to arise if there was one gene responsible, and the mutation was recessive. Using X2 analysis, it was supported that body color is inherited through autosomes and that the mutation is recessive. The X2 value of .081 correlated to a probability of 77.6% that the observed ratios would occur due to chance, given the expected ratio. Therefore, body color is autosomal recessive. Secondly, the data showed that the wing veination gene segregated properly as is controlled by one gene. The data corresponded to an observed ratio of 2.96:1 ratio (wild-type:mutant) which again was extremely close to the expected 3:1 ratio hypothesized to arise if there was one gene responsible, and the mutation was recessive. Using X2 analysis, it was supported that wing veination is inherited through autosomes and that the mutation is recessive. The X2 value of .013 correlated to a probability of 90.9% that the observed ratios would occur due to chance, given the expected ratio. Therefore, wing veination is autosomal recessive.
Finally, the F2 data shown above helped determine whether the two genes, body color and wing venation, were linked or segregating independently (combined table found in skeletal report). The data presented showed an observed ratio of 10.9:3.8:3.95:1 which was close to the expected 9:3:3:1 ratio expected for two independently assorting genes with recessive mutations. Using X2 analysis, it was supported that the two genes are assorting independently. The X2 value of .898 correlated to a probability of 82.6% that the observed ratio would occur due to chance, given the expected ratio. Therefore, it was concluded that these two genes are localized to different chromosomes in the genome, so this accounts for the independent assortment observed.
The F2 data showed the expression of four phenotypes for eye color. The expression of the orange and brown phenotypes in the F2 progeny warranted the proposal that there were two genes controlling eye color. With four phenotypes, there must be at least two genes, but, because there are only four phenotypes, there should only be two genes involved. The genes expressing the orange and brown phenotypes were assigned the names gene a and gene b , respectively. The following table shows how the genes interact to produce the different colors.
If the genes were independently assorting, there would be a typical 9:3:3:1 ratio expressed in the F2 generation. The data does not support this, as there are far more white-eyes flies than orange-eyed or brown-eyed. Since having white eyes requires that both genes are homozygous recessive, the genes are unlikely to be assorting independently. The data supports that the two eye color genes are linked, so crossing over may be occurring in the female flies. It makes sense that the brown and orange eyes are least expressed because they are recombinant phenotypes. The map distance between the two genes can be calculated using the combined data of crosses A and B. Orange-eyed and Brown-eyed flies are recombinant phenotypes, so crossing over had occurred in the mother flies. The two genes were shown to be 21.3 map units apart (refer to skeletal report).
A male backcross is used to ensure that crossing over does not take place when it crossed to a pure-line unknown female. Glued-Stubble males were recovered from the F1 progeny, and they were crossed with U-7812 virgin females. Because of the way the genes are set up on the marker chromosome (refer to skeletal report), unknown mutations that are on chromosome 3 will never show up with Glued/Stubble (i.e. genes that are linked on chromosome 3 will not show up with Glued/Stubble). If the unknown mutation is on a different chromosome, then it is possible for the mutation to show up with Glued/Stubble. According to the data above, body color and eye color genes must be assorting independly from Glued/Stubble because the mutant forms of both genes show up with Glued/Stubble. On the other hand, mutant veination is never seen with Glued/Stubble, so wing veination must be on chromosome 3. The body color and eye color genes are therefore on chromosome 2 because they are neither sex-linked (which was found from Cross A) nor linked to chromosome 3.
The data gathered (refer to skeletal report) shows that there is a linkage relationship between eye color and body color. If there had been no linkage, then the double-mutant, white eyes and dark body, would be expressed in the lowest quantity in the F2 progeny of crosses A, B, and I. This was not seen, as there were far more double-mutant flies than the recombinant orange or brown phenotypes. If the genes were not linked, then the expected ratio is 9:3:3:1. Using X2 analysis, it was supported that the two eye color genes are linked on autosome 2. The X2 value of 341.6 correlated to a probability of much less than 5% that the observed ratio would occur due to chance, given the expected ratio. This means the hypothesis of independent assortment must be rejected. This confirms that the eye color genes are linked on chromosome 2. With the combined data, it was found that the map distance between the eye colors was 18.9 map units.
Performing a three-point cross established the map distances between the linked genes of eye color and body color. Refer to the skeletal report to see a table with data for the three-point cross. By dividing recombinant phenotypes by the total number of flies, the map distances between genes could be calculated. The calculations showed a 9.8 mu distance between body color and gene a, a 10.4 mu distance between the body color and the gene b, and 20.2 mu distance between gene a and gene b. Mapping of these genes to their assigned chromosomes was accomplished by acquiring the location of one of the genes. The other genes were then mapped in relation to that gene.
The data presented above provides the organized conclusions of this experiment. It was shown that body color and eye color are linked on chromosome two, and that the wing venation trait was present on chromosome three.

In the field of plant biology, one of the fundamental processes of life is photosynthesis. This process occurs through the fixation of carbon dioxide in the presence of water and may or may not require light (photosynthetic dark reactions can occur in the absence of light). The end result of photosynthesis is the production of organic materials, such as sugars and oxygen, which are necessary for the life processes of many organisms. Although the majority of plants carry out photosynthesis, they do so at different rates. The rate of photosynthesis is dependent upon several environmental factors, including temperature, amount of light present, amount of carbon dioxide present, and the color of the light. In this lab, the purpose was to manipulate one environmental factor to determine the effect on the process of photosynthesis. It was decided that the environmental factor to be tested would be the concentration of carbon dioxide initially present. Then a hypothesis was generated: An increase in the concentration of carbon dioxide initially present will lead to an increase in the rate of photosynthesis, and as a result, an increase in the amount of oxygen generated. Throughout the experiments, the aquatic plant Elodea was used to carry out photosynthesis. This particular plant is especially conducive to scientific experiments involving photosynthesis because of its ability to produce oxygen bubbles as it carries out photosynthesis, making it simple to monitor the rate of photosynthesis in an experiment.
In this experiment, all other possible environmental factors, such as temperature, were kept constant, while the environmental factor to be tested was varied. To test the original hypothesis, different concentrations of sodium bicarbonate (NaHCO3), a source of carbon dioxide, were used. Approximately three one inch sections of Elodea were cut with a razor blade. As the method of examining the rate of photosynthesis was counting the number of oxygen bubbles produced by the plants, special attention was paid to cutting the stem of the Elodea at an angle so that carbon dioxide bubbles could escape properly. Three graduated cylinders were each filled with 10 mL of solutions of 0.2% sodium bicarbonate, 0.1% sodium bicarbonate, and 0.0% sodium bicarbonate (pure water). These concentrations were accomplished by diluting a stock solution of 0.2% sodium bicarbonate with distilled water. All solutions used room temperature distilled water; therefore temperature was not a factor that was tested in this experiment. The three pieces were placed in the individual graduated cylinders. The number of bubbles that broke the surface of the water for each cylinder was counted during a five minute time period, and the results were recorded in a data chart. This was repeated for a total of five trials, with new pieces of Elodea being cut for each trial. During each of the trials, a bright lamp stationed approximately one foot away was aimed at the three cylinders to help stimulate the process of photosynthesis.
For each of the five trials, the rate of photosynthesis was measured by counting the number of oxygen bubbles that was produced by the Elodea plant. Bubbles were counted as they traveled up the cylinder and broke the surface. Table 1 shows the data chart for the number of bubbles observed at each particular concentration of sodium bicarbonate in the five trials.
Afterwards, the average number of oxygen bubbles observed for each concentration of sodium bicarbonate was calculated. This data can be seen below in Table 2. It was observed that the rate of photosynthesis steadily increased as the initial concentration of sodium bicarbonate increased.
As shown by the data in Tables 1 and 2 in the Results section, the number of oxygen bubbles generally increased as the initial concentration of sodium bicarbonate increased. Thus, one can conclude that the rate of photosynthesis is directly related to the concentration of sodium bicarbonate. The results seen in this experiment are therefore supportive of the original hypothesis presented in the introduction of the paper. Since sodium bicarbonate acts as a source of carbon dioxide, one of the required starting materials for the process of photosynthesis, it was expected that solution with more sodium bicarbonate, and this more starting carbon dioxide, would lead to the production of more oxygen compared to a solution that had no sodium bicarbonate in it. Comparing the results seen with 0.0% and 0.2% sodium bicarbonate solutions (Table 1 and 2), it is apparent that there is a much greater amount of oxygen produced with the addition of more carbon dioxide at the start of the experiment. The average number of bubbles for pure water is less than 1, while the average for 0.2% sodium bicarbonate is 10.8 bubbles.
Based on the data shown in Table 1, it is evident that not every trial yielded ideal results. For instance, the results of Trial 2 show that the 0.2% sodium bicarbonate yielded 3 oxygen bubbles, an atypically low number for that particular concentration. Neither the 0.1% of the pure water solutions yielded any oxygen bubbles. There are several possible explanations for these results. As the rate of photosynthesis is measured by the amount of oxygen escaping in the form of bubbles from the stem of the Elodea plant, it is possible that there were problems with the stems of the plants. They may not have been cut at a sharp enough angle to allow the bubbles to escape, of they may have been blocked by some particles that were present in the plant. As the number of bubbles depends on the eyesight of individuals, it is also possible that human error is involved. Tiny bubbles of oxygen may have escaped the notice of the individuals performing the bubble counts.
In relation to the question of how photosynthesis is affected by the initial amount of carbon dioxide present, it would be interesting to further explore how different concentrations of sodium bicarbonate can increase the rate of oxygen produced. In the experiments described in this paper, a stock solution of 0.2% sodium bicarbonate was used, so higher percentages of sodium bicarbonate could not be tested. Therefore, it would be useful to try percentages of sodium bicarbonate such as 0.5%, 0.75%, and 1.0%. Based on the results seen in this experiment, one would expect that increasing the percentage of sodium bicarbonate present would lead to an even greater increase in the rate of photosynthesis, and correspondingly, more oxygen bubbles produced. However, one would also expect that once the maximal rate of photosynthesis is reached, adding a higher concentration of sodium bicarbonate would not affect that rate of photosynthesis.

Drosophila Melanogaster, commonly known as fruit fly, was the organism studied in this experiment. Drosophila were ideal because they were small and had a life cycle of about 10 to 14 days. This allowed for many generations to be produced from crosses. Drosophila are diploid organisms that only have four pairs of chromosomes which includes three pairs of autosomes and one pair of sex chromosomes. Drosophila were also ideal to study because they possess traits, which can be observed with a microscope, that were characteristic of the sex and genotype of the flies. In this experiment the pattern of heritance and the genetic loci of the mutations in body color, eye color, and wing venation in an unknown strain u4184 of flies. The unknown strain was crossed with a wild-type strain in a set of reciprocal crosses to determine if each trait was recessive or dominant and autosomal or sex-linked. The unknown strain was then crossed with three strains that each had different known marker mutations. The results from the F1 generation of these crosses and the results from their subsequent male backcrosses or F1 x F1 cross determined on which chromosomes the genes for body color, eye color, and wing venation were. The genetic distance between these genes was determined by the results in the F2 generation of the reciprocal crosses combined some of the results from the marker crosses by calculating the percentages of recombinants and parental types.
During the first lab period, the traits of eye color, body color, and wing venation were observed in the unknown strain and wild-type strain using the microscope and CO2 according to the procedure described in the lab manual. The unknown flies had whit eyes, dark brown body color, and wing venation with short longitudinal veins. The wild-type flies of Ore-R had red eyes, tan body color, and wing venation with longitudinal veins that went to the end of the wing. During this time the differences between the female and male flies was determined. The male flies have rounder abdomens with the last two dorsal segments darkly pigmented, genital claspers ventrally on the abdomen, and sex combs on the first pair of legs. The female flies have pointier abdomens with dorsal triangular dark pigmentations and lack genital claspers and sex combs. After observing, two new cultures of unknown flies were made with about 15 females and 20 males each. The cultures bottle was placed on its side until the flies wake up so that they do not get caught in the food and die. These new cultures supplied a fresh source of unknown flies for later weeks during the experiment. Shortly after this, the adult flies in the eight unknown bottles and three wild-type bottles were cleared. Eight hours later 80 unknown virgin females and 20 wild type virgin females were collected as described in the lab manual on page 20. Clearing the adults and then waiting no more than eight hours to collect the females ensured that the females were virgins because they were not mature yet for mating with the males.
The next step in the experiment was setting up the set of reciprocal crosses and the marker cross using the collected virgin females. The first cross was Reciprocal Cross A which consisted of 15 unknown U4184 virgin females and 10 wild type Ore-R males. To perform the crosses the same procedure was used as described in the lab manual for the culture, but in this case the flies were from two different strains instead of one strain. The vials for each cross were labeled for identification. The second cross was Reciprocal Cross B which consisted of 15 unknown U4184 males and 9 wild type, Ore-R females. The third cross performed was Marker Cross I, which consisted of 25 males from marker stock I and 10 unknown U4184 females. The marker I male flies had the genotype cv f on their only X chromosome, which produced a phenotype of cross veinless wings (with allele cv) and forked bristles (with allele f). These mutations are sex-linked, recessive. The fourth cross was Marker Cross II which consisted of 30 marker stock II males and 10 unknown U4184 virgin females. The marker stock II males had the phenotype of short thin bristles (with allele Bl), lobed eyes (with allele L), and curly wings (with allele Cy) and a genotype of Bl Bl+ L L+ Cy Cy+. The fifth cross was Marker Cross III and consisted of 30 marker stock III males and 10 unknown 4184 virgin females. The marker stock III males had a phenotype of smooth ("glued") eyes (with allele Gl) and short, blunt bristles (with allele Sb) and a genotype of Gl Gl+ Sb Sb+ LVM LVM+. The LVM gene did not produce an effect on phenotype.
One week later and once the vials all had a sufficient amount of pupae, the parents from all the crosses could be removed, killed with CO2, and disposed in the fly morgue. Because the fly life cycle is about 9 days, this step removes the parental generation and prevents breeding between the parental and F1 generation. During this time, the phenotypes of the marker crosses (which were noted above) were observed under the microscope so that offspring could be scored correctly. Also 40 unknown virgin females were collected according to the same procedure done before.
The next step in the experiment was to score the F1 generation of all five crosses and make a record. The scoring took looked at the phenotypes for sex, body color, eye color, wing venation, and the marker mutations. At least 100 flies from Reciprocal Cross A, at least 100 from B, and at least 50 each from Marker Cross I, II, and III were scored.
After scoring, the second round of crosses was set up. For Reciprocal Cross A 30-40 male and female flies from the F1 generation were taken and put into a new bottle to make a F1 x F1 cross. This was done again to make a duplicate. The same was done for Reciprocal Cross B and Marker Cross I and their duplicates. For Marker Cross II, first a cross was made between 30 bristle, lobed males of its F1 and 10 unknown 4184 virgin females, and then a second cross was made between 30 curly males of the F1 and 10 unknown virgin females. For Marker Cross III, a cross of 30 glued, stubble makes from the F1 and 10 unknown virgin females was made and duplicated. These crosses for Marker Cross II and III are known as male back crosses. And one week later the adults were removed just as in the first round of crosses. Finally, the last step in the experiment was to score the F2 generation and the male back cross progeny according to their eye color, sex, body color, wing venation, and marker mutations.
The results from the Reciprocal Crosses A and B in the F1 and F2 generations shed light o the genes controlling eye color, body color, and wing venation. The reciprocal crosses allowed for the determination of the traits as sex-linked or autosomal, and dominant or recessive. For example, if a trait were autosomal, then the progeny of both Cross A and B should be the same because it would not matter which parent had which genotype since each is passed on equally to both sexes of children. This is not true for a sex-linked trait, in which the son inherits the x chromosome and its genes only from the mother. These crosses F1 offspring also indicated whether the traits were dominant or recessive, because the offspring of these pure-breeding lines would show the dominant allele in their phenotype. The results for the F1 and F2 generations for the Reciprocal Crosses A and B are discussed individually below for body color, wing venation, and eye color.
From Cross A F1, the dark body color gene can be determined as autosomal, recessive. It is recessive because all the F1 is wild-type. This indicates that the wild-type allele masks the mutant dark color allele. The trait is autosomal because both the males and females are wild-type. If the trait were sex-linked then the males would be mutant because they would only have one X chromosome which they inherited from their mutant mother from a pure-breeding line. The F1 from Cross B only indicated that dark body color trait was recessive because the males would be wild-type even if the trait were sex-linked since they would inherit a normal allele on the X chromosome from the mother. The results form the F2 generations of Crosses A and B determined if the dark body trait alleles were segregating properly. If this data were tabulated again to look at the ratios, the following would be observed:
Upon Chi-Square analysis, the chi-square value was 4.38 with a p-value less than 0.05. This indicated to reject the hypothesis of light body color being completely dominant to dark with no more segregation the two alleles for body color. The rejection may have been because the dark body mutation does not have 100 % penetrance, or perhaps during scoring the newly hatched dark bodies were scored as light instead of dark.
Based on the F1 results of cross A, the mutant wing venation can be determined as a recessive, autosomal trait based on the same reasoning used for the body color trait. The F1 results from Cross B only determined the trait as recessive, as it did for the dark body color gene. The F2 results determined if the alleles for the wing venation were segregating properly. Once the F2 results were combined and looked at in ratios, the hypothesis of normal segregation for a complete dominance of wild-type wing venation over the mutant could be tested. The following is the combined ratios:
The chi-squared value for this test was .0176 with a p-value between .5 and .9. Thus, the hypothesis of complete penetrance and dominance of wild type wing venation over the mutant venation with no more segregation between the alleles cannot be rejected.
To test if the alleles for wing venation and body color were independently assorting or if the genes were linked, the two traits have to be looked at together in the F2 results from the Crosses A and B combined. These numbers must then be made into ratios that can be compared to the expected ration for independent assortment. The ratio for independent assortment of the alleles for wing venation and body color must be derived from the obtained ratios, obtained earlier, of the segregation of the individual traits. The observed and expected ratios are tabulated below. Wt indicates wild-type, and m indicates mutant.
The chi-squared value for testing the hypothesis of independent assortment was .628 with a p-value between .5 and .9. Thus, the hypothesis of independent assortment cannot be rejected. The genes for body color and wing venation were not linked.
From the same reasoning used for the determination of body color and wing venation, the results from the F1 of cross A determined eye color as a autosomal, recessive trait, while Cross B F1 determined eye color as just a recessive trait. Looking at the F2 results for the eye color trait, however, made this trait a bite more complicated than the others. When observing the F2 of Crosses A and B, four eye colors were identified. There was red, white, and also brown and orange. Because there were four different eye colors, there could be two genes involved in the expression of eye color. Because there are two genes involved, the F2 results will determine the interaction between these two genes; the F2 results will conclude if the genes were linked or not. A hypothesis for the possible interaction is described by the following diagram:
The presence of the a+ allele with the b+ allele complements to make the wild-type red eye color. However, brown was possible with the genotype of aab+_, orange was possible with the phenotype of a+_bb, and white was aabb. The data eye color from F2 of Crosses A and B is tabulated below.
The marker II and III stocks were balanced marker stocks to keep the heterozygous stocks pure line. The pure-line was established with a balanced lethal system, in which being homozygous for the mutant alleles was lethal. For example, flies in marker stock II had a one chromosome 2 carrying Bl L Cy+ and another chromosome 2 with Bl+ L+ Cy. These alleles are for the dominant, autosomal traits of short thin bristles, lobed eyes, and curly wings, respectively. When a male and female from marker stock II mate, the offspring with both chromosomes carrying Bl L Cy+ are killed and so are those both carrying the Bl+ L+ Cy chromosomes. Recombination among the genes in the chromosomes were controlled by inversion, an introduced cross-over suppressor for females, and males do not undergo recombination. This balanced marker system was also used in marker stock III with their chromosomes: one carrying Gl Sb LVM+ and the other carrying Gl+ Sb+ LVM.
Marker Cross II and III used male backcrosses instead of F1xF1 crosses. In a male back cross, the male progeny from the F1 are selected and mated to certain homozygote genotype female (in this case an unknown virgin.) In Drosophilia, male flies do not under go recombination. By selecting the males form the F1 generation and using the homozygous unknown females, recombination should not occur. These crosses also used male back crosses so that all the offspring would be viable. This was true for the kind of backcrosses employed in this experiment because an unknown female was always used. The unknown female carried the wild-type alleles for the marker mutations and would pass them on to the offspring. So the male back cross offspring would never be homozygous for the lethal marker mutations. The back crosses conducted would indicate which genes were on the marker chromosomes 1, 2, and 3. If a certain mutation does not show up with a marker mutation in the male backcross progeny, then that mutation is linked with the marker mutation, and thus on the same chromosome. This is a result because no recombination should occur in a male backcross in Dosophilia. However, if the mutation does show up with the marker mutation in the back cross progeny then this would be a sign that the two mutations are not on the same chromosome. In this experiment Cross II did not produce any male back cross progeny, but the results for Cross III male progeny are below.
The mutant body color of dark brown and mutant eye color of white showed up with the glued eye and stubble bristle marker mutations, and this means that neither the body color gene or eye color genes were on the third chromosome. However, the mutant wing venation of four short longitudinal veins did not show up with the glued eye and stubble bristle mutations, so the wing venation gene was on the third chromosome.
The results from the F1 of Cross A determined that the eye color genes were autosomal. From Marker crosses II and III, the placement of gene for wing venation was found to be on chromosome 3, while body color was not. Since body color was not sex-linked, the gene for body color must be on chromosome 2 (since chromosome 4 does not carry many genes.) The same was true for the two eye color genes. Another clue that the eye color genes and body color gene are on the same allele was the fact that only 13 white eyed light body and 69 white eye dark body F2 progeny were observed for the crosses A, B, and I combined. This was very different from the expected results associated with independent assortment of about ¾ of the white eye progeny to be light body and only ¼ to be dark bodied. In a tree-point cross between d-a-b (d is allele for dark body, a and b are alleles for eye color), the distance between genes was found by doubling the number of recombinants because the F1 x F1 crosses hides half of the recombinants. The distance between d-b was found to be 10m.u, a-b was 39.5 m.u, and d-a was 49.5 after taking the twice the number of double crossovers into account. Thus, d and a were on the ends and b was in the middle.
With the information from the five performed crosses and the information of the location of one mutation of the chromosomes given by the instructor, the genes for mutations for dark body color, mutant wing venation, and eye color may be mapped on the chromosomes of the Drosopholia .

Organisms are constantly altering their diets in response to food ability, competition, predation, and a multitude of other factors. How do organisms physiologically change in response to variations in diet? Which physiological processes are affected when an Alaskan brown bear switches from a low protein diet of wild berries and roots to a protein rich diet of salmon returning to spawn? Countless other examples exist in nature where organism experience annual, seasonal, and daily differences in food availability. The purpose of this study is to elucidate the effects of dietary protein deficiency on the size, and thus the function, of various organs in mice. The organ chosen for this study were the heart, pancreas, and testes, which play a crucial role in circulation, digestion, and reproduction. Adequate circulation is absolutely necessary for the transfer and removal of nutrients and wastes under any condition. Therefore, the prediction was made that regardless of dietary protein intake the mass (an indirect measure of organ function) of the heart would be maintained. In contrast, maintenance of the pancreas was expected to decrease during protein deficiency, since its major role in digesting dietary protein is non-essential under these conditions. The main focus of interest is the question of whether the mass of the testes of mice will be maintained when faced with protein deficiency. One could argue that reproduction is a non-essential life function and thus would be turned off when resources are in low supply. On the other hand, in terms of evolution the success of an organism is measured by its genetic contribution to the future. If environmental conditions appear unfavorable an organism may sacrifice its own health and biological needs in order to reproduce. A classic example of this occurs every year as salmon migrate upstream to their original riverbeds to spawn and soon after die. It is hypothesized that in mice facing protein deficiency mass of the testes will be maintained, since in nature mice have a rapid turnover rate and the success of the future generations may take priority over individual success. This issue can be more broadly applied to the field of ecology by observing how changes in diet effect organ growth. Prolonged periods of protein deficient diets may be a problem for organisms that inhabit temperate or artic regions where certain nutrient rich foods are not available throughout various time of the year. It is also interesting to infer the impact of diet on reproductive life histories.
The 8 mice used in this experiment were on a 12/12 hour light/dark cycle and kept at room temperature during the duration of the study. 4 days prior to the tissue harvesting, ½ of the mice were randomly selected and were switched from the standard University of Michigan rodent chow (LabDiet; Richmond, VA) to a isocaloric, but protein deficient chow (Dyets; Bethlehem, PA) and were given free access to the chow until the completion of the study. At the completion of the study, the mice were killed using carbon dioxide and the target organs were harvested and frozen in liquid nitrogen.
Measures of the body mass of the mice were taken before the harvesting of tissue and measures of the organ weight were taken prior to tissue homogenization to give the organ wt/ body wt ratios.
Measures for total protein content in the organs was determined by homogenizing 100mg of tissue in 2ml of solution containing 0.1% Triton X-100 and 5mM MgCl2 and immediately sonicated for 15secs to further disrupt the tissue. Following the sonication the total protein content was determined spectrophotometrically using BioRad protein dye. Measures of the total DNA content were measured using a luminescence spectrometer and a DNA quantification kit from Sigma-Aldritch.
Equal amounts of protein were run on a SDS-PAGE (polyacrylamide gel electrophoresis) and afterwards transferred to a nitrocellose membrane. The membrane was blotted with milk for 1 hr and rinsed to remove any non-specific binding. Subsequently, placed overnight into a solution containing antibodies for GAPDH or s6p (a ribosomal protein). The following day the membranes were rinsed and incubated with the secondary antibody for 1 hr, washed, and developed using enhanced chemoluminescence. Electronic images of the membranes were taken and used for comparison of control and protein deficient tissue.
In the pancreas the mean organ wt/ body wt ratio in the control condition was 7.51 ± 0.62 (mg/g), whereas in the protein deficient mice the mean organ wt/ body wt ratio was 5.53 ± 0.77 (mg/g) (Figure 1). Differences between the control and protein deficient mice were statistically significant and had a p-value of 0.007 (Table 1). In the heart the organ wt/ body wt ratios were 5.17 ± 0.24 (mg/g) in the control condition and 5.33 ± 0.31 (mg/g) (Figure 1). The heart wt/ body wt ratios in the control and protein deficient mice are very close and have a statistically insignificant p-value of 0.459 (Table 1). The ratios of organ wt/ body wt in the testes in the control and protein deficient mice were also statistically insignificant having a p-value of 0.572 (Table 1). In the control condition the organ wt/ body wt ratio was 6.57 ± 1.23 (mg/g), whereas the protein deficient mice had a slight larger ratio of 7.13 ± 1.41 (mg/g) (Figure 1).
To gain insight into the types of changes protein deficient organs undergo measurements of the total DNA (a rough estimate of cell numbers) were taken to determine if protein deficient organs contained less total cells and/or equal number, but smaller cells. In the pancreas, the mean values for the control condition was 0.82 ± 0.10 (mg/tissue) and the mean value in the protein deficient condition was .80 ± 0.09 (mg/tissue) (Figure 2). The numbers are not statistically significant and have a p-value of 0.769 (Table 1). In the control condition of the heart a mean value of 0.26 ± 0.08 (mg/tissue) was measured and a mean value of 0.22 ± 0.05 (mg/tissue) was measured in the protein deficient mice (Figure 2). A statistically insignificant p-value of 0.404 was determined for the heart tissue (Table 1). The p-value of 0.637 determined in the testes was also statistically insignificant (Table 1). The measure of the DNA content in the control condition of the testes was found to be 0.51 ± 0.12 (mg/tissue) and a similar average value of 0.55 ± 0.09 (mg/tissue) was found in the protein deficient mice (Figure 2). In all three organs no significant change in DNA content was observed.
Measurements of the total protein in the different organs yielded results similar to those found in the organ wt/ body wt ratios. In the control condition of the pancreas a mean value of 38.43 ± 5.68 (mg/tissue) was observed and a mean value of 18.13 ± 1.96 (mg/tissue) was observed in the protein deficient subjects (Figure 3). A significant p-value of 0.001 was found between the control and protein deficient conditions of the pancreas (Table 1). In the heart a mean value of 76.00 ± 13.4 (mg/tissue) was measured in the control and a mean value of 68.63 ± 4.30 (mg/tissue) was found in the protein deficient condition (Figure 3). The similar mean values in the heart lead to an insignificant p-value of 0.336 (Table 1). In the testes a difference between the control mean value of 65.60 ± 20.94 (mg/tissue) and the protein deficient mean value of 45.85 ± 9.19 (mg/tissue) was noticed, but a p-value of 0.135 failed to reject the null hypothesis (Figure 3, Table 1).
In the immunoblot for the protein gapdh there are no clear differences in protein expression between the protein deficient and control mice (Figure 4). This is likely attributed to the fact that gapdh performs various housekeeping activities, and is thus conserved. In the immunoblot for the protein s6p, a ribosomal protein, it is clear that protein expression in the pancreas is less in protein deficient mice in comparison to the control mice (Figure 5).
Much research has been done looking at the effects of nutritional stress on the body. For example, in a study on California voles it was observed that their choice of diet varied in breeding and non-breeding seasons. After giving the voles feed with differing nutrient content, it was inferred that nutrition plays an important role in reproduction (Batzli, 1986). Researchers hypothesized that various nutrients play a key role in an organism's reproductive success. The voles used in this experiment were fed either a diet consisting of grass seeds or a laboratory chow. The chow and seeds fed to the voles differed in concentrations of calcium and sodium, but not protein (Batzli, 1986). Voles fed the low calcium and sodium grass seeds reproduced less than voles fed a standard laboratory chow (Batzli, 1986). This experiment led to the belief that reproduction may be affected by an organism's diet.
Another study looked at effects of nutritional stress on sperm production in moths. In the study, the sperm count of moths fed a low-protein diet was compared to the sperm count of moths fed a normal protein diet (Gage and Cook, 1994). The results indicated that diet played a significant role in spermatogenesis as lower sperm counts were found in moths fed a low protein diet. However, the size of the individual sperm cells was unaffected (Gage and Cook, 1994). My results that testes size was maintained during protein deficiency do not fit the findings of this study, but the difference in results may be due to the use of different organisms with different reproductive strategies.
In the following study the relationship between survival and reproduction was looked at in zooplankton undergoing starvation. In the experiment it was found that in starvation conditions some species ceased reproduction and had higher survival rates, whereas some species maintained or increased energy allocation towards reproduction and had lower survival rates (Kirk, 1997). Furthermore, the results indicated that allocating energy production decreases resistance to starvation (Kirk, 1997). These findings suggest that if energy is devoted towards reproduction in times of low energy availability, individual fitness is decreased. However, if an organism fails to reproduce, genetically speaking, it makes no contribution to future generations. In species with a short life span like mice, individuals may die before environmental conditions become more favorable. This is why it was predicted that protein expression in testes would be maintained in mice fed a protein deficient diet. This study highlighted the fact that organisms possess different strategies for reproduction and determining whether reproduction will be maintained or decreased is dependent on the organism's life history.
Yet another study examined the effects of dietary protein on rats. In the study, pregnant rats were either fed a standard laboratory chow or an equal calorie low protein chow. The offspring of the rats fed the low protein chow had a lower mean body weight than rats born from mothers who were fed a normal protein chow (Snoeck, 1990). These results led to the assumption that in protein deficient rats, proteins from various parts of the body were being broke down for use and thus, contributed to the lower body weight in the offspring of pregnant rats fed a low protein chow. These results were additionally supported as a lower mean mass of 29.8 (g) was observed in protein deficient mice, in comparison to the mean mass of 29.8 (g) in the control mice.
Moreover, the effects of protein efficiency were observed in the protein synthesis in the livers of rats. The subjects used for research were either fed a standard laboratory chow or a low protein chow and the effects of protein deficiency were quantified by measuring the concentrations of mRNA in the rat livers (Pain, 1978). A significant decrease of mRNA concentrations was observed in the protein deficient mice (Pain, 1978). The fact that liver functioning decreased in this study led to the assumption that other organs would shrink in response to protein deficiency.
In taking all the studies into consideration, protein deficiency causes lower mean body weights as proteins from organs are broken down in order to maintain vital life functioning. The prediction that regardless of dietary condition the functioning of the heart would be maintained was supported. In all the measurements used to infer organ function ability (organ wt/ body wt, total DNA and protein content) no significant differences were observed between the control and protein deficient subjects. This is more than likely attributed to the fact that functioning of an organism's heart is crucial for its survival. If the heart fails to work properly a myriad of problems exist as a result of poor circulation. Metabolic waste and byproducts of respiration must be maintained at low levels in the body, otherwise the ability of an organism to adequately function is severely jeopardized. Furthermore, cells through the body need a stead supply of nutrients (glucose, O2, hormones, etcetera) need to be circulated throughout the body, and thus, due to its importance in circulation it was predicted that heart function would be maintained.
The results also clearly supported the prediction that functioning in the pancreas would decrease in protein deficient mice. A statistical difference was observed between the experimental and control mice in the organ wt/ body wt values and in the total protein content. This is more than likely due to the fact that one of the key roles of the pancreas is the digestion of protein and in the mice fed a no-protein diet digestion of protein was not necessary. If an organism is protein deficient, proteins within the body will be broken down in order to synthesize other proteins critical for survival. Interestingly, in the measures of total DNA content no statistical difference between the control and protein deficient mice was observed. Since DNA is localized in the nucleus of cells, measurements of the total DNA content of an organ gives a rough estimate into the number of cells. The similar means of total DNA in the pancreas and the rather large difference in total protein suggests that while the numbers of cells appear to be the same, the overall size of the cells seems to be decreasing in protein deficient mice.
The hypothesis that reproduction would be maintained during protein deficiency was supported by the results. The argument that when faced with an environmental stress reproduction would be shut down until conditions improved, was not statistically supported. However, although not significant, a decrease in the total number of protein was observed in protein deficient mice. The standard deviation for the testes was extremely large due to the small sample size and having a larger sample size may have yielded significant differences. Despite the problems associated with sample size, the results indicated that reproduction is high on the hierarchy of biological functions.
I would like to thank the Williams lab for graciously allowing me to use their materials, equipment, and laboratory space for my research. I would like to send a special thanks to Dr. Steven Crozier, who helped greatly throughout the entire process of my research.

Parasites in nonhuman animals offer insight in understanding and treating human parasites, and are thus frequent subjects of scientific study. Avian species of malaria were the first models for studying the biology of the human Plasmodium (Slater 2005). In fact, they are so similar that for a long time, there was doubt as to whether the parasites of bird and human malaria were really different (Manwell 1935). Studies by Waters and colleagues (1993) showed that Plasmodium
Previous studies have also attempted to compare the parasite-host relationship of avian and human malarias, plus evaluate the evolutionary dynamics of a system of many hosts and several multihost parasites (Gandon 2004). Co-evolution between the malaria parasite and its bird hosts have also been of interest. For example, studies of the Hawaiian Honeycreeper suggest that avian parasites do not hinder their hosts' reproductive success possibly because of evolution (Kilpatrick 2006). In addition, another Haemosporidian Leucocytozoon in ducks has been studied for its effects on brood size (O'Roke 1934).
However, avian Plasmodium in Northern Michigan has not been studied in much depth. Its prevalence in the birds should be between 10 and 30% (Feldman et al. 1995). The current study attempted to examine the possibilities of a trend in the incidence of various species of Plasmodium in groupings of birds according to species, age, sex, or size. Furthermore, prevalence of blood parasites in passerine birds were compared to those found in Mergus merganser based on previous findings of Haematozoa in Michigan (DeJong et al., 2001).
Data collection was carried out from July 10 to August 3, 2006 in Emmett County, Michigan. Birds were caught using mist nets, which were arranged in a loop formation with ten nets per loop. There were a total of three sites where birds were captured: NOMA (North Maple River), CEMA (Central Maple River), and SOMA (South Maple River). Data of every bird captured were recorded into organized data sheets, including information such as species name, age, sex, and net number. If blood was taken from a bird, the blood smear number was also recorded. A total of 189 blood samples were taken from a variety of birds; 159 of these were obtained through mist-netting, and the rest were obtained with help of Harvey Blankespoor.
Blood samples were taken from birds' feet using sterile lancets, and then slides pre-cleaned with acid ethanol were held against the small droplet of blood. The blood collected on these slides was smeared with the edge of another slide, then fixed in methanol to be stained later in the lab. Each fixed slide was immersed in Coplin jars containing Giemsa stain for 15 seconds, placed in distilled water for 15 seconds, and finally, rinsed quickly in two more jars of fresh distilled water.
Dried slides were viewed using the 100x power oil immersion lens. Observation of malaria within the blood cells was carried out by manually scanning the surface of the slide for five minutes (roughly 1000 red blood cells).
Prevalence of infection was determined by dividing the number of infected passerine birds by the total number of passerine birds examined. Overall prevalence was determined by dividing the number of infected birds by the number of total birds examined. Mann-Whitney and Kruskal-Wallis tests were used to calculate differences of the distribution of avian blood parasites and the intensity of infection between age, gender, and location of the species sampled. These helped to determine any relationship between age or gender and a particular infection and these analyses were done in SPSS 14.0 for Windows.
The total prevalence of blood parasites in the samples was 23.8%. Table 1 summarizes the data of the 14 families of birds that were examined. The family Parulidae was most represented in the sample, but the Anatidae had the highest overall prevalence of infection and prevalence within the family. The family Parulidae also had the highest prevalence of infection within the mist-netted birds (Table 2, Figure 1). At species level, the Baltimore Oriole and Common Grackle had the highest prevalence of parasitic infection, then the Song Sparrow was next highest (Table 3, Figure 2).
Most of the birds caught were in their hatch year age, which also had the highest prevalence excluding the group of five birds which were of unknown or indeterminate ages; no birds of the second year category were found to be infected (Figure 3).
Birds caught at the CEMA banding site had the highest prevalence of malarial infection, and the birds of the NOMA banding site had the lowest (Figure 4). Mann-Whitney Tests resulted in two-tailed p-values of 0.001, 0.762, and 0.003 for comparisons of prevalence between NOMA and CEMA, CEMA and SOMA, and NOMA and SOMA, respectively (Tables 4, 5, and 6).
Kruskal-Wallis Tests regarding bird age, location, and families resulted in p-values of 0.259, 0.004, and 0.009, respectively (Tables 7, 8 and 9). Tests for sex showed no significant differences between male and female.
The data from this study show that prevalence of avian malaria in Northern Michigan was statistically significant. The prevalence of infection in all of the samples collected was 23.8% and the prevalence in only passerine birds was 10.7%, which supports previous research that blood smears demonstrate infection rates between 10 and 30% (Feldman et al. 1995).
Prevalence among species also varied dramatically. One factor that may explain the variance is habitat preference of infected bird species. Mosquitoes are found much more often near standing water, which suggests that birds nesting and breeding near standing water should be more susceptible to avian malaria. The Ovenbirds of the family Parulidae are common in all woods with dry floor; the Northern Waterthrushes are mainly in conifer bogs, wooden swamps, and along streams in heavily shaded ravines; the Black-and-White Warblers are common in conifer bogs; the Nashville Warblers are common in moist conifer woods and bogs or open, dry woods; the American Redstarts are common in deciduous woods. The Hermit Thrush of the family Turdidae are common in semi-open woods, and the Veeries are common in densely shaded, often moist woods. The White-throated Sparrows of the family Emberizidae are common in the shrubby edges of conifer bogs and heavily shaded coniferous-deciduous woods, and the Song Sparrows are common in brushy areas. Black-Capped Chickadees of the Paridae family are commonly found in wooded areas. The Baltimore Oriole of the family Icteridae is common at the edges of open deciduous woods, and the Common Grackle is common near lakes and streams, particularly where there are pines and other conifers. (Pettingill 1974). Overall, some of the infected bird species prefer habitats that are potentially mosquito-dense while others do not.
There was a propensity for prevalence of infection to decrease with bird age except for birds in their second year (Figure 3). These data were inconsistent with previous findings which showed juveniles to be more susceptible to infection than adult birds (Atkinson and Van Riper 1991). Birds should be highly vulnerable to insect bites during the two weeks until they leave the nest because of their bare, feather-less skin, allowing an opportunity for them to be infected by Plasmodium. The small sample size of this study most likely limited the accuracy of the data; with a larger, more thorough sample, one would be able to better interpret the relationship of host age and haematozoan infection. Prevalence of infection was also high in the group of birds with unknown age, since only two out of only five birds were infected.
The prevalence of infection at the three banding locations differed from one another, and was statistically significant. This could be a result of birds having varying degrees of exposure to the mosquito vector. Birds caught in mist nets near water should show a higher prevalence of infection than those caught elsewhere. Possibly, the CEMA and SOMA locations had more mosquito-rich areas in it, and as a result reflected a higher prevalence of bird malaria in the data.
Tests for different prevalence in sex showed no significance, which is a result of inadequate data. Many of the birds caught in the mist nets could not be characterized for sex, because they were too young and juveniles are difficult to assess gender even with the expertise of collaborators of this project.
Prevalence of Haemosporidian infection throughout the birds of Northern Michigan varied significantly by host family, species, age, and location. There are many possible factors that could contribute to this: bird behavior, habitat, concentration of mosquito vectors, climate, or co-evolution (Atkinson and Van Riper 1991). Further studies that look into this would be useful in comparison to distribution patterns of Plasmodium species that infect humans. No conclusions could be made about the effect of Haemosporidians on the bird populations of Northern Michigan, but continued pursuit of this question could supplement data and make more accurate analyses possible.
Keely Dinse deserves the most credit in this project for allowing the authors to join her in collecting birds and for giving them all of the needed bird identification data. I would like to acknowledge Harvey Blankespoor for giving his assistance in identifying the parasites, Mike Akresh for helping data collection and data analysis, Dave Gonthier for data collection and assistance in parasite identification, Paula Furey for her assistance in using the camera microscope. Lastly, the University of Michigan Biological Station deserves credit for making this project possible.

Acer saccharum, the sugar maple, is a shade-tolerant, gap-phase species (Walters 1993). The growth of A. saccharum seedlings is typically restrained entirely by the limitation of light penetrating the forest floor by the canopies of overstory adult trees (Ricard et al., 2004). The seedlings persist on the forest floor as small juveniles for many years and grow quickly to fill gaps in the canopy created by disturbance (Marks and Gardescu 1998). The species has evolved phototropic responses to canopy gaps that allow it to avoid shade and quickly fill new gaps before its competitors (Brisson 2001). While in temperate North American forests A. saccharum is often found in conjunction with Fagus grandifolia (which is an even more shade-tolerant species), A. saccharum tends to out-compete F. grandifolia in the presence of very small canopy gaps by producing many short lateral branches that more effectively exploit these openings (Canham 1989).
The relationships of several phenotypic responses to light gaps have been examined, including stem orientation (King 2001), leaf structure and biomass allocation (Osada et al., 2004), root architecture (Cheng et al., 2005), and lateral branching (Canham 1988). Though a recent study showed that trees, particularly shade-tolerant species, displace their canopies away from their neighbors to reduce competition and maximize resource exploitation (Muth and Bazzaz 2003), the tendency of sugar maples to branch early and form two trunks to fill the forest canopy more effectively has not been examined specifically. This experiment sought to demonstrate the effects of canopy density and light competition on the tendency of sugar maples to exhibit forked trunks. We hypothesized that trunk forking would allow sugar maples to fill canopy gaps more quickly and efficiently than a single trunk by creating a wider canopy area to absorb the light energy in these gaps and shade out nearby competitors. We expected that a forked trunk would be more effective than a single trunk at filling larger canopy gaps, while a single unforked trunk would be sufficient to fill smaller gaps.
We tested our hypothesis by comparing local basal area per sample area around forked and unforked sugar maples from three sites that differed in species dominance and overall basal area. We predicted that each site would have significantly different proportions of forked and unforked trees and that mean local basal area per m2 would be significantly lower for forked trees than unforked trees. In addition we predicted that each site would have a significantly different mean local density for forked trees.
We sampled three sites: Grapevine Point, Sedge Point and Colonial Point. We selected these sites because they had significant populations of sugar maples but varied in species composition and overall basal area. In each site we established a 50m by 50m test area. We sampled each test area with five parallel 2m by 50m transects. We established these transects within the test area at a distance of 10m from one another measured from their centers. We used the following parameters in the sampling:
Within each transect we recorded the dbh and species of each tree. Every sugar maple encountered was recorded as being forked or not-forked, and we established a point-quarter system around the trees with one axis parallel to and the other perpendicular to the transect. Within each quarter we located the nearest neighbor and recorded its species, dbh and distance to the sugar maple.
We repeated these methods identically for each of the three sites. We calculated the relative dominance and basal area of each site from the transect data. We calculated the mean local density and basal area around forked and unforked sugar maples from the point-quarter measurements separately for each site and for all the sugar maples sampled in the study.
We used an ANOVA to analyze whether the mean local density for sugar maples differed between the three sites. We used a chi-square test to establish whether the proportion of forked and unforked sugar maples in the three sites was identically distributed to test the hypothesis that the sites would have significantly different proportions of forked and unforked trees. We used a t-test to examine whether the mean local density differed significantly for forked and unforked sugar maples for all the sites considered as a population. We examined this difference for the sites considered individually with additional t-tests.
Colonial Point was dominated by relatively large trees and the understory was open with no light gaps and few recently fallen trees. The trees at Sedge Point tended to be much smaller and the understory was crowded with saplings of Acer and Fagus. The trees at Grapevine Point were intermediate in size and there were a few saplings rising to fill recently created light gaps. Because there was more distance between trees at Colonial Point and there were fewer trees overall, its basal area was almost equal to that of Grapevine Point. Though the trees were smallest on average at Sedge Point, this site had a substantially higher basal area than the other two because there were many more trees present (Table 1).
Colonial Point was dominated by very large old beeches, and several smaller red maples and sugar maples were growing between them (Fig 1). Colonial Point had the lowest dominance of sugar maples of all the sites. Sedge Point had an almost equal balance of dominance by red maples, sugar maples and beeches (Fig 2). The beeches at Sedge Point tended to be smaller than the maples but there were more beeches overall. Grapevine Point was different from the other two sites in that over 50% of its basal area was accounted for by sugar maples. Unlike the other two sites, there were no adult beeches or red maples, and several large ashes and basswoods were present (Fig 3.)
The difference in basal area per m2 around the sugar maples between the three sites was statistically significant with a p-value of 0.013, however only the difference between Sedge Point and Colonial Point was significant (Table 2). All of the sugar maples at Colonial Point were unforked, while 47% of the sugar maples at Sedge Point and 24% of the sugar maples at Grapevine Point were forked (Table 3). The difference between these proportions was statistically significant with a p-value of 0.03 (Table 4).
Unforked sugar maples had an average local basal area per sample area of almost double that of the forked sugar maples (Table 5). This difference was statistically significant for the sugar maples of all three sites considered together, with a p-value of 0.007 (Table 6). This difference was statistically insignificant for Sedge Point (Table 7) and Grapevine Point (Table 8) considered individually, with p-values of 0.172 and 0.084, respectively.
The results of the ANOVA (Table 2), which showed that the mean local basal area per m2 of all sugar maples was only significantly different between Sedge Point and Colonial Point, are in reality not very meaningful because all the sugar maples at Colonial Point were unforked. It would be more meaningful to compare the local basal area per m2 for all forked sugar maples or for all unforked sugar maples between the sites, however this would almost definitely be statistically insignificant because of the small sample sizes. The importance of this comparison is questionable because we would expect there to be variation in these numbers between sites based on differences in overall basal area in each site. In addition it would seem logical to compare the local basal areas per m2 of sugar maples with the overall basal areas per m2 at the sites, as lower values for the basal area around sugar maples would illustrate their preference for canopy gaps. However it would be unsound to make this comparison because we calculated basal area per m2 in the transects and around the sugar maples using different sampling methods. The use of different sampling methods can lead to drastically different estimated densities, particularly when distribution is non-uniform (Engeman et al. 1994).
Because the canopy at Colonial Point was dominated by very tall, old trees that cast a thick uniform shade over the forest floor, it is not surprising that there were no forked sugar maples there. While it would seem unusual that the highest incidence of trunk forking occurred at Sedge Point (Table 3) where overall basal area was highest (Table 1), this site would also tend to have the most gaps because the trees in this site had lower basal areas and smaller canopies. This is supported by the fact that the difference between local basal area per m2 for forked and unforked sugar maples was higher for Sedge Point than Grapevine Point. Furthermore average local basal area per m2 for the unforked sugar maples at Colonial Point was close to the values for unforked trees at the other two sites (Table 5).
If the study were repeated, photometer readings would be taken randomly at several points in each site. Though more light would not reach the forest floor on average, a higher value for standard deviation in these readings at Sedge Point would show that this site has more canopy gaps than the other two sites. Likewise we would expect that though the similar values for basal area at Grapevine Point and Colonial Point should lead to similar mean light intensities at ground level, a lower standard deviation at Colonial Point would reflect a uniformly shaded environment for maple seedlings.
The fact that the difference between mean local basal areas per m2 was statistically significant for all sites considered together (Table 6) but insignificant for the sites considered individually (Tables 7 and 8), despite the fact that the means were almost identical for each site (Table 5), shows that the sample sizes were too small in this experiment. If the study were continued, doubling the sampling regime at each site would probably lead to statistically significant results. This would further establish the role of trunk forking as an evolutionary adaptation of Acer saccharumto quickly fill light gaps.
Our study showed that trunk forking allowed sugar maples to fill large canopy gaps more completely. This was an original finding, however it was consistent with previous research. Shade-tolerant trees produce more lateral branches in canopy gaps (Canham 1989), and orient these branches towards areas with higher light (King 2001). Therefore in large canopy gaps trees tend to expand their canopies by producing several large radiating branches (Canham 1988). If two or more of the branches of a developing tree were highly successful in capturing light, it would be logical for them to continue to develop upwards towards different areas of the light gap and eventually form distinct trunks with individual canopies. The leaves of these canopies would detect light competition by means of the red to far-red irradiance ratio (Aphalo et. al 1999) and the branches of the tree would behave autonomously and avoid one another (Brisson 2001). At this point the canopy of one trunk would respond to shading by the canopy of its other trunks exactly as it would to shading from neighboring trees (Muth and Bazzaz 2003), and orient its trunks away from each other and neighboring trees to minimize the interaction between canopies. As long as the individual canopies were successful in capturing light, all the trunks would be maintained with their canopies oriented away from each other to minimize competition and exploit all available light.
Further support for our results could be provided by long-term studies in which the development of sugar maples is observed over time. Sugar maples would be grown under experimentally created canopy gaps with known values of canopy closure, and the incidence of forking could be observed to determine a threshold gap size under which forking is induced. Based on our observations we would expect that the threshold local basal area per sample area needed to induce forking would be somewhere between 0.00222 and 0.00408, however the experiment outlined above would give a threshold value for canopy closure which would be more relevant because this value more accurately represents the light environment in the microhabitat of a seedling (Jennings et al. 1999).
It is interesting to note that we observed several sugar maples which had trunks that forked, but one or more of the trunks was dead and rotting. The dead trunks always pointed into the canopy of a tall nearby tree. This suggested that the dead trunks had been aborted because their leaves were not receiving enough sunlight because they were being shaded out by a taller neighbor. A long-term study, perhaps performed as a follow-up to the one described above, could be performed to observe this process and determine that in fact the trunks die in response to their gaps being filled by surrounding taller trees, and not the other way around. This would be done by blocking sunlight to one of the trunks of several forked sugar maples and seeing whether the trees abort entire trunks in the same way that they abort unproductive smaller branches.
Trunk forking is an adaptation that has evolved in certain canopy gap-dependent species that improves their competitiveness and overall evolutionary fitness. Tree species with the ability to alter their form in response to their light conditions have an advantage and will tend to outcompete those that are morphologically rigid in settings where environmental heterogeneity is high. An improved insight into trunk forking as a response to light gaps will increase understanding of forest succession and tree species' adaptations to varying conditions.

There are many reasons by which Drosophila Melanogaster is a model experimental model for genetic study. The first of which deals directly with the life cycle of this small insect. Only lasting 9 days, new generations are able to be generated quickly and coincide well with the timing of MCDB 306 class. Furthermore, as female flies can lay as many as 500 eggs in 10 days, the necessary population size for genetic analysis can be created and maintained. Another very important facet of Drosophila Melanogaster is the fact that upon emergence from pupa the female does not reach sexual maturity until approximately eight hours. This allows for the collection of virgin females for further crosses. There are also significant genetic advantages in using D. Melanogaster as well. Foremost is the fact that male flies do not undergo recombination and thus drastically reduce the complexity of scoring future generations. Additionally, the fact that this type of fly only has four chromosomes is also helpful in decreasing the number of chromosomal marker crosses.
The intent of this experiment was to identify the correct chromosome, locus, and mode of inheritance for three unique mutations through the use of designed crosses and statistical analysis. Given the dramatic increase in genetic study, experience in locating unknown mutations within a genome can be very helpful in real world laboratories. This understanding was obtained via careful adherence to established laboratory protocol. The isolation of virgin females and the ability to produce viable crosses with the selected flies was a vital piece in the overall goal of establishing a chromosomal map of unknown mutations. This could be determined only after performing a series of specific genetic crosses between precise populations of D. Melanogaster. Cross A and B were designed to determine if the mutations were dominant or recessive, while cross A showed if the mutation was autosomal or sex linked. For the crosses designated as marker, after executing the initial parental and F1 crosses, F1 backcrosses were performed with flies originating from specific marker populations exhibiting mutations with known loci. As each marker cross utilized populations of flies with different known mutations as well as cross over suppressing inversions, using the principles of independent assortment, though the observation of the progeny from these crosses, allows for the determination of the correct chromosome. A mutation's location on a chromosome can be obtained by statistical analysis of three point crossovers of three mutant traits.
With exception to a few altered details the overlaying procedure for this lab is described in depth in the MCDB 306 Lab Manual. The major difference is that due poor viability, the marker 2 cross was not performed. As a result only four crosses were performed. Virgin females were used when necessary in order to have a controlled cross as the female cannot have already been fertilized by a male prior to the initiation of a new cross. The crosses performed are outlined in the chart below. Additionally in preparation of the marker one and three backcross an initial cross of unknown virgin females and the respective maker males was performed.
From the F1 results noted above several conclusions can be drawn from the analysis of the progeny from both cross A and B. As in both crosses no mutant phenotypes were observed, one could determine that the mutation for dark body is in fact recessive. Furthermore, from cross A one is able to determine that this same mutation is autosomal. This is due to the fact that, were the mutation for body color located on the X chromosome, all male flies would receive only the mutant allele from their mothers, as the female in the cross is homozygous for the mutation. This cannot be determined from the B cross as the females in this cross will always provide an allele with a wild type gamete.
From the F2 results noted above, it was determined that the mutation for body color undergoes normal Mendelian segregation and consists of only one gene. This was determined by the analyzation of the ratios of wild type to mutant flies. The observed ratio as noted in the skeletal appendix was 3.35:1. With this ratio being close to the typical Mendelian ratio of 3:1 a chi squared test was performed in order to make sure the hypothesis of normal segregation is not to be rejected. Obtaining a X2 value of 1.18 with a degree of freedom of one and a probability greater than 0.05, it was clear that one would fail to reject the hypothesis of normal segregation.
From the F1 data noted above it is clear that like body color, wing venation is also a recessive mutation found on an autosome. This can be supported by the fact that both the F1 progeny of cross A and B resulted in exclusively wild type flies being justified in a fashion similar to body color. Furthermore, similar to body color, one can conclude that the mutation is autosomal as all male progeny of Cross A were wild type for wing venation.
From the F2 results noted above, similarly to body color, it was hypothesized that this mutation undergoes normal segregation in a Mendelian fashion. Despite this, the observed ratio of wild type to mutant flies however was four to one compared to the expected ratio of three to one for normal single gene inheritance. Upon obtaining a X2 value of 7.67 (degree of freedom 1) and a probability less than 0.05 the hypothesis cannot be accepted. Despite this, it is possible that due to the variance in expressivity of the wing venation mutation a counting error may have occurred, thus accounting for the rejection of the hypothesis.
In consideration of the relative location both wing venation and body color, as well as whether or not these mutations assort independently or are linked, it is necessary to analyze the phenotypic ratios. As shown above, the observed phenotypic ratio was 9.7 : 2.6 : 2.1 : 1, while the expected ratio of 13.43 : 4 : 3.35 : 1 was derived from the individual observed phenotypic ratio of each separate mutation. Having an initial hypothesis of independent assortment for each gene, a X2 test was performed. The obtained value was 6.13 and with a corresponding probability of greater than 0.05 (three degrees of freedom) one would fail to reject the hypothesis. Therefore, one can assume the two individual mutations are not linked and undergo independent assortment in a Mendelian fashion.
From the F1 data obtained from the A and B crosses it was once again possible to establish that eye color was recessive and autosomal. This is due to the fact that in both cross A and B there were no mutant flies observed, illustrating once again the same pattern seen in the previous mutations of being recessive. Furthermore, from cross A it is possible to again determine that as there are no males exhibiting the mutant eye color the mutant phenotype is not on the X chromosome for the previously stated reasons.
From the F2 results it becomes clear that due to the presence of more than two phenotypes, there must be multi gene control over eye color. In the F2 generation eye color was not limited to red and white, but rather orange and brown became evident among progeny. With a hypothesized genotypic and phenotypic relationship seen below.
From analyzation of the obtained data a hypothesis of two linked genes was proposed. One can justify the linkage, as were the two alleles for eye color assorting independently a ratio of 9:3:3:1 would be observed in actuality however, a ratio of 5.98: 1: 1.2: 1 for red: white: brown: orange. Were independent assortment occurring the ratio of orange and brown would be expected to be significantly larger than that of white. Given that they are linked the map distance between the two alleles was determined to be 48.1 mu.
The male backcross allows for the determination of whether or not a mutation is located on a certain chromosome. One is first required to perform a cross between a virgin mutant female and a male from an individual marker stock. The male progeny from this cross are then mated with unknown virgin females. Upon examination of the progeny from this cross, one can conclude that if the mutation in question can be observed with the marker mutations, then due to the principles of independent assortment the unknown mutation cannot be on the same chromosome as the marker mutations. Consequently if the unknown mutation is not observed with the marker mutations then one can assume that the mutation is on the same chromosome. Due to the low viability of marker stock II it was required that data for this cross be extrapolated from results throughout the lab.
From the given data, as the mutation for body color is not observed with the glued stubble marker for chromosome three, it can be concluded that this gene is on the third chromosome. Analyzation of the extrapolated data of marker cross two shows that both wing mutation and eye color are not observed with the maker mutations of Bristle Lobed or Curly. This can be determined since previous data concluded that these mutations are not sex-linked. Also from the data obtained from marker cross III it is clear that these mutations are not on chromosome three as they are observed with the marker mutations. Finally, chromosome four is eliminated as a possibility as it was stated that no unknown mutations were located on chromosome four.
Through the use of the previous marker crosses, it is clear that the two genes along with wing venation were on chromosome two. Due to the linked nature of the eye color genes and wing venation, one can use the frequency of single and double crossovers to determine the order and map distance of the genes in question. As seen calculated in the skeletal report appendix, the map distance between wing venation and mutant gene a was 89.9 mu. The distance between wing venation and b was 41.9 mu and the distance from gene a to gene b was 48 mu. Therefore the final gene order on chromosome two would be wing venation at 3.8 mu gene b at 45.7 mu and gene a at 93.7 mu.
The final assignments of each unknown mutation is as follows
From cross A it was determined that each mutation was in fact autosomal. This was due to the fact that as the female in the cross was homozygous for each displayed mutant phenotype, were any of the mutant genes on the X chromosome, all male progeny would receive only the mutant gene and consequently express the mutant phenotype. As this was not the case, it is clear that each mutation was autosomal. From both cross A and B it was clear that as all F1 progeny were wild type, each mutation was recessive. From the F2 progeny of crosses A and B it was also made clear that body color was controlled by one gene and sorted independently. Wing venation, although controlled by one gene assorting independently from body color received a chi squared value that corresponded to a probability of less than 0.05 consequently rejecting the hypothesis of normal segregation. This error is most likely attributed to a counting error resulting from the varying expression of mutant wing venation. Marker Cross I, as it was already determined that no mutations were sex linked, instead was used to determine, along with cross A and B the mode of inheritance for eye color as well as the linkage between the two genes controlling eye color. From this information it was determined that eye color was controlled by two genes were only when both were homozygous were white eyes observed, when either gene a or b was homozygous the eyes were brown or orange respectively. Marker backcross III provides evidence regarding the chromosomal location of the dark body mutation, as it was not observed with the marker mutations in any progeny. Through extrapolation it was determined that both wing mutation and the two genes for eye color were located on chromosome two.

Michigan forests have experienced fire as a result of natural causes, Native American agriculture and intense logging in the 1800s (Burt Barnes pers. comm.). After 80 years of fire exclusion, land managers have reintroduced fire in the form of prescribed burning in an effort to maintain fire-adapted ecosystems and combat invasive species. It is important to understand the impact of fire on small mammal populations and communities because these animals (Muridae, Soricidae) play a variety of important roles in forest ecosystems, such as feeding on invertebrates, seeds, fruit, and lichen, disseminating seeds and mycorrhizal fungi, and serving as prey for a number of avian and mammalian predators (Carey and Johnson 1995).
Clearcutting and burning can drastically alter ecosystems, with the most severe fires burning much of the vegetative cover and soil organic matter. Because recently burned ecosystems lack dense cover, they tend to experience higher maximum and lower minimum temperatures than older stands, which can make the habitat unattractive to sensitive mammals (Kozlowski and Ahlgren 1974). The presence of downed woody material, which provides cover, moist microenvironments, runways, burrow sites and a substrate for food, leads to higher recolonization rates for rodents and insectivores during the first 10 years after burning (Fisher and Wilkinson 2005). Additionally, the redevelopment of vegetation structure is associated with the reappearance of small mammal species (Whelan 1995). Some mammals may utilize the edge of plots because they prefer to exploit multiple habitats (Kozlowski and Ahlgren 1974).
Post-fire forest succession influences small mammal population and community composition. Species richness of small mammals increases over time, but levels-off after the first 30-40 years (Whelan 1995). In early successional habitats, the lack of complex/layered vegetation may result in interspecific competition that was not present before the fire, leading to reduced numbers of all species and a change in dominance (Kozlowski and Ahlgren 1974). A source-sink scenario has been observed in some low-quality habitats created by fire, with more recently burned areas being dominated by non-reproducing individuals (Ecke et al. 2002; Buech et al. 1977 in Fisher and Wilkinson 2005).
We selected a chronosequence of naturally and experimentally burned plots at the University of Michigan Biological Station (UMBS) to investigate the relationships between post-fire succession and small mammal communities. Through live-trapping, we looked for changes in the abundance, diversity and community composition of small mammal species across the burn sequence. Additionally, we measured the age structure and reproductive status of individuals within each plot to look for the existence of a source-sink scenario. We also surveyed the vegetative cover and coarse woody debris (CWD) in each plot, which can be more important than the age since burn (Simon et al. 1998), so as to provide a mechanistic explanation for any observed differences in small mammal composition.
Our study was conducted on a chronosequence of experimental and natural burn plots (see Fig. 1) located on UMBS property (Section 32, T37N, R3W; Cheboygan Co., northern lower Michigan). We chose five plots about 1 hectare in size-one from 1917 that burned naturally and four experimental burns (1936, 1954, 1980, 1998). All experimental burn plots were clear-cut, with the poor-quality timber and slash left on-site and burned. The 1998 burn also includes a deer exclosure and has undergone yearly aspen ramet clipping in some portions. The entire burn sequence is located on a high-elevation outwash plain characterized by excessively-drained, acidic, sandy soil. Pre-settlement, the area was an eastern white pine-red pine-eastern hemlock-northern red oak forest, but extensive logging and 40 years of frequent fires resulted in the dominance of Bigtooth aspen (Populus grandidentata) in early successional forests, and it maintains a noticeable presence in older forests as well (White 2000).
Figure 1. The 1998 Burn included a 50x80 deer exclosure. The 1917 plot was located to the south of the 1948 plot and to the west of the 1954 plot
We used a point-centered quarter sampling method (Anderson 2006) to survey canopy and subcanopy trees in each plot. Four transects 50m long and at least 10m apart, with 10m between point trees, were laid on each plot. From each point tree, we recorded the diameter, distance from point tree and species of one canopy and one subcanopy tree encountered in each NW, NE, SW and SE direction.
Coarse woody debris (CWD) was surveyed along 4 transects in each burn plot. Transects were 50m long and at least 10m apart. We counted any piece of downed woody material greater than 3cm in diameter that crossed a transect and noted its size category: 3-10cm and >10cm.
Trapping grids were set-up on each plot. For plots 1917-1980, trap lines were laid every 10m until reaching the edge of the plot, and folding Sherman live-traps (22.9 × 7.6 × 8.9 cm; H. B. Sherman Trap Co., Tallahassee, Florida, USA) were placed every 10m along the lines until reaching the plot border. We used the same procedure in the 1998 plot except no traps were laid in a 50x80 restricted area in the middle of the plot and some trap lines had portions located within the deer exclosure fence. The 1917 plot was 100x100m with 121 traps, the 1936 and 1954 plots were 70x80m with 72 traps, 1980 plot was 60x100m with 77traps, and the 1998 plot, excluding the restricted portion, was 5000 m2 with 69 traps.
Trapping was conducted for three consecutive nights starting 24 July 2006. Traps were laid out after 7pm the first night and loaded with oats. Each morning after 7:30am, traps with animals in them were removed during the day for processing, and empty traps were closed. We also noted the number of closed but empty, disturbed or missing traps in order to monitor the level of predator interference.
For each captured animal, we recorded the species, gender, reproductive status and weight. Additionally, a small portion of fur was clipped from a position on the back corresponding to one of the 5 plots, allowing us to determine if the animal was a recapture in the following days. Peromyscus and Tamias species were given oats and apples during the day, and Blarina were given worms. After 7:30pm, processed animals were returned to their trapping station, and all traps were re-set and loaded with oats.
All statistical tests were performed using SPSS 14.0 (SPSS, Inc., Chicago, Illinois, USA). Statistical comparisons were either a Chi-square test or a Spearmann Rank Correlation with a significance level of P ≤ 0.05. The data collected on recaptured animals was not included in the analysis.
The following species were encountered during our point-centered quarter sampling of the plots: Bigtooth aspen (Populus grandidentata), Northern red oak (Quercus rubra), Red maple (Acer rubrum
The percent coverage by the most abundant tree species correlated well with the year of the burning in the plots. The percentage of P. grandidentata in a plot was significantly positively correlated with the year of the plot (Rs=0.9, P=0.037). Both the percent composition of Pinus spp. and Q. rubra were negatively correlated with the year of the plot (Rs=-1, P=0.000; Rs=-0.8, P=0.104). Acer rubrum also generally increased (Rs=0.7, P=0.188) with the year of the plot.
We captured a total of 136 individuals (not including recaptures) and 3 different species. Peromyscus leucopus (n=118) was the most abundant. We also captured a few Tamias striatus (n=15) and Blarina brevicauda (n=5; may include recapture).
On average, 49% of all captures on the 2nd and 3rd days were recaptures. All P. leucopus recaptured had been captured earlier in the same plot, except in the 1917 plot, where P. leucopus males were recaptured from the 1980 and 1954 plots as well. One Tamias captured in the 1980 plot was recaptured from the 1998 plot, but all other Tamias were recaptured in the same plot as their first capture. The majority of all recaptured individuals (77%) were non-reproducing (abdominal males or nipples tiny females), and this majority held for each plot.
In total, 154 traps were disturbed (see Table 1 for percent disturbed by plot), with the most likely predators being Procyon lotor, Canis latrans and Martes americana. One M. Americana may have been sighted in the 1917 plot on the 2nd day of trapping. Plots 1936 and 1954 were severely disturbed on the 3rd day of trapping, with 31% and 42% of traps disturbed, respectively. Canis lantrans may have been responsible for disturbing the 1954 trap lines on the 3rd day.
In order to make comparisons across plots, we adjusted the number of individuals captured according to the following formula, which puts all of the numbers in terms of 121 traps and accounts for the effect of predator disturbance:
Spearmann-Rank Correlations indicated a relationship between CWD and reproductive status of P. leucopus. The amount of CWD >3cm in each plot was significantly negatively correlated with the percent of abdominal males in a plot (Rs=-0.9, P=0.037) and was also somewhat positively correlated with the percent of scrotal males (Rs=0.7, P=0.188). There may also be a slight correlation between CWD and the reproductive status of female mice, with non-reproductive individuals in the plots with less CWD (percent nipples tiny: Rs=-0.6, P=0.285; percent nipples enlarged: Rs=0.6, P=0.285).
The burn chronosequence exhibited a progressive change in habitat, with the number of years since burning being highly correlated with the canopy and subcanopy composition of the forest community. The percent of P. grandidentata in a plot decreased with age while Pinus spp. and Q. rubra increased in abundance with time since burning. Because we only captured 3 different species (P. leucopus, T. striatus, B. brevicauda), we were not able to conduct a rigorous investigation of changes in species diversity. However, we did observe significant differences between plots in the abundance, age structure and reproductive condition of P. leucopus. In general, there were more subadults in the intermediate-age plots, more adults in the oldest plot and more juveniles in the most recently burned plot than expected. Overall, there was a higher percentage of reproductive females in the 1917 plot, a higher percentage of non-reproducing P. leucopus in the intermediate plots and no deviation from the expected values for the youngest plot.
The most coarse woody debris and the highest number of reproductive individuals were found in the 1917 and 1998 plots. This pattern is supported by the significant correlation of high levels of CWD with the presence of scrotal males. Further research should investigate the extent to which P. leucopus utilize CWD for nest-sites, runways, foraging, etc. in the burn plots. Casual observation indicated that several burrows were located near CWD and that large numbers of insects, a preferred food for P. leucopus (Wolff et al. 1985), were present beneath the debris in the 1998 plot .
We were not able to confirm the existence of a source-sink scenario occurring in the intermediately aged plots. However, the high percentage of subadults (the age for emigration) paired with the low overall P. leucopus abundance and dearth of high quality food sources (few oaks, pines and CWD), makes this a plausible explanation for some of the variation between plots. However, caution must be taken when determining what sort of habitat is acceptable for P. leucopus since they are capable of living at high densities across a wide-variety of habitats (Lackey 1978), and they are flexible in their food preferences (Drickamer 1976). Additional research should incorporate a ground cover survey in order to account for the presence and quality of fruiting shrubs, since these could comprise a major part of the diet of P. leucopus during certain parts of the year.
The 1998 plot may have been a more suitable habitat than originally expected (supporting high numbers of adults) due to the large amount of coarse woody debris, which has the potential to provide food/foraging habitat, thermal regulation and shelter. Additionally, the presence of P. grandidentata may have accelerated the recovery of the 1998 plot because aspen can occupy a site quickly after fire (through root-suckering) and aspen groves create more moderate temperature and moisture conditions (Burt Barnes, personal communication).
We did not observe much movement between plots. When we did capture individuals, they were males that had traveled from the intermediate plots to the 1917 plot, matching the literature findings that males often travel longer distances (Hirth 1959).
One important area requiring further investigation is the influence of post-fire succession on the suitability of habitat for the predators of small mammals. For example, martens are often found in the old growth stage (76 years+), but rarely in other habitats (Fisher and Wilkinson 2005). The 1998 plot could have supported higher numbers of reproducing individuals and juveniles due to reduced levels of predation as compared to the 1917 plot. Overall, very little is known about the effects of fire on carnivores (Fisher and Wilkinson 2005), but the prevalence of disturbance in our trap lines indicates that predators are an active component of the burn plots. Different rates of predation could have an important impact on P. leucopus demography across plots.
We thank Burt Barnes for allowing access to the 1998 plot and for providing Master's theses and other information regarding the history of the burn plots. We are also grateful to Phil Myers and Stephanie Seto for their guidance during all aspects of this project.

Myotis lucifugus and Myotis septentrionalis are two Vespertilionid bats that are sympatric over much of their ranges. Both occur throughout Michigan, and both are insectivorous. Morphologically these two species are very similar. However, M. septentrionalis tends to have longer ears and tragi. The tragus is more than half the length of the ear and comes to a narrow point (Kurta 1995). In M. lucifugus, the tragus is less than half the length of the ear and is rounded (Barclay & Fenton 1980). These species also differ in the frequency of their echolocation calls. The calls of M. septentrionalis are of shorter duration, higher average frequency, and cover a broader range of frequencies than those of M. lucifugus (Faure et al. 1993).
This difference in vocalization suggests that these two species have different foraging strategies. Myotis lucifugus is traditionally labeled as an aerial hawker that feeds primarily on aerial insects (Ratcliffe & Dawson 2003). The short, high frequency, broadband calls of M. septentrionalis are characteristic of gleaners (Schnitzler & Kalko 2001). A shorter call prevents overlap between the call and the echo at short distances, while higher average frequency and broader frequency range provide higher resolution of small objects.
Data has been presented demonstrating that both species can both glean and hawk prey, although M. septentrionalis tends to be more specialized for gleaning (Ratcliffe & Dawson 2003). Given that both M. lucifugus and M. septentrionalis exhibit behavioral flexibility in foraging technique, and considering that they are sympatric throughout much of their ranges, it is likely that these two species segregate spatially or temporally in a way that reduces interspecific competition.
To test whether such segregation exists, we chose to study the relative abundances of the two species in three habitats: wooded, river, and lakeside. Of the three, lakeside sites are the most open, followed by the river sites and then the wooded sites. If the two species are segregating spatially, they should be found in different proportions and different relative abundances in the three habitats. Aerial insects are more abundant in open habitats. Thus, we expected to see a higher relative abundance of M. lucifugus at the lakeside sites where there were more aerial insects to hawk. Additionally, given that the calls of M. septentrionalis are more specialized for gleaning, we expected to see a higher relative abundance of this species at the wooded sites where there were more stationary insects to glean. At the river sites we expected to see roughly equal relative abundances for both species.
We then monitored how the relative abundance of each species changes throughout the night to determine if the bats exhibited temporal avoidance and foraged at different times. We also looked at whether or not there was a correlation between the relative abundance of each species and the weather conditions (temperature and rain). Relative abundance was measured by counting the number of call sequences recorded at each site and identifying which species each belonged to.
Nine locations in the vicinity of the University of Michigan Biological Station (Pellston, Michigan) were chosen: three lakeside habitats (L1, L2, L3), three river habitats (R1, R2, R3), and three wooded habitats (W1, W2, W3).
All of the lakeside sites were on Douglas Lake and within walking distance of Biological Station facilities. Site L1 (45º33.65'N, 84º40.61'W) was located on the western edge of Biological Station facilities, about one meter away from the edge of the lake. Sites L2 (45º33.74'N, 84º40.08'W) and L3 (45º33.89'N, 84º39.88'W) were located on the eastern shore of Douglas Lake. Site L2 was 1 meter above the water surface and 4.5 meters away, and L3 was 1.8 meters above the water surface and 5 meters away. Site L3 was north of L2.
The study was conducted for eight nights from June 25, 2004 through July 4, 2004. There was no observation on the nights of June 27 and July 3. The study ran from 9:00 pm until 5:30 am on each night of observation.
An Anabat II detector connected to a voice activated tape recorder was left at two sites per night. The first site was visited on the hour and the second was visited on the half hour to allow for travel between sites.
Temperature was measured with a mercury thermometer in degrees Fahrenheit. Wind speed was measured with an anemometer in meters per second. Also recorded were rainfall (no rain, light rain, or heavy rain), light (visible moon or no moon), and visibility (fog or no fog).
The data on each tape were recorded onto a computer using the program Anabat 5. The calls were analyzed using Analyze 2.1, which provided us with maximum frequency, average frequency, minimum frequency, curvature, duration, end slope, and start slope of each pulse in a call. Within each call, all of the pulses with a modal quality of 0.9 or greater were analyzed. If over half of the pulses had an average frequency greater than 50 KHz, the call was attributed to M. septentrionalis. If over half of the pulses had an average frequency less than 45 KHz, the call was attributed to M. lucifugus. If over half of the end slopes of the pulses were less than -10, the call was attributed to M. septentrionalis.
Out of a total of 334 call sequences recorded, 289 were identifiable using the above characteristics.  The remaining 45 were intermediate or contained conflicting pulse values. These calls were subjected to a discriminant function analysis, which was based on 17 known M. septentrionalis and 18 known M. lucifugus calls provided by Mr. Matt Wund, University of Michigan Museum of Zoology. This procedure classified 11 of the 45 unknown calls as either M. septentrionalis or M. lucifugus with a posterior probability of 0.95 or greater. These 11 were added to the identified calls, producing a total of 300 identified call sequences. Of these, 79 were identified as M. lucifugus and 221 were identified as M. septentrionalis.
For both M. lucifugus (P = 0.738, Mann-Whitney, U = 1535) and M. septentrionalis (P = 0.468, Mann-Whitney, U = 1476), the number of calls was not related to whether it was raining at the time the calls were recorded. Additionally, the relative abundance of neither species was correlated with temperature (P > 0.05, Kolmogorov-Smirnov, D = 0.105161).
The absence of M. septentrionalis at lakeside sites and its abundance at wooded sites confirms that this species is specialized for gleaning and prefers to forage in more enclosed habitats, as indicated by its high frequency, broadband calls. This preference serves to separate the niches of the two species and reduce competition between them.
While M. septentrionalis prefers to forage in wooded sites, M. lucifugus showed less of a preference. The roughly equal numbers of M. lucifugus at lakeside (34) and wooded (33) sites suggest that this species is a generalist and will both hawk and glean prey.
However, 70% of all calls (211 of 300) were recorded at wooded habitats. The 33 M. lucifugus calls at the wooded sites made up only 16% of the total number of calls recorded at wooded sites. At the lakeside sites, the 34 M. lucifugus calls makes up 81% of the total number of calls recorded.
Thus, the relative abundance of M. lucifugus was the highest at the lakeside sites. The unequal proportions of M. lucifugus at lakeside (.81) and wooded (.16) sites further confirms that M. septentrionalis prefers wooded sites over lakeside sites. However, it cannot be concluded from these two pieces of data that M. lucifugus prefers lakeside sites, because M. lucifugus numbers at lakeside sites were equal to those at wooded sites.
Both species showed the most activity during the early hours of the night. However, while M. lucifugus becomes active again at about 1:00 am, M. septentrionalis numbers steadily decline and drop off at around that time. This trend may be related to being a gleaner. Because a gleaner must snatch insects off of stationary objects, it would be helpful for them to supplement their echolocation with sight. Further, because sight depends on the availability of light, using sight would be more effective during the earlier hours of the evening.
Rainfall and temperature data, neither of which was significant, nonetheless could have had an influence on the temporal data. Rainfall and temperature were recorded once an hour. Thus, calls recorded during an hour in which rainfall was observed were not necessarily recorded in the rain.
Another problem with the study arose from limitations of the Anabat II detector. The detector recorded only the number of calls, not the number of bats present. It is possible that multiple passes were made by a single bat. For each species, if the number of calls was evenly distributed amongst the number of bats present, then the number of calls recorded is indicative of the relative abundance of that species, and this problem should not influence our results. However, if most of the calls recorded were made by one or a few bats, then the number of calls was not evenly distributed, and this could have affected our results.
It would also be helpful to see if the spatial and temporal segregation we observed exists in areas where the two species are allopatric. If this is the case, then we can conclude that the segregation we observed is due to intrinsic differences between the two and that competition does not influence habitat preference. If, however, either of the species expands its foraging habitats (if either species shows no preference for any of the habitats), then the segregation we observed was due to competition.

It is widely assumed that cellular functions are organized in a highly hierarchical and modular mannar (Hartwell, Hopfield et al. 1999). Each module is a discrete object composed of a group of tightly linked components and performs an independent task, seperable from the function of other modules (Hartwell, Hopfield et al. 1999; Ravasz, Somera et al. 2002). With the advent of genome scale data, many efforts have been devoted into identifying modular structures and their biological significance (Barabasi and Oltvai 2004). To thoroughly study the modular structure(Ravasz, Somera et al. 2002; Rives and Galliski 2003; Yook, Oltvai et al. 2004) of large scale networks require effective and automatic method that can separate modules. Clustering could be one of the methods to discover module structure in networks using topological structure (Ravasz, Somera et al. 2002; Giot 2003; Yook, Oltvai et al. 2004). And some studies combined clustering with functional genomics data also gave good result (Stiart, Segal et al. 2003; Tornow and Mewes 2003). By using RNA expression profile data, Han etc. (Han, Bertin et al. 2004) divided the hubs into data hub and party hub and showed modularity organization in yeast protein-protein interaction networks. Fraser (Fraser 2005) studied the evolutionary conservation of data hubs and party hubs and suggested the occurring of modules through exon shuffling.
However, the module separation by clustering usually gives ambiguity result (Barabasi and Oltvai 2004), part of which is because of the network's hierarchical structure. But lack of objective judgment could be another reason. Newman (Newman and Girvan 2004) proposed a method to measure the modularity of the separated modular structure and devised a greedy method to separate module according to edge betweenness (Newman 2004). Based on the modularity definition, many other algorithms were invented to get the global maximized modularity because greedy methods could be easily trapped in local maximization (Duch and Arenas 2005; Guimera and Amaral 2005; Massen and Doye 2005). Heuristic algorithms often give better result, especially for the networks with relatively low hierarchical structure. Using stimulated annealing to maximizing the network's modularity, Guimera and Amaral (Guimera and Amaral 2005) was able to identify the functional organization of metabolic networks. According to the topological properties, functional roles were determined for each node and they showed the evolutionary conservation among different roles of nodes. Nevertheless, their result that intra-module hubs are less conserved than the intermodule nodes, which contradicts with Fraser's (Fraser 2005) result for protein networks that intramodule hubs are more conserved.
In this paper, modules are separated solely based on the topology of protein networks. The biological significance of modular structures is accessed by functional and evolutionary data. The modular structures show highly evolutionary conservation when comparing the orthologous proteins in yeast and fly modules.
The modules of protein-protein interaction networks of yeast Saccharomyces cerevisiare and fly Drosophila melanogaster was identified using simulation annealing algorithm. For the protein networks, the largest component contains 3862 nodes (about 94% out of 4216 nodes for the entire network) forming 7208 edges for yeast and 6279 nodes (95% of all) forming 10094 edges. The overall modularity is 0.666 for yeast and 0.685 for fly suggestting a high modular organization of the network (table 1).
To correlate the functional properties and topological modules, functional classification established by MIPS was used in which each protein is assigned a function category according to the enzyme function. I filtered the data that only functional categories containing more than 10 proteins were used, and totally there are 17 functional categories including unclassified proteins.
For each module, the sum of genes in each functional category was calculated. To get the expected number, random module separation ran for 1000 times, and the average was used as the mathematical expectation. I used
A discrete module that performs a certain function is much likely located in the same cellular location. Based on this idea, global protein localization data in budding yeast is used to check the biased distribution of each module. The same as functional category, each location of proteins in modules is counted and simulation was used to calculate the expected distribution.
Nonsynonymous changes in sequences will result in protein sequences change, and could be used to measure the distance or evolutionary conservation of proteins. And a low evolutionary rate usually indicates strong functional constraint. Nonsynonymous substitutions per site (dN) was calculated for all the genes with ortholog in S. bayanus a closely related species. The Spearman's rank correlation was used to evaluate the evolutionary conservation of topological structure (table 1).
The result shows only dN and within-module degree is very significantly and negatively correlated which indicate that within-module hubs are more conserved. And this result is robust because there is no significant correlation between between-module degree and dN which would not confound the correlation between within-module degree.
Another way to measure the evolutionary conservation is the phylogenetic conservation across species. If the gene subjects high functional constraints, it is very unlikely to be lost during the
evolution. So, by counting the gene loss events on a phylogenetic tree also reflects the evolutionary conservation for the gene. I used all S. cerevisiea gene to blast against the other 9 species and identified a number of gene loss events on each brach. And in the following work I will analyze the distribution and correlation with the network topological properties.
Because of the functional correspondence to module, the importance of each module may be varied depending on the functional importance. To access the differences among modules, I examined the distribution of essential genes which would cause death if knocked out. If one module has more essential genes, it tends to be more important in terms of function. While if there is no functional differences among modules, essential genes should be randomly distributed in each module.
Although the interactions between proteins are less conserved across distant species (Gandhi, Zhong et al. 2006), they may show some conservation at modular structure level in which orthologous genes across species share the same module. To test this hypothesis, I used the same method to separate modules for D. drosophila protein-protein interaction network. And the homologous genes are identified using reciprocal blast best hits. If the homologous genes are grouped into the same module, the hierarchical modular structure will be conserved. The result shows this (figure 4). The same as function and location, conservation profile is used to show the relationship. And the
The hierarchical structure of the protein networks could correspond to the functional category, which is consistent with previous studies(Tornow and Mewes 2003; Yook, Oltvai et al. 2004). However, my result does not suggest a direct one-to-one or one-to-multiple correspondence between functional categories and modules. Noise in the data maybe one reason; another reason could be the module structure may not reflect the enzyme based functional classification. The direct evidence that pairs of proteins within the same module tend to be in the same protein complex structure indicate that protein complex is better correspondence to the module structure (Spirin and Mirny 2003).
Because highly pleiotropic genes tend to have multiple functions and they could be the nodes link to many other modules, they are usually thought to be more conserved (Fraser 2005). Participation coefficient measures how the between-module links distributed (Guimera and Amaral 2005; Guimera and Amaral 2005). This result suggest that either participation coefficient is not corresponded to pleiotropy or there is no correlation between pleiotropy and evolutionary conservation.
The result of no significant correlation between dN and betweenness centrality is somehow surprising. Because Hahn and Kern's (Hahn and Kern 2004) study suggests a significant correlation between betweenness centrality and gene essentiality in three eukaryotic protein networks, although the correlation is very week.
Finally, although the differences among modules are statistically significant, only a few of them show a strong biased distribution. And querying these modules to functional category distribution gives no significant biased distribution.
In fact, protein-protein interaction data set is highly noisy (Barabasi and Oltvai 2004). This is partly come from the random errors in large scale experiments. But they may also come from the method of yeast two hybrids to detect interacting protein pairs. A number of studies (Aloy and Russell 2002; Han, Dupuy et al. 2005) suggested artifacts in protein interaction networks and sampling may also result in biased data set. This result in the genome scale analysis becomes very hard because noise reduces the signals significantly.
Protein-protein interaction networks data set for Saccharomyces cerevisiea was downloaded from MIPS (http://mips.gsf.de/). Only the nodes in the largest component of the network were used to separate module by simulated annealing algorithm.
Functional annotation for S. cerevisiea genes were also downloaded from MIPS and protein complex data came from IntAct database (http://www.ebi.ac.uk/intact). I compared the distribution of gene in each module and the functional category for each gene to evaluate the functional correspondence of module structure. Protein complex data was another way to measure the biological meaning of network module. I also calculated the betweenness centrality for each node using Pejak.
The evolutionary rate dN for S. cerevisiea genes were calculated against orthologous genes of S. bayanus. The loss of genes on other braches of yeast was also used to evaluate the phylogenetic conservation. Specificly, I use BLAST against the whole genome sequences of other 9 species, S. paradoxus, S. mikatea, S. bayanus, C. glabrata, K. waltii, K. lactis, D. hansenii, Y. lipolytice, N. crassa and S. pombe using 0.1 as criteria to detect gene loss. Parsomony method was used to calculate gene loss events on each brach with S. pombe as the outgroup. I compared the correlation among within module-degree, between-module degree, participation coefficient, betweenness centrality, dN and gene loss events to get evolutionary conservation for the roles of nodes.
Finally, the protein-protein interaction data for Drosophila melanogaster (downloaded from flybase (http://flybase.net)) was used to detect the evolutionary conservation of hierarchical modular structure between yeast and fly.

Since the first vertebrates invaded the terrestrial realm around 350 million years ago (Clack 2002a, 2002b; Long and Gordon 2004), several subsequent groups of tetrapods have become readapted for an aquatic lifestyle (Mazin and de Buffrénil 2001). Among mammals, the spectrum of secondarily aquatic forms ranges from fully aquatic cetaceans and sirenians to semiaquatic taxa that spend much of their time on land but forage in the water (Gingerich 2003). One of the most speciose groups of secondarily aquatic mammals today, composed of about 36 extant species, is the Pinnipedia (Berta and Adam 2001; Berta et al. 2006). Pinnipeds are highly derived, mostly aquatic mammals within the Order Carnivora and are divided into three modern families: Odobenidae (walruses), Otariidae (sea lions and fur seals), and Phocidae (true or earless seals) (Berta et al. 2006). Based on morphological and paleontological evidence, pinnipeds were traditionally thought to have a diphyletic origin (McLaren 1960; Tedford 1976). This view maintained that the odobenids and otariids were derived from ursid (bears) ancestry and that phocids had a separate origin from somewhere within the mustelid (weasels, otters, etc.) lineage. Wyss (1987) reassessed the morphological evidence and suggested a common origin for all pinnipeds. A number of morphological (Wyss 1988; Berta et al. 1989; Wyss 1989; Wyss and Flynn 1993; Berta and Wyss 1994) and molecular studies (Sarich 1969; Árnason and Widegren 1986; Vrana et al. 1994; Árnason et al. 1995; Lento et al. 1995; Flynn and Nedbal 1998; Davis et al. 2004; Árnason et al. 2006) have since given strong support for pinniped monophyly, but the arctoid group from which pinnipeds arose has not yet been agreed upon, with studies suggesting ursid (Wyss and Flynn 1993; Berta and Wyss 1994; Vrana et al. 1994; Lento et al. 1995), mustelid (Árnason and Widegren 1986; Flynn and Nedbal 1998), or general arctoid ancestry (Davis et al. 2004; Árnason et al. 2006).
One way in which the pinniped groups significantly differ from each other is how they swim. Otariids swim using a form of locomotion called pectoral oscillation, in which the highly modified fore-flippers are the sole source of thrust while the hind limbs and axial skeleton play no apparent role (English 1976; Feldkamp 1987a, 1987b). Conversely, phocids and odobenids swim using primarily pelvic oscillation, in which the cranial end of the body is held rigid while the hind limbs perform a side-to-side "sculling" motion to generate forward thrust (Tarasoff et al. 1972; Gordon 1981). Both modes of swimming are highly derived and require very specialized morphologies in order to perform them (Fish 1996; Berta and Adam 2001). During the evolutionary transition from land to sea in pinnipeds, taxa intermediate between the fully terrestrial ancestors and the mostly aquatic descendents would have lacked some of these skeletal adaptations and would have swum in a manner different from modern forms. Since locomotion is central to an animal's ability to forage, evade predators, disperse, and migrate (Fish 1992), understanding how these highly derived modes of locomotion evolved from less efficient modes can inform us about the ecologies and lifestyles of intermediate species and help us understand how the land-to-sea transition progressed.
Models have been proposed to explain how highly derived modes of swimming could have evolved through semiaquatic taxa performing a series of increasingly efficient swimming styles (Fish 1996, 2000, 2001), but these studies have been based primarily on studies of swimming in living aquatic and semiaquatic mammals. Few studies have assessed the evolution of swimming mode in secondarily aquatic taxa using fossil evidence. Berta and Adam (2001) used the morphologies of extant pinnipeds to interpret the locomotor capabilities of extinct forms and placed these swimming styles onto a phylogeny to assess how forelimb- and hind limb- dominated swimming evolved within pinnipeds. Their results suggested: (1) that forelimb-dominated swimming was ancestral for the group, (2) that hind limb-dominated swimming arose once at the base of the Phocomorpha (the sister group to the Otariidae), and (3) that there was one reversal back to forelimb swimming in the Dusignathinae (Figure 1). However, the interpretations of locomotor mode in some of the fossil taxa in this study (mainly Enaliarctos and the Desmatophocidae) are not as straightforward as these authors presented them to be, and their potential locomotor capabilities could stand to be reassessed.
The most primitive known group of pinnipedimorphs is the Enaliarctinae (Barnes 1989, 1990; Berta 1991; Barnes 1992). Enaliarctos mealsi, from the late Oligocene of California, is known on the basis of a virtually complete skeleton (Figure 2A-B), and when it was initially described, it was thought to swim using a combination of fore flippers, hind flippers, and undulation of the axial skeleton to generate thrust (Berta et al. 1989; Berta and Ray 1990). However, Berta and Adam (2001) asserted that Enaliarctos
Likewise, there has been disagreement as to how the desmatophocid Allodesmus kernensis swam (Figure 2C). This species is a member of the Desmatophocidae, a group which was once considered to be a subfamily of Otariidae (Mitchell 1966; Barnes 1972; Mitchell 1975; Barnes 1989) but is now thought to be the sister group of the Phocidae (Berta 1994; Berta and Wyss 1994; Deméré and Berta 2002). The initial descriptions of Allodesmus postulated it used solely its forelimbs in aquatic locomotion, much as modern otariids do (Mitchell 1966; Barnes 1972). However, Giffin (1992) demonstrated that the anatomy of the neural canal in this species suggests that its torso, fore-, and hind limbs all possessed significant innervation. This morphology was found to be most similar to odobenids, and Giffin called into question the previous interpretations of forelimb-dominated swimming. Berta and Adam (2001) noted that Allodesmus has features indicative of both forelimb and hind limb swimming, but they coded it as being a hind limb swimmer in their locomotor analysis, an interpretation that has persisted in recent references (e.g. Berta et al. 2006).
For at least these two species, there is some question as to whether they swam using primarily forelimb-or hind limb-dominated locomotion, and given their phylogenetic positions in relation to other pinnipedimorph taxa, understanding the evolution of locomotion within the entire group is contingent on having accurate inferences about their swimming mode. Given their relatively complete skeletons, locomotor interpretations for Enaliarctos and Allodesmus can tested by including them in a principal components analysis (PCA) on skeletal proportions in extant semiaquatic mammals like that performed by Gingerich (2003). This method has been used to interpret locomotor mode in fossil whales (Gingerich 2003), desmostylians (Gingerich 2005), and pantolestids (Rose and von Koenigswald 2005), and by finding which modern mammals Enaliarctos and Allodesmus are most similar to in overall skeletal proportions, we will be able to make well-informed inferences about their swimming modes. These results can then be looked at in a phylogenetic context, as was done by Berta and Adam (2001), in order to assess how locomotor evolution occurred in pinnipeds.
Of the fourteen skeletal measurements included in the PCA performed by Gingerich (2003), thirteen of the measurements for Enaliarctos were obtained from Berta and Ray (1990) (Table 1). The missing measurement, the length of pedal phalanx III-2, was estimated by scaling it down from the length of metatarsal III in the same proportion as pedal phalanx II-2 is scaled down in length from metatarsal II. A PCA was carried out on 14 skeletal measurements for 61 extant semiaquatic taxa following the methods described in Gingerich (2003). These 61 taxa included the 50 specimens used by Gingerich (2003), as well as 11 additional pinnipeds and lutrine mustelids to increase the representation of these diverse groups in the data set (Table 1). Eigenvector coefficients for each of most informative principal component axes were multiplied by the normalized, natural log-transformed measurements and summed across all 14 measurements to generate scores for each species for each principal component. Enaliarctos was not included in the PCA itself, but was added as a supplemental taxon, following the treatment of fossil taxa in previous analyses (Gingerich 2003, 2005; Rose and von Koenigswald 2005), and was plotted in the same space as the modern taxa using the eigenvector coefficients generated by the PCA to calculate its PC scores. For Allodesmus, no pedal phalanges are known, so an additional PCA was carried out using only 12 of the 14 measurements. Measurements for the thorax, lumbus, humerus, radius, femur, and tibia of Allodesmus were obtained from tables in Mitchell (1966), while the remaining measurements were estimated from photographic plates therein (Table 1). Like Enaliarctos, Allodesmus was not included in the PCA itself, but its PC scores were calculated using the eigenvector coefficients resulting from the analysis. The locomotor modes inferred for these two taxa based on the PCAs were then placed onto a phylogeny of pinnipeds to assess when different swimming styles appeared within the history of the group, thus giving us an overall picture of how this important behavior may have been evolving during this land-to-sea transition.
The first PCA performed in this study included 14 skeletal measurements for 61 extant semiaquatic mammals. The eigenvalues and eigenvector coefficients (loadings) for each principal component are listed in Table 2, with the eigenvector coefficients also being shown graphically in Figures 3-4. The variation in this data set was reduced to three informative axes. PC-I accounted for 93.1% of the total variance in measurements, and all of the eigenvector coefficients are positive and of a similar magnitude. This is very similar to Gingerich's (2003) results, and this axis can best be understood as representing overall size, with smaller individuals possessing more negative PC-I scores and larger individuals possessing more positive PC-I scores. PC-II accounted for 4.2% of the total variance, and the eigenvector coefficients indicate that the strongest contrast on PC-II is between individuals with long manual and pedal phalanges and individuals with a long femur and ilium, just as in Gingerich's (2003) analysis. This axis can thus be interpreted as separating more terrestrial species on the positive end of the axis from more aquatic species on the negative end. PC-III accounted for 0.8% of the total variance, and the eigenvector coefficients indicate that the strongest contrast is between taxa with a long lumbus, a long third metatarsal, and long pedal phalanges and taxa with a long third metacarpal and long manual phalanges. Again, this follows Gingerich's (2003) results, so PC-III can be interpreted as separating hind limb-dominated swimmers with more negative PC-III scores from forelimb-dominated swimmers with more positive PC-III scores. PC scores for each taxon for the first three principal components are listed in Table 4 and shown graphically in Figures 3-4. When the PC scores for Enaliarctos are plotted in the same space as the modern taxa, it plots right with other pinnipeds on the size axis and appears to be less aquatic than the majority of the modern otariids and phocids but more aquatic than Odobenus on the terrestrial/aquatic axis. On the locomotion axis, Enaliarctos actually plots within the space occupied by the Phocidae, suggesting that Enaliarctos was primarily a hind limb-dominated swimmer and calling into question the recent interpretations that this species swam primarily using its forelimbs to generate propulsion.
The second PCA included only 12 skeletal measurements for the 61 extant semiaquatic mammals. The eigenvalues and eigenvector coefficients (loadings) for each principal component are listed in Table 3, with the eigenvector coefficients also being shown graphically in Figures 5-6. The variation in the data was again reduced to three informative axes. PC-I accounted for 93.8% of the total variance in the dataset, and since the eigenvector coefficients were all positive and of a similar magnitude, this axis can again be interpreted as representing size. PC-II accounted for 3.6% of the total variance, and compared with the eigenvector coefficients from the first PCA, the coefficients for each measurement in this case fell in relatively the same locations as in the previous analysis, simply with loadings for pedal phalanges being absent. The largest contrast on PC-II is thus between individuals with long manual phalanges and individuals with a long femur and ilium. It appears safe to assume that this axis can again be interpreted as separating more terrestrial taxa on the positive end from more aquatic taxa on the negative end. PC-III accounted for 0.9% of the total variance, and while the more intermediate eigenvector coefficients shifted around slightly, the most extreme values are the same as in the first PCA. The largest contrast on PC-III in this second case is between individuals with a long lumbus, long third metatarsal, and long tibia and individuals with a long third metacarpal and long manual phalanges. This axis can again be interpreted as separating hind limb-dominated swimmers with more negative PC-III scores from forelimb-dominated swimmers with more positive PC-III scores. PC scores for each taxon for the first three principal components are listed in Table 4 and shown graphically in Figures 5-6. When PC scores are calculated for Allodesmus and plotted in the same space as the scores of the extant species, it plots among the larger pinnipeds but appears less aquatic than all pinnipeds except for Odobenus. On the locomotion axis, Allodesmus plots outside of both the Phocidae and the Otariidae, but is much closer to the Otariidae on the forelimb-dominated side of the axis. This implies that Allodesmus has skeletal proportions similar to pectoral oscillators and swam primarily using its forelimbs, which is counter to the most recent interpretations of locomotion in this species.
These new locomotor interpretations were then analyzed in a phylogenetic context following the approach of Berta and Adam (2001) (Figure 7). The topology of the tree follows the results of Berta and Wyss (1994) and Deméré (1994), and the inferred locomotor modes of taxa not included in the PCA follow the interpretations of Berta and Adam (2001). If Enaliarctos is indeed a hind limb-dominated swimmer and Allodesmus a forelimb-dominated swimmer as the PCA results suggest, then it appears that hind limb-dominated swimming had to have arisen at least two times independently. If we posit that hind limb swimming arose once at the base of the Phocomorpha ([Desmatophocidae + Phocidae] + Odobenidae) and once in the Enaliarctinae lineage, then we also must posit an additional reversal to forelimb swimming in the Desmatophocidae. Another possibility would be to propose that hind limb swimming arose independently within the Phocidae and the Odobenidae, which would decrease the number of reversals to forelimb swimming (only once within the Dusignathinae), but would increase the number of independent acquisitions of hind limb-dominated swimming to three. The main point is that if these locomotor interpretations are correct for Enaliarctos and Allodesmus, then the evolution of swimming in pinnipeds involves a number of additional locomotor transitions than what Berta and Adam's (2001) initial analysis proposed.
Like many other tetrapod groups, ancestral pinnipeds embarked on a journey beginning on land and eventually finishing in the sea. Looking at modern secondarily-aquatic vertebrates and their supposed ancestral groups, it is clear that the starting and ending points for each of these land-to-sea transitions are unique, and the pathway that each of these lineages took to readapt to an aquatic environment was novel as well (Gingerich 2005). One aspect of reconstructing and understanding these transitions is to elucidate the evolution of locomotor modes, which informs us about the way in which an organism could interact with other species in its environment.
Previous attempts to analyze the evolution of locomotion in pinnipeds have relied on the presence or absence of osteological characters associated with each swimming mode (Berta and Adam 2001). This technique may work well for taxa close to the modern groups of pinnipeds, but it presents difficulties when interpreting locomotor capabilities in taxa that are less closely-related to the modern clades. Both Enaliarctos mealsi (Berta et al. 1989; Berta and Ray 1990) and Allodesmus kernensis (Giffin 1992) had been described as possessing features consistent with using forelimbs, hind limbs, and their axial skeleton to generate thrust during aquatic locomotion, but were shoehorned into either forelimb-or hind limb-dominated swimming based on possession of a handful of characters (Berta and Adam 2001; Berta et al. 2006). Their relatively complete skeletons allow their overall skeletal proportions to be looked at in comparison with modern semiaquatic taxa, and by elucidating which modern taxa they are most similar to proportionally, we have a way to quantitatively assess locomotor interpretations in these fossil taxa.
The placement of Enaliarctos on the size axis (PC-I) indicated that it was an average-sized species when compared with modern pinnipeds, and it appeared to be less aquatic than most of the modern pinnipeds on the axis for PC-II. It plotted among the phocids on the locomotion axis (PC-III), suggesting that it swam using a hind limb-dominated mode of locomotion. This interpretation is in contrast to recent suggestions that it was more specialized for forelimb swimming (Berta and Adam 2001; Berta et al. 2006), but given that its hind limbs are relatively larger than its forelimbs (Berta and Ray 1990), hind limb-dominated swimming seems like a more plausible interpretation. Allodesmus appeared to be of a similar size to some of the larger modern pinnipeds, but like Enaliarctos, its PC-II score indicated that it was less aquatically-adapted than most modern pinnipeds. Its PC-III score placed it closest to modern otariids, which suggests that it might have used predominantly forelimb swimming. This, again, is in contrast to the most recent interpretations (Berta and Adam 2001; Berta et al. 2006), but it supports the notions put forth by earlier authors that this animal was similar to modern otariids in habit (Mitchell 1966; Barnes 1972; Mitchell 1975).
Placing these new interpretations onto a phylogeny to assess the evolution of swimming mode in pinnipeds complicates the picture depicted by Berta and Adam (2001), as it calls for several additional transitions in locomotor mode than they proposed. But subscribing to this overall picture of locomotor evolution is contingent upon the inferred phylogenetic relationships being correct. While pinnipeds are generally agreed to be a monophyletic group derived from arctoid carnivores, the relationships among members within the group are still contentious. Traditionally, otariids and odobenids were allied within the Otarioidea (McLaren 1960; Barnes 1972; Mitchell 1975; Tedford 1976; Barnes 1989), but many recent analyses have placed the odobenids as the sister group to the Phocoidea (Desmatophocidae + Phocidae), forming the Phocomorpha (Wyss 1987; Berta et al. 1989; Wyss 1989; Berta and Ray 1990; Berta 1991; Berta and Wyss 1994; Deméré and Berta 2002; Berta et al. 2006). Molecular analyses have consistently suggested that odobenids are more closely related to otariids (Sarich 1969; Árnason et al. 1995; Lento et al. 1995; Flynn and Nedbal 1998; Davis et al. 2004; Flynn and Wesley-Hunt 2005; Árnason et al. 2006), but some authors argue that these results are due to long-branch attraction since the odobenid lineage appears to have split off early in the history of the group (Berta and Adam 2001; Berta et al. 2006). Using inferred locomotor mode like a character on a phylogenetic tree to interpret the evolution of locomotion is obviously contingent on the topology of that tree, so the picture of swimming evolution derived from this strategy could change drastically if the inferred relationships among the groups were to change. Thus, the interpretations of locomotor evolution across all pinniped lineages should be viewed with caution in light of the equivocal relationships among the different groups.
We should also be careful not to think that any animal must use exclusively one swimming mode. Modern otters use a variety of different swimming modes depending on what they are doing and where they are in the water (Fish 1994), and even the modern walrus, while using primarily pelvic oscillation, is known to use pectoral paddling as well when swimming at slow speeds (Gordon 1981). Given that Enaliarctos (Berta et al. 1989; Berta and Ray 1990) and Allodesmus (Giffin 1992; Berta and Adam 2001) have both been described as possessing adaptations for multiple swimming modes, we must not too quickly interpret them as using solely forelimb- or hind limb- dominated swimming, despite their placement on the locomotion axis in the PCA. The PCA simply demonstrated which modern animals were most similar in proportions to these fossil taxa, and by association, we were able to infer something about their locomotion based on that of the modern taxa with which they clustered. For Enaliarctos in particular, it is actually quite attractive to hypothesize that it may have used multiple locomotor modes given its relationship to modern pinnipeds. If it could be shown to be the common ancestor of all later pinnipedimorphs rather than the outgroup to them, it may help explain how distinctly different locomotor modes (fore- and hind limb- dominated swimming) could have evolved from a common ancestor that showed a propensity for both fore- and hind limb swimming. By incorporating stratigraphic data into these phylogenetic reconstructions, we could test ancestor-descendent hypotheses, and this may offer a more complete picture of both locomotor evolution and the evolution of these groups in general.
The PCA performed in this paper was successful in demonstrating the modern pinnipeds to which Enaliarctos mealsi and Allodesmus kernensis were most similar in skeletal proportions. Their similarity to modern phocids and otariids respectively called into question some previous hypotheses of dominant swimming mode, thus slightly complicating our view of how this critical behavior evolved within the group. Yet while these results may obscure our understanding of the evolution of this behavior, they demonstrate the importance of using multiple approaches to assess behavior in extinct taxa. Using solely osteological characters or statistical techniques gives an incomplete picture of these animals and their supposed abilities. Interpretations from statistical analyses like the PCA performed here should be used to supplement an assessment of osteological characters when inferring locomotor modes and other behaviors, as it is important to garner as much information as possible when reconstructing the lifestyles of extinct creatures.

Cichlidae is a monophyletic group of perciform fishes with a species diversity approaching 2,000 described species (Kullander 1998, Sparks 2001). The group is widely distributed on former Gondwanan fragments: South and Central America (300 spp.), Africa (1,000 spp.), Madagascar (>18spp.), India (3spp.), and the Middle East (3 spp.) (Figure 1). There has been resurgence of discussion of the historical biogeography of this group in recent years with the discovery of the oldest known cichlid fossils and new molecular evidence (Murray 2001, 2001b; Sparks 2001; Vences 2001; Kumazawa et al. 2000; see review in Chakrabarty 2003).
Recent phylogenetic work finds that each continental assemblage of cichlids forms its own monophyletic group with the exception of Madagascar (Sparks 2001; Farias et al. 1999, 2000; Streelman et al. 1998; Zardoya et al.1996, see Chakrabarty 2003 for alternatives and explanation). This has led some to propose a Cretaceous origin of Cichlidae before the break-up of Gondwana (Sparks 2001; Stiassny 1991). Dispersalists point out the minimum age of the Cichlidae based on the oldest known fossils is Eocene (45 million years ago), and use the lack of fossil cichlids before this time as evidence for recent dispersal events to explain current cichlid distributions (Murray 2001b; Lundberg 1993).
Malagasy cichlids are shown to be sister to cichlids on India in every analysis to include them (Stiassny et al. 2001; Sparks 2001; Farias et al. 1999, 2000; Streelman et al. 1998; Zardoya 1996; Cichocki 1976; Streelman & Karl 1997; Lippitsch 1995; Oliver 1984; Kullander 1998). This Malagasy-Indian relationship is the most well corroborated sister grouping of cichlids and in the dispersalist framework this relationship would mean that cichlids were capable of dispersing across the entire Indian Ocean but not across the Mozambique Channel (430 km wide at its narrowest point) to Africa.
Cichlids are primarily a strictly freshwater water group with some species able to tolerate brackish waters and a few that are considered salt tolerant (Murray 2001b). With no known cases of cichlids dispersing from one landmass to another across a salt-water divide, the marine environment should still be considered a barrier to cichlid dispersal.
There are landmasses that are not Gondwanan in origin that have cichlids, these are the islands of Cuba and Hispaniola (Figure 2). These islands have a complex geological history and there are a number of alternative hypotheses of how the fauna and flora of these islands evolved. The mechanism that allowed cichlids to invade these islands will be explored in this study.
The relationships of the Greater Antillean cichlid fauna are important because the Antilles are not geologically Gondawanan in origin, although they may have in various times of their history been connected to Gondwanan fragments. Leon Croizat's (1962) metaphor of vicariance biogeography being like reconstructing a pane of glass that has been repeatedly shattered seems particularly relevant to the Greater Antilles.
Geologically speaking, the Greater Antilles rest upon a small plate located between the much larger North America, South American and Cocos and Nazca plates. The Caribbean plate itself can be divided into a series of minor plates that have separated and merged at various times in their history (Perfit & Williams 1989). Despite their history and position on a tectonic plate, these islands are usually labeled as "oceanic." This nomenclature, like "secondary freshwater," assumes a priori that overwater dispersal is the only mechanism for organisms to populate these islands. Paulay (1994) defined oceanic islands as islands that have never been connected to a mainland continent and therefore are populated solely by dispersal. Given the aforementioned tectonic reconstructions, this definition does not fit the Greater Antilles.
Two alternative tectonic reconstructions involving a land connection between the Greater Antilles and either mainland South America or Central America will be discussed. Rosen (1975, 1985) developed a vicariance model based on Malfait and Dinkelman (1972) and Tedford's (1974) geological models of the Antilles. This view gives a Mesozoic age to the fauna of the Greater Antillean islands. This reconstruction of the Antilles, supported by many others (Pindell et al. 1988; Ross & Scotese 1988; Pindell & Barrett 1990; Pitman et al. 1993; Schuchert 1935; Dickinson & Coney 1980; but see alternatives in Hedges 1982), proposes that the Greater Antilles originated in the Pacific Ocean as a series of islands that connected North and South America as a temporary closely linked island chain circa 80 million years ago, in a position occupied by present day Central America. This arc then broke in the early Cenozoic and drifted to its current position. In Rosen's model, Jamaica, the Caymans, southwest Hispaniola, and western Cuba were connected to the southern and eastern parts of Yucatan (northern Central America) until they separated at the end of the Cretaceous, 65 million years ago.
In the alternative view proposed by Iturralde-Vinent and MacPhee (1999) based on geological evidence, a short-lived connection between the Greater Antillean Islands (Cuba, Hispaniola, Puerto Rico and possibly Jamaica) and northwest South America existed circa 32 million years ago. The authors name this Early Oligocene landbridge GAARlandia (from Greater Antilles + Aves Ridge). The consequence of this alternative is that the Greater Antillean Island chain would have had a more recent connection with South America than with Central America. The authors do not believe that there was a land connection between these islands and Central America in a way that would explain the current faunal distributions. This sets up two testable alternative hypotheses, between the relationships of Antillean cichlids with either Central or South American lineages. Non-congruence of phylogenetic pattern and sequence of divergence would favor dispersal.
There are five known cichlids from the Antilles, Cichlasoma tetracanthus, C. ramsdeni, C. haitiensis, C. vombergi and C. woodringi (see Myers 1928 and Darlington 1957 for discussion on other possible species, and distributions). The first two are restricted to Cuba, and the others to Hispaniola. The fossil C. woodringi is either Upper or Middle Miocene (23 to 5 mya) in age (Rivas 1986; Myers 1928) or Pliocene (Casciotta & Arratia 1993; Murray 2001b). It has the notoriety of being the only known freshwater fossil from the Antilles (Burgess & Franz 1989). Bussing (1985) and Rivas (pers. comm. in Burgess & Franz 1989) comment that this fossil is indistinguishable from C. haitiensis, an extant Hispaniolan species. Bussing (1985) also likens it to the Parapetenia of Central America. The generic name of these species (Cichlasoma) is sometimes referred to as Nandopsis, these names are homonyms and will be used interchangeable in this paper.
The genus Cichlasoma (sensu lato), to which all the Antillean cichlids belong, dominates the Central American cichlid fauna (75 of about 100 or more species) (Roe et al. 1997; Miller 1966; Martin & Bermingham 1998; Kullander 1983). This genus is also found throughout South America and north to Texas, but its diversity in Central America is unmatched (Bussing 1985).
To date no formal phylogenetic analysis has included the Cuban and Hispaniolan species with Central American and South American species. Rosen (1975) presented a cladogram that had a sister relationship between Central America and the Antillean cichlid fauna, however it included only the Central American and Antillean fauna. This four taxon cladogram was cited using a "personal communication" from Cichocki who did not include this analysis in any published material. Loiselle (1985) also lists Central American sister species for the Antillean cichlids, but does not discuss his phylogenetic method; it appears as if his notion is based solely on his own similarity judgment. Without a phylogenetic diagnosis, we lack a measure for selecting between alternative mechanisms for explaining this disjunct distribution. Before discussing the results of the analyses done here, it is important to discuss some important contributions on prior hypotheses of vicariance and dispersal mechanisms for these cichlids and freshwater fauna of this area; they are reviewed below.
Myers (1938, 1966; see also Darlington 1957) hypothesized that the freshwater fishes of the West
A number of authors have stated that Cuba, particularly its eastern half, was once united with Hispaniola in the early history of the Caribbean (Williams 1989; Perfit & Williams 1989). According to Pitman et al. (1993), Cuba and Hispaniola did not separate until a shearing in the late Middle Eocene. Nearly 90 % of the 71 species of Antillean freshwater fishes occur on Cuba and Hispaniola (Burgess & Franz 1989). Sixty-five of these are endemic to an island or island group (Burgess & Franz 1989). Surprisingly, Puerto Rico, the fourth largest Antillean island, separated from Hispaniola by only the narrow Mona Passage (130 km), is completely lacking in native freshwater fishes. Puerto Rico does have available habitats, as an introduced African cichlid and many other introduced species maintain populations there (Burgess & Franz 1989). Fishes dispersing from Central or South America would also probably reach Jamaica or the lower Antilles first because of their location (Fig.8). There are no cichlids on Jamaica (it does have six other native freshwater species), and there are only two native freshwater fishes on the entire Lesser Antilles. The absence of cichlids from Jamaica and particularly Puerto Rico does not bode well for the Iturralde-Vinent and MacPhee (1999) hypothesis, which posits a connection between Cuba, Hispaniola and South America, with Jamaica and Puerto Rico in-between. A complete extinction of the freshwater fish fauna would be required on Puerto Rico, after the suggested landbridge, for their hypothesis to be plausible.
Based on his vicariance model, Rosen (1975) gave a Mesozoic minimum age to the freshwater fish fauna of the Antilles including cichlids, atherinids (silversides), poeciliids and other Cyprinodontiformes, synbranchid eels and gars. Rauchenberger (1988) attempted to create a composite area cladogram from 12 other cladograms using these taxa to support Rosen's vicariance model. Most of the trees she used in her analysis are poorly resolved (the cichlid area cladogram she used is an uninformative polytomy), as she notes herself. Only the Gambusia tree (a poeciliid) provided much resolution to her composite. Unfortunately the Gambusia tree she cites (but does not show) from Fink (1971, 1971b) ignores some key elements of the original cladogram (Fink pers. comm.). Her analysis also included only one South American species, which, due to its placement at the base of the cladogram, did not affect the composite tree. It is not surprising then that she found a close relationship between the Antillean and Central American taxa (the only possibility, given her sampling).
Iturralde-Vinent and MacPhee (1999) give an age no older than the Middle Eocene for the Greater Antillean islands (including Cuba and Hispaniola), therefore claiming that that fauna must be younger than this age. On this basis they reject Rosen's (1975) Mesozoic vicariance hypothesis. Although they agree that these island masses may be as old as the Jurassic, they believe that subduction into tectonic nappes, subsidence into the water, thrusting, folding, volcanic activity and K/T bolide impact-related activity make comparisons of present day (or at least post-Eocene) Antillean islands and their Mesozoic precursors meaningless.
There are many methods that have been proposed for selecting between biogeographic hypotheses (see Crisci 2001). I adopt a cladistic biogeographic approach sensu Rosen (1978) and Nelson & Platnick (1981). This method was selected over others because it uses area cladograms that can be created from the published phylogenies reviewed here. This method was also selected over others because it is the only one that utilizes the principle of parsimony, which by minimizing assumptions finds the most efficient explanation of the evidence (Sober 1988).
The cladistic biogeographic approach assumes a shared correspondence between phylogenetic history and geological history. The relationship between these histories can be seen in congruent patterns of different taxonomic and area cladograms (cladograms with taxon names replaced by distributions) fitting a given pattern of geological history. In this method, dispersal is assumed not to explain a disjunct distribution until vicariance can be falsified (Kluge 1989; Croizat, Nelson & Rosen 1975). Vicariance is a more parsimonious interpretation than dispersal for congruent area cladograms of different taxa, because the congruence can be explained by a single event (i.e., the rifting of a continent or orogeny). The same interpretation of distributions by dispersal would require concordant dispersal in the same sequence for many diverse taxa (Fig. 2, Fig. 3). The essence of vicariance biogeography is that barriers arise secondarily to divide up species. Vicariance events, because they are tied to earth history, can only be supported by a very limited range of phylogenetic patterns. Dispersal scenarios, because they can occur without any underlying congruent process, can be claimed to support an unlimited range of phylogenetic patterns.
Area cladograms that fit a hypothesized geographical fragmentation sequence may support vicariance, or at least do not falsify it. All distribution patterns can be explained by dispersal. Dispersal scenarios therefore should not be employed unless vicariance scenarios have been falsified.
Vicariance scenarios for freshwater fishes have the following potential falsifiers: (1) the phylogenetic pattern (sequence of lineage divergence) does not follow the timing of known geological processes (i.e. the sequence of fragmentation) (Figure 3), (2) members of particular lineages are younger than hypothesized related vicariance events (3) a species of the group under study is found on either side of a supposed barrier to dispersal (4) molecular clocks or sequence divergence times reliably show that lineages have diverged after the particular vicariant events under study.
Dispersal will be the favored mechanism to explain a disjunct distribution when falsifiers of vicariance -- by adding assumptions to a vicariance hypothesis -- make dispersal a more parsimonious alternative. The possibility also exists that the current evidence is insufficient to select between alternative explanations of disjunct distributions.
Specimens were purchased from an aquarist company, Tangled Up In Cichlids or from specimens that were bought from Michigan pet stores by the author. Specimens have yet to be vouchered and officially placed in the UMMZ (Univeristy of Michigan Museum of Zoology) collections. Photographic records of the left side of the included fishes were taken before tissue samples were removed. Molecular sequences were obtained from the University of Michigan Sequencing Core after DNA purification and amplification were completed by the author. Additional sequences were obtained from GenBank. Table 1 lists the species used in this study and tissue voucher numbers and GenBank accession numbers. Outgroups included members of Labridae (wrasses), the possible sister group to Cichlidae (Streelman & Karl 1997). Closely related Indian cichlids in the genus Etroplus were included to verify monophyly of the Neotropical species.
Tissue was preserved in 70% ethanol and frozen at -20°C prior to DNA extraction. DNA was extracted from fin clips, or from muscle tissue from the right side of the specimen using Proteinase K dissolution. Tissue was then purified using a Qiagen Tissue Extraction Kit following the manufactures protocol. Polymerase Chain Reaction (PCR) was performed using Platnium Taq polymerase following the manufactures protocol to amplify about 600 base pairs (bp) of 16S ribosomal subunit and a 650 bp segment of cytochrome c oxidase subunit I (COI). The primers 16S ar-L 5'-CGCCTGTTTATCAAAACAT-3' and 16S br-H 5'-CCGGTCTGAACTCAGATCAGT-3' (Koucher et al.1989; Palumbi 1996) were used to amplify 16S. The primers LCO1490 5'-GGTCAACAAATCATAAAGATATTGG-3' and HCO2198 5'-TAAACTTCAGGGTGACCAAAAAATCA-3' from Folmer et al. (1994) were used to amplify CO1. Amplification for 16S were carried out in 30 cycles according to the following protocol denaturing for 2 minutes at 95°C, annealing at 55°C for 1 minute and an extension for 2 minutes at 72°C, with an additional terminal extension for 72°C for 10 minutes. Amplification for COI was carried out in 35 cycles to the following temperature profile: denaturing for 20 seconds at 95°C, annealing for 1 minute at 45°C and an extension for 2 minutes at 72°C, with an additional terminal extension for 72°C for 10 minutes.
Sequences were aligned (using complete alignment option) and compiled in CLUSTAL X (Thompson et al. 1994). Indels were treated as missing characters in all analyses.
Parsimony and Maximum likelihood analyses were carried out in PAUP* 4.0b3 (Swafford, 1998). Heuristic searches were carried out with 1000 replicates (random addition of taxa, ACCTRAN optimization). For maximum likelihood 100 replicate heuristic was done and the assumption set is as follows: the transition/transversions rate was set as 2:1; base frequencies were set to empirical frequencies; for the variable sites option, equal rates for all sites option was selected; starting values for all parameters was parsimony based approximations, Rogers-Swafford method was selected for starting branch lengths.
Twenty 16S rDNA sequences were obtained by the author and 22 from GenBank (Table 1). Unfortunately no cichlid COI sequences for the taxa used in this study are available on GenBank.
Figure 3 shows the strict consensus parsimony tree of the 16S data alone. Total tree length was of 856 steps. Although the Neotropical taxa are shown to be monophyletic with a bootstrap support of 80, and cichlids are monophyletic with a support of 68, there is very little resolution within the Neotropical clade. There are two possible explanations for the resolution, that either this molecule is unable to resolve this level of divergence or that the species tree is a hard polytomy. Given that the combined data set of both 16S and COI genes are better resolved, it cannot be concluded that these species relationships is an irresolvable hard polytomy.
Figure 4, shows the resulting consensus tree of a maximum parsimony heuristic analysis of 1000 bootstraps. Central and North American species form a monophyletic group that includes the Greater Antillean species. This entire clade is nested within the South American clade. Cichlidae and the Neotropical cichlids are found to be monophyletic. The Antillean cichlids do not form a clade. The Cuban Nandopsisramsedeni is found to be sister to a subclade of Mexican and Central American species and the Hispaniolan species Cichlasoma haitiensis is in the poorly resolved grouping outside of the clade that includes N. ramsedeni. The consistency index is 0.471963 and the retention index 0.567878.
Figure 5 shows the maximum likelihood tree, which is largely congruent with the parsimony tree. The Mexican and Central American species still form a clade nested with the South American clade. A major distinction is the position of the Greater Antillean species. Cichlasoma haitiensis is recovered as sister to C. oblongus a Guatemalan species; Nandopsis ramsedeni is recovered in a similar position to where it was in the parsimony analysis as sister to a larger clade of Central and Mexican cichlids.
Unfortunately due to the disparity of where the Antillean cichlids were recovered in the likelihood and parsimony analyses, little conclusively can be said in regards to their biogeography. Both analyses (likelihood and parsimony) found that the Antillean cichlids do not form a clade, so it can be stated with certainty that these cichlids are derived from different lineages. The Antillean fauna therefore did not arise from a single ancestral species dispersing across a marine environment or landbridge, or from one species dispersing during a vicariant event. At least two separate dispersal events or ancestors are necessary to explain these data.
Since both analyses (Figure 4 and 5) find the Antillean cichlids nested within the Central American clade, these analyses find evidence to falsify the GAARlandia hypothesis of Itturalde and MacPhee (1999). Evidence to support this hypothesis would have been to recover the Antillean cichlids as sister to some South American clades instead of nested within the Mexican-Central American clade.
With the Central American clade nested within the South American clade and the Neotropical clade recovered as monophyletic in this and other analyses (Sparks 2001; Farias 2001; Streelman et al. 1998; Zardoya 1996) it is apparent that South American taxa dispersed to Central America to colonize habitats there -- fitting the hypotheses of Myers (1938, 1966) and Bussing (1985). Although there is evidence against dispersal across a South American landbridge of cichlids to give rise to the Antillean cichlids, there is no evidence supporting or falsifying the Rosen coalescence hypothesis or overwater dispersal being the mechanism for cichlids getting from Central America to the Greater Antilles. Both scenarios are still plausible. However, the cladistic biogeographic approach uses a simplicity criterion to favor one of these scenarios over the other.
Dispersal from Central America across the marine environment given the topologies of these analyses would require that the species that dispersed either speciated on these islands from at least two separate dispersal events, or that the species that currently are on the islands had their source mainland populations subsequently extirpated. There is no fossil record of any Cuban or Hispaniolan species on Central America or vice versa. Interestingly the relationship of the Antillean cichlids rules out successful dispersal events from one island to the other. Not only are there not shared populations of species between the islands, but also the species on Cuba and Hispaniola are not even closely related.
That the Antillean cichlids are not monophyletic still fits the Rosen hypothesis of drift vicariance. According to Rosen (1975, 1985) Jamaica, the Caymans, southwest Hispaniola, and western Cuba were connected to different parts of southern and eastern parts of the Yucatan and northern parts of Central America. This scenario would entail different cichlid species giving rise to the separated lineages in Cuba and Hispaniola.
The simplest scenario is that of Rosen (1975, 1985) rather than of a dispersalist scenario of overwater dispersal. Given the dispersalist view point cichlids on the Antilles would have had to disperse the hundreds of kilometers from Central America to Hispaniola and Cuba in two separate dispersal events, but not the 77 kilometers between these islands. The vicariance approach requires a unique event, to explain the current Antillean distributions of cichlids, atherinids (silversides), poeciliids and other Cyprinodontiformes, synbranchid eels and gars, since they all have largely congruent cladograms (Rosen 1975; Rauchenberger 1988). The dispersalist scenario assumes each of these lineages independently dispersed from Central America to these islands.
There are several interesting non-biogeographic outcomes recovered in the tree topologies that will interest aquarists. Cichlasoma (= Archocentrus) citrenellum is perhaps the most popular aquarium trade cichlid. Three individuals of this species were samples in the analyses. C. citrenellum10 (as labeled in the trees and Table 1) is from a pet store, and has presumable been in the aquarium trade for many generations leaving it susceptible to hybridization with other captive bred species. C. citrenellum1 is an F1 generation from wild caught individuals, and Archocentrus citrenellumAF0490 is a GenBank sequence from this same species reported with its synonomized generic name. The later two individuals are identical but C. citrenellum10 is recovered as forming a sister relationship with Amphillophus lyonsi. Supporting that at least some aquarium populations of this species, sold as the Midas cichlid for its golden color, are hybrids of other species. Another popular aquarium population known as the blue dempsey Nandopsis octofasciatus has been reported in the aquarium literature as a hybrid of two distantly related taxa and not related to the common dempsey Cichlasoma octofasciatus. In these analyses the blue dempsey is found to be identical to individuals reported in GenBank and other populations of dempsey supporting the idea that this population is simply a color morph of the common wild population.
Clearly the other Antillean species need to be sampled, as should the COI data from those species from which only 16S data is sampled. Finding that the additional two species from the islands are not related to the two species sampled here would favor a dispersalist scenario over a vicariant one. A vicariance scenario would predict that the ancestor of the species on each island arrived there as part of a single vicariant event and speciated on that islands. A morphological analysis would be useful because the fossil species Cichlasoma woodringi should be included in these analyses, and a morphological study would act as a congruent and independent test of the congruent molecular topologies found here.

The seasonality of cholera remains a mystery, it exhibits robust regularity but with important geographic variation. Furthermore, its environmental drivers are poorly defined because the seasonal pattern varies in space, with different lags relative to rainfall, one major potential driver. For example, two peaks per year are the typical pattern described for cholera in Bangladesh and former Bengal, with a decline in the summer during the monsoons, but only one peak is present in other regions of former British India and current Brazil, which coincides with the rainy season (see review by Pascual et al., 2002; and Codeço, 2001).
A better understanding of cholera's seasonality is key to identify the regional mechanisms behind the described effect of the El Niño Southern Oscillation (ENSO) (Pascual et al., 2000; Koelle and Pascual, 2004). It is also fundamental to build scenarios for cholera with global change. Both climate variability (ENSO) and climate change are likely to act on infectious diseases through the modulation of the seasonal cycle and the crossing of environmental thresholds (Pascual and Dobson, 2005).
Studies in volunteers confirmed that the ingestion of a dosage between 107 and 1011 bacteria, depending on the method (for example, neutralizing stomach acidity, or with different foods), develop in an infection (Kaper et al., 1995); consequently, bacteria density is considered a major indicator of potential outbreaks. Moreover, the existence of two possible routes of transmission for cholera includes rainfall in the seasonal cycle of cholera cases. Primary transmission presumably occurs from a reservoir of the pathogen Vibrio cholerae in the aquatic environment. Brackish water and estuaries, has been shown to provide adequate environmental conditions for the bacterium to survive outside the human host (Colwell et al., 1977). Secondary or "human-to-human" transmission occurs via the ingestion of fecally contaminated water, and some times through food (Glass et al., 1991). The relative importance of these two routes of transmission is highly debated; however, if a strong feedback exists from infected hosts to the natural environments, the distinction between the two is blurred.
Based on these two transmission routes, Dobson et al. (in prep.) have previously proposed the following mechanisms behind the bimodal seasonal pattern of cholera in Bangladesh. The first peak occurs in the spring, during the dry season when temperature warms up, because the bacterium thrives in the environment where it is also more highly concentrated, and humans' interaction with water bodies increases (i.e. primary transmission). The monsoon leads to a decline in cholera in the summer, as heavy rainfall dilutes the concentration of the pathogen in the environment and both salinity and pH, favorable conditions for the bacteria, decay. This dilution effect of rainfall represents a negative influence. However, a positive effect would follow with an increase in cholera as humans concentrate in the flooded landscape and existing sanitary conditions break down (i.e. secondary transmission).
This seasonal model provides predictions for both endemic and epidemic areas. In endemic regions, cholera should exhibit a negative association with rainfall at zero lag (dilution effect) and a positive correlation at positive lags reflecting the increase in secondary transmission after the rains. However, in regions with long and sustained periods of rainfall, and consequently with low concentration of the pathogen in the aquatic reservoirs, there should be an increase in the local extinction of the disease. Hence, frequent fade-outs and an erratic behavior should be favored in regions with lower human populations.
To examine these predictions, we analyze the association of cholera and rainfall in space and time, investigating also the notion of a Critical Community Size for cholera (Keeling, 1997).
The predictions conceived from the hypothesis about the temporal dynamics of primary and secondary transmission can be evaluated studying historical records of cholera disease.
The area studied was the region of India known as Madras Presidency. The Madras Presidency was a
For each district, monthly cholera mortality data were collected from January 1892 until December 1940 and, during the same period, information about population size for some of the districts was obtained. In addition, from January 1901 to December 1970, several meteorological stations located in the region collected daily rainfall data. A monthly estimation of rainfall during that period was calculated averaging the stations and taking in consideration the historical borders of the districts. Because of the partial overlapping, analyses considering both data only included the period from January 1901 to December 1940. On the other hand, whenever cholera mortality or rainfall was analyzed independently from each other, the full time series was used (1892-1940 and 1901-1970 respectively).
A spatial correlogram (Bailey and Gatrell, 1995, Fortin et al., 2002) was performed to detect up to which amplitude, or distance, the area of influence of each district extends when cholera mortality is the variable under study. With the distance obtained (200 Km.), proximity matrices, that determinate which districts are considered as neighbors, were defined. Once that a neighborhood was defined spatial autocorrelation analyses were performed considering both cholera and rainfall dataset. The well-known (global) spatial autocorrelation index, Moran's I (Cliff and Ord, 1973), and its modified version, which incorporated the computation for local variations, Local Indicators of Spatial Association (LISA) (Anselin, 1995, Fortin et al., 2002), was obtained for the mean values of both time series.
In order to study the interactions between cholera and rainfall the coefficient of correlation between the time series was calculated including a lag between the series ranging from 0 to 12 months (with rainfall preceding cholera). Then, both global and local spatial autocorrelation indexes, using the coefficients of correlation as input, were calculated. Whereas Moran's I evaluates the degree of global clustering, LISA index permits local variations and hence the identification of "hot-spots" and "cold-spots" or clusters where high and low values for the interaction cholera-rainfall were observed is possible.
Spatial autocorrelation shows a static picture of the distribution of cholera, however, dynamical measures are necessary to have a complete perspective. Two of such measures were considered here: the Critical Community Size and the seasonal variability of rainfall. The Critical Community Size (CCS), the population size below which a disease dies out in the troughs between epidemics, reflects the dynamics underlying outbreaks (Keeling, 1997). Hence, a qualitative measure of the CCS was obtained for the districts were population data was available. For this analysis, a period of at least two consecutive months without mortality was considered a die out of the disease, known as fadeout. In addition, the spatial autocorrelation of the amount of fade-outs also was analyzed. Finally, the incorporation of the dynamic pattern manifested by rainfall fulfills the previous analysis. All the districts exhibited a strong peak in rainfall due to the so-called southwest monsoon (for a detailed explanation see Krishnamurthy and Kinter III, 2002). Some districts exhibit high precipitation values can last for even 6 months, and after such a long rain season the rainfall diminish to irrelevant levels during the rest of the year, whereas in others districts the monsoon effect is typically shorter (3 or 4 months) and a second peak of rainfall occurs within an interval of few months. Exceptions to this second peak are the districts on the Arabian Sea. Determining how many times and for how long the monthly-accumulated rainfall exceeded a threshold value-fixed at a value of 25% over the expected rainfall per district-allowed to discriminate districts with two rainfall peaks per year from those with only one. This characterization of districts was later analyzed considering the variability of cholera observed in conjunction with the rainfall patterns.
The methods described in the previous section provide evidence to describe the variability manifested in rainfall, mortality due to cholera, and their interconnection in light of the situations expected by our hypotheses.
The low values obtained for the spatial autocorrelation, measured by the global index Moran's I, indicate that there is not significant spatial clustering when both cholera mortality and rainfall are independently considered; however, when Moran's I is evaluated for the correlation between them, the index reaches significance (Table 1). Hence, a notable spatial association emerged when the interconnection between cholera and rainfall was explored.
Considering the locality for cholera mortality, the results provided by the LISA index delimited one small-size cluster (districts number 17, 24 and 26) with high values in the central southern part of the region under analysis, and a second cluster close to the first, also small in size (districts 15 and 18) and with low values compared to that in the high-value cluster. In contrast, for rainfall, a medium-size cluster (districts number 4, 5, 11 and 14) of low values appears in the central northern region. As well as the global clustering changes when the correlation between cholera and rainfall is analyzed, the pattern exhibited locally is also dramatically different (Fig. 2): one big-size cluster with high values is formed in the northeast region (districts 7, 8, 9, 10, 11 and 23, denoted as HIGH-HIGH in the figure); a second big-size cluster with low values is found in the central southern area (districts 15, 16, 17, 18, 19, 21, 24, 25 and 26, LOW-LOW); and three one-district clusters with relative low values, respecting their neighborhood, are also delimited (districts 12, 13 and 20). This pattern could be read as follows: In the northeast area, the correlation between rainfall and cholera without considering any temporal lag between the series is reach positive significant values, whereas in the central southern area the values are negative. When the local clustering evaluated with LISA index is studied introducing time lags between cholera and rainfall time series, with rainfall preceding cholera, a similar pattern is obtained when the lag is 10, 11 and 12 months, whereas the pattern is inverted-negative correlation in the northeast region and positive correlation in the central southern area -- for values ranging from 3 to 7, and finally the pattern is not clear for 1, 2, 8 and 9 months (Fig. 3 shows some of the lags).
Considering the dynamic aspects of this study, the CSS plotted in Figure 4 shows that districts with higher density have a lower number of fadeouts, while in the less dense districts the presence of the disease is frequently interrupted. It is very important to notice the high variability present in the dataset, but in these low populated districts, cholera can be seen as a disease with irregular outbreaks, whereas in the former districts cholera exhibits an endemic behavior. The Moran's I value considering the amount of fadeouts for each district, Table 1, indicates that there is not spatial clustering. Local clustering analysis considering the amount of fadeouts does not reach statistical significance; however, mapping the amount of fadeouts more fadeouts occurs in the northern districts (Fig. 5). A similar geographical pattern is obtained when the duration of fadeouts is considered, with the northern districts having longer fadeouts than the southern ones.
Finally, a classification considering the mean annual duration of the rain season determined three different regions (Fig. 5): a northern area with long rain season (districts 6, 7, 8, 9, 10, 13, 15, 20 and 23); a Central area with a moderate length of rainfall (districts 1, 2, 3, 4, 5, 11, 12, 14, 16, 19 and 21); and a southern region with two short wet season (districts 17, 18, 22, 24, 25 and 26).
The results obtained in this work allow splitting the Madras Presidency into two main regions, Northeastern and Southern, with different cholera seasonality and different patterns of association between cholera and rainfall. In particular, the southern region exhibits a pattern which is similar with the one described in the literature for Bangladesh (Bouma and Pascual, 2001), with the whole seasonal pattern shifted in time in accordance with the earlier dominant monsoon season. This seasonal pattern is then characteristic of "endemic" regions, with regular and persistent infection, and contrasts with the stochastic nature of "epidemic" regions with only one sporadic peak coincident with rains and recurrent fadeouts.
The northeastern region comprehends the following districts: Godivari East (7), Godivari West (8), Guntur (9), Kistna (10), Kurnool (11), and Vizgapatam (23). In this area, a positive correlation between cholera and rainfall is shown and in general, one epidemic peak of cholera mortality per year is presented. Moreover, these districts display longer and more frequent fadeouts, suggesting a non-permanent presence of the disease.
The southern region including the districts of Nilgiris (15), North Arcot (16), Ramnad (17), Salem (18), South Arcot (19),Trichinopoly (24), Coimbatore (25), and Madua (26). In this region, a negative association between cholera and rainfall is observed and two peaks of cholera mortality are commonly shown. Also in this region, shorter and infrequent (sporadic) fadeouts are observed, probably indicating a permanent presence of the disease.
The remaining districts (numbers 1, 2, 3, 4, 5, 6, 12, 13, 14, 20, 21, 22 and 23 in Fig. 1), located in the central and west regions exhibited an intermediate behavior between the ones presented in the two regions described above.
The bimodal Southern pattern is consistent with the predictions of our seasonal model (Dobson et al., in prep). This result provides the first quantitative evidence for both a positive and negative influence of rainfall on the seasonality of cholera, through its dilution effect on the pathogen and its enhancing effect of 'human-to-human' or secondary transmission. While the first effect has been described in the literature, the second is novel for the bimodal cholera pattern.
The unimodal monsoonal Northern pattern combined with its stochastic nature suggests that in places where secondary transmission cannot be sustained over time, an environmental reservoir of pathogenic Vibrio cholerae is not effectively maintained. Thus this human feedback from infected individuals to aquatic reservoirs appears critical to sustain the so-called primary transmission, which underscores the importance of secondary transmission itself. Sustained periods of rain further dilute the pathogen in aquatic reservoirs and epidemics occur during the rainy season, presumably through immigration of infected individuals and the consequent secondary route of transmission.
Traditionally, the explanation for the occurrence of outbreaks after rainfall periods only considers the environment driven infection processes and basically states that moderate rainfalls are able to create optimal (or quasi optimal) environmental conditions for the spread of Vibrio cholerae (Lipp et al., 2002). The findings presented here supported the hypothesis that a long wet season is able to create a dilution effect avoiding the bacteria to anchor in the environment (Pascual et. al, 2002), but due to the aggregation of people, probably because of floods, which disproportionally raises the human-to-human oral-fecal mechanism of contagion and drastically reduces the quality of sanitary conditions, the disease is able to settle when is introduced from an endemic area. Consequently, rainfall plays two different roles concerning cholera dynamics: short wet seasons favor cholera leading to an environment driven disease, whereas long wet seasons entail an infectious dynamics.
In conclusion, the dynamics of cholera can be defined by two very different regions inside the Madras Presidency: the southern endemic region and the northeast region that exhibits an epidemic behavior. A possible explanation for the re-emergence of the disease in epidemic zones can be the migration of infective agents (food, water or people) from the endemic zones. Although more information is necessary to evaluate explicitly such scenario, our findings may be used to design health care polices in order to avoid massive mortality situations.

The palmnut vulture (Gypohierax angolensis) is a monotypic species found along the coast, estuaries and rivers of sub-Saharan Africa in areas where oil and raffia palms also occur. This vulture is relatively common but not abundant in Mozambique and rare in northern Zululand, Botswana and Zimbabwe; it is considered South Africa's rarest breeding bird. The diet of Gypohierax largely consists of palm fruits, but also includes many fish, crabs, snails and other small animals, as well as other birds. (Ginn et al., 1989)
An alternative name of Gypohierax is the vulturine fish eagle (Ginn et al., 1989), reflecting its many similarities, both dietary and morphological (especially talon structure), to the sea and fish eagles (genus Haliaeetus). The phylogenetic placement and evolutionary history of Gypohierax is unclear. Brown and Amadon (1968) included Gypohierax as a member of a monophyletic clade of Old world vultures, but also suggested a sister relationship between the sea eagles and Gypohierax . Jollie (1977), Suschkin (1899), and Brown and Amadon (1968) note that Gypohierax resembles the Egyptian vulture (Neophron) and may be a member of a monophyletic clade of all Old World vultures (including Neophron). Holdaway (1994), in a phylogenetic analysis of osteological characters, finds support for a monophyletic clade of vultures where the palmnut vulture is the earliest diverging vulture.
Several phylogenetic analyses based on mitochondrial DNA and morphological traits support paraphyly of the Old World vultures (Seibold and Helbig, 1995; Mundy et al., 1992). Two clades are identified: the Gyps-Aegypius-Necrosyrtes clade includes the genera of its name as well as Torgos, Trigonoceps, and Sarcogyps, and is sister to the Snake eagles; and, the Gypaetus-Neophron clade, is sister to Pernis apivorus, including only the two genera for which it is named. Gypohierax is not included in these analyses, but Seibold and Helbig propose that it most likely represents a third independent evolutionary line separate from the other Old World vulture clades.
In summary, it has been proposed that Gypohierax (1) forms a clade independent from other vulture lineages, (2) is sister to the sea eagles (Haliaeetus), (3) is a member of a monophyletic clade of Old World vultures, and (4) is most closely related to the Egyptian vulture (Neophron). The Old World vultures have been proposed to be (1) a monophyletic clade, (2) two separate distinct monophyletic clades (overall polyphyly), and (3) three separate distinct monophyletic clades (overall polyphyly).
DNA was extracted from tissue and feather samples using the Qiagen DNEasy Extraction kit. An addition of dithiothreitol was used for samples from feathers. Polymerase chain reaction (PCR) was carried out using two primers pairs for cytochrome B (CytB; Sorenson et al. 1999). PCR and sequencing was done following protocols in Mindell et al. (1997). Additional CytB sequences for analyses were obtained from GenBank (Aegypius monachus, gi 1050567; Trigonoceps occipitalis, gi 1050710; Sarcogyps calvus, gi 1050699, Gypaetus barbatus, gi 1050621; and Neophron percnopterus, gi 1050666).
Conceptually translated protein sequences were aligned by eye and this alignment was imposed on the nucleotides. Phylogenetic analyses were conducted using parsimony and maximum likelihood (ML) criteria for the dataset. For parsimony analysis, a bootstrap analysis was done using Winclada software (Nixon, 2002), 1000 replications with 100 search reps and ten starting trees per rep. A Bayesian inference (BI) approach, which is related to ML analyses, (Mau et al., 1999; Yang and Rannala) was performed with Metropolis-coupled Markov chain Monte Carlo, or (MC)3, to approximate the posterior probabilities (PP) of the trees in MrBayes 2.1 (Huelsenbeck and Ronquist). Bayesian inference has advantages over other methods of phylogenetic inference in interpretation of results, consistency (Wilcox et al.) and computational speed (Larget and Simon); however, as always, reliability of the results depends on appropriateness of the model, and some simulations have demonstrated artifactually high PP support values (Suzuki et al., 2002). Base frequencies and gamma distribution (with eight rate categories) were estimated each run. A general time reversible model was used, which includes six substitution types. The search was run twice, starting from random trees with four simultaneous Markov chains, sampling every 50 generations for 1,000,000 generations. The proportion of searches in which any given node (set of relationships) is found during the chain is an approximation of its PP, and provides an indication of support for that node based on the dataset.
The sequence data set of 1017 nucleotides from 27 species aligned with no indels or stop codons. Of the 1017 nucleotide positions, 458 were variable and 339 were parsimony informative.
Each amino acid is coded for by up to six different codons (synonymous codons). The relative frequency of each synonymous codon should be equal to that of the its related synonymous codons if none of the codons are favored by selection or mutation. However, many organisms have been found to have biases in synonymous codon usage (Sharp et al., 1988). Codon bias was also found in this data set of Accipitridae for most amino acids (table 1). The largest biases were found in valine (V) and serine (S), where one codon was at least three times more frequent than expected. Synonymous substitutions (10815 occurrences) were observed ten times more often than nonsynonymous substitutions (1060 occurrences).
The parsimony analysis shows polyphyly of the Old World vultures based on the cyt b dataset (Figure 1). Sarcogyps, Trigonoceps and Aegypius form a sister clade to Necrosyrtes and Gyps. There is strong support for the existence of two sister clades (95 boostrap value), but weak support for the placement of species within these clades. Circaetus remains part of an unresolved polytomy not within this derived vulture clade. The remaining three vultures Neophron, Gypaetus and Gyophierax form a much earlier diverging unresolved polytomy.
The Bayesian analysis shows polyphyly of the Old World vultures based on the cyt b dataset (Figure 2). Neophron, Gypaetus and Gyophierax form an early diverging monophyletic clade sister to Pernis, while the other Old World vultures form a much more derived monophyletic clade. Support for the early diverging clade of vultures is moderate (74 PP). The more derived vulture clade consists of two sister groups, one formed by Trigonoceps, Aegypius and Sarcogyps, and the other including Gyps, and Necrosyrtes. Support for the monophyly of this clade is high (95 PP), as is support for the existence of two clades within the derived vulture clade (100 PP) Circaetus is sister to the derived clade of vultures with moderate support (85 PP).
Both the Bayesian and ML analyses support polyphyly of the Old World vultures based on this cyt b dataset. The analyses agree that the more derived vultures form a monophyletic clade consisting of two sister groups as found by Seibold and Helbig (1995). The first group includes Sarcogyps as the earliest diverging species, then Trigonoceps and finally Aegypius as the most derived species in this group. The second group has Necrosyrtes as sister to the Gyps pecies. These results coincide for the most part with the anlyses done by Seibold and Helbig, however, they support a sister relationship between Necrosyrtes and Gyps not found by Seibold and Helbig.
The Bayesian analysis suggests that Gypohierax , Neophron and Gypaetus form a monophyletic clade of early diverging vultures sister to a species long-supposed to be early diverging: Pernis. The sister relationship between Gypaetus and Neophron was supported by Seibold and Helbig's analysis and is reflected in shared morphological, embryonic development, vocalizations and feeding behavior characteristics of the two species. Although Neophron and Necrosyrtes have been considered closely related, they are best considered in terms of convergence according to this analysis (White, 1950).
The placement of Circaetus although unresolved in the ML analysis, is potentially sister to the derived vulture group as it is shown in the Bayesian analysis. This placement was proposed by Mundy et al. {, 1992}, although not supported by Seibold and Helbig's cyt b analysis. Mundy et al. also suggested a sister relationship for Circaetus and Terathopius. Both genera specialize on snakes as prey and have morphological adaptations for this lifestyle. Given the convergence seen between Neophron and Necrosyrtes in this analysis, it remains uncertain if Circaetus and Terathopius are sister or convergent species.
The placement of Gypohierax is supported strongly by the Bayesian analysis, as sister to the Gypaetus-Neophron early diverging clade of vultures. A sister relationship between Gypohierax and Neophron was suggested by Brown and Amadon (1968), but has not been tested before with molecular evidence. Osteological evidence suggests a sister relationship between Gypohierax , and Aegypius, Gyps, and Necrosyrtes that was not supported with this cyt b dataset (Holdaway, 1994). If Gyophierax is truly sister to Gypaetus and Neophron, as suggested by this dataset, this relationship may help identify morphological characters that are convergent in many of the Old World vultures which have led to misplacement and confusion in previous phylogenetic analyses of vultures.

As Platt (1964) argued, hypothesis-testing is essential to rapid scientific progress, with dispositive studies guiding it efficiently along a "logical tree" akin to a chemist's flow chart or "conditional computer program." Moreover, this "strong inference" approach attains its greatest efficiency when multiple alternative hypotheses are tested simultaneously, enabling researchers to quickly eliminate fruitless branches of the logical tree (Platt 1964).
Now that many ecologists rely upon computer programming to generate models and run simulations, the question arises of whether such experience in constructing parsimonious sequences of conditional propositions has promoted the use by these scientists of a hypothesis-testing approach in their empirical ecological investigations. Furthermore, because scientific investigation is strongly influenced by the culture in which it is embedded (Kuhn 1970), perhaps even ecologists who do not themselves engage in programming-aided theoretical studies would nevertheless display heightened hypothesis-testing if they are affiliated with institutions that emphasize computer reliant theoretical research.
Based upon these considerations, I propose the Programming Promotes Hypothesis-Testing Hypothesis (PPHTH), which states that ecologists occurring in habitats in which computer-aided theoretical modeling is emphasized should show increased frequency, rigor, and efficiency of hypothesis testing when compared with ecologists from other milieus. In the present paper, as a test of this hypothesis, I compare empirical ecological research produced at an institution that places an extremely high emphasis upon computer-assisted, theoretical studies with empirical ecological studies emanating from a more broad-based institution.
For the computer-rich environment, I chose the University of Chicago's Department of Ecology and Evolution (Chicago), home to a self-styled "naturalist" publication in which the most frequently studied species are named "x" and "y", and in which a computer monitor passes for an observation blind. Moreover, this university served as Platt's intellectual home, as he was a professor there (Platt 1964) and, as a whole, it has historically been steeped in theory, from Fermi in physics to Friedman in finance. I compared research of ecologists who are Chicago professors with research produced by ecologists who are professors of the Department of Ecology and Evolutionary Biology of the University of Michigan at Ann Arbor ( Michigan).
The latter institution, while certainly strong in ecological theory, does not, unlike the former, place such a strong emphasis on computer programming that its graduate school application largely comprises an assessment of such skills (Rosenthal, pers. obs.). In other respects, however, the institutions are similar, especially in that they both have highly prominent ecological research programs.
For Chicago, I visited the Department of Ecology and Evolution web page listing faculty research areas (http://pondside.uchicago.edu/ecol-evol/research/), identifying as ecologists all those professors listed under the Ecology heading. For Michigan, my search similarly started at the Department of Ecology and Evolutionary Biology website listing faculty research interests (http://www.eeb.lsa.umich.edu/eebfaculty.asp). However, because this site did not have a separate list for ecology or ecologists, I identified as ecologists those professors for whom the word ecology appeared in the description of their field of interest. For both sites, sampling was conducted in February, 2002.
After the ecologists were identified, ecological publications were selected in the following manner: I visited each ecologist's individual web page, and went through his or her posted list of publications in reverse chronological order (with publication titles within the same year examined in alphabetical order), up to and including 1997, if necessary, until one suitable publication title was encountered. Publications were excluded if they were books rather than journal articles; or if they were not available either online or at the Shapiro Science Library; or if their titles reflected material that was either not ecological in nature (e.g., single species ethological studies); or clearly not explanatory in nature (e.g., an account of fish recruitment rates) or purely theoretical, without an empirical component. For some of the ecologists, no suitable publication was found, and therefore these individuals were not represented in the study.
Because of the frequent changes in institutional affiliations prevalent in academia, it is quite likely that some of the publications selected in the above manner were written, submitted or published prior to the ecologist's affiliation with their current institution. However, I decided that this should not preclude the inclusion of such publications in the current study, because: 1) their current institutions presumably selected faculty who reflected their own institutional cultures; and 2) the faculty members presumably chose institutions that encouraged their own approaches. Therefore, the different degrees of emphasis on theoretical approaches, and the predicted consequences for hypothesis testing should be reflected even in such prior publications.
Co-authorship of papers was not a bar to their inclusion in the study, as elimination of studies having multiple authors would have reduced the number of reviewed articles to an unworkably small number (quite close to zero). Moreover, as co-authored studies were included from both institutions, this approach should not affect the outcome of a comparison between the two.
For the purposes of this study, the hypothesis testing process was defined as containing the following four essential elements: 1) proposal of an a priori hypothesis (a posteriori hypotheses being excluded here because, by definition, they are not tested within the published studies within which they appear); 2) formulation of at least one factual prediction predicated upon this hypothesis, such that results contrary to the prediction would necessitate rejection of the hypothesis; 3) performance of a test to determine whether the prediction is borne out; and 4) comparison of the test's results to those predicted by the hypothesis, yielding a conclusion that the hypothesis has accordingly been rejected or supported.
In operationalizing the four elements outlined above, I emphasized substance, rather than form, such that:
The hypothesis-testing activity in the two populations of publications was assessed with respect to frequency, rigor, and efficiency as follows:
For each publication, the presence or absence of each of the four functional elements was recorded. Only those publications having all four present were classified as hypothesis-testing ("HT"). I then computed the proportion of the total that were HT. If PPHTH were to obtain, the HT proportion should be higher in the Chicago than the Michigan sample.
Each publication was examined to determine if it purported to evaluate hypotheses (i.e., by explicitly making this assertion). For each institution, the proportion of such publications that also qualified as HT was then computed. This approach was undertaken in order to evaluate the relative success of the two institutions in achieving hypothesis testing when explicitly attempted. If PPHTH were to obtain, the proportion of "rigorous" studies should be higher in the Chicago than the Michigan sample.
Each HT publication was examined to determine if it tested multiple alternative hypotheses ("MAHs") was computed. For a publication to be designated as testing MAHs, it would need to have two or more non-null a priori hypotheses (as defined above) with each of them tested pursuant to the criteria listed above. I then computed the proportion of the HT publications that tested MAHs. If PPHTH were to obtain, the MAH proportion should be higher for the Chicago than the Michigan studies.
The individual evaluations of the selected studies are presented in Table 1.
The group comparisons for hypothesis-testing assays are displayed in Table 2. As Table 2 shows, whereas the values for hypothesis-testing frequency are the same for the two samples, those for both rigor and efficiency are higher in the Chicago sample.
The results of this study provide limited support, at most, for PPHTH. Although there was a higher efficiency value for the Chicago publications, this was simply due to the fact that its only hypothesis-testing article evaluated multiple alternative hypotheses. It would be an over-generalization-based solely upon this single episode of MAH evaluation -- to characterize the Chicago ecologists, as a whole, as more efficient in their hypothesis-testing.
On the other hand, the difference in the values for the rigor index is more likely to reflect an actual disparity between the ways in which ecological research is framed at the two institutions. Although at both departments only one out of the five reviewed studies met all the hypothesis-testing criteria, at Michigan a majority (3 of 5) of publications asserted that they were evaluating hypotheses. At Chicago, in contrast, the only study to mention hypotheses was the one that actually tested them. There, 3 of the remaining 4 studies (Dwyer et al. 1998; Pfister 1998; Wootton 2001) focused instead upon the parameterization of mathematical models. Thus, the institutional difference in rigor values might be due to the Michigan ecologists more frequently attempting to fit their research into a hypothesis-testing framework (whether justified or not) -- whereas the bulk of Chicago research addresses simulations and other mathematical manipulations.
The one prediction of PPHTH that clearly was not borne out was that of a higher frequency of hypothesis-testing in the Chicago publications. Instead, the frequency was identically low for both institutions, due perhaps to Chicago ecologists largely eschewing hypothesis-testing in favor of more purely theoretical investigations, while Michigan ecologists tackled topics that are of such scale that they pose practical obstacles to adequate testing (e.g., Pascual et al. 2000 ; Vandermeer et al. 2000).
In conclusion, the results of the present study suggest that although a programming-oriented environment does not increase the frequency of hypothesis-testing, it does reduce the frequency with which it is claimed, thereby increasing its apparent rigor. Additionally, this environment might increase the efficiency of hypothesis-testing, but this effect is far from clear.

The goal of this lab was to acquaint ourselves with self compacting concrete (SCC), its properties, and testing methods used in the design and mixing of SCC. Given a given design mix we made our own SCC. Moreover, we became familiar with a number of tests designed specifically for use with fresh SCC. We also became familiar with the method of testing to find the compressive strength of cylindrical hardened concrete specimens.
Mix proportions were provided by the instructor, thus no design procedure was required.
There are no American standards for self compacting concrete, so we mixed our concrete per instructor's directions. First all coarse aggregate was placed in the mixer with about 1/3 of the water. Then sand was added to the mix and all was allowed to become saturated. Next we added the concrete and fly ash to the mix slowly, and allowed that to combine. A small portion of the superplasticizer was then added to the remaining water, and the majority of water was added to the mixer. Finally, the remaining superplasticizer was added to the water, and the water added to the concrete mix. This was allowed to mix for a few minutes. Then we let it set for 5 minutes, and mixed for a little longer. Once the concrete mix was uniformly blended, fresh concrete tests were performed and samples of SCC were placed into molds.
The unit weight of the fresh concrete can be determined following ASTM C-138. First, the weight and volume of a chosen cylinder must be recorded. The cylinder is then filled with fresh SCC, however it is not rodded due to its self compacting nature. Once the container is full, the excess concrete must be struck off with the rod. The filled cylinder is then weighed, and the unit weight of the concrete can thus be calculated.
The air content test can be performed following the ASTM C-231 procedure. This process requires a special apparatus made up of a measuring bowl, top section, pressure gage, and air pump. First the bowl is wetted and filled completely with SCC, which does not need to be rodded due to its self compacting nature. Excess concrete should be struck off, and the seal of the bowl should be cleaned. The top section is then attached to the bowl and locked in place. The petcocks must be in the open position on each side of the top section, and water must be slowly added to one side until it begins to spill out the other. The petcocks must both then be closed. Begin pumping air into the chamber until the reading is 3 tick marks past the 0 mark. Next hit the pressure release tab on top, allowing all air to escape. The final reading on the pressure gage represents the air content in the SCC. Repeat all steps by releasing the petcocks on the base once again and filling with water. Once a consistent reading can be taken, that is recorded as the percent air content in the SCC
This test requires a standard cone from the ASTM C-143 slump test. Also required is a strong nonabsorbent base at least 700mm square with a 500mm diameter circle drawn on the center. A stopwatch and ruler will also be needed. Begin by placing the cone small opening down on the center of the board. Then fill the cone completely with SCC in a timely fashion. Simultaneously start the stop watch and lift the cone up from the board at a constant speed. Stop the clock when the SCC reaches the drawn circle on the board. The recorded time is generally acceptable between 3-7 seconds. Finally, measure the diameter of the SCC circle in two places and take the average. Acceptable values range from 650-800mm.
This test requires an apparatus developed in Japan known as a V-shaped funnel. The funnel shaped like a V consists of a trap door on the bottom, which is locked closed and is then filled with about 12 liters of SCC to the top. Place a container underneath the funnel. Be prepared with a stopwatch and begin timing from when the trap door is released, then stop time when light can be seen through the bottom of the funnel. Immediately fill the funnel back up with the same SCC, after closing the trap door once again. Allow the SCC to set in the funnel for 5 minutes and repeat the same timing procedure. Record both times, and compare. A large segregation of materials will result in a much greater time for the 5 minute test. An acceptable time for the first flow test is about 10 seconds.
This test also requires a special apparatus, developed in Japan to be used for underwater concrete. This L-shaped box assesses the flow of concrete, especially when subjected to blocking by reinforcement bars. The box consists of a vertical and horizontal section, separated by a moveable gate. With the gate closed, the vertical portion is filled with SCC and allowed to set for one minute. The gate is then lifted allowing the SCC to flow through the horizontal section past the reinforcement bars. Once flow has stopped, two readings are taken: the height of the concrete at both ends of the horizontal section. These heights are then used to calculate the slope of the SCC; the height closest to the vertical end divided by the height furthest from that end. It is suggested that the maximum value of this ratio be 0.8.
Preparing specimens can be done according to ASTM C-192. In this lab we were asked to produce three sizes of specimens: 8 small cylinders (4"x8"), 2 large cylinders (6"x12"), and 4 beams (3"x4"x12"). Each mold was first greased so as to make concrete removal go smoother at a later time. All cylinders and beams were filled completely with SCC to the top of the mold. The large advantage of SCC is that it requires no pouring of layers, rodding, or tapping of the molds. It consolidates on its own, and only the excess concrete must be struck off the top to create a surface with a clean look.
All concrete specimens were capped by placing them in a pool of hot wax, allowing the wax to harden on the ends of the concrete, creating a smooth and consistent surface. This preparation allows the compression machine to get a flat even surface on the specimen so the load can evenly be distributed throughout the cylinder. Once capped and dry, a specimen can be placed in the machine, and its dimensions entered in accordingly. The machine then begins its test, using algorithms to determine the maximum load the specimen is able of handling. This force can then be recorded as the maximum compressive stress for that specimen, keeping in mind the number of curing days the concrete has had. Multiple cylinders' tests can be averaged for a more accurate result of fc'.
Generally following all procedures as stated above, we performed our design and mixing. Our design is based off data provided by the instructor. Table 3.1 provides all the values of mix proportioning for our SCC.
The following Table 3.2 gives results from all the tests that we performed on our fresh SCC. All test procedures were followed generally. Shown in parentheses are the acceptance criteria.
The following Table 3.3 gives results from the compressive strength tests that we performed on two of our previous concrete specimens.
Comparing our test results to the standards set forth by their procedures, our mix of concrete is adequate and meets all criteria.
The addition of fly ash and superplasticizer to our mix of concrete had large influences on the fresh concrete as well as it will on our hardened specimens of concrete. Superplasticizer mainly acts as an additive that significantly increases the workability of fresh concrete. Moreover, it allows higher strength concrete with lower amounts of water, and the same strength concrete with less cement content. It has been shown that an addition of a superplasticizer will not hurt the overall durability of concrete or increase its shrinkage and creep.
Fly ash's main effect is increasing the concrete's strength over time. It can also decrease concrete's permeability keeping harmful aggressive compounds on the surface. Shrinkage can also be reduced indirectly by reducing the amount of water required in the concrete.
SCC has many advantages as well as disadvantages. Advantages include: early stripping strengths, increased productivity, design flexibility, and pumpability. Other benefits are a decrease in noise levels, time required during placement, overall production costs, and wear and tare on equipment. Disadvantages consist of: sensitive mixing, possible segregation, long setting time, touchy quality control, no ASTM standards for mix design or testing, and changing location of ingredients can alter a whole mix of SCC.
Throughout this lab we gained experience in mix proportioning self compacting concrete. Using given data we were able to design concrete to meet specifications, and take these values into the lab. We then were capable of creating the design mix and forming our own batch of SCC mix. Furthermore we were able to properly sample the concrete, as well as perform V-funnel test, slump flow test, L-box test, unit weight test, and air content test. We gained practice in preparing molds of concrete to be later used in testing. Finally, our group gained familiarity with testing cylindrical concrete specimens for their compressive strength. The results of our lab were given in this report.

You recently requested that our department conduct a series of tests on a soil found at the proposed site of a new 100' long masonry dam, as well as tests on a coarse soil to be used as a filter material. You are concerned with the classification of the soils, their suitability for this construction, and the resulting seepage loss under the dam. Our results show the coarse sand is suitable for filter material, classified by ASTM as SP-Poorly Graded Sand. The foundation soil is classified by ASTM as SC-Clayey Sand. The use of the Hazen equation to find permeability was found to be significantly inaccurate. However, using our test data for permeability, Design #1 will have seepage of 0.274 ft3/day while Design #2 will have seepage of 0.164 ft3/day. We recommend lowering these seepage values in both designs by incorporating a 75' clay blanket on the water bed of the head end of the dam.
Your letter, sent on January 25, 2006, asked for us to test two soils to be used in the construction of a new masonry dam at your Glacier Way site. You are concerned with ASTM soil classification, suitability of the coarse soil as a filter material, and the effective seepage underneath the two different dam designs, both 100' in length. It's apparent that a series of tests must be conducted to find the gradation curves and permeability of each soil. The appropriate testing has been completed, and we submit our findings in this report.
Tests were performed on two different soil samples. Both were a dry and brown sand common to this area of Michigan. The first was very fine and was found locally in the excavation site of the new dam. The second was an external sand brought in to be used as a filter, being similar to the first, however more coarse in nature. Further tests were performed on each sample to find their ASTM classifications, gradation analysis, and respective permeability.
The following tests were performed on the soil samples:
Test-422 uses the shaking sieve method well known among engineers to analyze the gradation of soils. The-422 hydrometer test uses a standard hydrometer setup and data points are taken over a 18 hour period to determine the size gradation of finer soils.
Test-2434 uses an apparatus that keeps a constant head of water while allowing water to flow through a sample of soil. Time and the volume of water flowing are used to get flow rate, and thus the coefficient of permeability for the soil.
Test-5856 is similar to-2434 and the same data is recorded, however it allows the head of the water to fall over time. Likewise, the method of calculating the coefficient of permeability varies slightly.
All procedures were followed generally, and can be referenced in the CEE 445 laboratory course pack.
Shown below in Figure 2.1 is the graph of the grain size distribution curve of each soil, with the coarse sand on the left and the fine on the right. Full data sheets of the sieve analysis can be seen in Appendix A and the data for the hydrometer test can be seen in Appendix B.
Using the sieve analysis results, Table 3.1 below shows each soil classified according to standards set by ASTM. Calculations to determine classifications can be found in Appendix D.
The US Army Corps of Engineers sets criteria on soils used as filter material:
The permeability of the fine soil can be estimated using the Hazen's Equation:
Shown below in Table 3.2 are the calculated values for the soils' permeability. Full data sheets can be referenced in Appendix C.
The calculated value of the permeability using the Hazen's Equation is shown to be pretty inaccurate compared to that of the test results. The Hazen's Equation gives a value almost 3 times larger than the actual tested value. The main reason for this being that Hazen designed the equation to be generally accurate with uniform soils in a relatively loose condition, and our soil is graded well and is far from a uniform state. Moreover, it is merely an empirical formula and can not be accurate in all cases.
Given your attached masonry dam designs with drawn flownets, we were able to compute the seepage underneath each dam design. Calculations can be found in Appendix D.
Our results indicate that both dam designs will have to large of a resulting seepage. We recommend that a 75' clay blanket be introduced on the water bed at the head of the dam. This in turn will increase the travel distance of the flow channels for water, thus lowering effective seepage of the dam.
Design #1 was found to have a higher seepage of 0.274 ft3/day than Design #2, which had a seepage of 0.164 ft3/day. The foundation soil of the dam is classified as SC-Clayey Sand. The soil proposed in Design #1 as a berm is classified as SP-Poorly Graded Sand, and meets all Army Corps of Engineers criteria of soils to be used as filter material. In conclusion, both dams are adequate for construction with the addition of a clay blanket, and all soils meet criteria set forth by the Army Corps of Engineers and ASTM.

Several tests were administered per ASTM standards on supplied samples of soil obtained from the Glacier Way dam site. These tests ultimately determined if either of two proposed dam designs were acceptable. Gradation analysis was completed from data found by sieve and hydrometer tests conducted per ASTM standards D 421-85 and D-422-63 using an ASTM 152H hydrometer. This allowed us to classify the soil per the ASTM/USCS classification system D 2487-93. We also found the coefficient of permeability using permeameter tests and compared them against the empirically derived Hazen equation. Based on the gradation and permeabilities we determined the suitability of the coarse sand as a filter for dam design 1, and also estimated seepage loss under the dam for both designs. Each dam has a slightly different configuration but will ultimately be supported by these soils, therefore the stability and resistance to washout is crucial to the success of this project. The results of these tests determine what actions must occur before construction proceeds.
The soils have been found to be clayey sand and poorly graded sand for the fine and coarse samples, respectively. The gradation of the samples meets the Army Corps of Engineers criteria for soil restraint but fails to meet allowable filter permeabilities. Therefore, either a different filter media may be evaluated or design 2 may be chosen. For both soils, the Hazen approximation for permeability is not nearly within reason to allow this shorthand estimation to be used in the field. Unacceptable design 1 has an estimated seepage rate of 32.7 ft3/day compared to an improved 19.6 ft3/day for design 2. If this is deemed excessive several recommendations have been included that will get your project underway with minimal effort. All other data may be found on the data and calculation summary page, Appendix A.
William Piper Associates is planning to build a dam at the Glacier Way dam site. Two designs are being evaluated based on their subbase soil characteristics to ensure they do not fail under the expected conditions. Before construction of the dams, soil criteria for using a filter must be met and the seepage loss under the dam must satisfactorily meet or exceed requirements. On October 24, 2006, Michigan Soil Consultants was contacted and asked to carry out these tests.
Seepage losses can determine the performance of the dam over its expected lifetime. Without proper filter characteristics and seepage flow, the dam may fail in a variety of ways including possibly washing away or not functioning to its intended degree. This may result in an unattractive, inefficient, unsafe, and possibly unusable dam. The purpose of this report is to detail our procedures and test findings while drawing conclusions and making some recommendations to ensure the success of this project.
The remainder of this document consists of test procedures, data results, conclusions, and recommendations. All preliminary data sheets and subsequent calculations have been included at the end of the document in Appendices A through I. Appendix B shows the soil gradation curves for the soils and Appendix J shows the proposed dams and corresponding flownets.
To determine if these soils have satisfactory characteristics two important quantities were found. First, gradation analysis was completed according to ASTM standards D 421-85 and D 422-63. This included a hydrometer test for the fine soil using an ASTM 152H soil hydrometer. Additionally, constant and falling head tests were performed to determine the coefficients of permeability of both soils. It is important that the permeameter tests conducted in the lab use the soil sample compacted to the degree so that the relative density matches that of the in-situ relative density of 70%. Since this could affect the results much care was taken to ensure this was accomplished. The constant head test was performed in general accordance with ASTM D 2434-68, and the falling head used general laboratory procedures. These guidelines were generally followed and deviations that occurred are noted.
The soil classifications were found to be SC clayey sand and SP poorly graded sand for the fine and coarse sands respectively. These were found based on the soil gradation curves included in Appendix B as well as the uniformity coefficient Cu (equation 1) and coefficient of gradation Cz (equation 2).
Overall, the coarse soil is not acceptable to serve as a filter medium for the fine grain soil. Therefore, only design 2 is acceptable which has a seepage rate of 19.7 ft3/day. If this is acceptable then no further design modifications are necessary. A summary of numerical data, formulas, and calculations can be found in Appendix A, with additional data record sheets following. Summaries of these results are outlined below.
Soil restraint criteria were met however permeability criteria was not met. As a result either different filter media should be considered or dam design 2 should be evaluated further. The US Army Corps of Engineers has filter suitability criteria for dam designs utilizing a coarse grain filter with finer grain soil. This criteria is divided into two categories, soil restraint and permeability
Both soil restraint criteria were met, shown below in equations 3 and 4. Since these are satisfied the filter material is satisfactory for a dam application.
Unlike soil restraint criteria, permeability criteria was not met. Equation 5 must be satisfied for the soils to be deemed acceptable. This expression has been evaluated and is shown in Appendix A. This confirms that we may not choose to build dam design 1 with this filter media.
Fine soil permeability resulted in a value of 0.000002 cm/sec and for the coarse soil it was 0.016 cm/s. This result is intuitive as a coarse grained soil like gravel should transmit more fluid than one of little voids due to small particle size diameter.
The Hazen Equation, equation 6, is an empirical derivation of permeability and does not predict our measured values within reasonable tolerances. This formula results in permeability in mm/s, which can be converted to cm/s to be compared with the measured data.
This derivation resulted with a permeability of 0.029 cm/sec for the fine soil and 7.02 cm/sec for the coarse soil. Obviously these are extremely high when compared to actual values. Therefore, this shorthand equation should not be used for any field soil checks. This may have resulted from the rather poorly graded nature of the soils.
Furthermore, our fine soil D10 value of 0.017 mm falls below the recommended range of the equation applicability, (0.1 to 3.0 mm). Other soil types such as the clay in the fine soil may have influenced the results.
We have found the seepage to be 32.7 ft3/day for dam design 1 and 19.6 ft3/day for dam design 2. These were found using flownets that have been provided by the design engineer. After converting units for the permeabilities measured previously, we can use equation 7, which is a variation of Darcy's Law to compute the seepage.
The results from the data analysis demonstrates that the soil proposed to be used at William Piper Associates Glacier Way dam site is SP clayey sand and SP poorly graded sand for the fine and coarse sands respectively. These soils do not meet criteria satisfactorily for a successful dam using design 1. Here, the materials do not satisfy the criteria for the US Army Corp of Engineers permeability requirements, despite the acceptability of soil restraint. However, design 2 does not use a filter, which eliminates this inadequacy. Additionally, this design allows less seepage than dam design 1 at only 19.7 ft3/day compared to 32.7 ft3/day, which will provide a better end product that does not promote any piping and erosion that will yield a functional dam for a long time to come. Lastly, during field operations the empirically developed Hazen's Equation should not be use for field checks due to its giant discrepancy from actual measured values. All data has been summarized in Appendix A.
If dam design 2's seepage rate of 19.7 ft3/day is considered excessive several simple remedies can be offered. We recommend adding another seepage blanket upstream of the dam, which would be the most cost effective way to lengthen the dam. This will alter the flow beneath the dam by extending its flow path, which reduces seepage. Other modifications could be made to the materials, such as grout injection into the soil to reduce permeability further, allowing less seepage beneath the dam.

From your letter dated February 5, 2007, we understand that you intend to construct a dam at your Glacier Way site. There are two designs being proposed, both 100 ft in length. The main objectives of this report are to classify the coarse and fine soils given to us, determine the suitability of the coarse soil as a filter for the fine soil, determine the permeabilities of the two soils, determine the accuracy of Hazen's Equation on the approximation of the coefficient of permeability, and to determine the seepage loss beneath each of the two dam designs that are being proposed. We have completed the required tests to determine the above objectives. The purpose of this document is to present to you our findings, conclusions, and recommendations.
The classification of the coarse soil was found to be poorly graded sand, SP. The fine soil was found to be clayey sand, SC. The coarse sand passed each of the three criteria which proved its suitability as a filter for the fine sand.
The coefficients of permeability for the coarse soil and the fine soil were found to be 8.7x10-3 cm/sec and 6.57x10-7 cm/sec respectively. It was found that this approximation was inaccurate due to key assumptions made in the formulation of this equation. With this we would recommend to not use this approximation during the calculation of further coefficients of permeability.
The seepage losses beneath dam Design 1 and dam Design 2 were estimated at 10.35 cubic ft per day and 6.21 cubit feet per day, respectively. These values are excessive by our standards and further design changes need to be made. We would recommend the addition of a seepage blanket in front of each dam design which would effectively reduce the amount of seepage loss beneath the dam and in turn ensure the stability of your proposed dam.
ASTM D 422: Standard Test Method for Particle-Size Analysis of Soils.
ASTM D 2434-68: Standard Test Method for Permeability of Granular soils (Constant Head).
Note: No ASTM standard for the falling head test performed on the fine soil.
An in house standard test was performed.
The necessary data for construction of the proposed dam at your Glacier Way Project follows. This data includes the soil classifications, filter suitability, coefficients of permeability, and the seepage losses beneath each of the two proposed dam designs.
It has been determined that the coarse sand is poorly graded sand, SP, and it has been determined that the fine sand is clayey sand, SC. These sand types were determined using the ASTM soil classification system and further calculations can be found in the appendix.
The coarse sand is a suitable filter for the fine sand. The suitability of the coarse sand as a filter is important because it shows us that the soil can properly drain without clogging of the pores in the coarse sand and without washing out through the coarse sand. So here the filter is the coarse sand diameter and the soil is the fine sand diameter values that were obtained from Table 1 above. We will be using the US Army Corps of Engineers' procedure to help determine the soils suitability as follows:
(1) and (2)
Through our testing, we were able to determine the coefficient of permeability of both the coarse soil and the fine soil provided. The following values are as follows:
(4)
The calculations and data from these tests can be found on pages 7 and 9 in the Appendix.
Hazen's Equation has been found to be inaccurate when compared to our test results of the fine soil. We can determine the permeability of the fine soil in two ways. One way is to use an approximation equation known as Hazen's Equation and the other is to find it experimentally in the lab. The following are the results of the two calculations.
As can be seen from the above approximations of the coefficient of permeability of the fine sand, Hazen's Equation is off by an order of magnitude when compared to our laboratory result. Hazen's Equation is only off by one order of magnitude which is not as far off as one would think due to the wide range of magnitudes that soils can have, yet it clearly is not as accurate as actual laboratory tests. Hazen's Equation was found through observations made during experiments for loose, uniform sands. Because of this, if we are not using loose, uniform sands, then this approximation loses its accuracy as can be seen from the results.
The seepage loss beneath both dam designs has been determined to be excessive. The seepage loss beneath dam Design 1 was found to be 10.35 cubic feet per day while the seepage loss beneath dam Design 2 was found to be 6.21 cubic feet per day.
The seepage loss under your dam may be the single most important parameter in determining the stability of your dam because if the loss is too great, the soil beneath the dam may wash away, destroying the dam with it.
The following shows the seepage loss under your proposed dam for both design 1 and design 2. The following equation was used in the determination of the seepage loss:
(6)
The addition of the seepage blanket would effectively increase the number of equipotential drops below the dam which would decrease the seepage loss. The blanket lengthens the distance in which the water must travel which decreases the amount of water that can flow beneath the dam in a given day. This in effect decreases the seepage loss. A length of at least ½ the length of the dam is recommended for safety purposes.
From the conclusion of our testing we have been able to determine the classification of the two soils, the suitability of the coarse sand as a filter, the permeability of the fine soil, and the estimated seepage loss beneath the dam in each design. The coarse soil was found to be poorly graded sand, SP, while the fine soil was found to be clayey sand, SC. With this we were then able to determine that the coarse sand met all criteria required to be a filter for the fine sand. We were also able to determine the coefficient of permeability of the coarse and fine soils as 8.7x10-3 cm/sec and 6.57x10-7 cm/sec respectively and also compare them to Hazen's equation approximation. We were able to determine that Hazen's equation is not an appropriate approximation in this situation due to certain assumptions needed to use that equation. Also, we were able to determine that the estimated seepage loss beneath dam Design 1 and dam Design 2 were 10.35 cubic ft per day and 6.21 cubit feet per day, respectively, which is excessive by our standards. With this we recommend adding a seepage blanket in front of both designs which would allow the seepage loss to drop to an acceptable level.

GG Brown and Associates have performed compaction and Atterburg limit tests on the Gold Art Clay as requested. Tests were performed in general accordance with ASTM D-698-07e1. The Proctor test was used to determine the compaction curve. It was determined that the maximum dry density for Gold Art Clay is 110.6 pcf. The Atterburg Limits of liquid limit is 39% and plastic limit is 25.3%. The optimal water content is 15%. The clay was classified as CL in the group of leen clay.
A letter dated October 16, 2007 from William Piper Associates, requested GG Brown and Associates determine group symbol and name for Gold Art Clay as well as the Atterburg Limits, the optimum water content and the maximum dry density. We have completed testing on November 6, 2007 using the Protector Test to determine the compaction curve of the soil of interest and therefore to determine the maximum dry density and the optimal water content as well as Atterburg limit tests to determine the Atterburg limits of the soil.
The soil was previously sent to G.G. Brown Associates. The soil is the Gold Art Clay.
The Compaction Test (proctor test) was used to determine the maximum dry unit weight and the optimal weight and was tested according to the following;
ASTM D698-07e1: Standard Test Methods for Laboratory Compaction Characteristics of soil using Standard Effort
The Atterburg Limits were found and tested according to the following:
ASTM D 4318-05: Standard Test Methods for Liquid Limit. Plastic Limit and Plasticity Index of Soils.
The Proctor test was performed to determine the compaction curve for a soil of interest, to include the determination of the maximum dry unit weight and the optimal water content for the Gold Art Clay. Mechanical compaction is the most common and effective way of stabilizing soil and prior to compaction in the field; the compaction characteristics should be tested and determined.
The Atterburg Limits Tests were performed to determine the liquid limit and the plastic limits of the soil. The water contents separating the transition from a semi solid state to a plastic and a plastic to a semi liquid state are called the plastic limit and the liquid limit respectively. The water content in soil significantly influences its behavior. And the difference between the liquid limit and the plastic limit is an indicator of potential problems
The detailed procedures for both tests can be found in Appendix A of this report.
The group symbol and group name were determined to be CL and leen clay respectively. This is because the PI is greater than 7.
The Atterburg limits were determined to be the following:
These values calculated from our test results, a full table of data results is included in Appendix B.
The Plastic Limit corresponds to the water content at which the soil will begin to crack when rolled out into a diameter of 3mm.
The liquid limit was calculated by comparing the number of blows to the water content. Table 1 summarizes the data. The liquid limit corresponds to the water content at 25 blows.
From this table, the data can be plotted and seen in Graph 1. From this graph, a trend can be seen. As the water content increases, the number of blows to bridge the gap in the test decreases.
The maximum dry density and optimal water content were to be the following:
When the data from Table 2 is plotted against each other along with the Zero Air Voids line, the compaction curve can be found as in Graph 2.
From this graph it is clear where the maximum water content and dry unit weights are obtained.
These values calculated from our test results, a full table of data results is included in Appendix C. From the results found in the laboratory, the optimum moisture content is lower in comparison to that of the empirical correlation ( about 18%) provided in Figure 1 of Appendix E. This could be because of the fact that some of the moisture was lost while conducting the experiment. However the values are very similar as they were only about 3% away from each other. See Appendix E.
Sample calculations form both tests are included in Appendix D.
Using the Proctor Test, the Max Dry Density was determined to be 110.6 pcf and the Optimal Water Content was determined to be 15%. The ASTM soil symbol and group name was determined to be CL and leen clay, respectively.
From the laboratory determined optimal water content that was found, it is determined that it is only slightly lower in comparison to the empirical correlation provided in Figure 1 of Appendix E. The empirical relation estimates that the optimal water content (based on the values of the liquid limit and the plastic limit) is about 18%, where as that found in the laboratory was 15%.
From the Atterburg Limit test the liquid limit was determined to be 39% and the plastic limit was determined to be 25.3%.

The purpose of this experiment is to determine both the specific gravity and absorption capacity of coarse aggregate. This information will be used in the proportioning of concrete mixtures. Specific gravity is the characteristic generally used to calculate the volume occupied by aggregate. Absorption capacity describes the change in mass of an aggregate due to water absorbed in the pores.
This test is performed in accordance with ASTM C127: Standard Test Method for Density, Specific Gravity, and Absorption of Coarse Aggregate. The following materials are required:
Table 1 gives the data from the lab and the appropriate calculations. From this data the Bulk Specific Gravity at the SSD condition (BSGSSD) is 2.57 and the Bulk Specific gravity at the OD condition (BSGOD) is 2.49. The absorption capacity is 3.23%.
The values of absorption and specific gravity for aggregate that is not oven dried before it is soaked in water can be significantly higher that aggregate that is oven dried. (1) Larger particles, especially those over 75 mm, may be too thick to allow water to penetrate the pores to the center of the aggregate particle. Therefore it is critical that the procedure state if the aggregate was oven dried and, if so, how long it was submerged in the water. In this case, since the data is being found for a concrete mixture (where the aggregate will be in its natural, moist state) the aggregate was continuously submerged.
Another important procedural point that must be followed to assure reproducible results: The experimenter should assure the aggregate is fully submerged when finding the weight in water.
The purpose of this experiment is to find both the specific gravity and absorption capacity of the fine aggregate. This information will be used in the proportioning of concrete mixtures. Specific gravity is the characteristic generally used to calculate the volume occupied by aggregate. Absorption capacity describes the change in mass of an aggregate due to water absorbed in the pores.
This test is performed in accordance with ASTM C128: Standard Test Method for Density, Specific Gravity, and Absorption of Coarse Aggregate. The following materials are required:
Table 2 gives the data from the lab and the appropriate calculations. From this data the Bulk Specific Gravity at the SSD condition (BSGSSD) is 2.684 and the Bulk Specific gravity at the OD condition (BSGOD) is 2.669. The absorption capacity is 0.553%.
(a) Bulk dry specific gravity, bulk SSD specific gravity, and apparent specific gravity are all calculated in experiments 1 and 2. The key concept to understanding these terms is understanding the concept of pores in aggregate particles. A single particle of aggregate has a total volume that includes solids and pores. At the SSD condition the pores are filled with water and at the OD condition the pores are filled with air.
From these equations, these bulk and apparent specific gravities can be compared. In general
(b) Reproducible Results: In order to produce reproducible results, an accurate SSD condition needs to be obtained. It is critical that this is performed correctly, because the basis of all calculations relies on the appropriate SSD weight.
The purpose of this experiment is to determine the particle size distribution for fine and coarse aggregates using the sieve method. This data will be used to select proportions for concrete mixtures.
This test is performed in accordance with ASTM C136: Standard Test Method for Sieve Analysis of Fine and Coarse Aggregates. The following materials are required:
Tables 3 and 4 give data from experiment 3. Table 3 gives the sieve gradation analysis for coarse aggregate. Table 4 gives the sieve gradation analysis for fine aggregate. Figures 1 and 2, following the data charts, show the particle distribution graphically. As both the tables and figures demonstrate, both the fine and coarse aggregates do not meet ASTM specifications at some point in the distribution.
(a) In figures 1 and 2 the upper and lower limits for the ASTM standards are plotted in the lighter color. As demonstrated, both the coarse and fine aggregates do not fully meet ASTM specifications. The coarse aggregate does not fall in the range specified by ASTM standards for 3 of 4 given standards. The fine aggregate meets specifications for all sieves besides the #50 and #100 sieves. This tells us that both specimens are not appropriate for use in construction.
(b) In order to obtain reproducible results it is crucial that the aggregate is agitated for the appropriate amount of time. It is also necessary to weigh all aggregate sample retained in a sieve. Loss of any particles causes error in the calculations.
The purpose of this laboratory is to determine the minimum and maximum unit weights of coarse aggregate. This data will be used to select proportions for concrete mixtures.
This experiment is performed in accordance with ASTM C 29: Standard Test Method for Bulk Density (Unit Weight) and Voids in Aggregate. The following materials are required:
Table 5 demonstrates the data obtained in the lab, as well as the resulting calculations of specific gravity. From this data the rodded unit weight () is 1424 kg/m3 and the loose unit weight ( is 1328 kg/m3.

The development of necking and localization in specimens subjected to a uniaxial tensile load are triggered by a bifurcation. This bifurcation occurs when a critical load is reached where the displacement path becomes unstable. Necking and its subsequent phenomena localization show the mechanics behind material and geometric non-linearities. These non-linearities which can make a specimen decrease in cross sectional area (necking), can induce strain localization at later stages. Specifically, necking occurs due to geometric non-linearities and localization occurs due to material non-linearities.
This report contains implicit finite element analysis of models with plane stress and plane strain elements, and of different materials. Two materials were used, an elasto-plastic and hyper-elastic material, to analyze their effect on necking and localization. The first part of this report contains some explanations about what bifurcation, necking, and localization is and the second part discusses the results of the analysis.
Bifurcation is defined as the loss of uniqueness of solution in a non linear problem. It corresponds to a sudden change in behavior when a critical load parameter (λc) is reached. In buckling or necking the bifurcation point is defined as the point when a compressive or tensile load reaches a maximum triggering a sudden change in displacement. Figure 1 shows the bifurcation paths for buckling (red) and necking (green). As the load increases from zero both paths are stable with displacements close to zero. The mathematical theoretical solution states that the load will increase with zero displacement until λc is reached (yellow solid line). In real life because of imperfections and in finite element because of approximations there will always be some displacement before the critical load is reached. This can be observed in the initial solid red and green lines. Once λc is attained, the paths will bifurcate into non unique solutions. The buckling path will become stable while that of necking will become unstable. Ahead the finite element results are presented which excellently capture this behavior.
This reports aims to analyze two different bifurcation phenomena's. The first one is necking which depending on the material used might lead to the second bifurcation phenomena, strain localization.
Necking occurs due to geometric non-linearities and can be observed as a decrease in cross-sectional area of a specimen under tensile load. If a specimen is loaded in tension as seen in Figure 2 it will undergo three stages that will culminate with necking. First, the behavior will be stable with a constant decrease in cross sectional area throughout the length of the specimen. When the critical load is reached bifurcation occurs and the center of the specimen will neck. In order to see this complete behavior the sides of the specimen must be under shear free boundary conditions (i.e. rollers on both edges). In lab experiments it is very hard to give shear free boundary conditions so fixed boundary conditions (clamped) are given at the edges which then force the specimen to start at the second step and go directly into necking behavior. The same is true for finite element analysis where boundary conditions play a crucial role. These boundary condition requirements will be discussed in more detail in the boundary condition section. The analysis performed in this report starts the model when the bifurcation that triggers necking occurs. Capturing the pre-necking behavior is much more involved and is not of much use since most real-life elements undergoing necking will be fixed at both ends (does not allow for pre-neck behavior).
Localization which occurs after necking is defined as a bifurcation phenomenon which creates diagonal shear bands in the necked area which eventually cause the specimen to fracture along these bands. These bands are formed because of plastic strain localization. Strain localization is due to the materials non-linear behavior. Not all materials that undergo necking will experience localization. A more detailed discussion on why certain materials undergo localization is presented in the next section. Figure 3 is an example of strain localization for a steel specimen.
An elastoplastic J2 flow theory behavior was used to model the behavior of steel. An elastoplastic material experiences linear behavior until yield stress, where the material enters into the plastic range. The non-linear plastic behavior is defined by the following work hardening uniaxial equation:
Where the stress σ is a function of strain ε. The following values were assumed for steel: yield stress σy of 50 ksi, modulus of elasticity E of 29,000 ksi, poisons ratio of 0.3, and hardening parameter n as 10. Different n factors were analyzed but n=10 gave a smoother looking curve. The G is the elastic shear modulus which is a function of modulus of elasticity and poisons ratio. As mentioned before this material allowed the model to undergo necking and strain localization. Figure 4 shows the behavior of this elastoplastic:
The hyperelastic incompressible Arruda Boyce model was used as an example of a material that will cause necking in the specimen but will not allow for strain localization. The Arruda-Boyce model stress-strain relationship has the following form:
Where U is the strain energy potential, λU is the stretch in the uniaxial direction, and Ii are the deviatoric strain invariants. This model assumes full incompressibility (J= λ1 λ2 λ3=1). The coefficient λm is referred to as the locking stretch. Approximately at this stretch the slope of the stress-strain curve will rise significantly. This model is also known as the eight-chain model, since it was developed starting out from a representative volume element where eight springs emanate from the center of a cube to its corners. The values of the coefficients C1...C5 arise from a series expansion of the inverse Langevin function. The series expansion is truncated after the fifth term. The coefficient λm is referred to as the locking stretch. Approximately at this point the slope of the stress-strain curve will rise significantly. Figure 5 shows the stress-strain curve for the uniaxial form of the Arruda-Boyce model.
Both plane stress and plane strain elements were used to analyze the model. They both provided similar results. The mesh had to be refined as much as possible in order to capture localization. A denser mesh was used in the middle section were localization occurs. Reduced integration was used to avoid volumetric locking.
Defining the correct boundary conditions in the finite element model is crucial to capture the necking and localization phenomena's. As mentioned before to undergo through all the stages that lead to necking, shear free supports must be given at the edges of the specimen. Given that this is almost impossible, the analysis can be started right before necking occurs by either providing fixed boundary conditions or a geometric imperfection at the center of the model. If the geometric imperfection is used then rollers can be assigned to the edges providing a shear free edge support. Usually this geometric imperfection will be a small dip at each side of the plate on the center for a shell element analysis or a change in thickness at the center for a solid element analysis. This report was completed using fixed boundary conditions, not geometric imperfections. The results should be very similar because they both have the same effect which is to trigger a concentration of stresses in the center of the specimen.
A trial run was performed to validate the results using neither a clamped support nor a geometric imperfection. The results were as expected where the plate experienced a constant decrease in thickness throughout the length without any necking or localization.
This problem incurs both geometric and material non-linearities. The material non-linearities as discussed above arise from the non-linear behavior of stress as a function of strain. For the elasto-plastic material the analysis does not include material non-linearities until it reaches the yield stress were it changes to non-linear behavior. The hyper-elastic material is always non-linear. The geometric non-linearties arise from non-linear strains. The Abaqus step module has an option "Nlgeom" which allows the user to include the non-linear effects of large displacements.
For the stated conditions the plate was analyzed and the results are shown in Figure 6 (contours of plastic strain). Figure 6.a shows the preliminary plate before any displacements (dense mesh in middle section). Figure 6.b shows the initial strain formation at the upper and lower edges of the plate, and some other symmetrical accumulations of strains in the middle portion. Figure 6.c shows how the strain in the corners radiated inward and met with the ones previously found in the center. At this point the non-uniforms strains can be observed specially in the edge support (dark blue-smaller strain). Figure 6.d shows the circular formation of strains in the middle section right before localization occurs. This figure also shows how the whole plate is loaded (plastic strain) but the boundary edges have zero strains. This loading and unloading process is very important for localization. At Figure 6.e bifurcation occurs and there are early signs of shear band formations. This figure shows how the top and bottom edges of the center of the plate have lower plastic strains than the middle section from where the shear bands radiate diagonally. Figure 6.f is a close up of the shear bands already formed. Figure 6.g shows the whole specimen at the end of the analysis where in a real test it would have likely already fractured.
To better understand when the bifurcation occurs the following load curves were plotted. This curves shows the bifurcation that starts the strain localization. The following load curves are normalized both in the x and y axis by length and yield force, respectively. Figure 7 shows the normalized load curve for longitudinal displacement. The maximum load occurs at a factor u/L of 0.083 with a normalized critical load of 1.7. Through previous research it has been shown that the actual bifurcation occurs just after the maximum load. This figure also shows the fundamental path which would be followed if localization would not have occurred. Figure 8 shows the normalized load curve for width displacement at a maximum u/w of 0.054.
The plane stress case had similar results to the plane strain case. Figure 9 shows the plate with plane stress elements at the end of the analysis. The main difference between plane strain and plane stress is that in plane stress the shear bands are more pronunciated as can be seen by the slopes of the plate's edges at the end of the diagonals. This can also be observed by looking at the slope along any edge from the boundary to the point were localization occurs. In the plane strain case this slope is less gradual when compared to plane stress. Figure 10 shows the normalized curve with a maximum of u/L 0.089 very similar to plane strain. The maximum normalized critical load is of about 1.45. The curve after the bifurcation point which seems to be somewhat unstable does not provide any real physical meaning since a real specimen would be in the process of fracturing at this time for which a whole new analysis is needed.
The hyper elastic (Arruda Boyce) material was used to explore necking without subsequent localization. Figure 11 shows the stages from beginning to end where no strain localizations can be observed. Figure 12 has the normalized curve for the width displacement which is not conclusive. This curve is linear up until u/w of 0.13 where it becomes an exponential curve. Given that Abaqus had to stopped the analysis after the elements width had become to small to continue integrations the second part of the curve could be due to errors of the finite element analysis. This behavior may also be due to the materials non-linearity. I was not able to find information of this type of analysis with a hyper-elastic material. Figure 13 shows the normalized curve for longitudinal displacements. This plot is linear with a change of slope. Again this change in slope which occurs at around u/L of 0.75 might signal an error. Nevertheless it can be concluded that for small displacements the Arruda-Boyce model shows an increasing linear behavior for necking.
A complete analysis of necking and localization for plane strain and plane stress elements with different materials was presented in this report. The elasto-plastic (steel) material underwent necking and strain localization while the hyper-elastic (Arruda Boyce) only experienced necking as expected. The steel specimens reached a maximum load or critical load where bifurcation occurred and strain localization started. The critical load was reach at about a longitudinal displacement of 8.5% of the initial length, and it varied from 1.45 to 1.7 for plane stress and plane strain elements respectively. This strain localization culminated with the formation of shear bands. For the Arruda Boyce material only necking was observed due to the materials properties. The plane stress / strain tests had similar results. The analysis of 3-D models would have provided a better understanding of these phenomena but substantial computational power is needed to run this type analysis in a realistic time frame. Finally the results of this analysis where compared with past research in this field and there was a strong correlation for the elasto-plastic plane stress/strain results.

Methanotrophs have been known as gram-negative and aerobic bacteria and they use only methane for their carbon and energy source [3]. The initial oxidation of methane to methanol is catalyzed by methane monooxygenase (MMO) that can be expressed differently depending on the environmental factors. The most well known factor is copper concentration by which two different MMOs can be expressed: a soluble cytoplasmic MMO (sMMO) and a membrane-associated, or particulate, MMO (pMMO) [1,3,4,6]. Under low ratio of copper to biomass (≤ 0.9 nmol of Cu/mg of cell protein), the sMMO is expressed; at higher value, the pMMO is [6]. That is mainly because pMMO is a copper-based enzyme [4].
While copper plays an important role in the physiology of methanotrophs, the mechanism of copper uptake system by methanotrophs is still unclear [2]. However, sMMO's mutant of Methylosinus trichosporium OB3b [5, 7] which can express either sMMO or pMMO depending on copper concentration, suggested the presence of an extracellular copper-binding compound (CBC). The CBC was further isolated from M. capsulatus Bath and was shown to be small polypeptides with a molecular mass of 1,232 Da [5, 8, 9]. The two strains, M. capsulatus Bath and M. trichosporium OB3b, have been known to make CBC, but other methanotrophs have never been reported to make CBC. Since methanotrophs such as Methylomicrobium album BG8 and Methylocystis parvus OBBP need copper to express pMMO for the oxidation of methane and appear not to possess CBC, they either have a different copper uptake mechanism or have to utilize the CBC made by other methanotrophs.
This independent study was performed to achieve two goals. The first one was to determine if there was an interaction among methanotrophs for copper uptake; i.e. CBC, the sole copper uptake mechanism. To do so, three methanotroph strains were chosen, M. trichosporium OB3b, M. album BG8, and M. parvus OBBP to compare their growth trends in response to the presence of CBC in the growth media. The second goal of this study was to acquire the basic knowledge and technology of molecular analysis. Hence, the characteristics of the bacterial strains were investigated using naphthalene assay for a verification of sMMO expression, trichloroethylene (TCE) degradation, polymerase chain reaction (PCR), gel electrophoresis (GE), and capillary electrophoresis (CE).
M. trichosporium OB3b, M. album BG8, and M. parvus OBBP were grown at 30℃ on the agar plates of nitrate mineral salts (NMS) medium with the presence of 10 µM copper as Cu(NO3)2 5H2O under a methane-air mixture (1:2 ratio) [10] and cells were transferred to a fresh NMS liquid medium. The liquid culture medium of NMS did not exceed 15 % of the total flask volume to prevent mass transfer limitations of methane and oxygen from the headspace to liquid medium [10].
Whole-cell sMMO activity of M. trichosporium OB3b was examined using the colorimetric naphthalene assay of Brusseau et al. [11]. Since only sMMO can oxidize naphthalene to 1-or 2-naphthol, it could be determined whether or not M. trichosporium OB3b expressed sMMO rather than pMMO depending on copper concentration by adding tetrazotized o-dianisidine to form a purple naphthol diazo complex [12].
NMS media with different copper concentrations, either 0 µM or 20 µM, were prepared to evaluate sMMO expression. As M. trichosporium OB3b can express either sMMO or pMMO, all flasks were acid-washed in 2 N HNO3 for 2 days and 20 µM copper was added aseptically as Cu(NO3)25H2O. Cells were grown until they were in an exponential growth phase (optical density (OD) between 0.2 and 0.5) and then 2 ml of the liquid culture was aseptically transferred to 20 ml vials. Naphthalene was then added to the cell-transferred vials, sealed and incubated at 30℃, 270 rpm for 1 hour. 35 µl of 5 N NaOH was added to the samples to disrupt cell activities. 1.5 ml of the samples were taken and centrifuged at 12,000×g for 5 min. Lastly 130 µl of 4.21mM tetrazotized o-dianisidine was added to 1.3 ml of the supernatant for the absorbance measurement at 528 nm using Milton Roy Company Spectronic 20. Duplicate samples were measured.
Trichloroethylene (TCE) degradation assay was performed with M. trichosporium OB3b given with either 0 µM or 20 µM copper as Cu(NO3)25H2O to evaluate the ability of sMMO and pMMO-expressing cell to degrade chlorinated solvents. Stock liquid culture was prepared as described earlier and methane was removed from the stock culture flask by evacuating the flask and reequilibrating with air performing seven cycles [13]. 3 ml of the stock culture was aseptically transferred to 20 ml vials with Teflon-coated rubber butyl stoppers and aluminum crimp caps, and sealed [13]. TCE concentrations for standard calibration curve varied from 0 to 33 µM in aqueous phase. 9.8 µM of TCE in the aqueous phase of 20 ml vials was added to evaluate the ability of M. trichosporium OB3b to degrade TCE. In addition, the role of formate was investigated adding 20 mM of formate in the form of sodium formate to the samples. By using a dimensionless Henry's constant of TCE as 0.42, the partitioning amount of TCE between the liquid space and the headspace was calculated [13]. The 20 ml vials containing cells and certain amount of TCE were incubated at 30℃, 270 rpm for 6 hours. Control samples for monitoring any abiotic losses were treated with 50 µl of 5 N NaOH to lyse the cells [13]. TCE analysis was performed using an Hewlett Packard 5890 Series II gas chromatograph with an FID detector and the temperature of the injector, oven, and detector were 250, 120, and 250℃, respectively. Triplicate samples were prepared and analyzed in the experiment. All gas phase concentrations were calculated by the standard curve equation.
Since M. album BG8 has been contaminated, it had to be purified before the experiment. The purification was verified using nutrient agar evaluation, and polymerase chain reaction (PCR) and gel electrophoresis. The contaminated cells were diluted 10, 100, and 1,000 times and then spread on NMS agar medium plates to pick up some colonies of M. album BG8-like cells, all of which were aseptically transferred to other NMS agar medium plates and nutrient agar medium plates at the same time and the contamination was observed for more than a week. By performing the procedure repeatedly, cells that could grow on a NMS agar medium but not on a nutrient agar medium were finally selected as potential M. album BG8.
A microscope was used to verify the rod shape of M. album BG8. A simple staining was conducted using methylene blue. A small amount of cells was placed in a drop of Milli Q water on a glass slide and fixed by heating the glass slide. The heat fixed smear was covered by 1 % of methylene blue for approximately 1 min, and the excess stain was washed off. The stained cells were observed using a microscope (Olympus Tokyo Model E 324059).
PCR and gel electrophoresis were conducted to verify if the purified M. album BG8-supposed cells possessed pmoA gene but mmoX gene. For positive controls and a negative control, both M. trichosporium OB3b and M. capsulatus Bath, and Escherichia coli were used, respectively. DNA samples of the bacteria were prepared using a freezing-thawing method and a beadbeater-using method, both of which were developed earlier [14] were performed and the effectiveness of the methods were compared each other.
In the freezing-thawing method, approximately ten loopful-amount of cells in 1 ml of TE buffer that played a role in repressing the activity of DNA degrading enzyme were frozen at-70℃ for 30 min, boiled for 10 min, and mixed strongly for 3 min using a vortexing device. This cycle was repeated 3 times and the samples were finally centrifuged at 13,000 rpm for 5 min. The supernatants were taken as whole-cell DNA samples. In the other method using a beadbeater, 0.1 mm glass beads were filled half of 2 ml Ependoff tube and TE buffer was also filled two-third of the tube. The same amount of cells used in the freezing-thawing method was placed in the tube, shaken at 5,000 rpm for 30 sec, and cooled in ice water for 1 min. Total 6 cycles of the procedure were conducted and the samples were finally centrifuged at 13,000 rpm for 5 min. The supernatants were taken as whole-cell DNA samples. The recipe of PCR is shown in Table 1 and the condition of temperature control is indicated in Table 2. Thermocycle device made by a company automatically performed the temperature cycles and it kept the samples at 4 °C after finishing the thermocycles.
After completing the PCR, 5 µl of sample loading buffer made of 20 % of glycerol and bromophenol blue was added to each sample and DNA molecular weight marker (DNA molecular weight marker VIII, 19~1114 bp, Roche) which was prepared with 20 µl of Milli Q water and 5 µl of the marker. All samples were briefly centrifuged to be mixed and loaded into 1.8 % of agarose gel with TAE electrophoresis buffer (Tris-acetate-EDTA made of 0.04 M Tris-acetate and 2 mM EDTA, pH 8) and 0.5 ppm of ethidium bromide which could bind very tightly to DNA molecules and form a strong fluorescent complex resulting in being visualized by exposure to UV light. Current was applied as 125 volts for 45 min. The separated PCR products in the agarose gel were exposed to a strong UV light using UV device.
Gel electrophoresis is able to show clear bands of PCR products under UV light but it usually requires a relatively long time to see the results of PCR. However, much more precise and rapid results of PCR products can be obtained using capillary electrophoresis (CE).
P/ACETM MDQ capillary electrophoresis system from Beckman Coulter was used under the condition of the reverse polarity mode with either 4 kV or 6 kV applied voltage. The CE separation buffer was prepared as described earlier in Han and Semrau [10]; 50 mM HEPES sodium salt (N-2-hydroxylethylpiperazine-N'-2-ethanesulfonic acid), 65 mM boric acid, 0.5 % HPMC (hydroxypropylmethylcellulose), 6 % mannitol, and 1 µg/ml ethidium bromid. The final volume of the buffer was adjusted to 100 ml. Since 6 % mannitol made the buffer so sticky and bubbling that the buffer had to be degassed overnight by a stirring device using a low stirring speed. The buffer was then sonicated for 30 min to completely remove remained bubbles and stored in a refrigerator before CE analysis. The capillary used was an uncoated silica capillary whose total length, effective length, and inner diameter were 31 cm, 21 cm, and 75 µm, respectively [10].
Both 0.2 N NaOH and 0.2 N HCl as well as the separation buffer were used to rinse the CE capillary before analyzing samples. Each rinsing solution rinsed the capillary for 5 min in order of 0.2 N NaOH, 0.2 N HCl, and the separation buffer under a pressure of 25, 25, and 30 psi, respectively. A sample was then injected at 1 psi for 50 sec and measured at 254 nm of UV detector. To be sure of the CE result quality, all CE analysis was performed after confirming that the current was maintained consistently during CE analysis.
Copper analysis was performed using an atomic absorption spectrophotometer (Perkin-Elmer, model Z5100) with a furnace mode. The amount of sample injection was 20 µl with 5 µl of dilution Milli Q water. All samples were properly diluted to a final concentration within 100 ppb because the detection range of the AA instrument was from 0 ppb to approximately 100 ppb. Energy lamp strength was always maintained over 50 %.
Copper samples were analyzed under the presence of nitric acid that could dissolve copper bound to cell materials. Since heavy metals tend to be precipitated with ligands depending on pH conditions, copper might also be bound to cells or precipitated in a sample, which might cause error in a copper analysis due to the unbalanced distribution of copper concentration in a sample vial. Thus, by making the sample be acidic, copper could be present in a dissolved form in the liquid sample. The proper concentration of nitric acid was investigated analyzing a known copper concentration solution with different concentration of nitric acid ranging from 2 % to 10 %.
MB was isolated from the spent medium of M. trichosporium OB3b and treated with either copper or EDTA: copper-bound MB and EDTA-treated MB. The copper concentrations of the treated MB and the original MB were analyzed. In addition, the copper concentration of a fraction of MB eluted from HP20 column after regeneration was measured as well. Stock solutions of each sample were prepared dissolving 10 mg of each sample into 1 ml of Milli Q water. The samples were diluted 1,000 to 3,300 times depending on the copper concentrations before injecting to the AA instrument.
M. parvus OBBP was grown under the presence of CBC to evaluate the effect of CBC on the growth of M. parvus OBBP. A stock liquid culture was prepared as described earlier including 10 µM of copper as Cu(NO3)2 5H2O. After harvesting cells in the exponential growth phase, cells were washed with pre-warmed fresh NMS medium to remove loosely cell-bound copper, and resuspended on 50 ml of NMS medium samples with different MB and copper concentration.
Two kinds of MB were added to the M. parvus OBBP cultures: copper-bound MB and EDTA-treated MB. Since MB could be destroyed by heating, it was added to the culture after autoclaving the medium. The copper concentrations for the samples were either 0 or 10 µM. In addition, a negative control was prepared adding neither copper nor MB to the M. parvus OBBP culture and a positive control adding just 10 µM copper to the culture. An optical density of each sample was measured at 600 nm. After completing the growth monitoring, copper concentrations of the spent media were analyzed.
Table 3 indicates the results of optical density at 528 nm depending on different copper concentration. The naphthalene assay is based on the fact that only sMMO can oxidize naphthalene to 1-or 2-naphthol. While the color of the 20 µM-copper sample was not changed, the naphthol diazo complex samples of 0 µM copper were in bright purple color. Based on the results, 0 µM copper condition made M. trichosporium OB3b to express sMMO and oxidize naphthalene to 1-or 2-naphthol; on the other hand, 20 µM copper condition to express pMMO that could not oxidize naphthalene. These results were well matched with the known fact that M. trichosporium OB3b can express either sMMO or pMMO depending on copper concentration [3].
The standard concentrations for the calibration varied from 0 to 33 µM in the gas phase. Figure 1 shows a standard curve between TCE concentration in gas phase and peak area. Figure 2 indicates TCE concentrations in the gas phase of the 20 ml tube samples after 6 hour incubation with the initial TCE concentration of 4.12 µM in the gas phase. Although all gas phase concentrations were calculated by the standard curve equation, two data of GC peak area, 0 µM and F 0 µM samples in Figure 2 (b), were converted to the concentration using just two points of the standard concentration because the peak areas were too small to be calculated by the standard curve.
Since M. trichosporium OB3b has been known to express either pMMO or sMMO depending on copper availability [3], M. trichosporium OB3b was expected to express pMMO under 20 µM copper concentration (Figure 2 (a)) and sMMO under 0 µM copper concentration (Figure 2 (b)). As shown in the two figures, sMMO showed approximately 10 times higher rate in TCE degradation than pMMO and TCE degradation efficiency by M. trichosporium OB3b grown with 20 mM formate was 7 % higher than that with no formate. Table 4 shows the TCE degradation rate of the samples. These results were also well matched with those of the naphthalene assay.
Two kinds of potential M. album BG 8 could be picked up from contaminated plates; one was from a very small colony in light yellow, and the other from a streak. The two potential samples were spread on NMS and nutrient agar media at the same time. After 5 days of the incubation, the colony samples showed much higher cells on NMS media and much less cells on nutrient agar media than the streak sample, suggesting that the colony sample had contained much more methanotrophs than the streak sample. The colony samples were then diluted and transferred to fresh NMS media repeatedly. Finally cleaned cells from the contaminated cells showed no growth on a nutrient agar medium but on a NMS medium.
The purified M. album BG 8-like cells and M. album BG 8 that had been stored in a refrigerator for 6 years were observed using a microscope. Simple staining with 1 % of methlylene blue showed the shape of the cells. Both of the cells seemed to be a round shape and seemed alike much. Also there were no other shapes of cells except for the round type, so they might not to be contaminated by other microorganisms.
The PCR product bands shown in Figure 3 were obtained using the freezing-thawing method. While pmoA gene indicated strong bands of both M. trichosporium OB3b and M. album BG8, mmoX gene rarely showed any bands except for one weak band (marked with a circle) in Figure 3 (b). After some consecutive results of no PCR products in mmoX gene such as Figure 3 (a), the quality of the primer set of mmoX gene were suspected, so they were replaced with another one which was also not a new one. With the replaced mmoX primer set, a weak PCR product of mmoX gene of M. trichosporium OB3b could be obtained (Figure 3 (b)). However, no PCR product of mmoX gene of M. album BG8 was shown in all the PCR experiments. The unexpected results of mmoX gene of M. trichosporium OB3b might result from either the poor quality of the primer set or not proper method of DNA sample preparation, or both of them.
With the same DNA preparation method, a freezing-thawing method, the following PCR products shown in Figure 4 and 5 were obtained using three different methanotrophs: M. trichosporium OB3b, M. capsulatus Bath and M. album BG8. Both M. trichosporium OB3b and M. capsulatus Bath were used as positive controls in producing the products of pmoA and mmoX genes at the same time. As shown in Figure 4, M. capsulatus Bath showed two clear bands of pmoA and mmoX as expected, but no bands of M. trichosporium OB3b was indicated. However, the reverse results of those in Figure 4 were obtained in Figure 5; M. trichosporium OB3b showed two distinguish bands of pmoA and mmoX, but M. capsulatus Bath did not.
Although one of two positive controls for the detection of pmoA and mmoX genes did not work properly at the same time in the two PCR experiments and nothing was surely confirmed, a clear PCR product of only pmoA gene of M. album BG 8-like cells was shown in all PCR experiments and also the negative control of E. coli showed no products of pmoA and mmoX genes. Therefore, the cleaned M. album BG 8-like cells obtained another positive evidence for real M. album BG 8.
CE performance for DNA molecular marker VIII from Roche (0.25 µg/µl, molecular size 1,114 ~ 19 bp) was conducted to make sure of the separation quality of the marker prior to an analysis of PCR products of pmoA and mmoX genes. Han [10] applied 1.0 psi for 50 sec and 4 kV for 25 min for sample injection pressure and separation voltage, respectively. With the same conditions as Han [10], the separation of DNA molecular marker VIII was performed and Figure 6 showed the results and a semi-log graph between molecular size (bp) and retention time (RT). According to the composition datum of the DNA marker offered by Roche (Figure 7), the separation quality using Han [10]'s method was quite good except for the baseline at the end of the analysis. So, better separation quality and faster method was investigated changing the sample injection pressure and separation voltage based on the Han's method [10].
Figure 8 indicated the results of the modified method of Han [10]. With 0.5 psi for 30 sec and 6 kV for 15 min for sample injection pressure and separation voltage, respectively, the separation quality was good enough to distinguish each peak, it was faster to separate all peaks, and the baseline during the analysis was fairly good. The current was maintained as-20 µA during the CE analysis. In addition, the DNA marker separation was conducted with the same method as the modified method except for the applied voltage 8 kV instead of 6 kV to make the analysis much faster. As shown in Figure 9, the separation of the peaks performed at 8 kV was good enough and the analyzing time was shorter, but the base line was not as good as that of 6 kV.
PCR products of pmoA and mmoX genes of M. capsulatus Bath were detected using CE analysis. The growth condition of M. capsulatus Bath was the same as shown in Materials and method. DNA samples were prepared using the beadbeater-using method. Although the same method as used in the previous PCR experiments was applied, PCR products did not clearly shown on both gel and capillary electrophoresis (data not shown). So, each PCR chemical and DNA samples were analyzed using CE to figure what shape of peaks could be indicated on CE. Figure 10 shows each peak of the chemicals and DNA samples.
One suspected thing from Figure 10 was that the peaks of the primer sets were much smaller than those of dNTPs. Actually the primer sets were not fresh ones and also they had been stored even in a refrigerator with no power for about 2 days due to the US-nationwide power failure in 2003. Hence, the concentration of the primer sets was adjusted to 0.4 µM (proper concentration range of a primer set is 0.1 ~ 0.6 µM) and the amount of each primer set was recalculated according to the information shown on the labels of the primer sets. The information on each label was as follows: pmof 334 (MW=6,885.5); 2.3 OD260 = 10.8 nmol = 0.07 mg. That information was not enough to calculate the required amount of the primer sets, so the total volume of each primer was assumed 1.8 ml which was the volume of the primer-contained vial. Based on the information, the required amount of each primer set was calculated as 0.4 µM in 25 µl of a sample. The amount of each primer of pmof 334, pmor 640, mmoxf 882, and mmoxr 1403 were 1.77, 0.64, 1.11, and 0.49 µl, respectively. Figure 11 shows the results of CE analysis of pmoA and mmoX genes using either the pre-existed amount of each primer (Figure 11 (a)) or the recalculated amount of each primer (Figure 11 (b)).
With the same PCR products as shown in Figure 11 (b), CE analysis was conducted and the peaks of the PCR products are indicated in Figure 12. As shown in Figure 11 (b) and Figure 12 (a and b), clear bands of pmoA and mmoX genes appeared on both GE and CE results. The sizes of pmoA and mmoX genes were 327 bp and 547 bp according to the DNA marker. The two obvious results were the clearest results after changing the DNA sample preparation method and adjusting the concentration of the primer sets. In addition, both gel and capillary electrophoresis results were well matched each other.
5 % of nitric acid showed a proper concentration of copper, so all copper analysis were performed under the presence of 5 % of nitric acid. Copper concentrations of four different MB are shown in Table 5 and Figure 13. According to the results, MB was proved to have a high affinity to copper binding approximately 260 time-higher copper amount of original MB. Also, MB could still retain about 11 % of the total copper amount even after the ethylenediaminetetraacetic acid (EDTA) treatment.
M. parvus OBBP growth was monitored under the presence of 20 mg/L of MB. Figure 14 and 15 shows the growth curve of the optical density and the relative optical density, respectively.
As shown on the graphs, M. parvus OBBP grown with MB-Cu did not show any growth in both 0 µM and 10 µM copper concentrations of NMS media; however, M. parvus OBBP grown with MB-EDTA was grown well similar to the positive control, M. parvus OBBP grown in 10 µM copper-NMS, except for longer lag phase than that of the positive control. In addition, the growth rates of the three samples were so similar one another even though the growth rate of the positive control was a little higher than those of the others (Table 6). After completing the monitoring of growth rates, whole-cell copper concentrations of the samples were analyzed to compare copper concentrations between initially intended and actual concentrations. Table 7 and Figure 16 indicate the actual copper concentrations of M. parvus OBBP-containing samples.
Basic molecular analysis on methanotrophs was performed to understand the characteristics of the bacterial strains and acquire the analysis techniques. To examine MMO expressions in M. trichosporium OB3b depending on copper availability, naphthalene assay and TCE degradation assay were conducted using non-copper NMS medium and 20 µM copper-containing NMS medium. Since M. trichosporium OB3b expresses either sMMO or pMMO under low copper and high copper concentrations, respectively (the critical concentration was reported as 0.89 µmol copper per gram cell dry weight [10, 13]), M. trichosporium OB3b was expected to express sMMO at non-copper NMS medium and pMMO at 20 µM copper-containing NMS medium. In addition, while the substrate specificity of sMMO is lower than that of pMMO, halogenated hydrocarbon degradation rate of sMMO is higher than that of pMMO [3, 10, 15]; in other words, sMMO can oxidize much more kinds of halogenated hydrocarbons at faster degradation rate than pMMO. In both of the assays, sMMO-expressing cells showed much more degradation efficiency of naphthalene and TCE than pMMO-expressing cells (Table 3 and 4, Figure 2). Also, 20 mM formate played a role in an outside source of reducing equivalent that facilitated the TCE degradation rate. Thus, making methanotrophs express sMMO and adding a proper reducing equivalent in a chlorinated compound-contaminated site would help in situ bioremediation strategy be optimized. During the experiments, it was important for better activities of cells to transfer a part of a liquid stock culture to sample vials when cells were in the initial exponential phase rather than other phases.
PCR and gel electrophoresis were performed to verify a purified M. album BG8. Since both M. trichosporium OB3b and M. capsulatus Bath possess two types of MMO genes, they were used as positive controls to show the PCR products of pmoA and mmoX genes on agarose gel. However, initial continuative experiments could not show clear bands of PCR products of either pmoA or mmoX gene but indicated a thick and blunt band on agarose gel. Three explainable reasons were investigated changing the components of PCE experiment. The first one was the method of DNA sample preparation; using a beadbeater with 0.1 mm glass beads rather than a freezing-thawing method to disrupt cell membrane showed better PCR results. Although the freezing-thawing method could also indicate quite good PCR products, it sometimes failed to show proper products (Figure 3, 4, and 5). The reason why the freezing-thawing method sometimes failed to show good results might be either that the method was too strong to remain a proper gene size or that it was too weak to disrupt the cell membrane. In Figure 3 and 4 using the freezing-thawing method, both pmoA and mmoX genes did not appear at the same time but one of the gene products was shown, suggesting that the preparation method was not too weak to disrupt cell membrane. Thus, the former reason might be a better explanation. The second reason could be the quality of the primer sets that were exposed to room temperature for about 48 hours, because oligoneucletide primers were vulnerable to freezing-thawing cycles and room temperature. After changing another primer set of mmoxf 882 and mmoxr1403, a mmoX gene product could be obtained (Figure 3). The last possible explanation would be not proper amount of the primers used in PCR experiment. Although PCR recipe in Jong-In's lab note [14] suggested taking 0.5 μl of each primer whose concentration should be 20 μM, the concentration of the primers used in this study was so doubtful that the concentrations of the primers were recalculated based on the label information and the required amount of each primer was then calculated. As shown in Figure 11, PCR containing recalculated amount of each primer showed very clear product bands on 1.0 % agarose gel (Figure 11 (b)) even though the DNA sample was the same as used in Figure 11 (a).
According to the PCR results and nutrient agar assay for the verification of the purified M. album BG8, it not only indicated a pmoA gene product in all experiment (Figure 3 (lane 3), Figure 4 (lane 2), and Figure 5 (lane 2)), but also showed no growth on nutrient agar plates. As M. album BG8 can express only pMMO, the M. album BG8-like cells might be purified based on the two assay results. However, the purified M. album BG8 did not grow as well as other methanotrophs in a liquid NMS medium (data not shown). It took more than 8 days to start to grow in the liquid medium, suggesting that the purified M. album BG8-like cells might not be M. album BG8. Thus, it is not clear whether the purified M. album BG8-like cells were completely purified or not. Since the M. album BG8-like cells have been contaminated on agar plates for a long time, it might be possible that they could not be easily accustomed to the liquid medium. Based on the assumption, the M. album BG8-like cells will be continuously tried to grow in the liquid medium and 16S rRNA analysis will be performed if necessary in the future study.
As a more convenient and faster analysis of PCR products, capillary electrophoresis (CE) was used to separate the PCR products of pmoA and mmoX genes. CE method was modified and developed from Han's method [10]. Faster and better peaks could be obtained using 6 kV separation voltages with 0.5 psi sample injection pressure for 30 sec; especially the peaks of 900 and 1114 bp of the DNA molecular weight marker were sharper than those of the previous study [10]. Also, 8 kV separation voltages were applied to achieve much faster results, but as shown in Figure 9, the baseline was not as stable as that of 6 kV voltages. With the modified method, the PCR products of pmoA and mmoX genes were separated and the size of the genes were estimated. Two distinguishable peaks could be observed in Figure 12 (a and b). A pmoA gene product appeared at 10.34 min of the retention time and a mmoX gene product at 12.03 min; the gene sizes were 327 bp and 547 bp for pmoA and mmoX, respectively. These results of CE analysis were well matched with those of gel electrophoresis analysis but showed much faster and sharper peaks.
Methanobactin (MB) is a copper-binding compound (CBC) which was isolated from the spent medium of M. trichosporium OB3b [9]. Although the role of CBC is not known, it is assumed that it might play a role in up-taking copper outside cells because CBC has high affinity for binding copper. Table 5 showed the high affinity of MB to copper. One unusual and interesting thing in Table 5 was that the copper concentration of EDTA-treated MB was higher than that of the original MB. Since EDTA is a strong chelating agent [16] resulting in binding copper much more strongly than MB, the copper concentration of MB-EDTA was expected to be lower than that of the original MB; however, the reverse was true, suggesting that MB could retain the bound copper against the strong chelating agent, EDTA. In addition, M. parvus OBBP could utilize the copper bound to MB-EDTA and showed almost the same growth pattern as that grown under the presence of 10 μM copper. The results indicated that the EDTA-treated MB did not badly affect on the growth of M. parvus OBBP but assisted the growth supplying copper to M. parvus OBBP for the expression of pMMO. However, the lag phase of M. parvus OBBP with MB-EDTA was longer than that of the normal growth of M. parvus OBBP, probably because MB still affected on the growth to some extent, so it took much time for M. parvus OBBP to be accustomed to the MB environment. Also the two growth of 0 μM-ED and 10 μM-ED in Figure 15 showed exactly the same pattern each other. It might be explained that M. parvus OBBP could be grown normally once copper concentration in NMS media met the minimum of the required amount of copper to express pMMO. On the other hand, M. parvus OBBP could not survive under the presence of copper-bound MB (0 μM-Cu and 10 μM-Cu in Figure 14), suggesting that copper-bound MB seriously badly affected on the growth or killed the cells. The cause(s) might be either the effect of MB or the toxicity of high copper concentration, or both of them. According to the copper analysis of the whole-cell and NMS medium (Table 7), the copper concentration of 0 μM-Cu and 10 μM-Cu were 22.0 and 22.9 μM, respectively. 22 μM of copper was usually not a high concentration to methanotrophs, so it could be hypothesized that MB might play a serious role in impeding the cell growth.
Two methanotrophs, M. parvus OBBP and M. album BG8, were purified from contaminated plates but it will be necessary to verify them using PCR and CE with the recalculated amounts of primer sets (Figure 11 (b) and Figure 12 (a and b)). Also, based on the preliminary data of the growth of M. parvus OBBP under the presence of different-copper concentrations of MB, it will be required to determine if M. parvus OBBP is able to show the same growth pattern with the same copper concentrations under no presence of MB. It will be able to suggest which of the factors caused no growth of M. parvus OBBP under the presence of copper-bound MB in Figure 14. In addtion, the growth monitoring of other methanotrophs that can express pMMO but not produce CBC under the presence of the treated MB will make the role of CBC better clear. With the results, it will be possible to give an answer to a hypothesis if there are some important groups that provide copper-needed methanotrophs with CBC among methanotrophs. Ultimately, the way that methanotrophs interact one another based on the necessity of copper will be figured out to understand the copper uptake mechanism of methanotrophs.
Methanotrophs oxidize methane to methanol using either soluble cytoplasmic MMO (sMMO) or a membrane-associated, or particulate, MMO (pMMO). pMMO expression requires high ratio of copper to biomass (> 0.9 nmol of Cu / mg of cell protein), but the copper uptake mechanism remains vague. One of the assumptions of the copper uptake system lies in copper-binding compounds (CBC) that are small extracelluar polypeptides and have high affinity for binding copper. In this independent study, preliminary data suggesting the effect of CBC on the growth of Methylocystis parvus OBBP were obtained and the characterizations of some methanotrophs were performed as well.
Methylosinus trichosporium OB3b showed the effect of copper concentration on the expression of either sMMO or pMMO using naphthalene and trichloroethylene (TCE) assays. Non-copper environment caused M. trichosporium OB3b to express sMMO and the cells then not only showed to oxidize naphthalene to naphthol changing the color of the NMS medium to bright purple in the naphthalene assay, but also indicated approximately 10 times higher TCE degradation rate than those grown with 20 μM copper. Also, 20 mM formate played a role in an outside source of reducing equivalent that facilitated the TCE degradation rate.
Polymerase chain reaction (PCR), gel electrophoresis (GE), and capillary electrophoresis (CE) were performed to verify purified Methylomicrobium album BG8. The purified cells showed a distinct PCR product of the pmoA gene on GE, but no product of the mmoX gene, suggesting that the purified cells might be M. album BG8; however, the cells were not grown as well as other methanotrophs in a liquid nitrate mineral salts (NMS) medium. Thus, the cells need to be monitored continuously in the liquid medium and further analysis such as 16S rRNA analysis will be required to verify them. In addition, the method of CE analysis could be modified from Han's [10] using 0.5mpsi sample injection pressure for 30 seconds and applying 6 kV separation voltages; it showed as better peaks as Han's [10] and faster analysis.
Methanobactin (MB) could, to some extent, hold bound copper against a strong chelating agent, ethylenediaminetetraacetic acid (EDTA). EDTA-treated MB assisted the growth of M. parvus OBBP supplying copper to the cells even though the lag phase under the presence of EDTA-treated MB was approximately 20 hours longer than that of a normal one. However, copper-bound MB killed the cells; it might be caused by either MB toxicity or high copper concentration, or both of them. With the results, it is assumed that MB might be able to play a different role according to a certain environment in an interaction among methanotrophs based on the copper nessecity.

This program was revised based on the beam-column program. After the modification, it can solve any 2-D structure assembled by any number of rectangles (Figure 1). In this revised program, lots of effort was put on the preprocessor modification, such as structural assembly and mesh generator. The 4-node rectangular C0 element was used in this FEA program. In this program, it is assumed that the applied loads are only applied at the nodes. In other words, this program does not consider consistent nodal loads due to body or surface loads.
The program has the following structure:
In the preprocessor, it allows user to assemble any number of rectangles to form a structure and gives the degree of freedom to assign different material properties (Young's modulus, e, and Poisson's ratio, v), element types (plan stress and plan strain), and thickness (t) in different rectangle (however, elements within the same rectangle have the same properties). Another feature within this program is the mesh generator. This function allows user to create specific mesh in each rectangle. The preprocessor has the following structure:
In plotting the "Meshing result for the first rectangle", nodal number is also plotted aside each node (Figure 2). This information makes it easier to assign the boundary conditions and nodal loads in the following procedure.
The algorithm in the "Insert another rectangle" can be shown as follow,
Figure 3 to 6 show the ability of this preprocessor by displaying the assembling process of a frame-like structure.
As shown in figure 9, a 4-node rectangular C0 element has 8 degree of freedoms. The displacement field within the element can be interpolated by the shape function as follow.
(1)
where
The stiffness matrix can be calculated through
(2)
To check the correction of this program, a square plate (2 inch in thickness) stretched by a uniform force was analyzed (Figure 10). It is expected that the convergence problem in this simplest loading configuration is limited. Hence, this configuration was computed using FEA by only one 4-node rectangular C0 element to check the correction of this FEA program. The simulated geometry, meshing, and boundary and loading conditions are shown in figure 11. The support conditions are fixed in x-and y-directions in node 1 and fixed in x-direction and free in y-direction in node 3.
Table 1 and 2 give the comparison between computational and theoretical results. As can be seen, the results are almost identical except some truncation error. Figure 12 shows the deformed shape (dot line, 2000 times) of this problem.
A cantilever beam problem was conducted in this section to do the convergence studies. This cantilever beam problem is identical with homework 2 and 3 (Figure 13) with 35 inches in length, 10 inches in depth, and 2 inches in thickness. The material is isotropic and linear elastic. Poisson's ratio is 0.3. Enforce plane stress condition.
Table 3 shows the computational result of nodal displacement in the y-direction for the right-bottom corner (dy of the black dot in figure 13) calculated with different number of elements. This result is compared to the theoretical values and the value calculated from ANSYS (The element type used in ANSYS analysis was chosen to be the 8Node-82 quad element).
Two theoretical theories were used to calculate the theo. deformation and give comparisons with computational results. The first one is the Classical beam theory, in which the shear deformation is not considered.
The second theoretical calculation considered the shear deformation in the cantilever beam; however, it still assumes that plan remains plan in this calculation. This consideration is probably important for this case because the Depth to Length ratio is 3.5, which is kind of a deep beam rather a slim one.
Figure 14 plots the results in table 3. As can be seen, the computational results (both FEA program and ANSYS) converge to the theoretical value (shear consideration) as more and more elements were used. Second, the convergence speed of deformation is faster in ANSYS than in FEA program. This result can be attributed to different element type used in FEA program (4-Node rectangular C0 element) and ANSYS (8Node-82 quad element). Third, shear consideration is necessary in this analysis (deep beam). Forth, a careful examination of the FEA result shows that the convergence speed of deformation is faster when more elements are created along the horizontal direction than along vertical direction. For example, dy increases from 0.022 to 0.023 when mesh number increases from 4*1 to 4*2. However, dy increases from 0.023 to 0.028 when mesh number increases from 4*2 to 8*2. This is because the geometry, boundary, and loading configuration make the deformation more sensitive to the meshing density along horizontal direction. Figure 15 displays the sequence of deformation with different number of elements (100 times magnitude).
After checking the correction (section 4) and the convergence study (section 5), we have more confidence to use this FEA program. In this section, two complicated structures were computed to demonstrate the ability of this FEA program.

With increased urban development, the fraction of impervious surfaces has dramatically increased throughout the United States. When soil surfaces become impervious, the degree of infiltration of water into the soil substantially decreases, causing increased urban runoff. This runoff is typically either discharged directly into receiving waters resulting in degraded water quality and erosion, or is treated at publicly owned water treatment plants prior to discharge which requires substantial capital investments and often still results in poor water quality during storm surges.
As a response to the concern over increased urban runoff, all new private developments located within Washtenaw County with impervious surfaces totaling more than 5000 square feet must adhere to the Washtenaw County Drain Commissioner (WCDC) procedures and design criteria for storm water management [1]. Further, the City of Ann Arbor requires all such developments to apply on-site stormwater management plans through both structural controls, such as trenches or detention ponds, and non-structural controls, such as vegetated swales or natural storage [2]. The combined effect of both the County and the City stormwater requirements places the burden of stormwater management on the site developer rather than the City, requiring the detention of stormwater on site although not mandating treatment.
As a result, developers must provide stormwater management plans that detail the practices used to treat, prevent and reduce the volume of stormwater on site. As guidelines for the creation of stormwater management plans, Best Management Practices (BMPs) have been developed to characterize effective, efficient and both logistically and economically practical methods for managing stormwater. These BMPs focus on reducing the volume and improving the quality of stormwater as well as reducing the need for capital investments and improvements in the City water management infrastructure. [2]
Although stormwater management plans are intended to require detailed descriptions of the controls implemented, there is no specific requirement on the type of control that should be implemented for a given site plan. The permitting system is intended to allow for innovative, unique and site specific solutions to managing stormwater cost effectively by placing this burden on the developer. However, this requires the developer to understand the technical challenges of stormwater management and the appropriateness of various controls.
As a result, there is a need for consolidated design guides and synthesized case studies to aid developers in selecting appropriate water treatment and detention methodologies. Additionally, characterizing the fate and mass transport, removal pathways, and removal rates of contaminants is critical in designing treatment systems and implementing appropriate site specific management practices. This work is intended to supplement a design guide presented to both the City of Ann Arbor and to developers, describing the suitability and design process for constructed wetlands as part of stormwater management plans.
This paper describes the physics based numerical models used to quantify mass transport in constructed wetlands. A case study is presented of a proposed wetland for West Park, located in the City of Ann Arbor. Finally, a parametric analysis is presented to compare the influence of the design and model parameters (such as the wetland length, diffusion coefficient and contaminant decay rate) on treatment efficacy.
A wetland is an area of land covered either all or some of the time by standing water during the growing season, is made up of predominantly undrained soil, and serves as a transition between aquatic and terrestrial ecosystems [3]. Wetlands differ from rivers and lakes both in water depth and in average water velocity.
Climate and hydrology, or water saturation, dictate the types of soils and plants that can be found in wetlands. Wetlands are classified into two main categories: tidally influenced and inland wetlands. As can be expected, tidally influenced wetlands are typically found along coastlines and surrounding bays. Whereas inland wetlands typically border lakes, streams and rivers. Types of wetlands include marshes, wet meadows, swamps, bogs, prairie potholes and fens. [4]
It has been estimated that over 50 percent of the wetlands in the contiguous U.S. have been lost [5]. Between 1986 and 1997, the U.S. Environmental Protection Agency estimates that 58,500 acres of wetland were lost in the lower 48 states each year, with an estimated 105.5 million acres existing in 1997 [4]. This rapid decrease in wetland area is due in large part to drainage, dredging, deposition of fill, logging, construction, mining, damming, tilling, overdrawing groundwater aquifers, and many other human related factors [4].
Constructed stormwater wetlands are water treatment wetlands designed to improve the water quality of urban stormwater while increasing on-site detention to mitigate the effects of storm surges on water treatment plants or riverways [6].Constructed wetlands can also be used to treat municipal wastewater prior to discharge [7], or for treating industrial wastewater such as wood waste or landfill leachate [8].
Constructed wetlands are considered to be favorable control mechanisms for storm water management because they reduce stormwater contaminant loading, provide increased detention capacity on site, and increase recreational opportunities and wildlife habitat. Although not always suitable as a sole stormwater management control mechanism, constructed wetlands have gained increased attention as an option for controlling stormwater within a diverse stormwater management infrastructure.
This section outlines the typical composition of urban stormwater, the contaminant removal pathways, and design considerations for the construction of treatment wetlands.
The composition of urban stormwater is influenced by land use and varies throughout the year. Typical stormwater pollutants include nutrients (nitrogen and phosphorus), solids (sediment), pathogens (bacteria and viruses), metals (lead, copper, cadmium, zinc, mercury, chromium, aluminum), hydrocarbons (oil, grease, napthalenes), organics (pesticides, PCBs, synthetics), and salts [9].
The solids collected from stormwater can be classified as litter (greater than 6.35 mm) or non-litter (less than 6.35 mm) as shown in Figure 1. Litter is then further classified as gross, wet, or dry, then floatable or non-floatable, and finally biodegradable or non-biodegradable. Non-litter particles are classified as sediment, gravitoidal, colloidal, and dissolved. [10]
Release rates of other pollutants are impacted by the particle dynamics of suspended solids; additionally, rainfall intensity directly impacts the ability to mobilize particles as well as the size of the particles mobilized and occurs as a random process [11]. Figure 2 shows the fraction of dissolved copper, lead, nickel and zinc measured from state-wide California highway runoff characterizations from 2001-2003 [11]. Although the median colloidal and dissolved concentrations are between 30-60%, the concentrations can vary from 0-100%, making modeling of the fate of suspended solids non-trivial.
The removal of stormwater pollutants involves a complex interaction of physical, chemical and biological processes. Stormwater wetland contaminant removal pathways include sedimentation, adsorption, filtration, and microbial, plant and algae uptake [6]. A description of several removal mechanisms for macrophyte based wastewater treatment wetlands, where macroscopic plants play a critical role in the water treatment process, is provided in Table I.
Water soluble organic compounds are typically removed by bacteria attached to plant and soil/sediment surfaces. The diffusion of oxygen from the atmospheric/water interface and photosynthesis within the water column, along with leakage of oxygen from plant roots support the aerobic removal or these organic compounds. [12]
Nitrification and denitrification converts nitrates into nitrogen gas. As with the processing of organic compounds, the oxygen required for this process is provided from the atmosphere or from leakage by plant roots. Plant uptake is typically a less dominant removal mechanism than denitrification, but does occur. Depending upon the water pH levels, ammonium can be converted to ammonia gas and released. [12] The typical nitrogen cycle in a wetland is displayed in Figure 3.
Phosphorus removal occurs via adsorption, complexation and precipitation reactions with aluminum, calcium, iron and clay in the sediment layers. For low concentrations of phosphorus, sedimentation and filtration can be a significant removal mechanism. [12] The typical phosphorus cycle in a wetland is displayed in Figure 4.
As a result of these removal processes, wetland soils typically contain a shallow oxidized soil layer over a reduced soil layer, creating constituent concentration gradients. Figure 5 graphically displays the soil profiles of reduced manganese, iron and sulfur along with the redox potential.
Due to the complex nature of contaminant transport and the dynamics associated with the physical, biological and chemical interactions of contaminants in the wetland system, a trial and error approach has been taken in designing treatment wetlands. Numerous case studies have been published to detail wetland designs and treatment effectiveness. From these case studies design guides have compiled general rules of thumb for designing treatment wetlands that remove particular pollutants, in certain concentration ranges, for specific climates. A brief summary of some of these design strategies is provided here.
There are four basic stormwater wetland designs [6]:
Numerous factors such as the pollutant removal capability, land consumption, site water balance, contributing watershed area, maintenance requirements, and wildlife interactions play a role in the selection of an appropriate wetland design for a specific site. It is however, important to note that constructed treatment wetlands have a different functionality than constructed wetlands designed to mitigate the loss of natural wetlands. Additionally, treatment wetlands should not be located near or adjacent to natural wetlands due to their protection by local, state and federal regulations. [6]
Figure 6 provides comparative profiles of the stormwater wetland designs.
Important objectives to consider in the design of stormwater treatment wetlands are [6]:
For optimal pollutant removal, design recommendations are to provide a [6]:
These sizing criteria for the design of stormwater wetlands are summarized in Table II.
Additionally, it is suggested that the water depth in the deepwater cell and forebay be one to six feet below normal pool, the lo marsh should be 6 to eighteen inches below normal pool, the hi marsh should be zero to six inches below normal pool, and the semi-wet region should be zero to 2 feet above normal pool [6].
Lists and descriptions of wetland plant species suitable for surviving exposure to certain pollutants, draught and inundation, along with tolerance to certain water depths are outlined in [6] and [7]. The selection of particular plant species and modeling of the complex dynamics associated with their pollutant removal capabilities is beyond the scope of this work and will not be addressed.
The Allen Creek is currently plumbed beneath the surface of the park. The proposed wetland serves to daylight the creek, improve water quality, increase detention capacity within the Allen Creek Watershed, increase habitat, and educate residents on the use of constructed treatment wetlands within the context of stormwater management. A site analysis is provided in this section along with a description of the proposed wetland design.
The fraction of impervious surface throughout the Allen Creek watershed has dramatically increased in recent decades, causing a significant burden to the existing stormwater infrastructure. Street, yard and house flooding events occur throughout the area, along with blown man-hole covers, placing increased attention on West Park and its potential to reduce the flow of stormwater during and following precipitation events.
West Park is located along the western edge of downtown Ann Arbor, a city with a population of approximately 114,000 full time residents, as shown in Figure 7. An aerial view of West Park along with five foot topographic contours is provided in Figure 8.
Single and multi-family residential housing borders the park. The park is located in a bowl, with 15-20 foot ridges along the northern and southern boundaries. The city of Ann Arbor has located concrete and wood structures along with vegetation to stabilize these slopes and reduce or prevent erosion and land slides. There is a 30 foot elevation difference from the western to eastern boundaries.
There are many existing landmarks within the park, such as a marker indicating the site of a Native American trail, an outdoor amphitheater, playground, water fountain, pergola, several hundred year old trees, and the City's oldest baseball field. The park is frequented by residents and is considered to be an integral part of the local community.
Unfortunately, West Park has its' own localized flooding concerns. Much of the soil in the park is considered undisturbed and little modifications to the topography have been made over the years. Due to the difference in elevation of the park with respect to the surrounding neighborhoods, overland stormwater flow collects in several locations within the park. Spot fixes including vegetated swales and depressions have been created to store this stormwater in small quantities, however does not provide a large enough storage volume to prevent the large expanses of turf grass from remaining saturated many days after a rain event.
To reduce the amount of space required by the wetland system, increase stormwater detention capacity, and enhance the removal of suspended solids, a pond/wetland system was selected. First, the amount of space to be allocated to the treatment wetland, the location of the wetland within the park, and the general water flow path were determined. Then the surface area to volume ratios were calculated according to the design criteria set forth. Finally, the site was regraded to achieve the recommended side and bottom slopes as well as to accommodate the necessary wetland cell depths.
There should be little elevation change in the hi and lo marshes. The largest surface area available for the marshes and thus the critical sizing parameter, was located on top of the existing baseball field. Although considered to be a desirable amenity, the baseball field was removed and used for the marsh system. The outlet of the wetland system must be located close to the Huron River and thus was placed on the eastern edge of the Park. Due to the elevation, the inlet to the wetland system must then be located along the western boundary, either from the northern or the southern corner. Because the amphitheater restricts the available space for the forebay and deepwater marsh, the southern corner was selected for the wetland inlet.
Following the recommended design criteria, maintaining a 30 foot flood buffer zone, while minimizing the necessary regrading of land, the surface area to volume ratio were achieved with the constraint that the forebay and deepwater marsh must be located on the shelf leading towards the existing baseball field. The resulting volumes at normal pool level and full bank are listed in Table III.
A sketch of the wetland boundaries (solid lines) and the buffer zone (dotted line) superimposed on an aerial view of the park is shown in Figure 9.
Treatment wetland models are typically constructed for one of four main reasons, to: examine the hydrologic response of the wetland to storm surges, investigate the biological response, explore contaminant transport and removal processes, or describe the wetland hydraulics for use in designing control structures. The degree of complexity of these models varies from static algebraic expressions to high order dynamical equations accounting for the conservation of mass, energy, and/or momentum.
Water budgets are used in hydrologic models to characterize the movement of water through the wetland system. The objectives of these models are often related to stormwater detention, and thus focus on the inputs and outputs to the wetland and treat the wetland itself as a lumped parameter single volume. Konyha [13] employed a first order lumped parameter model using
where Q is used to denote volumetric flow rate, and S for the stored volume. Water enters the wetland via
, (2)
where P is used for precipitation, SRO for surface runoff, DRN for subsurface drainage, B for baseflow, and G for groundwater seepage and springs. The output flows are characterized by
, (3)
where AET is used for evaporation and transpiration, Rp for flow over the spillway, Re for flow over the emergency spillway, L for lateral seepage, and D for deep percolation. Additional models are then used to relate these variables to physical properties, such as expressing evaporation as a function of temperature. This methodology is not unique and has been described in various forms in standard hydrology textbooks on flood routing [14]. The implications of the use of these models depends upon the degree of complexity incorporated. Additionally, they focus on hydraulics and often neglect the presence of pollutants.
Walker [15] applied a two-dimensional momentum balance along with continuity, to formulate a numerical model of the flow processes in a constructed wetland. The momentum balance in the x direction and continuity are described by
, (4)
where
The processes of sedimentation or uptake are typically modeled as first order decays using specific removal rate constants for given constituents (BOD, TOC, nitrogen, pathogenic microorganisms, heavy metals, and trace organics) regardless of the actual removal mechanism [16], [17]. Kadlec [17] compared the impact of assuming the wetland volume of interest behaved as a plug flow reactor (PFR) or a continuous stirred tank reactor (CSTR) using dye tracer studies and found that the wetlands investigated were best described by series and parallel combinations of PFRs and CSTRs. The PFRs were modeled assuming steady-flow, first order irreversible reactions using,
, (5)
where C is used for concentration, k is a decay rate constant, t represents the nominal retention time, and Cin is used to denote the inlet concentration. From inspection, it is clear that this equation was derived by neglecting diffusion/dispersion and advection. An advantage to this formulation is that it can be easily tuned with experimental data using input and output measurements of concentration. The decay rate can then take on a functional relationship according to statistical analysis or flow regime characteristics [18].
Kadlec [17] went further to describe long narrow reactors (wetlands) by modifying the PFR with a dispersion coefficient of the form,
, (6)
where D represents the dispersion coefficient, u is the average velocity, and L is the length.
Walker [15] used the following two-dimensional transport equation to describe the spatial and temporal evolution of a constituent concentration,
, (7)
where C represents the constituent concentration, U and V are the component velocities in the x and y directions, D is the water depth, and S is the dispersion coefficient. The dispersion coefficients are then represented as a function of the component velocities. The intent of these numerical models was to quantify the residence times and predict the flow pattern. The algorithm incorporated by Walker in modeling sediment transport is shown in Figure 10.
Fig. 10. Flow chart for modeling sedimentation [19]
The relationship of the design parameters and flow characteristics on contaminant transport was of interest in completing this work. A simple zero or first order relationship, as described in Equations 5 and 6, between constituent concentration and time is not capable of simulating the dynamic response to changes in inlet concentration or water velocity. The two dimensional transport model, although accurately quantifying transport in constructed wetlands as compared to the low order models, is computationally intensive. As a result, a one dimensional advection diffusion reaction equation was employed of the form,
, (8)
where U is the average velocity in the direction of the flow path, k is a decay rate constant, D is the diffusion coefficient, and C is the constituent concentration. The inlet concentration was assumed constant for this analysis, and the initial concentration in the wetland was assumed to be zero. The location of the outlet, x = L = 800m, was set to be equal to greater than two times the actual wetland length (350m) to ensure the boundary condition did not have a significant influence on the solution profile.
Johengen [20] published data quantifying nutrient removal in a stormwater treatment wetland. These data included nitrate, ammonium, and phosphate removal efficiencies. Although these data were taken by comparing inlet and outlet concentrations, the diffusion coefficient and decay rate were tuned such that these efficiencies were achieved. It is important to note that there is not a unique combination of parameters that satisfy the removal efficiency requirements.
The model was simulated using the "pdepe" initialboundary value solver in Matlabr for parabolic and elliptic one-dimensional partial differential equations.
For this analysis a general constituent was modeled, rather than a specific constituent such as phosphorus, nitrogen, or sediment. The model developed in Equation 8 assumes the diffusion coefficient and decay rate are constant parameters that depend on the constituent of interest. This analysis examines the impact of varying these parameters throughout the range of values expected for urban stormwater pollutants.
A minimum flowrate of 0.0006 m3/s is recommended throughout the wetland [6]. The smallest average cross sectional area in the wetland is approximately 2.3 m2, resulting in a average minimum velocity of 2.6x10−4 m/s. Assuming the maximum average velocity to be ten times greater than the minimum velocity, or 224.6 m/day, a constituent concentration was simulated for the length of the wetland assuming a uniform decay rate constant and diffusion coefficient. The resulting steady-state velocity profile assuming three different values for the velocity is given in Figure 11. For the range of velocities considered, the concentration profile does significantly depend on the velocity and therefore should not be neglected from the model.
The impact of the decay rate and dispersion coefficients on the contaminant transport were then considered, as shown in Figure 12. As expected, the decay rate has the greatest influence on the concentration profile and the diffusion coefficient has very little impact on contaminant transport for the range of parameter values considered.
Tuning the parameter values to achieve measured inlet and outlet concentrations is rather straightforward. However, assuming a single decay rate and diffusion coefficient for the entire wetland constrains the solution of the concentration profile. It has been well documented that particular pollutants are targeted for removal within certain wetland cells [6]. For example, the large majority of suspended solids are removed in the forebay and deepwater marsh. As a result, if a more accurate prediction of the actual concentration profile is required,
different parameter values should be used for each wetland cell.
By assuming that each wetland cell (forebay, deepwater marsh, hi/lo marsh, and micropool) has an individual constant lumped decay rate and diffusion coefficient, different pollutant dynamics can be incorporated for each wetland cell. Figure 13 and 14 display the simulation results and the sensitivity of the concentration profile to the individual parameter values. The nominal decay rates assumed were
As expected, similar inlet and outlet concentrations can be achieved with significantly different concentration profiles. Additionally, the concentration profile in upstream wetland cells, such as the forebay, is not impacted by changing the decay rates in downstream cells. Finally, manipulating the diffusion coefficient in the various wetland cells has little impact on the concentration profile as compared to the decay rate. As a result, it is recommended that a constant diffusion coefficient be incorporated with a spatially varying decay rate to model the concentration profile through the length of the wetland.
The temporal and spatial evolution of concentration was simulated for a 10 day period, as shown in Figure 15 with a constant diffusion coefficient and velocity throughout the wetland and employing the four nominal decay rates for the individual wetland cells.
By increasing the decay rate in the forebay to
Wetland design guides typically provide qualitative assessments of various wetland designs for a wide range of applications, such as stormwater, wastewater, greywater, commercial, and residential treatment wetlands. While easy to follow and replicate, these guides provide little assurance that the proposed design will function as intended and typically target Best Management Practices, not design and control. Existing models for characterizing contaminant transport in treatment wetlands vary greatly depending upon the application of interest. Typically, these models focus on wetland and watershed hydraulics, assume zero or first order transport relationships and are tuned using measured input and output pollutant concentrations. The models that do capture the complex interactions of transport phenomena are difficult to tune and cumbersome to use in practice.
Using a one dimensional advection diffusion reaction equation to describe the pollutant transport through the proposed wetland, both the reaction rate and the diffusion coefficient impact the spatial concentration distribution. Surprisingly, the average water velocity does influence the concentration profile and should not be neglected. If a simple transport model is sought, it is recommended that a spatially varying reaction rate be employed with a constant diffusion coefficient.
Future work could incorporate hydrologic models to assess the impact of stormwater surges on contaminant transport. Additionally, data for expected stormwater constituents in the City of Ann Arbor, along with information on particular species could be used to provide a more detailed analysis of particular constituent concentration profiles.
I would like to acknowledge the help and support received from Danielle Kahn, a graduate student in the Landscape Architecture Program at the University of Michigan. Her role in this project was instrumental in developing an innovative and unique final design for West Park, which incorporated many of the design elements presented here.

It is widely understood that education is an essential component to the development of any society. A more educated population leads to a more advanced health care system, more efficient use of resources and, generally, more wealth for the entire economy. Therefore, it is vital that the adult population of any developing nation today invests in the education of their children if they wish to expand their economy. Over the past twenty years, conditional cash transfer programs have begun to address this issue by offering stipends to families with school-age children, provided that these children have documented regular attendance in school. These programs have been especially popular in Latin America where Progresa, the cash program in Mexico, exhibited wonderful success in its preliminary stages starting in 1998. Since then, these programs have spread like wildfire throughout Latin America, with individual countries adapting the model slightly to meet their own needs. The hope is that these conditional transfers can replace the wages children might earn from working or helping with the family business, and thus reduce or eliminate the family's opportunity cost of sending these children to school.
Still, questions remain regarding the magnitude of the opportunity cost of attending school, and regarding the components that create this opportunity cost. Is the opportunity cost of attending school higher for families with fewer children, where each child's work has a greater marginal return to the family's income? Or, do these families have fewer children because they do not need as much help generating a sustainable income, hence diminishing the effect of the children's contribution to family income? Is family size simply a result of the family's income level prior to having children; that is, are a family's fertility decisions independent of the future income these children may generate? These are the types of questions that this analysis seeks to answer in the context of Costa Rica. The paper proceeds as follows: Section II describes the theoretical and empirical motivation for studying this question, and specifically, for studying Costa Rica. Section III describes the data and methodology used to answer these questions, Section IV describes trends in the data, Section V presents and interprets regression analysis results, and Section VI concludes.
Gary Becker's theory on the quantity and quality of children describes a negative relationship between the quantity of children a family has and the quality, or amount of investment, which the parents put into each child. Becker and Lewis argue that this inverse relationship results from the increasing shadow price of a child as the level of quality of children increases (Becker, 280). Taken in the context of education, this theory implies that families who desire higher levels of education for their children will have fewer children due to the fact that they will have to pay more for each child's education as the level of education rises. Empirical studies have also found this relationship to be true. For example, Lam and Duryea find a negative relationship between parents' schooling and fertility in Brazil. They link this relationship to an increase in the level of schooling of each child in the family. At lower levels of schooling, this decrease in fertility results from an increase in child quality, which is measured by an increase in schooling (Lam and Duryea, 176).
A study of this phenomenon in Costa Rica is unique because of specific characteristics of Costa Rica, some which separate it from some its Latin American neighbors. First of all, its literacy rate is one of the highest in Latin America at 96 percent. According to the CIA World Factbook, the 2006 literacy rate in Brazil is 88 percent, 91 percent in Mexico, and 67.5 percent in Nicaragua. Secondly, it has a young population, with nearly 30 percent of its population under the age of 14, and a median age of 26.2. With a population growth rate of 1.8 percent in 2007, the relationship of family size and education level certainly affects a large subset of the population. To compare, growth rates in Brazil and Mexico are 1.0 percent and 1.1 percent, respectively, whereas the growth rate in Nicaragua is 1.9 percent (CIA Factbook). Therefore, while Costa Rica has literacy rates that are similar to more developed countries in Latin America, it behaves like smaller, less developed countries in terms of population growth. For these reasons, it poses an interesting case to study.
Additionally, the relationship between family size and education in Costa Rica is interesting because a conditional transfer program, called Superémonos, implemented in 2001 found that providing one monthly food coupon to selected Costa Rican families if all children between 6 and 18 attended school increased school attendance among these families by 2.9 percentage points, an effect equivalent to increasing the mother's education by six years. However, when comparing the mean attendance rate of families who received transfers to those who did not receive a transfer, the difference was not found to be significant (Duryea and Morrison, 11-12). While there could be several explanations as to why there is no significant difference in mean attendance rates, this impact evaluation is perhaps the greatest motivator for analyzing the relationship between family size and educational attainment. Regression analysis captures a bit of the opportunity cost of sending a child to school, but the lack of significant difference in means could suggest that perhaps one food coupon per family, regardless of the number of children between 6 and 18 might not always be an effective motivator to send children to school. The next sections explore family size in Costa Rica in more detail, in hopes of deepening the understanding of this relationship.
In order to look at these questions, this paper uses IPUMS census data from Costa Rica in 1973, 1984 and 2000. The three years of data will provide a cross-sectional analysis of education and family size over time. To begin, it is important note drastic changes in demographic trends over time. Latin American women averaged approximately 6 births in 1950, but fertility rates have decreased rapidly over the past fifty years to hovering just above replacement fertility in 2000 (Population Reference Bureau). Therefore, each year will provide an interesting contrast in comparing family size and education, holding other factors constant. The entire population of individuals with education information available in 1973 is 154,885. The sample size increases to 208,958 in 1984, and 343,642 in 2000. The mean age of the sample in 1973 is 26, increasing to 27 in 1984 and 30 in 2000. In other words, the population is aging slowly over time.
Variables included in the IPUMS dataset that are especially important to this analysis include educational attainment, representing the current (or completed) level of education of every person in the household, and the variable indicating the number of people in each household. Those who are have no education information listed, most likely children under age 6, are excluded from the sample. In this analysis, the number of children born to a child's mother is used as a proxy for the number of siblings that child has, and in this case the number of children born is interpreted as a measurement of family size. It is important to note the assumptions when using number of children born to the mother as a proxy for the number of siblings. In certain cases, some of those children born to the mother may no longer be alive, or some of these children may be half brothers or sisters that do not live in the same household. Obviously, these absent children have no impact on the observed child's educational attainment. Still, this variable attempts to capture some of the possible competition between siblings for the opportunity to go to school, if parents need one child to stay at home or work. In additional specifications, the number of people per household is understood to be another measurement family size, although in some cases multiple generations may live in one household, or the household may contain people who are not directly related to children in the family. One additional model is run using this measure of family size as a sort of robustness check to the effect of number of siblings.
Educational attainment is grouped into four categories: less than primary, primary, secondary and university. Most analyses will use years of school in place of this variable for a more accurate interpretation of effects. When years of school is used, 6 years of school can be compared to completing primary school, and 11 years of school generally is understood to correspond to completion of secondary school. Any additional years of education indicate participation in some type of specialized technical education or university. Other variables to be used as controls in regression analysis include an indicator for whether families live in an urban or rural neighborhood, and a proxy for income. While the IPUMS data on Costa Rica does not include information on household income, this analysis uses home ownership, availability of electricity and possession of a refrigerator as proxies for income. Certainly, these variables cannot be interpreted as any monetary equivalents of income, or even as a continuous distribution of income levels. They simply group the population into a higher income and lower income bracket. In other words, if a household owns its home rather than rents its dwelling, has electricity and has a refrigerator, it is assumed to be a higher income household than a household that does not possess these basic durable goods.
Analysis will focus on children ages 9 to 15 as a whole, and the older subset of this age group, children 14, 15 and 16. On average, children ages 9 to 15 are most likely both still living with their parents and attending school, so there may not be as much variation in educational attainment in this population group. However, as children reach their early teenage years, their probability of working increases, especially after the completion of primary school. Children in this age group are old enough that, should the family need extra help earning money or working in a family business, parents could pull these children from school to work more easily than they could pull younger children from school.
These variables will be used to study the relationship between family size and educational attainment by comparing trends of family size over time, by level of education and with control variables. Then, multivariate ordinary least squares regressions will estimate effects across the entire age group by year, including only children ages 14-15 by year, analyzing children aged 16 separately, pooling all years together. One final robustness check will number of people living in a household as a robustness check for the effect of family size.
Before analyzing the effects of these variables on educational attainment with regression analysis, it is important first to gain a picture of the relative trends and distributions of families in Costa Rica. To begin, Table 1 presents the mean values of important variables in the sample in each of the three years studied. As seen in this table, the population in 2000 is older, more urban, with smaller families, more schooling and more household durables than the population in 1973 or 1984. The following figures investigate these changes further.
Figure 1 shows the changes in the distribution of family size over time. This graph shows a relatively normal distribution of children born across all years, although this distribution is slightly skewed to the left. Furthermore, this graph shows a clear decline in the variance of the number of children born to women in Costa Rica over the three years surveyed. In 1973, approximately 44 percent of women in Costa Rica had at most five births over their lifetime; by 2000 this percentage increased to nearly 78 percent, with 40 percent of those women having only two or three births. This shows quite clearly the women's changing preferences of childbearing over time.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica.
When looking at the mean number of children born to mothers Costa Rica by years of education, it is clear that schooling adds another dimension to this relationship. Not only is the number of children born to Costa Rican women decreasing with her level of education, but the average number of children born to women at each level of education is decreasing over time. While this graph suggests that households where the mother has higher levels of educational attainment are generally smaller than those where the mother has less education, it does not shed light on the causal relationship between these two factors. This highlights the need to separate the effects over time from the effects of educational attainment to determine if smaller households result in family members attaining higher levels of education, or if these two factors are simply correlated, but caused by some third variable, such as income.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica.
These two graphs naturally beg the question of the distribution of educational attainment over time. Because it appears that family size is negatively correlated with educational attainment and over time, it is very plausible that the population of Costa Rica is becoming more educated over time, thus explaining these two results. In fact, this is exactly what the data shows. Figure 3 shows the highest level of education attained by mothers of children ages 9-15 over time. The sample was limited to the education of mothers with children ages 9-15 due to the direct impact of their education on the children to be analyzed in the regressions in this paper. As seen below, approximately 15 percent of mothers completed six years of education, or primary school, in 1973. This number grows to 25 percent in 1984 and 35 percent in 2000. Approximately 10 percent of mothers complete secondary school in 2000. Additionally, the percentage of mothers completing more than 11 years of education increases from approximately 2 percent in 1973 to 6 percent in 1984 and 12 percent in 2000. This shows that mother's education is increasing over the same period of time during which they are experiencing decreasing fertility.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica. Mothers represented here are those with children ages 9-15 in the year of the survey. Schooling level shown is the maximum level of schooling the mother in each household has attained.
In order to examine whether the level of schooling has risen unambiguously in Costa Rica over time, Figure 4 shows a Lorenz Curve of the years of completed schooling for individuals over age 15. Because the top line in this graph represents the year 2000 and the bottom line represents 1973, it is clear that the distribution of education in Costa Rica becomes increasingly more equalized from 1973 to 2000 because these lines do not cross. Still, the magnitude of the gains in equality over the entire period is relatively small. This graph confirms the aforementioned conclusions that the population of Costa Rica has become more educated over time. However, all three curves remain very close together and are clearly concave; indicating that more work needs to be done to equalize education in Costa Rica.
It is also useful to briefly analyze the breakdown of educational attainment and family size across different control groups. For example, Table 2 shows the mean years of schooling and standard deviations across the three income proxy variables and across the indicator for urban households. This provides a basic understanding of how educational attainment varies across different income groups, even if income is not explicitly described by these proxies. Also, this table represents mean years of schooling over all three census years.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica. Yrschl=Years of Schooling. Sample is limited to individuals over the age of 15, to be interpreted as individuals who have completed their education. Top curve in the graph represents 2000, the middle curve represents 1984, and the bottom curve represents 1973.
Intuitively, as shown by the previous figures, one can assume that these means would be higher if the sample were limited to include only 2000, and lower if the sample included responses only from 1973.
As shown by this table, the average number of years of schooling nearly doubles in households that have a refrigerator or electricity. While the effect is not as large for a household owning its dwelling, there still is a positive effect. This confirms that the three binary variables selected serve as a good proxy for income, since literature shows that higher income families generally have higher levels of education. When looking at the corresponding means for urban locations compared to rural locations, Table 2 shows that households in urban areas have, on average, 2.5 more years of education than households in rural areas. Again, this relationship is paired with smaller households in urban areas compared to rural areas, as seen in Figure 5 below. Nearly 40 percent of the urban population of Costa Rica has a household size of 4 or 5 people, compared to approximately 30 percent of the rural population. These two distributions are marked by similar skews as the previous figures.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica. Relationship above presented across all three years of the sample. Again, Table 1 shows that the households become smaller and increasingly urban over time.
Hence, all of these trends and summary statistics indicate that households in Costa Rica that are smaller, richer and urban have higher levels of education than those that are not. The next section employs regression analysis in order to tease out the magnitude of these effects, as well as the importance of family size in consideration with these other factors.
Initial ordinary least squares regressions were run on years of education, controlling for number of siblings, age, age squared, sex, mother's schooling, father's schooling, urban areas, owning a refrigerator, owning a home, and having electricity. Separate models were run for children ages 9-15 in each year of the survey. The coefficient of greatest interest for this analysis is the coefficient on the child's number of siblings in the household, and the other variables simply serve as controls. The results of this regression are shown in Table 3 below.
The first, most striking observation is that all of these variables significantly explain some of the variation in years of schooling. Additionally, the negative coefficient on the number of siblings is increasing over time. While the coefficients are statistically significant, the substantive significance of these statistics is quite small. For example, the coefficient on the number of siblings in 2000 means that increasing the number of siblings a child has by one decreases the number of years of schooling of a child ages 9-15 by 0.09 years. In other words, increasing the number of siblings by 10 decreases the years of schooling of a child ages 9-15 by one year. However, as seen previously by Figures 1 and 2, the average number of children born to a woman in 2000 is less than eight, across any level of educational attainment. Therefore, it is unlikely that a child's number of siblings would have a large effect on their educational attainment.
In Figure 4, children ages 16 were examined separately to investigate if older children's education is more sensitive to changes in the number of siblings. The coefficients on number of siblings for sixteen year olds are slightly larger when compared to those from children ages 9-15, but the coefficients suffer from the same questions of substantive significance as in the previous model. However, it is interesting to note that coefficient on being male is negative, and that the magnitude of this coefficient is larger for 16 year old boys than it is for boys ages 9-15. This coefficient demonstrates the diminishing returns of schooling for boys; after age 15, the opportunity cost of attending school is significantly larger than it is for younger children. Hence, many boys over age 15 drop out of school to work, or are encouraged to do so by their families. This fact is compounded slightly by one additional child in the family, increasing the negative effect of being male on schooling in larger families.
In Table 5, the samples from all three years are pooled into one model, and dummies for 1984 and 2000 are added to account for variation in education levels over time and other unexplained year effects that could affect educational attainment in a given year of the survey. As different controls are introduced into the model, and interesting result emerges. While in the first specification, the coefficients on years 1984 and 2000 are positive, indicating that the level of education is increasing over time, the signs on these coefficients are reversed once parents' education and the number of siblings are included in the model. Not only is the sign reversed, but the magnitude of the effect of years 1984 and 2000 increases. This is contrary to what is expected, one would expect these additional controls to absorb some of the variation of educational attainment over time. This changing sign suggests the possibility of the presence of multicollinearity in the regression; that is, the fact that parents' education is highly correlated with the later years of the sample, 1984 and 2000.
In fact, Figure 2 exhibits this multicollinearity through the fact that the number of children born to mothers is decreasing both over time, and by the mother's years of education. This illustration, combined with the changing signs on coefficients as shown in Table 5 make a strong case for the presence of multicollinearity as the reason for the confusing the results. Again, the coefficients on number of siblings are negative and significant, but substantively small. In order to examine one more measure of household size, the variable for number of children was replaced by the variable indicating number of people in a household. Results are shown in Table 6 below.
Here, the coefficient for adding one additional person to the household has more variation than the siblings variable had, but the overall magnitude stays within the range of the sibling variable. While the coefficient is smaller in 1973 than the coefficient on one additional sibling for children ages 9-15, the coefficient on adding one additional household member in 2000 is approximately equal to adding one additional sibling to the family of a 16 year old. Hence, the measure of siblings and the measure of overall household size, regardless of the individuals' relationships within the household, capture essentially the same effect. Other control variables in this regression also have similar directions and magnitudes as in the sibling model. The fact that similar results were found when using two separate measures of family size proves that this negative correlation is a robust finding.
In short, OLS regression results presented in these models predict that the negative effect of larger families on education for children ages 9-15 increases over time, judging by the increasing coefficients on number of children from 1973 to 2000. This means that having an additional person in the household in 2000 decreases an individual's years of schooling by a greater magnitude than having an additional child in 1973. The effect of other control variables, such as parents' education on an individual's schooling decreases slightly over time, when using the household size variable as the dependent variable, but increases when using the sibling variable. As the distribution of educational attainment spreads out over time, becoming less concentrated in fewer years of schooling, the possible outcomes become more variable. The changes in the magnitudes of these coefficients account for the increasing diversity of the Costa Rican population over time. Still, the overall effect of household size on education in comparison with other control variables is small. In the end, while these regressions show that there is indeed an effect of adding an additional person to a household on schooling, this effect is small in comparison to a multitude of other factors that influence a household's educational decisions.
Based on the figures and tables presented in this analysis, it is clear that households in the Costa Rican population have become both smaller and more educated during the past thirty years. However, a cursory glance at these facts reveal very little about the actual changes in the population that have occurred. While there is a clear negative correlation between the size of a household and the level of schooling of individuals in that household, the direction of the causality is more complicated. In fact, not only do smaller households attain higher levels of education, but urban households, households with more educated parents, and households with electricity also attain higher levels of education. Due to the fact that all of these factors develop simultaneously as a society grows, it is likely that they all play a role in increasing education of the individuals within that society, to some extent.
The ordinary least squares estimates in this analysis reveal that all of these factors do indeed significantly influence the amount of schooling that a child in Costa Rica achieves, including the effect of household size, which is the focus of this analysis. As expected by classical theory, the years of education a child receives decreases as the number of people in that individual's family increases. For 9-15 year olds, adding one additional sibling to a family decreases the years of schooling for that child by between 0.04 and 0.09 years. The effect is slightly larger for 16 year olds, but still remains substantively small. In fact, this effect is smaller in magnitude than the effect of several control variables, for example, increasing mother's schooling by one year. Other significant predictors of educational attainment include household income, which is replicated here by the presence of electricity, a refrigerator, owning a home, living in an urban area, and being female. Despite the fact that the magnitude of the effect of increasing family size is relatively small, this nevertheless settles part of the debate of the causal relationship between household size and educational attainment.
It is difficult to place a magnitude on the opportunity cost of schooling in Costa Rica based on these results. However, the regression estimates here can be understood to mean that the opportunity cost of schooling is higher for families with more children, and for families whose parents have lower income. As Becker suggests, this could result from a tradeoff between the quantity of children, and the quality, or investment that parents instill in each of their children. The larger negative effect found for older children (age 16) demonstrates the diminishing returns to education for children in large families, especially for boys in large families. As their earnings from labor increase with age, the value of schooling decreases in comparison to these wages. While the actual costs associated with schooling, costs like books, shoes and pencils, may be relatively small, the cost of wages these children could have earned had they not been in school is much higher. This certainly increases as children age and are able to perform more difficult, or valuable, tasks. Additionally, if parents have fewer years of education, they most likely have lower wages, which would make the children's contribution to income even more valuable.
The story is less clear for households with many children. Children's wages could be more highly valued because there are more mouths to feed in the household; or, parents might decide to have more children in order to help support the family. Regardless of whether this decision was made before or after educational decisions, the effect is the same: larger families tend to value children's wages more than smaller families, a fact which increases the opportunity cost of attending school. The OLS results confirm these suppositions, and inform possible changes to a variety of conditional cash transfer programs in Latin America. While a fixed monthly stipend or coupon may encourage a family with one or two children to send their kids to school, the fixed amount of the transfer may not always be enough to overcome the opportunity cost of education for children in large families.
Despite the possible limitations to education that children in larger families face, the data from Costa Rica mainly presents positive changes over time through increased education, and suggest that the country is continuing to grow and attain even higher levels of education. Every generation of children that grows up to be more educated parents can help make informed decisions to serve as a model in the community and throughout the region.

The article, "Economic Development in Brazil, 1822-1913", by Nathaniel Leff attempts to explain the poor performance of the Brazilian economy from its independence until the 20th century, and then further attempts to explain the dramatic shift to a period sustained long term economic growth. Leff claims that the original poor performance was due the fact that the domestic agriculture sector of the economy was a very large, if not dominant, part of the economy and that its performance was hindered by extremely high transportation costs which kept it from being profitable. Leff explains the shift to long term growth with the introduction of a new constitution in Brazil which shifted it to a decentralized federal republic and allowed for greater public investment that spurred the economy.
The information presented in Leff's article is important because it provides evidence that suggests that states that have tightly controlled governments can restrict economic growth. It further shows how a switch to a decentralized federal republic can provide necessary public finance that allows for investments that can help instigate economic long term growth.
In his article, Leff argues that transportation costs could have been reduced by the introduction of an extensive railroad system, but this system was late in coming because difficult terrain made the construction costs very high and the tax revenues generated by the government through the taxation of imports and exports was simply not enough to fund it. Taxing the domestic agriculture sector of was unattractive economically because of the great distances involved, poor communications, and low literacy rates. Taxing imports and exports, however, had much lower administrative costs, and thus provided the bulk of Brazil's public funds. When the government adopted a new constitution in 1889 which changed Brazil from a centralized imperial regime into a federal republic it allowed for greater opportunities for overseas borrowing. These increased public finances led to an increase in government spending which help jump start the Brazilian economy into a period of long-term economic development.
In order to demonstrate the size of the domestic agriculture sector Leff explains that at 1820, 70 percent of the population were free people, and that free labor was seldom employed in export activities. He also explains that the "limited information available on the sectoral composition . . . suggests a large fraction . . . was engaged in domestic agriculture." One last piece of evidence he uses to support his estimate of the size of the sector is his claim that from 1911-1913 exports only accounted for an estimated 16 percent of GDP, leaving an obvious gap to be filled by the domestic agriculture sector. The evidence he presents to support the governmental change causing the economic shift is simply history. When Brazil abandoned its absolutist imperial regime for the new constitution of decentralized federal republic, the states could and did raise more in tax money and overseas borrowing. He further supports this by charting out the increase in public expenditure dedicated to transportation, namely the railroads. He demonstrates this increase with the year and the corresponding amount of railway track existent in Brazil, with it showing that after the time of the new constitution the length of railway track dramatically increases, and that this increase corresponds with the boom of the economy.
As just discussed, one of the main points of Nathaniel Leff's article, "Economic Development in Brazil, 1822-1913," is that the dismal performance of the economy of Brazil from the time of its independence from Portugal until the late 19th century was the result of the poor performance of the domestic agriculture sector. Leff argues that the domestic agriculture sector could have this effect because it was a very large part of the economy. I think that the evidence presented by Leff to support his estimate of the size of the domestic agriculture sector is very weak, and his argument could have been vastly improved if he had done additional research to produce a more accurate estimate of the amount of the population invested in activities other than domestic agriculture from 1822-1913. In addition to the actual size of the domestic agriculture sector, I believe that his argument involving its unprofitability due to the high transportation costs of Brazil is also weak. Leff provides very little explanation about why the complex system of rivers found in Brazil, including the Amazon River, could not be utilized to provide cheap transportation. In addition to the system of rivers, I think that a further discussion of the condition of existing roads in Brazil would be a worthwhile undertaking.
If the domestic agriculture sector was actually in fact much smaller than Leff claims, his argument that Brazil's poor economic performance was derived from its poor performance is less credible because the smaller the sector can be proven to be, the less of an effect it would have on overall economic performance. Accurately measuring the amount of labor actually invested in the domestic agriculture sector is therefore crucial to his argument. In addition to minimizing the domestic agriculture sector's size, it would have an equal detrimental impact on Leff's argument if it could be proven that it wasn't actually an unprofitable sector to be engaged in. Investigating the transportation costs would demonstrate that if he was in fact correct about the size of the sector, then the fact that it was so unprofitable would accurately explain why the economy did so poorly until the time when railroads were introduced (which effectively cut transportation costs).
In order to test my hypothesis I am going to examine the levels of urbanization in Brazil during this period with the intention of determining the extent to which people were engaged in the non-agricultural sector. Leff claims that as late as 1890 only 11 percent of the population resided in urban centers of 10,000 or more inhabitants, meaning that the number of people working in transportation, commerce, crafts, manufacturing, and government must be small. I believe that urban centers of much less than 10,000 were plenty and provided many opportunities for the inhabitants to be engaged in activities other than agriculture. Further I believe that non-agriculture activities were not limited to the urban centers and could instead also be found in rural areas. First I am going to address the issue of transportation costs, specifically addressing the Amazon River, and then addressing the rivers in São Paulo and Minas Gerais. I am going to focus my research on the non-agricultural sector into two case studies of Minas Gerais and São Paulo.
Through the consultation of three online encyclopedias, Britannica Online, MSN Encarta, and Wikipedia, I have determined which rivers were the most significant in São Paulo and Minas Gerais. These sources are valuable because they are providing facts about the rivers that are not debated. The information has been provided by actual documentations of the real conditions present.
The article by William R. Summerhill, "Railroads in Imperial Brazil, 1854-1889," appears in book Latin America and the World Economy since 1800. The main point of Summerhill's article is to evaluate the economic consequences of railroads and the role of government policy in Brazil from 1854-1889. Through his analysis he provides useful information about the existing transportation costs in Brazil.
The primary purpose of the article, "Monarchy, monopoly and mercantilism: Brazil versus the United States in the 1800s," by Zanella, Ekelund, and Laband draw support to the notion that the difference in growth rates between the two countries are due to relative factor endowments and institutions. The article claims that political and economic structures present in Brazil allowed the persistence of monopoly restrictions, and that the United States did not, accounting for the differences in growth in the 1800s. The article is particularly relevant to this research paper in its discussion of monopoly and the Amazon River.
The book, Slavery and the Economy of São Paulo, 1750-1850 by Luna and Klein provides a vast array of valuable information. Its primary purpose is to outline the growth of the economy and society of São Paulo from the time of its colonization by Portugal, to introduction of coffee in the mid-19th century. The book claims that there had been a lack of interest in this topic because the most substantial social and economic data surviving were from the first national census in 1872, but because of the discovery of a "vast store of previously unknown and unused population and production censuses in the state archives that go from at least the 1760s until the 1850s," the authors of this book could now undertake such a topic.1 The previously unknown censuses they use provide valuable information for this paper such as the breakdown of agricultural and non-agricultural heads of household in São Paulo, the households engaged in liberal professions and the military, as well as information about those engaged in commerce, transport, and as day laborers. Furthermore they give us population estimates so that we can determine the percentages of the population engaged in the non-agricultural sector.
Another source for São Paulo that will be addressed is Elizabeth Kuznesof's Household-Economy & Urban Development: São Paulo, 1765 to 1836. Her claim is that between 1765 and 1836, the household economy of São Paulo was "transformed from subsistence to a market-oriented economy."
The article, Freedmen in a Slave Economy: Minas Gerais in 1831,by the Herbert Klein and Clotilde Andrade Paiva also uses two unpublished 1831 censuses from two major municipios; those of Campanha in the southwestern part of the province, and Sabara in the central zone near present day Belo Horizonte.5 This article is mostly about the freed colored population, but the analysis provides many insights to the fraction of the population not engaged in agriculture.
The first issue to address now will be Leff's claim that extremely high transportation costs were what caused the domestic agriculture sector to be unprofitable. This argument seems counterintuitive if one considers the extremely complex system of rivers that can be found in Brazil, including the world's second largest river, the Amazon. Within his article, Leff does concede that rivers and coastal shipping were used for transportation, but he claims that "some of the country's rivers (the Amazon, for example) were poorly located from the viewpoint of promoting economic development," and further he argues "other rivers flowed in a direction that was not advantageous from the perspective of production for markets."
The key river with the highest economic potential was that of the Amazon River. The Amazon River is the second largest river in the world (second to the Nile River), measuring 4,000 miles, with roughly half of it located in Brazil.7 Over 200 of its tributaries are located in Brazil, and the main body of the Amazon is navigable by ships as large as ocean liners over two-thirds of its length.8 The ease at which goods could be transported on the Amazon is undeniable, but upon further investigation it can be seen that Leff's denial of its lack of influence in Brazil's 18th century economy can become clear. First we will address the part of Brazil actually able to utilize the Amazon River.
As mentioned earlier, the article by Zanella et al addresses the Amazon River and the monopolies granted in regards to transportation of goods. They argue that even by the mid-19th century, the Amazon was largely unexplored despite its tremendous economic potential. With the permission of the Brazilian government, the U.S. Navy Department was given permission to explore the Amazon in 1850. However, Matthew Fontaine Maury, the first head of the United States Naval Observatory and Hydrographic Office caused alarm to the Brazilian government by speaking openly and enthusiastically about the economic potential of the Amazon River to both businessmen and the U.S. Congress. As a result, the alarmed Brazilian king granted monopoly rights over the navigation of the Amazon River to Baron Mauá to protect Brazilian interests in the Amazon (from the United States).9 As a result of these rights, Mauá was receiving freight-rates on transported goods of 18-30% of their value. While this would have been exceedingly profitable for Mauá, those wishing to take advantage of the Amazon to transport goods cheaply found themselves not much better off than if they had been transporting them overland. However, the monopoly rights held by Mauá were ended in 1867. Bearing this in mind, we have an idea of why the Amazon did little to help high transportation costs in the 19th century. First being that it was simply unexplored, and second were the high rates charged by Mauá until 1867. The reason why the ending of monopoly rights on the Amazon did little to help the overall Brazilian economy will become clear after examining the Amazon River's location.
Figure 1 shows a map of Brazil and the Amazon River and its tributaries. It can easily be seen that the Amazon proper is strictly isolated to northern Brazil. The tributaries, the Xingu and the Tocantins are somewhat more central to Brazil, but still do not reach the southeast, specifically those regions of Minas Gerais and São Paulo. Summerhill argues in his article that "although the north was well served by rivers, by the end of the eighteenth century most of the population was well south of the Amazon basin."
Even though the Amazon was not available to the southern territories does not mean there were no other rivers located in these regions. In São Paulo there are the Tietê River and the Grande River. The Tietê River is about 700 miles in length before joining into the Paraná, and several of São Paulo's largest cities are located on it, but it has relatively poor navigability due to frequent falls and rapids.13 The Grande River, or Rio Grande, which flows through Minas Gerais and along the border of São Paulo is also hindered by waterfalls and rapids.14 The São Francisco flows through Minas Gerais and is navigable for about 850 miles on its middle course, but the rest is interrupted by rapids.15 There are various other rivers located in the southeast of Brazil, but like those already mentioned, their economic viability is eliminated due to the existence of waterfalls and rapids. It seems at this point that Leff's argument about the complex system of rivers of Brazil doing little to provide cheap transportation can be taken as credible, although it could have been strengthened within his paper had he explained about the overwhelming existence of waterfalls and rapids in the southeast.
Because of the lack of navigable rivers, states like São Paulo had to make do with roads. Kuznesof tells us that by 1802 roads had greatly improved but were still very costly and that it was between 1802 and 1836 that a new important road had been constructed between Cubatao and Santos, and another road which would be passable by cars was in the planning stages.16 Summer further tells us that as a result of São Paulo's efforts to improve roads, such as paving them with macadam, by 1864, São Paulo had the lowest freight charges in all of Brazil at the time.17 However, in most other areas of Brazil, freights were hauled by mules over unimproved trails and the charges were drastically higher than those of São Paulo.
The next important issue to tackle is the size of the domestic agriculture sector. As we have determined, because of the high cost involved in the transportation of goods, the domestic agriculture sector would have been indeed quite unprofitable. This is why it will be important to determine that the domestic agriculture sector was as large as Leff claims, and its unprofitability because of transport costs would indeed explain the poor performance of the Brazilian economy at this time. To do this I am going to specifically address Minas Gerais and São Paulo and by examining their urbanization levels, try to determine the extent of those who were engaged in other activities besides agriculture. First I will examine São Paulo.
The choice someone entering the non-agriculture sector of São Paulo had included many diverse activities. There were merchants, store owners, civil construction workers, potters, metalworkers, woodworkers, spinners and weavers, clothing workers, shoemakers, and leather workers. There were also educated liberal professionals such as clergymen, government officials, lawyers, doctors, and teachers of all types. They could also be members of the military.18
This table, taken from Luna and Klein's book, shows the breakdown of agricultural and non-agricultural heads of household by color and sex for the 41 counties of Sao Paul in 1829. It can be seen that at least at this time, more than half of the heads of households in São Paulo were engaged in activities other than agriculture. If taken at simply this it might be determined that of the 16,278 households in São Paulo at this time, roughly 41% were engaged in the non-agricultural sector, and that this percentage would carry over to the population as a whole. However, it must be taken into account that only 22% of non-agricultural households owned slaves, compared to 29% of households owning slaves in the agricultural sector, on top of which most non-farmers averaged about half of the total number of slaves as agriculturalists.20 This pushes the amount of total amount of the population engaged in non-agricultural activities to a lesser degree, but it would still be more than the figures alluded to by Leff. Leff argues that by 1890 only 11% of Brazil's population resided in urban centers of 10,000 or more inhabitants, and because jobs in the fields of transportation, commerce, crafts, manufacturing, and government, were typically located in cities, that this figure suggested a large fraction of the population was engaged in agriculture.21 However, Luna and Klein point out that a "large proportion of the population in even the most rural communities did not work the land, but rather provided crucial services for those who were engaged in agriculture." Despite this statement, Luna and Klein tell us that most non-farmers were concentrated in the first few districts of an urban settlement.22 This, combined with the amount of households engaged in non-agriculture, suggest that Leff's figure of 11% urbanization by 1890 is perhaps not correct. Luna and Klein, however, also consider those engaged in export activities as members of the agricultural sector, so if they are accounted for, the figure for those engaged in the domestic agriculture sector decreases.
One theory presented in Luna and Klein is that as export activities grew, such as sugar and coffee, former subsistence farmers (who would have been counted as agricultural) were forced off of their lands and into urban centers to become either skilled, or more likely, unskilled laborers. One of the examples they list is the fact that several families who had been listed as subsistence farmers in a 1777 census were listed as weavers and spinners or leather workers in the next census in 1798, which they claim resulted from the increase in sugar production in that area. Furthermore, Luna and Klein point out that although non-agriculturists in São Paulo could sometimes attain great wealth, on the whole they tended exhibit more characteristics of those in poverty than farmers and planters. 23 Keeping this in mind, a counter-theory to Leff for the explanation of Brazil's poor economic performance can be formulated. Perhaps because the non-agricultural sector was larger than he thought, and that most of those engaged in this sector were in relative poverty, this would instead change the explanation from the dominance of the agricultural sector to the dominance of the impoverished non-agricultural sector.
Now it is time to switch focus from Sao Paul to Minas Gerais. Minas Gerais is another state of Brazil located in the southeast. In Freedman in a slave society, it is claimed that "at least half of the rural population engaged in non-agricultural activities" in the 19th century.24 This estimate does not take in account those rural households engaged in both agricultural and non-agricultural activities, but at least demonstrates that participation in the non-agricultural sector was prevalent. In 1833 the total population of Minas Gerais by administrative divisions was 768,666 and by 1872 the population was up to 2,102,689.25 Bergad does not provide any information on the occupational categories of the population for 1831, but he does however provide a selected breakdown for both slave and free populations in 1872:
When comparing this graph to the figure I just listed for population in 1872 (2,102,689) it can easily be seen that the graph does not account for the entire population, and instead only a sample. From this sample, however, we can see that farmers only make up 37%. The non-farmer population it accounts for totals to 830,414. If the population was indeed 2,102,689, then the non-agricultural sector in Minas Gerais at this time was at least 40%. While the farming population is obviously underrepresented in this table, the non-agricultural sector, at least, is not overrepresented, and could also possibly be underrepresented as well. What is known is that during the 19th century, Minas Gerais had the largest slave population (382,628) accounting for 18.2% of its total population.27 Typically the type of "farming" that slave labor was engaged in were export activities, which should not be counted in the domestic agriculture sector. Bergad also gives us another occupational breakdown based on a limited sample of 3,062 slaves and finds that only 21.6% are engaged in agriculture, while the rest are in many other non-agricultural occupations such as: carpenter, blacksmith, coachman, cook, barber, shoemaker, tailor, muleteer, seamstress, stonemason, miner, and others.28 While Bergad tells us that the sample is too small to indicate any general occupational data of slaves in Minas Gerais because only 2.7% had occupations listed in the inventories, it still provides us with an idea of the vast amount of other positions besides agriculture that the population was engaged in.
It can be seen through the analysis of the this paper that Leff's argument about transportation costs in 19th century Brazil are founded, and have been strengthened by the evidence presented. Even after dealing with the problem of monopoly pricing, the Amazon River was still restricted to northern Brazil. Where there existed rivers in southern and southeastern problems, they were on the whole not navigable to any profitable degree because of the abundance of rapids and waterfalls. There were roads in existence in Brazil, but even the ones in the best condition, those of São Paulo, were still highly costly. The argument that high transportation costs would have rendered the domestic agriculture as unprofitable stands credible and has been further substantiated.
Where Leff's argument seems to lose credibility is his dismissal of a substantial non-agriculture sector. One of his claims is that because of the lack of large urban centers, claiming that only 11% of Brazilians resided in cities of 10,000 or more by 1890, very few people would be engaged in activities considered non-agricultural. However, by examining the cases of Minas Gerais and São Paulo this paper has shown that there existed a large and significant population that was engaged in the non-agricultural sector, in both rural and urban areas. Because most of those engaged in this sector were in relative poverty, it is possible to explain at least a portion of the poor economic performance of Brazil in the 19th century to the fact that this sector was so large, instead of the domestic agriculture sector.

It is well known that high school graduates may attend college to signal their ability to potential employers, regardless of any benefits intrinsic to the educational process itself. Spence's classic paper on job market signalling demonstrated that agents of inferior quality may choose to acquire high levels of education in order to join the pool of college graduates, and gain high wages [1]. But colleges themselves are heterogenous: Western Michigan is not Yale, and employers know this. Choosing which college to attend is thus a matter of choosing which signal to send, and it is a problem that is complicated by competitive admissions processes.
Students and their families across the United States face this problem every year. Getting into the good schools is tough, but clearly would provide a head start in the job market. Students know their own attributes, but are uncertain about how the schools weight them. In the face of application costs in the form of both time and money, how high should one aim? The admissions process forces agents to self-select the appropriate range of schools to apply to. It is this feature that makes the college attended a sensible screening device for firms hiring new graduates.
This is similar to the ideas in Arrow's paper, "Higher Education as a Filter" [2]. He considers the impact of college admissions processes in the simpler case where colleges are completely homogenous. In his formulation, higher education filters students in two ways: through admissions, and through the graduation process. Provided that the skills required to perform well in college are positively correlated with the revenue product of labor (the "positive screening assumption"), firms know that students that have successfully negotiated these two obstacles are, on average, of greater value to the firm. Hence, in a competitive labor market, college graduates are paid higher wages.
His concern is with the social value of higher education. In a one-factor world, where all labor is homogenous, and where higher education does not actually add to productivity, higher education is simply a drain on societal resources. In contrast, if one extends the model to allows for multiple factors of production (say "skilled" and "unskilled" labor), higher education may allow firms to screen their workers and deduce which type of job they are best suited to. This may increase allocative efficiency and thus be good for society.
In this paper, our focus is on the filtering and screening properties of higher education in the case where colleges are heterogenous. We analyze in detail the effect of allowing students to apply to only one of two colleges with different reputations. In deciding which school to apply to, agents must weigh the future benefits of a diploma from the top college against the risks of not getting in, and thus not attending college at all.
We may view this as a process whereby agents choose which pooled signal they would like to try to send. An agent that graduates from Harvard can expect to be paid more than his peers at other institutions, because attending Harvard sends a signal to potential employers that the agent has high ability. Joining the top pool (top school) pays off in the job market. It is clear that all agents would like to attend the top school, and free-ride off the school's reputation. It is thus crucial that their college choice is mediated by a competitive admissions process. Agents must balance the increased payoff of getting in to the better school against the increased risk of not getting in to college at all.
It is this institutional feature that distinguishes this model from the approach taken in Spence's original paper. In his model, low ability agents could mimic high ability agents (acquire the same amount of education as them) provided that they were willing to bear the costs of acquiring that level of education (and their costs were higher than those of the high-ability agents). Here this is not possible. Weaker agents may try to apply to the top school, but the admissions process means that they have very low odds of getting in and mimicking the top agents. We argue that it is this feature that forces a type of separating equilibrium where all the top agents apply to the top schools, and lower ability agents do not try to mimic them.
The importance of institutions that allow agents to pool has recently received attention in the literature. Pradeep Dubey and John Geanakoplos have shown that quantity limits on contributions to insurance pools can guarantee the existence of equilibria in insurance markets [3]. This existence occurs because high-risk agents, who wish to hold a large amount of insurance, have to join pools with high quantity limits and thus declare their type. Something similar occurs with college admissions, where a limit on the admitted class size forces low ability agents to apply to less competitive schools, and thus declare their type.
Other pooled institutions include sporting tournaments and academic journals. In the case of golf tournaments, for example, professionals often have to choose which tournament to attend on a given weekend. One may offer greater reward, but is bound to be more competitive. Similarly, in submitting a paper for publication, authors must assess the quality of their paper in order to decide whether to submit it to a top journal. If they aim too high at first, they are likely to face delays in getting their paper published.
The model presented here is focused on college choice, looking at the case where there are only two colleges. The basic model of college choice is presented in Section 2, and the resulting equilibrium is analyzed. Section 3 introduces some dynamics and examines how educational institutions may endogenously become differentiated over time. Section 4 applies the model to race-based admissions, and Section 5 concludes.
We assume that agents may choose between only two colleges, college 1 and college 2. For the moment, we abstract away from the wage generation process of firms (examined in the subsection below) and assume that the wages paid to graduates are given exogenously as w1 and w2 . Let college 1 be the top school, so that w1 > w2 . Agents that get into college receive payoffs of w1 and w2 respectively; while those that don't are assumed to receive a wage of 0, for simplicity1
There are a continuum of agents, indexed by their ability level θ, where θ has positive (possibly unbounded) support, has a finite mean, and a continuous cumulative distribution function F (θ). Each college computes a score for every agent applying to that college S . This is given by
where ε has support ( −∞, ∞), continuous cumulative distribution function G(ε) and E[ε] = 0. Assume that the error distribution G is independent of the ability distribution F . The colleges are assumed to have a capacity of measure Ci ≥ 0, i = 1, 2. After ranking the applicants by score, the top Ci students are offered places. Agents may apply to only one college.2
While the proof of this result is surprisingly lengthy , the intuition behind it is not. Consider the first claim. In this case, because the total college capacity is sufficient to accommodate the agent population, in equilibrium those applying to the unattractive college, college 2, are guaranteed a place. As a result, only agents that are sufficiently likely to get in to college 1 are willing to take the risk of not being accepted, in return for the wage premium w1 − w2 . There is precisely one point at which an agent is indifferent; this point is
In the second case, the agents face aggregate risk, since the total number of places at college is insufficient for the population. One might imagine that in this case, some of the "good" agents would choose to apply to college 2 to make sure of their acceptance to college. But given that behavior, some agents of lesser ability may take a chance on college 1, since "good" agents are applying to both colleges and getting in will be difficult in any case. Thus, in this case, we would not have a separating equilibrium. The theorem makes clear the circumstances under which this intuition fails. Where a marginal increase in ability always enhances the agent's proportional odds of getting in to the top college relative to the bottom one, good agents always find it preferable to take advantage of this and apply to the top college. Similarly, weaker agents have an incentive to apply to the bottom college, and a separating equilibrium obtains.
This result proves formally that the college applications process acts as an effective screening device in a setting where agents choose between colleges. Due to the separation, the eventual outcome of this game is a set of graduates from each college that are markedly different in ability level. Firms may use information about which college an individual attended to improve their estimate of agent ability. Thus in this model screening is obtained endogenously as a result of strategic agent behavior, within the given institutional framework. Notice that the admissions process itself plays a minor role in the screening, although on average the better applicants tend to be accepted. Rather, it is the presence of a fairly reliable testing mechanism that induces agents to reveal their type through their own college selection.
It is also clear from the proposition that, in general, the screening properties of college admissions will be enhanced when the size of the college-going population is less than the available number of places. This seems likely in two scenarios. Firstly, in countries where tertiary institutions have some freedom in setting tuition fees (as in the United States), we would expect the "price" of tuition to be such that the market clears. This suggests that the supply of college places will be roughly equal to demand. Furthermore, in reality colleges tend to admit more applicants than they actually expect will accept their offers. This necessitates them having excess capacity to cope with years when they have high acceptance rates. These facts would support the contention that college capacity is indeed sufficient for the college going population and the conditions of the proposition hold.
Secondly, in countries where the government regulates tuition fees or waives them completely, it is often necessary to pass a standardized test to be allowed to apply to any of the colleges at all. For example, in South Africa, one requires a certain number of points in the matriculation examinations to be eligible for college. Governments may use this type of policy to equate the size of the population applying for places and the number of places. Thus it seems that the requirements for the existence of a separating equilibrium are not unduly restrictive.
In this section, we tackle the question of how firms behave. For simplicity, we assume that the labor market is perfectly competitive. In this model, we assume that college serves only one educational purpose: to provide a diploma and a set of skills that allow the graduate to enter into the "skilled" sector. Education does not add value, in the sense that it does not increase the workers' ability level. Unskilled workers are indistinguishable, and are paid a reservation wage r, which, without loss of generality, may be normalized to 0. In contrast, firms looking to hire skilled workers receive information about which college the prospective employee has attended. Based on this limited information Ω, they pay some function of the expected ability of the agent, w = w(E(θ|Ω)) > 0.
It is clear that this simple model misses important aspects of the wage generation process. Firstly, the role of education here is somewhat at odds with the human capital literature, where education actually increases the agent's existing ability, rather than simply providing a skill-set. In section 3 below, I explicitly consider this modification to the model. More importantly, firms observe far more information about the agent than simply a college dummy variable. A typical resum ́e of a college graduate might include his or her grades, ma jor fields, extracurricular accomplishments, and summer job experience. These are all individual signals. Clearly, this will reduce the effect of the pooled college signal. This model should therefore be viewed as an extreme case.
Notice immediately that with the firm behavior as given above, we may apply the results of proposition 1 to deduce agent behavior. Thus it is now possible to analyze the game that results when firms set wages endogenously after observing college outcomes.
This result is in some ways trivial. Consider the case where the cut-off level,
Unfortunately, this may be the only such equilibrium. To see this, note that for an agent of ability
where we are using the fact that firm's optimal wage offers depend endogenously only on θ∗ (as well as exogenously specified distributions). But notice that the L.H.S. is monotone increasing in
This may indicate that important aspects of the model have been left out. For example, if one included the possibility that education added to human capital and that colleges with small enrollments were better able to provide such education, agents may choose the low-wage college in order to benefit from a smaller enrollment. The payoffs may then be adjusted in such a way that an interior solution for
It also seems reasonable to say, however, that this result is in some sense a product of the solution concept. Firms are assumed here to know the structure of the model, and in equilibrium to successfully guess the separating point
Suppose that agents believe that the firm's wage generation process is, on average, static: that is, their best guess of this period's wage offer is last period's. This seems like a plausible behavioral assumption, in light of the fact that each generation of agents plays the college admission game only once, and must look to the past for indications of how firms might behave. In this case, agents will have wage expectations given by last period's wages w1(t −1) and w2(t−1) , and behave as in proposition 1. In particular, provided that the requirements given in proposition 1 are met, the unique equilibrium will be a separating equilibrium.
Suppose also that one year after hiring them, firms observe some imprecise measure of the true productivity of their new hires. Hence when making their wage offers, their only new information (as compared to last period) is how last period's hires from each college did. For simplicity, assume that firms thus set their wage offers as a moving average of the observed productivities of graduates of each college over the last 5 years. That is,
where pit is the observed productivity of college i students graduating in year t. Again, this represents a sensible strategy for firms, since the observed productivity signal is not entirely accurate, and thus averaging over the past five years reduces error while still responding to new information. We now make things more concrete by considering a specific case.
Given this explicit formula, it is easy to do comparative statics and show how the exogenous parameters affect the separating point. Notice that
We use the framework above to simulate the evolution of wage offers to graduates of different colleges in an economy. Of necessity, we consider a situation with a finite number of agents (in this case 1000), and assume that they behave as if there were a continuum of them. We use admissions error bounds given by a = 0.1, capacity at each school of 0.5 (half the population) and errors in firm's observations of true productivity drawn from a uniform distribution on [ −b, b], where we set b = 0.2. These may also be thought of as macroeconomic productivity shocks. These parameters conform to the requirements of proposition 3 above for the range of wage offers generated. In order to generate initial wage offers, we create a history of the last 5 period's productivity levels, where college 1 has been better over those 5 periods. The differences are rigged to be very slight (on average, productivity in college 1 is around 0.005 higher than that of college 2). Notice that in all other respects, the colleges are identical.
The simulated wage path shown in figure one above demonstrates a key result: initially almost identical colleges can endogenously become differentiated over time as a result of random initial fluctuations in productivity. This is a striking example of "tipping". In this case, agents seeking to profit from a small initial wage differential cause a separating equilibrium. Firms notice the difference in productivity levels across the colleges, and change their wage offers accordingly. Suddenly, the small wage differential becomes large, and one college attracts the best students. College 1 emerges with a reputation for producing top quality graduates while the other college languishes behind. One should note that firms cannot tell whether this is because of the separating effect argued for here, or because college 1 really does provide a better education. Thus even if Yale were to provide exactly the same education as Western Michigan, firms and society at large might incorrectly attribute the difference in productivity between Yale and Western Michigan graduates to differences in education, and conclude that Yale was a better school.
This model seems apt for looking at students' choice of graduate school. In this environment, the pooled signal conveyed by the college that students graduate from is, in general, regarded as very important by potential employers. The academic job market for positions in top-ranked schools tends to be closed to graduates of schools with weaker reputations. Furthermore, since graduate students are often either not assigned grades, or the grades are seen as unimportant, this pooled signal is one of the few "objective" (easily measured) signals available to potential employers.
Turning to potential students, we notice that students take the rankings of graduate schools in publications such as US News and World Report surprisingly seriously. The prestige of the school is also very important. This model suggests a potential reason for this. Essentially, these factors act as coordinating devices much in the same way as wages do in the model we presented, and allow agents of similar ability to group themselves together, and thus maximize the value of their graduate experience. The model predicts that some schools will attract the top students year after year, since these agents will self-select the top schools.
This result is not surprising. It is interesting, however, that the model shows how hard it may be to reverse the trend. Small initial fluctuations result in large differences between colleges, once students self-select and separate. Now imagine that the weaker college, college 2, hires superb new faculty, and in fact this means that graduates of college 2 emerge with higher productivity than they started with. College 1 continues to give no value added. Yet, unless the value added by college 2 is very large, this will not change the outcome. College 1's reputation attracts the top students, and these students will remain the top students despite college 2's best efforts. They will get the top jobs, and college 1's reputation as the better school will remain intact, and the top students will continue to go there. The logic is relentless.
Figure 2 shows this scenario. After period 51, college 2 graduates are assumed to receive a bonus of 0.25 to their productivity (which is, on average, doubling it). Despite this, they simply aren't as good as those of college 1, and college 1 remains the top college. What might one do to try and catch up? Part of the answer is to reduce the size of the incoming class, and thus make admissions more competitive. The quality of the admitted students thus improves. The effects of this adjustment are shown in figure 3, where college 2 is assumed to cut its enrollment from 0.5 to 0.2 in period 51.
Notice that this has almost as much of an effect as the quality improvement depicted in figure 2. Together, these strategies can work to reverse the trend. Specifically, suppose that college 2 has access to a fixed endowment, and that the exiting productivity of students is a function of the amount spent on each student. For example, dropping the enrollment from 50% of the population to 20%, and raising per capita spending accordingly may perhaps allow college 2 to add a productivity bonus of 0.3. The result is that college2 is able to catch up and then surpass college 1, as is shown in Figure 4.
In fact, this suggests that temporary increases in spending combined with selective admissions can have a permanent effect on a college's reputation. The time-scale required to effect such a change depends on how quickly wages (or school rankings) rise to reflect the better quality of college 2's graduates. Once wages have fully responded, college 2 will start attracting the best applicants and may reduce spending to earlier levels.
This sort of strategy is not, in general, viable. The schools with access to large endowments tend to be the top schools, and are, in general, far more likely to be able to provide a high quality of education. Furthermore, while wages may respond to an increase in value added by college 2 fairly quickly, it is not clear whether intangibles such as prestige will respond at all (how do you dispel the Ivy League mystique?). Finally, this sort of approach entirely neglects the strategic aspect of this "reputation game"; one would expect college 1 to match college 2 in spending, knowing full well that a temporary increase now will pay off by cementing its reputation for the long term.
An alternate strategy may be to target individual students, acting on preferences not captured in the model by offering them fellowships or access to better facilities. But the model shows that the additional incentives offered will need to be large to counter reputational effects. It is perhaps, therefore, unsurprising that in most disciplines the top schools remain relatively constant.
The question of race-based admissions policies has recently come under legal scrutiny. The U.S. Supreme Court decision in University of California Regents v Bakke ruled that racial quotas were inequitable and should not be permitted, but that other forms of admissions that take race into account may be allowed in the interests of diversity. The University of Michigan's admissions policies are currently under legal challenge, with the Bush Administration, amongst others, arguing that the university's points system for admissions constitutes a de facto quota. It is with this in mind that we apply the model developed above to analyze a simple quota system.
We set up the model as follows. Suppose that a minority group (hereafter to be termed "Blacks") constitutes 20% of the overall population. However, not all members of the total population are eligible to go to college. In particular, suppose that the college-going population is comprised of only 10% Blacks and 90% Whites. This may be true for a variety of reasons, a lower percentage of Blacks completing high school, a failure to pass standardized eligibility tests of the type mentioned earlier, or access to credit. All members of the college going population are otherwise identical.
Now, suppose that in the interests of diversity, all colleges adopt a policy whereby they reserve 20% of their capacity for Blacks (i.e. their percentage in the general population). Notice that this means that the capacity available for Black students vastly exceeds the college-going Black population. This constitutes a quota system. But the outcome would be similar in any case where there was an admissions process that virtually guaranteed entry to Black applicants with certain qualifications 6.
For the simulation, we set the capacities as C1 = C2 = 0.5, as before, which means that the quota of 20% at each college is 0.1. This is a particularly simple version of the model, where the college-going Black population is also of measure 0.1and thus all of them will apply to college 1 and get in. Thus the resulting capacity for white applicants at college 1 is C1=0.4. Choosing parameters a=0.1, b=0.2 and using 1000 agents (900 White, 100 Black), we may run the simulation as before.
The figure above shows the wages earned by the graduates of college 1 and 2 by race. Notice that since all Blacks apply and get into college 1, we have no wage series for Black graduates of college. The wages paid reflect the fact that race is observable to potential employers. Thus the wages paid to Blacks are on average 0.5, since the admissions process yields no information about ability (it is neither separating nor selective). Wages paid to Whites are higher than they would be usually, since with a quota system, getting into either college is more difficult. Clearly though, this is not necessarily beneficial to the average white student, since his odds of getting in are reduced.
This deserves more explicit analysis. Suppose that socioeconomic status and race are highly correlated, so that it is possible to enforce the quota system by basing it on socioeconomic factors rather than race. Since socioeconomic factors may be unobservable to potential employers, under this system the employers may not be able to determine whether the job applicant got in to college 1 on merit or through a quota. Then we have three possible policy alternatives: no quota, a race-based quota, and socioeconomic quota. The key difference between the latter two is that with a race-based ("observable") quota, the employer, who may easily observe the race of the applicant, may use this information in deducing the appropriate wage offer; whereas with a socioeconomic ("unobservable") quota, this may not be possible8.
In the table below we report the average wage of both white and black graduates from college 1 (where they differ) and the a priori expected wage of a white applicants with ability 0, 0.25, 0.5 and 0.75 respectively under each of the three systems9. In all cases, measurements are taken from the steady-state.
It is interesting that for the given parameters of the simulation a quota seems almost Pareto-improving. Yet there are definite winners and losers. Consider first the college going Black population. If the quota is unobservable, the effects are unambiguously positive (average wage goes from 0.5 to 0.769). But if the quota is observable (which is almost always the case), then the quota has no effect on the average Black student, as firms can extract no information from the college signal. In particular, above average Black students are hurt by the quota (since they could have attended college 1 and earned on average 0.769), while less able students are advantaged (since they would have gone to college 2 and got 0.252).
The effects on the college going White population are more complicated. For the observed quota system, both college 1 and college 2 become more competitive. This is reflected in the higher average wage for graduates of college 1 (0.801). Further, the white student of ability 0.5 actually does better, since he now finds it preferable to apply to college 2, and since college 2 is more competitive than before, he ends up with a higher wage. But there are two main groups of losers. The first are the students of ability around 0.6 (not shown in the table), who used to be almost certain of getting in to college 1, and are now borderline applicants. Their ex ante expected wage will fall. One may argue that in a more realistic model, with many colleges and the possibility of multiple applications to limit risk, this change will be small. Fair enough. But the second group to lose out are the White students of low ability (θ = 0 in the table). There the changes are dramatic. Suddenly a certain place in college 2 has become uncertain, as better White applicants now apply to college 2 because of the quota system. The ex ante expected wage falls from 0.252 to 0.048. Moreover, the effects on this group are unlikely to be mitigated in a more complicated model -- there is simply no space for them in the college system anymore.
Turning to the unobserved quota case, the results are similar. Now college 1 White students are on average paid less than their ability, since the firms are less able to determine ability from the pooled college signal. Furthermore, this system is in fact even worse for the bottom White applicants, as the lower wages in college 1 (relative to the observed quota case) mean that more people apply to college 2, further diminishing their chances of getting in.
The major effects of a race-based admissions policy are captured in this simple model. In particular, if the quota system is based on easily observable characteristics, it has little effect on average wage outcomes since employers can and will discount the credentials of the favored group. Furthermore, in this case it is actually prejudicial to the top minority applicants, although benefiting weaker applicants. The model identifies the main losers from a quota system in college admissions as the members of the college going majority group that used to barely get in to college, and now cannot. It is interesting that these are almost never the people showcased as the victims of these policies. Rather, it is the students that fail to get in to top colleges (like the University of Michigan) who are seen as the victims. This is because the impact on the weaker White students is indirect: it is the resulting change in the behavior of their White peers that actually knocks them out of the system, rather than the quota system itself. Overall, this suggests that the overriding concern of policymakers worried about the effects of any preferential admissions system should be the weakest applicants of the group that does not receive this preferential treatment.
Pooled signals matter. The reputation of the college a student attends will almost certainly affect his post-graduation wage. In this context, it is critical to understand how agents may behave when confronted with colleges that they see as heterogenous in terms of reputation. This model makes a start in this respect, establishing the circumstances under which a simple separating equilibrium will exist. It also explains why colleges that are similar in most respects can still attract vastly different calibers of students if they differ in reputation. The key is to recognize that both firms and students are making decisions based on signals: students choose the school with the better reputation in order to signal that they are of high ability, while firms pay more for the graduates of the top school based on their pooled signal. Admissions processes are important in ensuring that low ability agents do not try to mimic high ability agents in attending the top school, and thus perform a valuable screening function.
The analysis of a race-based quota system shows how important it is to look at the overall picture when attempting to analyze these policies. It was shown that the people who are hurt by a race-based policy are the most able of the minority group and the least able of the majority. This is in contrast to the perception that it is the students that get shut out of top schools due to the quota that are most damaged by the policy.
Looking forward, it seems clear that this model has wide applicability. Institutional structures where agents compete for some prize (or to send some signal that yields a prize) are all around us. From sporting tournaments to academic journals, agents must repeatedly make choices about which pool they should attempt to join. Future work could include looking at the effect of having more than two such pools, the possibility of multiple applications, and attempts to characterize the solution in the case where the number of applicants exceeds the number of places, and the likelihood ratio condition fails.

1. Often, we would notice from financial bulletins that the world's key stock indexes generally move in tandem with each other, in particular in the event of a major crisis where major stock indexes plunge drastically or a major economic breakthrough where stock markets boomed. Possible reasons contributing to this trend include (i) the strong interlink between the world's major financial markets where buyers and sellers are operating across the globe like in a single market, (ii) the wide implications to the entire world economy due to a major event such as a terrorist attack or a financial credit crunch and (iii) the general knee-jerk reaction of speculators.
2. While there is a general trend of stock markets moving together in major events, there are exceptions of stock indexes contradicting the trend. These then could possibly due to internal economic factors such as internal good/bad economic performance and bilateral economic relationships with closely linked economies.
3. To analyze this inter-relationship, this report looks specifically at Singapore's Straits Times Industrial Index (STI), the key stock index in Singapore and how the moves in STI is correlated with other major stock indexes and the economic relationship variables.
4. Essentially, the first question we would like to analyze is the correlation between STI and the major stock indexes.
5. Dow Jones Industrial Composite (New York, United States), Financial Times Stock Exchange 100 (London, United Kingdom), Nikkei 225 (Tokyo, Japan) and Hang Seng Index (Hong Kong SAR, China) are selected to compare against the STI index. As world's major financial hubs, the selected stock indexes listed in New York, London, Tokyo and Hong Kong are the main stock indicators and are good representations of the individual cities' general financial landscape. In addition, the selected countries/regions are of similar economic development stage as Singapore (i.e. developed financial markets as opposed to emerging financial markets).
6. The next parameter to set is the time period of analysis. Specifically, there are factors such as availability of data, updated and sufficient data to reflect latest current trend, stability of economy (as opposed to choosing a period of turbulence which may over-skew the data and reflect wrong correlation due to a rare occurrence or low probability). Taking into account the above factors, the selected period is from Jan 2004 to Aug 2007. There are sufficient data points between this selected period as stock markets operate on all working days and that this period does not contain extreme world events such as 911 terrorist attacks. Also, this is also the period which all the selected economies are generally enjoying good growth with minimal economic disruptions or imbalances.
7. To measure the impact, the percentage changes in stock indices are being put into the regression model as opposed to having the absolute value of stock indices which would not reflect the changes. During the selected period between 2004 and 2007, there are more than 900 data points. As different countries have different public holidays, the days of operations vary, on such days, the rate of index is then computed as taking the value of the previous opening day1. In this analysis, only days which all stock exchanges were in operation are being used as the purpose of the report is to look at the correlation between STI and all other major stock indices.
8. Before running the regression model, it is assumed that the model of the correlation between STI index and other stock indexes (namely DJ Composite, FTSE100, Nikkei and Hang Seng), the model in the population is estimated to be as
9. This time series regression model takes into the linear time trend, commonly found in time series analysis where
10. Table 1 below summarizes the regression results of the first model.
11. In analyzing the robustness of the coefficient estimates, there are statistical tests needed to ensure the unbiasedness in estimating the regression model. These are serial correlation and heterskedasticity.
12. Under the Gauss Markov Theorem, there are assumptions made to the time series regression, which only when satisfied, the OLS estimators are BLUE (Best Linear Unbiased Estimator).
In the sample, no independent variable is constant nor a perfect linear combination of the others.
For each t, the expected value of the error ut, given the explanatory variables for all time periods is zero,
In the sample, no independent variable is constant nor a perfect linear combination of the others.
The variance of ut, is the same for all t,
The errors in two different time periods are uncorrelated,
13. In the presence of serial correlation, the OLS estimator is no longer BLUE and the usual OLS standard errors and test statistics are not valid even asymptotically. If there is autocorrelation, the expression is as follows:
14. The model in equation (1) is being tested for serial correlation with the results being tabulated in Table 2. It shows that there is indeed serial correlation with H0:
15. In correcting the serial correlation, we continue to assume the Gauss Markov Model Assumptions 1 to 4 but we relax the Assumption 5 and takes into account the error term model,
16. The OLS on the transformed data is being tabulated in Table 3. This GLS estimator is BLUE and the errors in this transformed equation are serially uncorrelated, of which t-statistics and F-statistics are valid asymptotically.
17. Similarly in cross sectional applications, heteroskedasticity in time series regression models while not causing bias or inconsistency in the coefficient estimate ß, invalidates the usual standard errors, t-statistics and F-statistics. In the test for heteroskedasticity, one key assumption is should not be serially correlated; any serial correalation will generally invalidate a test for heteroskedasticity. After the serial correlation is corrected, we will apply the test for heteroskedasticity.
18. The test for heteroskedasticity for time series regression is the same as the cross sectional analysis which (in the equation (2)) is regressed with the variables, serial_dj, serial_ftse, serial_hs, serial_nikkei where
19. Based on the results on F-statistics (Table 4), we fail to reject H0 for significance level of 10%. Therefore, we can conclude that the transformed equation is not heteroskedastic.
20. The regression model based on the results shown in Table 2 is estimated to be
21. The model illustrates that there is correlation between the percentage changes in STI index and Dow Jones Industrial, FTSE100, Hang Seng Index and Nikkei Index.
22. With regard to the correlation between the stock indexes and the STI index, there are a few observations. First, it is in line with the common notion that there is some correlation between the markets in Singapore and the financial stock indexes in US, UK, HK and Japan (i.e. when Dow Jones suffers a loss, there may be a ripple effect on STI index to react on the following trading day). From the magnitude of the coefficients, we can observe that 1% change in Dow Jones has a return of 0.05% change in STI index. This is relatively small and reasonable correlation as a large correlation is not expected given the differences between the comparing indexes in terms of structure, composition and economic performance and fundamentals. The correlation with the Asian stock indexes is stronger, possibly due to closer proximity in terms of economic structure. The recent economic diversification efforts initiated by the Singapore government to rely less on the major world markets such US have also reduced this correlation relationship. This could also be attributed to the particular time period selected for analysis. This regression model is based on the time period from 2004 to Oct 2007, where there are no significant turbulence economic shocks and even if during certain economic shocks i.e. sub-prime housing loan, the duration is not sufficiently long enough to capture the relationship effectively.
23. While efforts are being made to make data sets for analysis as robust as possible, nonetheless, there are some weaknesses in this selection. Only 4 other stock indices are being selected to conduct the regression model with STI index. These stock indexes are computed using different approaches (ie price weighted or scale weighted versus STI which is weighted-value stock index) and are therefore not an exact like-for-like comparison of stock indexes. There may be missing variables such as neighboring stock indexes (e.g. Kuala Lumpur Stock Exchange Index, KLSE) and emerging stock indexes with growing influence (e.g. China's Shanghai Stock Exchange).
24. The model can also be improved by separating into 2 periods where one of which is a stable period while the other could be a turbulent period, such that we can compare the correlation under different situations. Using the same analogy, the model can be expanded by separating into developed stock markets versus emerging stock markets.

Although its large population has provided China with a sufficient labor force, compared to its limited resources, it is widely believed that China's economic development and ability to modernize to a great extent depends on its success in curbing the population (Tsuya & Choe, 1988). Moreover, education has always been a crucial aspect of China's severe gender disparity. On one hand, China's huge population along with its increasing fertility and decreasing mortality could intensely limit China's further development. On the other hand, China's market transition and economic growth dating from the late 1970s along with the increasing population growth may have actually slowed down the progress toward gender equity instead of accelerating it (Hannum, 2005). The slowing progress is caused by the rising cost of education and increasing population which were resulted from the market transition and economic development. The combination of these factors may result in even more intense competition between boys and girls for parents' investment onto them, which boys are more likely to "win."
Therefore, by investigating the relationship between the educations of women aged 41-64 and their fertility behavior I could possibly answer whether or not China would be left in this "economic growth-> population growth and enlarging educational gender gap-> increasing population growth-> slower economic growth " trap. In this way, the answer may also provide an insight into how the relationship between education and fertility contributes to China's educational gender gap and a possible prediction of China's future development. Furthermore, to investigate the relationship from a cohort perspective could enable us to investigate the dynamics of the relationship, approach the changing historical background of fertility policies in China, and also compensate for the unavailability of data for other than 1982 while helping to forecast what relevant changes may lie in years ahead.
From the perspective of other social context of China, although its large-scale one-child policy did not start until 1979, the wan-xi-shao policy dating from 1970 might actually cause the relationship itself as well as its evolution over cohorts.1 Firstly, the wan-xi-shao policy may drive the relationship itself. This is because on one hand, fertility fell dramatically under the policy – the policy is negatively correlated with fertility in this way; on the other hand, in more developed regions and for the majority ethnicity group Han female education tends to be higher and the policy tends to be implemented in a more rigorous and far-reaching way – the policy is positively correlated with female education in this way. Therefore, the policy might be the intermediary part joining female education and fertility together and drive the relationship between them. Secondly, the wan-xi-shao policy may also cause the evolution of the relationship. This is because for women aged 41-64 in 1982, the younger of them may have not yet completed their fertility at the time of the policy and thus were more likely to be influenced by the policy, while the older of them may have already completed their fertility at the time of the policy and thus were more likely to be exempted from the policy.2 The different levels of policy effect may result in different patterns of the relationship across female cohorts of interest.
From the perspective of economic theory, the negative correlation between the quantity and quality of children in a family is one of the central features of economic theories of fertility (Becker & Lewis, 1973). Willis (1973) further extended the theory and argued that because education increased the value of women's time, women with higher education tend to substitute toward quality and away from quantity of children. This suggests a negative correlation between women's education and their fertility levels (Willis, 1973; Lam, 2005). Lam and Duryea (1999) have more explicitly attributed the negative relationship to the results of two trades-offs: trade-offs between quantity and quality of children and trade-offs between women's time allocation between housework and labor force participation. China, a country with more than one fifth of the world's total population and severe gender inequality of education (Hannum, 2005), provides a critical context for testing the theories and investigating how the underlying mechanisms of the relationship operate.
Specifically, this paper will investigate the following questions: How did the education of women in China aged 41-64 in 1982 correlate with their completed fertility? How would the relationship change across cohorts? What caused this relationship itself and its change over time? My empirical analysis is based on retrospective fertility histories of 89,594 Chinese women aged 41-64 in 1982. I document the strong negative relationship between education and fertility and summarize trends in education and fertility across cohorts. By relying on both the general trends across cohorts and including cohorts in the regression model, I demonstrate that the relationship for older cohorts is getting stronger and the relationship for younger cohorts is getting weaker. Analyzing the mechanisms through which education affects fertility, I first show that for both younger cohorts who were more likely to be influenced by the fertility policy and older cohorts who were more likely to be exempted from the fertility policy, the relationship between education and fertility remains negative. Combing this pattern with the above results indicating varying strength of the relationship, I conclude that fertility policy is not likely to be the cause of the negative relationship itself but the cause of the evolution of the relationship. Furthermore, by examining the relationship between women's education and their children's survival fraction as well as the relationship between women's education and their labor force participation, I find evident positive relationships in both results. My interpretation of this limited evidence, based on the rich research on the "two trade-offs", is that the "two trade-offs" are likely to be the cause of the negative relationship itself.
Many studies have focused on the relationship between women's education and fertility in developing countries. Bongaarts' (2003) conclusions were based on the data from Demographic and Health Surveys in 57 less developed countries and he investigated the relationship from the view of fertility transition that these educational differentials in fertility are slightly larger in the earlier than in the later stages of the transition. 3 He provides the perspective to analyze the relationship in China under such a macro background of fertility transition. Jain's (1981) research was based on the data from the First Country Reports of the World Fertility Surveys for eleven developing countries and reached the conclusion that advancement in female education can be expected to influence fertility behavior even without simultaneous changes in other factors such as increasing opportunity for participation in the paid labor force in the modern sector. 4 Lam and Duryea (1999) also confirmed the negative relationship between women's schooling and fertility in Brazil, a developing country experiencing rapid fertility decline in the absence of a major family planning effort.
Moreover, there are many studies paying attention to the mechanisms beneath the relationship. Graff (1979) concluded that "a more basic, critical, and realistic conceptualization of the role of education is required. Education ... should be seen ... less directly and less linearly, functioning and mediating through and with other structural and attitude-shaping factors." Weinberger (1987) reached a similar conclusion that education influences fertility through its effects on the intermediate factors, for example, age at marriage, breastfeeding and contraceptive practice. Both of their conclusions stressed the importance of finding out the mechanisms driving the causality from education to fertility and of controlling for other relevant variables. Martin (1995) argued that education enhances women's ability to make reproductive choices which resulted from their intensified bargaining power. This argument was consistent with Lam and Duryea's (1999) conclusion that education would increase women's household productivity. Martin (1995) also concluded that in some of the least-developed countries, education might have a positive impact on fertility at the lower end of the educational range, which was different from Lam and Duryea's (1999) finding that low levels of schooling are associated with large declines in fertility. Both of the above similarity and discrepancy raised the necessity to empirically investigate the evolution of the relationship and to test the mechanism beneath the relationship. Some articles especially emphasized the importance of controlling for other relevant variables. Freedman (1979) suggested controlling for changes in life conditions and changing perceptions, while Bongaarts (1978) preferred controlling for intermediate fertility variables (exposure factor, deliberate marital fertility control factors and natural marital fertility factors). Instead, Lam and Duryea (1999) controlled for socioeconomic factors including husband's schooling, region, race, marriage age and husband's income.
This paper will provide a complementary perspective compared to the above literatures by examining whether the policy effect is the driving force of both the relationship between education and fertility as well as its evolution. Furthermore, although all of the above studies were on developing countries, none of them were on China, for which my investigation might be a necessary complement and improvement.
Moreover, all of Jain (1981), Freedman (1979), Bongaarts (2003) and Lam and Duryea's (1999) articles emphasize the importance of controlling for other relevant variables. However, both Freedman (1979) and Bongaarts (2003) ignored the socioeconomic factors which are more fundamental since they actually determine life condition, perceptions and fertility variables in the first place. Therefore, I will turn to Lam and Duryea (1999) for reference on which socio-economic variables should be controlled for.
Table 1 presents the summary statistics of key variables of interest in the study. The dataset I will use is the IPUMS International China 1982. The sample will be restricted to Chinese women aged 41-64. By this restriction, women who are too young or too old for a research on completed fertility can be excluded. For those too young, their fertility or even education pattern may still be changing frequently and intensely; for those too old, their relevant patterns would already be very fixed, thus, including them may contribute little to the general analysis. The average age for women of interest is around 51.27. Therefore, their fertility behaviors can hardly be influenced by the large-scale on-child policy starting from 1978 since it is highly likely their childbearing age had already ended at the time. However, as discussed in the previous sections, the wan-xi-shao policy dating from 1970 might dissimilarly influence the fertility behaviors of the women of different cohorts. To capture the policy effect and the general evolution of the relationship between education and fertility, women aged 41-52 and those aged 53-64 will be analyzed separately through regressions. 41.86% of the women are aged 53-64, which generally guarantees the representativeness of both age groups. Furthermore, relationship changes across female birth cohorts will be investigated as a more detailed examination of the policy effect and relationship evolution.
Within the women of interest, 79.27% of them were illiterate or semi-illiterate, 15.05% of them had primary schooling, 3.56% of them had junior middle schooling, 1.57% of them had senior middle schooling, and only 0.56% of them had undergraduate schooling or higher. For their employment status, 53.44% of them were employed, 0.009% of them were unemployed and 46.55% were inactive. This under-representativeness of higher educational groups and unemployed groups may make the conclusions shaky. The women had a mean family size of 5.18 and had born 5.31 children on average of which 4.24 had survived. Therefore the average fraction of children surviving is as high as 83.25%.
Moreover, since most minority ethnicity groups were exempted from the fertility control policy, the patterns of the relationship between education and fertility for majority and minority ethnicity groups are also good indication of the policy effect. In the sample, as high as 93.91% of the women belong to the majority ethnicity group -- Han. This under-representativeness of the minority ethnicity groups may influence the reliability of the conclusions.
Specifically, to investigate the first research question that how did the education of women in China aged 41-64 in 1982 correlate with their completed fertility, firstly, descriptive statistics of the education level and number of children ever born to women aged 41-64 will be used to demonstrate the general trends and relationship between education and fertility of women of interest. Then a regression of the number of children ever born to women aged 41-64 on their educational levels will be done in order to achieve the partial and incremental correlation of each educational level on the number of children ever born. Both of the above descriptive and regression analyses can be done using the 1982 IPUMS International data for China, and the variables used are educational level and number of children ever born. Since the universe of educational level is all persons aged 6+, the universe of number of children ever born is females aged 15 to 64, and the dataset is based on the China census in 1982, there should not be serious self-selection problem. The variable indicating educational level will be treated as five dummy variables and their partial correlations with number of children ever born as well as their incremental correlations beyond the lower one will be shown.
To address the second research question that how would the relationship evolve across cohorts, firstly, descriptive statistics of the educational levels and fertility levels of female cohorts will be plotted to demonstrate the general evolution of the relationship between women's education and fertility across time. Next, two regressions of number of children ever born on educational levels will be done respectively for age group 41-52 and 53-64. In this way, general changes of the relationship across time can be achieved. Then two more inferential analyses will be done by regressing the number of children ever born to women aged 41-64 on the educational level and female cohorts respectively with and without interactions of each cohort with educational level, in order to demonstrate how the relationship between education and fertility changes across different cohorts. An F-test will be done between the above two regressions so as to examine whether the changing relationship between education and fertility across cohorts are statistically significant. Both of the above descriptive and regression analyses can be done using the 1982 IPUMS International data for China, and the variables will be used are educational level, number of children ever born and female birth cohorts. The eight 3-year female cohorts will be treated as seven dummy variables. Note that educational level will be treated as a categorical variable here for the convenience of interpretation.
The third research question that what caused the relationship and its evolution is highly important in that it investigates the underlying mechanisms driving the relationship between education and fertility. To examine whether the relationship itself is the result of the policy effect, the analyses for the second research question could be applied to demonstrate whether the directions of the relationship are different from younger to older women in 1982. Furthermore, since most Chinese minority groups were exempted from the wan-xi-shao policy (Tsuya and Choe, 1988), two regressions of number of children ever born on educational level, ethnicity with and without interaction of ethnicity with educational levels may also be instructive to test the possible causality carried out by the policy change. Ethnicity group will be treated as one dummy variable with minority ethnicity groups as the reference level.
The two trade-offs have provided insightful explanations for the negative relationship across many literatures. By attributing the declined fertility to the increased productivity of women in household and labor market as a result of the generally improved educational status of women, the rationales behind the negative relationship emerge in a clear and reasonable way.
To examine the trade-off between children quantity and quality, I will use both descriptive statistics of education, children quantity and children quality, in which children quantity is indicated by the number of children ever born, and children quality is indicated by number of children ever born now living and fraction of children alive. The analysis can be done using the 1982 IPUMS International data for China.
To investigate the trade-off between women's time allocation between housework and labor force participation, I will use descriptive statistics to show the general relationship between women's education, wage and labor force participation. With the increase of wage, if women's labor force participation does not change much, the research focus should be turned to the first trade-off that women, with even more improved market productivity, may tend to stay in housework and try to invest more on children quality. The opposite causality might also be achieved through empirical test. However, data needed for this analysis are not entirely available in IPUMS 1982. I can only use women's employment status and education to examine with higher education, whether or not women are more likely to participate in the labor market. To determine the pattern of the relationship between women's education and income, conclusions from Xie and Hannum (1996) will be referred to. However, since Xie and Hannum's paper was based on the 1988 Chinese Household Income Project (CHIP), the conclusions may not be quite comparable in this article due to the fact that China has experienced fast economic transition between 1982 and 1988. Moreover, CHIP and IPUMS data are based on different universe and survey design. As a result, reliance on their conclusions is quite limited.
Figure 1 shows the number of children ever born and the number of children ever born now living at the time of the survey, classified by different educational levels, for all Chinese women aged 41-64. The figure demonstrates the large differences in fertility across different educational levels. Illiterate or semi-illiterate women (about 79 percent of the women aged 41-64) report an average 5.8 live births. This falls rapidly with the educational levels from illiterate to undergraduate, at which mean number of children born alive was 2.5. Figure 1 also shows the large differences in child survival across educational levels. Illiterate women lost an average of 1.2 children, a survival rate of 78 percent. Women with graduate education report an average of only 0.07 children death, for survival rates over 97 percent. This leads to the issue of child health and child survival which indicates implicitly the interaction between education, fertility and investment in child quality. Therefore, it is a good starting point for my later investigation into the causes beneath the relationship between education and fertility. However, from undergraduate to graduate schooling, the declining trend of mean number of children ever born starts to be weaker, which is corresponded to by the following regression analysis.
Although family size does not entirely correspond to fertility levels, it does reflect the fertility and labor participation decisions made by women. Figure 2 shows the number of persons in the household at the time of the survey, classified by different educational levels, for all Chinese women aged 41-64. The figure demonstrates the large differences in family size across different educational levels. Illiterate or semi-illiterate women report an average family size of 5.2. This falls relatively fast with the educational levels from primary school to senior high school, while declining gradually in both educational levels of illiterate to primary school and senior high school to graduate schools.
To investigate the relationship between education and completed fertility, it is necessary to determine firstly whether education had shown a certain trend during the period of interest. As discussed by Lam and Duryea (1999), educational inequality is associated with the low mean and high variance of mean educational levels. Table 2 shows the trends in educational levels for female cohorts born between 1918-1941, based on the cross-sectional relationship between educational levels and age in the 1982 IPUMS International China. Mean educational levels for women has increased tremendously across cohorts by eight times over the years shown. The most rapid increase in the mean occurred for the cohorts born from 1930 to 1941. The role of these cohorts in the fertility decline as well as the evolution of the relationship between education and fertility will be investigated in the next section. However, Table 2 also demonstrates the enlarging educational inequality with the increase of standard deviations. Moreover, as seen in Table 2, the coefficient of variation declined steadily during the cohorts shown, which indicates that the educational distribution has a general tendency toward equality relative to their mean levels.
Table 3 represents the estimated correlations of educational levels on the number of children ever born to Chinese women aged 41-64. It is clear that both for the age group 41-64 in general, and for the separate age groups 41-52 and 53-64, a significant negative relationship exists between women's educational levels and their fertility level, that is, the higher educational levels women have, the lower their fertility levels are. Moreover, for the age group 41-52, the negative relationship does seem stronger than both the age group 41-64 in total and the age group 53-64 since for all of the educational levels, the absolute values of the coefficients are larger. This to some extent indicates that the wan-xi-shao policy did influence the fertility behavior of those women who were still in their active childbearing age at the time when the policy were implemented despite their respective educational levels. Therefore, it might be meaningful to treat different cohorts as dummy variables to be regressed on along with educational levels by fertility levels, in order to test whether or not the policy effect are significant.
Furthermore, although the negative relationship between educational and fertility levels is estimated to significantly exist, the marginal differences of the estimated coefficients5 of educational levels on fertility levels are actually diminishing, that is, for women aged 41-64, when the educational level turns from illiterate or semi-illiterate to primary school, the number of children ever born is estimated to reduce by 0.925, however when the educational level turns from primary school to junior middle school, the marginal correlation is only -- 0.905; from junior middle school to senior middle school, the marginal correlation is -- 0.622; from senior middle school to college undergraduate, the marginal correlation is -- 0.662 and from college undergraduate to graduate, the marginal correlation is only -- 0.001. The diminishing trend is the same for women aged 41-52, though with a larger marginal correlation from college undergraduate to college graduate. The trend is even more evident for women aged 53-64 for whom the marginal correlation from college undergraduate to college graduate gets a quite large positive value 0.445, which indicates that for this group of women, the higher education, instead of leading to lower fertility level, is actually resulting in higher fertility level. However, different from two other age groups, there is a rebound of marginal correlation from senior middle school to college undergraduate.6 Lam and Duryea (1999) attributed this diminishing trend of marginal educational correlations to the increased household productivity of women whose fertility decisions were mainly based on the trade-off between "quantity" and "quality" of children. Therefore, those results indicate the importance to investigate the driving forces of the relationship between education and fertility. However, these diminishing marginal correlations may also be due to the under-representativeness of women with higher education.
Figure 3 shows the fertility and education trend across female birth cohorts. As can be seen, there are changing relationships between educational and fertility levels longitudinally. It is clear that from the cohort 1927-1929, evident negative relationship between the average educational and fertility levels started and with an increasing speed. Before cohort 1927-1929, this trend of changing relationships was rather weak and gradual, and the relationship between education and fertility almost kept positive. However, both trends before and after 1927-1929 do not indicate the causality between the educational and fertility levels and also does not show whether the observed negative relationship was getting stronger or weaker across different female birth cohorts. To address this problem, regression analysis of fertility levels on educational levels and different female birth cohorts might be helpful.
Model 1 in Table 4 shows the regression results of number of children ever born on education which is treated as a categorical variable for convenience of interpretation, and cohort dummy variables which treat cohort 1918-1920 as the reference level. Interaction terms between education and cohorts are included in Model 2. As can be seen in Model 1, the negative relationship between education and fertility still holds. Moreover, along with the cohort evolving from 1921-1932, the average differences in number of children ever born across educational levels with the reference level of cohort 1918-1920 have been increasing. However, from 1930 to 1941, the differences in average fertility level between the cohort of interest and cohort 1918-1920 starts to decrease with an increasing speed. Especially, the difference falls below zero for cohort 1939-1941. In general, Model 1 shows the changing pattern of the average fertility level that although from cohort 1918-1932 the average fertility level keeps increasing, it increases at a decreased speed. For Model 2, the negative relationship between education and fertility also holds. This also shows that for those illiterate or semi-illiterate women, the average fertility level keeps rising from cohort 1918-1932 and then starts to decline. However, the changes of the negative coefficients on the interaction terms indicates that along with the cohort evolution, the negative correlation between education and fertility is getting stronger from cohort 1921-1932 and then begins weakening with the absolute values of the negative coefficients getting smaller. An F-test between Model 1 and Model 2 is done with an F-value of 6.96, which indicates the changing strengths of the negative relationship across cohorts are statistically significant.
As can be seen more clearly in Figure 8 and Figure 9, although the average fertility level is estimated to increase before cohort 1930-1932, it is with a stronger negative correlation between education and fertility; however, although from cohort 1930-1932 the average fertility level is estimated to decrease, it declines with a weaker negative correlation between education and fertility.
Model 1 and Model 2 also show that before cohort 1930-1932, the negative relationship is getting stronger with an increasing average fertility for all educational levels; however, from cohort 1930-1932, the negative relationship is getting weaker with a decreasing average fertility. Especially, both in Model 1 and Model 2, from cohort 1930-1932, the coefficients of cohort dummy variables decreased fast and eventually turn negative for the cohort 1939-1941. These results demonstrate that fertility policy might be the reason for the evolution of the relationship.
Figure 1 and Figure 6 show that with higher education, the number of children ever born tends to be lower and the average fraction of children ever born now living tends to be higher. This generally indicates women with higher education do substitute toward quality and away from quantity of children. The first trade-off is likely to drive the relationship itself.
Figure 7 respectively shows that women's labor force participation tends to increase with higher education. Moreover, Xie and Hannum (1996) concluded that education is estimated to be positively correlated with earnings in China during 1988. If the conclusion is used here, it indicates that women with higher education actually reacted actively to the higher income by participating more in the labor force. Therefore, the second trade-off might also be the key driving force of the negative relationship.
Based on the above descriptive statistics and linear regressions of fertility on educational levels with controlling for other key relevant variables, it can be concluded that the higher the educational levels of the women, the lower the fertility levels. This negative relationship holds true for both older and younger cohorts. More specifically, for older cohorts, the negative relationship is stronger but with an increasing average fertility for all educational levels; while for the younger cohorts, the negative relationship is weaker but with a decreasing average fertility for all educational levels. Moreover, the average fertility level for Han is estimated to be lower with a stronger negative relationship than that of minority groups, although the difference of the slopes is not statistically significant and the negative relationship remains for both majority and minority ethnicity groups. Furthermore, both the number of children ever born now living and labor force participation of women tend to increase with higher education, which generally follows the rationales of the two trades-offs.
Based on the above empirical results, the following conclusions can be drawn:
From the above conclusions, some implications about China's social reality can be reached. In the first place, decreasing China's gender gap of education and improving women's education tends to be an effective way to curb China's population and mitigate the intense competition for resources among its people. However, in terms of China's already limited resources and increased gender inequality resulted from the economic transition, this is a quite long and hard way to go. Relatively, implementing fertility control policy is effective not only in restricting China's population but also in weakening the reliance of fertility on women's education. Moreover, since it is implemented imperatively by the government, it is also more efficient in the short term. However, to improve women's education, in the long run, is more significant since this can fundamentally pull the relationship between education and fertility out of the vicious circle from large gender gap of education to increased fertility.
However, the above conclusions and implications should be taken cautiously due to the following reasons. Firstly, the regression of fertility on education has not been done repetitively by treating each educational level as the reference level in order to statistically test whether their respective correlations with fertility are significantly different from one another. Secondly, women with higher education and unemployment, as well as minority ethnicity groups are under-representative in the used sample, which may affect the reliability of the conclusions. Thirdly, to investigate the two trades-offs, descriptive statistics and regressions to examine the relationship between women's education, wage, labor force participation and their children's education should be achieved aside from the reference to other literature's conclusions. These analyses will be done for future study and will be based on a combined dataset of China Study (1964) and Chinese Household Income Project (1988, 1995). However, the three datasets are different in their specific universes and survey designs; moreover, although the time periods of the dataset covered 1982, in terms of China's intense transitions during 1980s, the possible results achieved from the above dataset may be unrepresentative and inconsistent with the situation in 1982, and thus lack the convincing power. Fourthly, except for cohorts and ethnicity, more variables should be controlled for in the model to secure the estimates to be unbiased while making the conclusions more reasonable and convincing. Last but not least, the conclusions should be related more deeply with China's social context in order to provide more insights and implications for gender gap of education and sustainable economic development of China.

Using a one-sector model, Lucas (1990) argued that it was a paradox that not more capital flow from rich to poor countries. His reasoning goes as follows. Let y = f (L, K) be a constant-returns-to-scale production function where y is the output produced using labor L and capital K. Let p be the price of the good, and w and r be the returns to labor and capital, respectively. Firm's profit maximization problem gives
Assuming that the product price is equalized across countries under free trade, the law of diminishing marginal product implies that r is higher in the country with a lower
capital-labor ratio. As an illustration, Lucas calculated that the return to capital in India should be 58 times as high as that in the United States based on their factor endowment. Facing a return differential of this magnitude, one should observe a lot more capital to flow from rich to poor countries. That too little flow is observed in the data has come to be known as the Lucas paradox. In fact, in the data today, this paradox is prevalent if not aggravated. In table 1, it shows clearly that the total value of capital inflows into rich countries exceeds those into poor countries manifolds. This trend has even been stronger in the more recent periods.
Lucas (1990) discussed three possible explanations (within a one-sector framework): (a) A worker in a rich country could be several times more productive than her counterpart in a poor country; (b) Human capital may be a missing factor and is likely much higher in a rich country; (c) Political risk and hence required risk premium may be substantially higher in a poor country.
The paper will mainly focus on the third explanation, namely whether the quality of political institutions plays a role in capital inflow. It discusses the motivations and empirical evidences in the more recent literature. The question of whether political institutions matter in terms of attracting capital inflow is crucial for policy makers especially in the developing countries. As indicated in table 1, poor countries often have difficulty in attracting capital inflows which might be crucial to their growth process. Thus, if policy makers had a better idea of the underlying cause of the lack of capital inflow, such as bad quality of institutions, they could improve the source of the problem.
Interestingly, Lucas himself dismisses the quality of political institutions as possible explanation for the lack of capital inflow into rich countries. His reasoning is that prior to 1945, most of the colonies were ruled by European countries who instituted the same laws in their colonies as in the mother country. Thus, if a British lender, for instance, wanted to lend money to an Indian, the British could expect the same contract laws to apply in India. Reinhart and Rogoff (2004) challenge the validity of Lucas' dismissal by pointing out the number of rebellions in the colonies at that time. They argue that despite having the same laws in the colony and mother country, it was far more likely that the laws in the colonies would be disobeyed due to instable governments. In fact, Reinhart and Rogoff Reinhart argue that the set of countries with frequent default on their external debt have instable governments and are generally low income countries.
A more extensive empirical study is by Alfaro, Kalemli-Ozcan, Volosovych (2005). Using data from the IMF on balance of payments, Alfaro, Kalemli-Ozcan and Volosovych (2005) find that good institutional quality is a key determinant of total capital inflows. In their study, they use the extensive dataset by IFS/IMF on foreign direct investment (FDI), portfolio equity investment, and debt inflows from 1970-2000. For their total sample of 98 countries, they construct a measure of institutions as a composite index which is the sum of the indices of investment profile, government stability, internal conflict, external conflict, no-corruption, non-militarized politics, protection from religious tensions, law and order, protection from ethnic tensions, democratic accountability, and bureaucratic quality obtained from International Country Risk Guide (ICRG). Alfaro et al. run the OLS regression of capital inflow on log GDP per capita and the institution index, i.e.
Alfaro et al. argue that if α, the coefficient on log income per capita is positive, it indicates the existence of the Lucas' Paradox. As seen in table 2, when regressing capital inflow on log GDP per capita by itself, α is statistically significant and positive (columns 1 and 3). When adding the institution index as a regressor, however, the coefficient estimate on log GDP per capita becomes statistically insignificant and β, the coefficient estimate on institutions, is positive and statistically significant (columns 2 and 4). Thus, Alfaro et al argue, the quality of institution provides an explanation for the Lucas Paradox. To get a better sense of the magnitude of β, Alfaro et al. point out that based on the estimate, if a country moves up from the 25th percentile (Guyana) to the 75th (Italy) in the distribution of the index of institutions, based on column (4), there will be an increase of $187.54 in inflows per capita over the sample period on average. This represents a 60% increase in inflows per capita over the sample mean which is $117.34. Alfaro et al. also run a separate multiple regression by including other explanatory variables such as log average years of schooling, log average distantness, and average restrictions to capital mobility. These three regressors represent plausible barriers to capital inflows, namely, human capital, asymmetric information and government policies, respectively. The results in Table 3 show, however, that none of these three additional regressors can by itself explain away the Lucas paradox. The coefficient on log GDP per capita is only insignificant when adding the quality of institutions index.
The two main potential problems with this simple OLS regression are multicollinearity and endogeneity. The multicollinearity stems from the fact that log GDP per capita and the institution index are highly correlated. The authors perform extensive diagnostic tests and simulation exercises to make sure that the results are not spurious and that they capture the direct effect of institutional quality on capital inflows. For instance, they regress the residuals from the regression of average inflows on average institutional quality against the residuals from the regression of log GDP per capita in 1970 on average institutional quality and vice versa. The slopes of the fitted lines match exactly their counterpart in the multiple regression as proposed by the Frish-Waugh Theorem if all conditions are met. In addition, they perform perturbation exercises and none of the robustness regressions show any big sign and magnitude changes, which are typical indicators of multicollinearity.
The other potential problem is endogeneity which can be caused in two ways. First, it is possible that the capital inflows affect the institutional quality of a country. Since most institutional quality measures are constructed ex-post, and there might have been a natural bias in "assigning" better institutions to countries with higher capital inflows. With this errors-in-variable type of endogeneity, one would expect attenuation bias, which means that the actual effect of institutions on capital inflow should be even higher.
The second source of endogeneity can come from the possibility that both inflows and institutional quality are determined by an omitted third factor. By conducting various robustness tests, Alfaro et al. include that omitted third factor is not an issue.
To control for the first possibility of endogeneity, Alfaro et al. use an instrument for institutional index. As the instrument, they pick log settler mortality, which was suggested originally by Acemoglu, Johnson, and Robinson (2001, 2002). Acemoglu et al. find that the mortality rate of settlers indicates the conditions in the colonies. More specifically, mortality of European settlers in the countries they colonized shaped their decision to settle or not. When they settled, they brought with them effective European institutions, whereas when they did not settle, they instituted systems of arbitrary rule and expropriation of local populations. The results from the first stage regression shows a large R-squared of 0.39, and as expected, the second stage estimate on institutional quality becomes even larger than the one without instrument.
Glaeser, La Porta, Lopez-de-Silanes, Shleifer (2004) raise the interesting issue regarding the construction of the institutional index and the validity of the instrumental variable. Their critique is a follow up on Acemoglu et al.'s paper which suggest that institutions play a role in the economic growth of a country. Glaeser et al. point out that institutions are long lasting constraints rather than brief policy changes as constructed via the institutions index. The other main issue that Glaeser et al. criticize is the use of the instrument log mortality rate employed in Acemoglu et al's paper. Contrary to Alfaro et al, Acemoglu et al. run a regression of log GDP per capita on the institutional quality variable. Glaeser et al. point out that human capital is an omitted variable Acemoglu et al's regression. In fact, when including human capital via average log years of schooling, in a two stage regression, the impact of institutional quality on log GDP per capita become insignificant but the estimate on schooling become significant instead. Alfaro et al. included average log years of schooling in their multiple regression as well, but it did not have explanatory power on capital inflows. Thus, although Glaeser et al.'s critique on Acemoglu et al's approach is relevant, it is not applicable to Alfaro et al.
The dilemma with this stream of literature on capital inflows is that apart from mostly OLS based empirical papers, there is no concrete model for the actual problem.
The basic setup of a model is
(1)
If agents can borrow and lend capital internationally and if all countries share a common technology, perfect capital mobility implies instantaneous convergence of the returns to capital, i.e. for countries i and j,
where f(.) is net of depreciation production function per capita and k denotes capital per capita. Based on Lucas' three different explanations of why capital does not flow into poor countries, different approaches to model capital returns, 1. Missing Factor, 2. Government Policies, i.e. impediment to flows, tax policies, capital controls, and 3. Institutional Structure and Total Factor Productivity.
In the missing factor explanation, there might an externality in the production process that affects the returns to capital but are generally ignored by the conventional neoclassical approach. For example, if human capital positively affects capital's return, less capital tends to flow to countries with lower endowments of human capital. Thus, if the production function is given by
Where Z denotes another factor that affects the production process, then (1) misrepresents the implied capital flows. The true return for countries i and j is
A second explanation of the lack of capital inflows might be that government policies are impediments to the flows. For example, differences across countries in government tax policies can lead to substantial differences in capital-labor ratios. One can model the effect of these distortive government policies by assuming that governments tax capital's return at rate τ, which differs across countries. Then the true return for countries i and j is
The last explanation that inhibits capital inflows might be the quality of institutions. Institutions can affect economics performance through their effect on investment decisions by protecting property rights. Weak institutions can lead to lack of productive capacities or uncertainty of returns in the economy. These differences can be captured in the parameter A, which depicts differences in overall efficiency in the production across countries. The problem with this approach is that one cannot differentiate between the effect of institutions on investment opportunities versus that of the Total Factor Productivity (TFP). The true return for countries i and j is
Clearly, much work lies ahead in constructing a sound economic model that shows the effects of institutions on capital inflows. The model would enable us to focus on particular data sets instead of averaging numerous countries and only taking into account the cross-sections. Thus, the next step could be to identify particular instances, where given the events, we are sure that the lack of capital inflows was due to a certain change in institutions. This way, we can test this particular event with a specific dataset. As of now, the empirical evidence is rather generalized, and the cross-section evidence is sensitive to various econometric problems.

Asymmetric stock price response to earnings announcements has long been an interesting and important topic in accounting, finance and economic research. This paper empirically investigates the existence of asymmetric stock price response to abnormal earnings both at aggregate and industry levels. These asymmetries are identified by comparing the response of the stock price to good news to the response of the stock price to bad news. In order to uniformly measure asymmetry in stock price response, the difference between actual earnings and analysts' mean forecasts are defined as abnormal earnings. Positive abnormal earnings are categorized as good news and negative abnormal earnings as bad news. This method gives an ideal measure for information about firms' earnings because it measures "quality" as well as quantity of information. In addition, the dependence of the asymmetric stock price response on the industry that the firm belongs to is also examined. This paper investigates if the average asymmetric stock price response in an industry is related to industry characteristics, such as the Herfindahl index, CEO (Chief Executive Officer) compensation, credit rating information, and firm size.
There has been few quantitative research performed at the industry level to detect the asymmetric stock price response. This paper gives an interesting motive to investigate the reason for the different stock price responses between industries. Quantitatively, this approach has some advantages in that asymmetric stock price response in each industry is easily detected using a simple OLS specification, and the interaction of the industry-specific variables with abnormal earnings can be analyzed to identify the effect of each industry-specific variable on the asymmetry in stock price response. In fact, there is a large body of research on asymmetric stock price response, and an equally large one on the relationship between key industry-specific factors and voluntary disclosure or earnings management. However, there has been little work linking these two studies. This paper examines an empirical link between these two studies.
I find that there exists asymmetry in stock price response in aggregate level. The econometric specification shows that the coefficient of good news is 0.1110 and that of bad news is -- 2.421. This implies that overall bad news has a larger impact on the stock price change than good news does. However, another econometric specification performed in industry level suggests that there is huge heterogeneity in asymmetric stock price response across industries. These asymmetries are more likely to be related to the variance of the stock price change than to the magnitude of the stock price change itself. The empirical link between the key industry-specific factors and stock price response is examined. The consistency of the results with prediction of relating theories is also checked. Most of the key industry-specific variables are likely to follow the pattern that the corresponding theory predicted.
The remainder of the paper proceeds as follows. Section 2 reviews related empirical or theoretical literature. Section 3 contains corresponding data. Section 4 discusses the asymmetric stock price response in aggregate level. Section 5 discusses the heterogeneity across industries. Section 6 characterizes the sources of heterogeneity discussed in Section 5. Section 7 concludes this paper with a discussion of some implications of the results for future research.
For asymmetric stock price response in earnings announcement, Kross and Schroeder (1984) provide an empirical investigation of the effect of quarterly earnings announcement timing on stock price returns. They found that early quarterly earnings announcements contain better news and are associated with larger abnormal stock returns. MacKinlay (1997) shows that both good news and bad news have strong impact on the cumulative abnormal return. He shows that the absolute value of the statistics for testing the impact of news is higher for good news than bad news.
Asymmetric stock price response might be attributed to a few factors such as earnings management and/or voluntary disclosure occurred before earnings announcement. First of all, what earnings management is should be clarified. Notice that there is no consensus on defining earning management1. Following Schipper (1989), I define earnings management as "a purposeful intervention in the external financial reporting process, with the intent of obtaining some private gain." If earnings management is an unobservable component, it is reasonable to assume that investors and other firms in the same industry cannot unravel the effect of earnings management on reported earnings.
Analytical or empirical research on managers' voluntary disclosure has shown that there are three different effects on stock price according to which information managers are more likely to reveal before the quarterly earnings announcement. First, if firms tend to voluntarily disclose more good news than bad news, the stock price response to positive abnormal earnings in actual earnings announcement is expected to be smaller than the response to negative abnormal earnings. Lev and Penman (1990) document that earnings forecasts are used by managers of "good news" firms to screen themselves out from other firms with "bad news" in a signaling scenario. However, they do not find that their results are consistent with the negative price reaction implication for the nondisclosing firms in the same industry. Second, if firms are more likely to disclose bad news than good news, the stock price response to positive abnormal earnings in actual earnings announcements is larger than the response to negative abnormal earnings since quarterly earnings announcements that convey large negative earnings surprises are preempted by voluntary disclosure while other earnings announcements are preempted less(see Skinner (1994)). Lastly, if firms are willing to disclose good news and bad news equally, the stock price response to either abnormal earnings in actual earnings announcement is same. According to Pownall, Wasley, and Waymire (1993) forecasts are less informative than earnings announcements for their full sample and differences across forecast forms are not significant at conventional levels. They find that voluntary disclosures are associated with a stock price response which is, on average close to zero.
Theoretical analysis on incentives for voluntary disclosure associated with industry-specific factors has shown that key industry-specific factors could explain a large portion of asymmetry in stock price response. A highly concentrated industry (high Herfindal indexed industry) is expected to have large stock price response regardless of signs of abnormal earnings. Darrough and Stoughton (1990) predict that competition in the product market encourage voluntary disclosure. The effect of voluntary disclosure might lower the stock price response in earnings announcement. The degree of asymmetry is, however, not yet analyzed. In a similar study, Dontoh (1989) supports that in an N-firm oligopoly, firms have incentives to disclose unfavorable information about future outcome as well as favorable information depending on the types of firms. Thus it is expected that an industry with a higher HHI will have lower stock price responses with almost no symmetry in earnings announcements.
Murphy (1998) supports that CEOs who receive relatively small bonus payments have potentially less explicit incentives to manipulate income so as to maximize bonus-based compensation. Murphy and Zimmerman (1993) point out that the first class of discretionary behavior -- reflecting the managerial horizon problem -- is likely to be relatively more pronounced in firms with good corporate performance and routine retirements while the other two classes of discretionary behavior -- outgoing CEOs covering-up poor performance and incoming CEOs taking a big bath -- are likely to be more pronounced in firms with deteriorating economic health. The irresolvable links between performance and discretionary behavior make it difficult to disentangle the effects of poor performance from the effects of managerial discretion. Note that earnings management is not necessarily tied with poor economic performance, but is highly correlated with deteriorating corporate performance. Thus higher portions of bonus payments in CEO compensation leads to larger amounts of the stock price response due to higher incentives in earnings management. While poor economic health is more likely to encourage CEOs to manage earnings.
By simple logic, lower earnings management is expected with higher credit rated firms since the better firm's credit rating is, the better performance of a stock and dividends are made. So there might be a negative relation between the stock price response and firms' credit rating. For theoretical support, Bagnoli and Watts (2000) indicate that firms may exaggerate their earnings in a world driven by multi-firm-comparisons because they expect other firms to do so. They also find that the equilibrium amount of earnings management depends not only on the earnings management method itself but also on the proportion of long-term investors' in the firm. This implies that firms with a higher crediting rating for long-term issuer's credit rating do less earnings management under the assumption that investors' decision is based on all the information available including the firms' credit rating.
In this section, quarterly earnings announcements are considered. I investigate the information content of quarterly earnings announcements for the entire US firms in the COMPUSTAT® Industrial data over the 5-year period from January 1997 to December 2001. These announcements correspond to the quarterly earnings for the last quarter of 1996 through the third quarter of 2001. For each firm, the daily stock prices for the corresponding earnings announcement are collected from the same source. For each firm and quarter, three pieces of information are gathered: the date of the announcement, actual announced earnings measured in earnings per share, and the analysts' mean forecasts. A measure of the deviation of the actual announced earnings from the market's ex-ante expectation is required in order to analyze the impact of abnormal earnings on the stock prices. I used the analysts' mean quarterly earnings forecasts from the Institutional Brokers Estimate System (I/B/E/S) to proxy for the market's expectation of earnings. I/B/E/S compiles forecasts from analysts for a large number of firms and reports summary statistics each month. In order to control the change in firm's economic performance and macroeconomic shock, the industry average of stock prices on the corresponding dates and the S&P 500 Indices from COMPUSTAT® Industrial data are included. All firms are classified by their industry classification codes. From this sample 22 industries were dropped due to insufficient financial data in the regression.
The COMPUSTAT® Industrial CEO compensation data taking into account options granted, firms' sales data, and S&P Long Term Issuer's Credit rating data as firms' credit rating information are used in another regression. CEO compensation is comprised of the following: salary, bonus, other annual, total value of restricted stock granted, total value of stock options granted (using Black-Scholes), long-term incentive payouts, and all other total. The quarterly average CEO compensation is calculated under the assumption that the annual average CEO compensation could be evenly spread throughout the years. The incentive variables are defined as the ratio of incentive related compensation to total compensation. This could measure how the CEOs' discretionary intention affects the stock price in the event of earnings announcements. For the Herfindahl index, I computed the market share of each firm using its quarterly sales data relative to the total industry sales. Note that the Herfindahl indices were computed in the original data set simply because the more available sales information obtained the better the results. Information about firms' credit rating was obtained from COMPUSTAT® Industrial data after comparing this with data from Moody's Default Risk Services database. According to the data manual in COMPUSTAT®, credit rating information is defined as a current opinion of an issuer's overall creditworthiness, apart from its ability to repay individual obligations. The S&P Long Term Issuer Credit rating was employed. From this sample 52 industries were dropped due to insufficient observation in the regression. Lastly, firms' quarterly sales data is used as a proxy for firm size.
I examine the stock price response of firms in aggregate level to the announcement of earnings in every quarter over 1997-2001. If stock price response to positive abnormal earnings and negative abnormal earnings differ systematically, it is likely that there exists asymmetric stock price response. To explore this, I examine the stock prices, abnormal earnings, industry average stock price and the S&P 500 Index. For example, when we observe positive or negative abnormal earnings ex-post, the corresponding stock price responses could either vary equally in both cases or represent some asymmetry. I test whether there is asymmetric stock price response to their earnings announcement through aggregate level OLS specification.
In order to detect the stock price response, I use a modified event study of earnings announcements repeatedly. The usefulness of a modified event study is that, given rationality in the marketplace, the effect of an earnings announcement will be reflected immediately in stock prices. The event's economic impact can be measured using the stock prices observed over a relatively short time period. The period of interest expanded to multiple days including the day before the announcement and the day after announcement. This captures the stock price effects of announcements which occur after the stock market closes on the announcement day. I assume that there exists no correlation between the reported earnings by firms and the mean value of analysts' forecasts. In order to control common shock in the stock market, I use the S&P 500 indices and to control industry-specific shock I include the average industry stock prices. The equation to be estimated has the following form for each industry:2
(1)
where
(2)
The model suggests that positive coefficient on the difference in reported earnings by firms and the analysts' mean forecasts would be expected when the positive abnormal earnings lead to positive impact on stock prices. But negative or 0 coefficient on the positive abnormal earnings variable could be observed when the abnormal earnings have negative impact or zero impact on stock prices. Note that the interesting part is not the value of the coefficients themselves but the difference between two coefficients. The difference in coefficients of two opposite signed variables that represent asymmetry, explains the strength of asymmetric response of the stock prices.
However, this gap reflects not only asymmetric stock price response from abnormal earnings but also other forces resulting from changes in the economic performance of the firm's industry. This justifies including the average industry stock prices and the S&P 500 indices to separate the effect of asymmetric stock price from the effect of industry-specific economic factors mentioned above. Since the number of observations available for estimation is large enough, the use of Ordinary Least Square (OLS) is justified for each industry. The scope of investigating abnormal earnings is constrained by the limited disclosure of information in quarterly financial announcements.4
A summary of the sample data for the regression is presented in Table 1. The total number of industries collected from the data source is 393 and 366 out of 393 industries remain since 27 out of 393 industries were dropped due to insufficient observation. Table 1 part A reports the summary statistics before the differences of the variables were made. The sample size is 96,784, which is calculated based on the number of actual EPS observations. Table 1 part B informs the sample statistics after the differences were made. The sample size is 42,358. This provides summary statistics for all the variables used in the OLS specification. The OLS regression, performed in the aggregate level is presented in Table 2.
In the aggregate industry level, the results support the asymmetry in stock price response to earnings announcements. These results also indicate that all the coefficients are statistically significant at 1-percent significance level with and without time dummies and industry dummies. The OLS regression with dummies shows that positive abnormal earnings variable has a positive coefficient. In other words, the stock price is more likely to rise when earnings announcements contain good news. The magnitude of the coefficient implies that, for instance, when a firm in this industry reports positive abnormal earnings about 1 percent of the analysts' mean forecasts, the stock price one day after the earnings announcement date is expected to rise by 11 percent more than it would otherwise. Also the fact that the coefficient of negative abnormal earnings is larger than that of positive abnormal earnings in absolute term implies that the stock price responds more to bad news than good news.
In this section, I study the stock price response of firms to the announcement of earnings in different industry level in order to see whether there is heterogeneity of stock price response across industries. At this stage, the same econometric specification and equation as in the previous section is used. Based on the arguments provided at the end of the previous section the following hypotheses for each industry relating to the asymmetric stock price response are tested:
No asymmetric stock price response exists across industries.
As in the previous section, I find that there is asymmetric stock price response in aggregate level by using OLS specification. Now I test the hypothesis that no asymmetric stock price response exists in earnings announcement in different industry levels. If there is no asymmetry in the stock price response, those two estimated coefficients of the abnormal earnings in every industry have the same magnitudes. Graphically, two estimated coefficients are scattered on the 45 degree line if no industry is expected to have the asymmetric effect on abnormal earnings. As shown in Figure 1 and Figure 1-1, there exists clear asymmetric response of stock prices in many industries. This finding supports that there is heterogeneous stock price response in different industries. Figure 1-1 represents that the absolute magnitude of the estimated coefficients of the positive abnormal earnings is larger than that of the negative abnormal earnings overall and some points are clearly located outside of the 45 degree line. Figure 1-1 also shows that coefficients of positive abnormal earnings are scattered broader along the axis than those of negative abnormal earnings. Namely, many industries are more likely to have larger stock price response to good news than to bad news. The asymmetry is related not to the magnitude of stock price change but to the variance of this change. Summaries of the sample data for the industry level regression are presented in Table 3 and Table 4. The number of industries that show the greater stock price response to negative abnormal earnings is 128 while 238 industries report that the stock price response to positive abnormal earnings would be larger than that to negative ones. However, only 61 industries show that they have statistically significant asymmetric stock price response. Results for the industry-wide OLS regression of the difference in stock prices on the abnormal earnings and the controls for rational stock market conditions are shown in Table 3. Table 3 exhibits that 23 out of 366 industries show the asymmetric stock price response, which is statistically significant at the 1-percent level. Up to 10-percent significance level, 61 industries out of 366 have the difference in estimated coefficients that are statistically different from 0. The detailed information about other industries which represent asymmetric stock price response is provided in Table 4.
In this section, sources of heterogeneity across industries are explained by the corresponding theories of key industry-specific factors, which are provided in Section 2.
The evidence of including voluntary disclosure is consistent with the idea that managers have incentives to preempt the announcement of large negative earnings surprises, and they have incentives to distinguish themselves by declaring good economic performance. This voluntary disclosure would affect the stock price immediately so that it will lead to lower impact of quarterly earnings announcement on the stock prices. Accordingly, unfavorable voluntary disclosure will reduce the impact of negative earnings surprise in earnings announcement. So if firms voluntarily disclose favorable information more often than they do unfavorable information, the stock price response to the negative abnormal earnings will be larger than the response to the positive abnormal earnings. Also if firms are more likely to disclose unfavorable information than favorable information, the stock price response to the positive abnormal earnings will be larger than the response to the negative abnormal earnings. If they disclose voluntarily both favorable and unfavorable information, the asymmetric stock price response is not expected.
Also, including voluntary disclosure justifies the phenomena that the stock prices could rise (fall) when we have bad news (good news). For example, when we observe the positive abnormal earnings ex-post, the stock price one day after earnings announcement date is expected to rise without prior information about future income or any earnings management. The stock price is, however, expected to fall with previous voluntary disclosure which already carried very favorable news about earnings and no more promising news, but note that this response could be much larger than it is in the case we observe negative abnormal earnings. Similarly, when we observe negative abnormal earnings ex-post, the stock price one day after earnings announcement is expected to fall without prior information or any earnings management by the same magnitude when we observe positive abnormal earnings. Also the stock price could even rise with previous voluntary disclosure to preempt the negative earnings surprise.
The results predicted by corresponding theories are following: A highly concentrated industry is less likely to have asymmetry in stock price due to less voluntary disclosure. Since CEOs who receive relatively small bonus payments have potentially less explicit incentives to manage earnings5, higher portions of bonus payments in CEO compensation lead to larger amounts of the stock price response due to higher incentives in earnings management. The amount of discretionary compensation is expected to explain the asymmetry in stock price response. The better credit rating would be related to less incentive to manage earnings, thus less asymmetry in stock price if other conditions are the same.
In this section, I investigate the source of heterogeneity in different industries using simple econometric method. Once asymmetric stock price response has been detected both in aggregate and industry levels, I examine the stock price response of firms in each industry to the announcement of earnings, taking into account the interaction terms between abnormal earnings and the four industry-specific factors, such as a change in number of competitors in the market which can be represented by a Herfindahl index, a change in firm CEO compensation, a change in firms' credit rating and a change in firm size proxied by quarterly sales. For example, I examine if an industry is more concentrated, an individual firm in this industry more likely to have large/small response in stock price. Also I check if changes in credit rating or CEO compensation explain the degree of stock price response to earnings announcements. Since CEO compensation includes the bonus payments and the total value of options granted, adding this variable could explain how much the asymmetry in stock price response is affected by the amount of discretionary portion of compensation. I also estimate the effect of incentive variables defined as the discretionary portion of CEO compensation. I assume that the previously detected the asymmetric stock price response is determined by a linear combination of some observable industry strategic factors, and also an unobserved component. Thus asymmetric stock price response which might be explained by key industry-specific factors has the following form:
(3)
where
(4)
where difeg12it :
None of the industry specific factors will explain the stock price response.
Table 5 provides summary statistics for the key industry-specific variables used in the OLS specification. Note that we are interested in studying how the interaction terms of the variables of HHI, CEO compensation, incentive variable, credit rating information and firm size with abnormal earnings would affect the difference in the stock prices. This industry-wide regression makes an attempt to ascertain whether the key industry-specific factors could explain the difference in the stock prices through the asymmetry in the stock price response. It repeats the industry-wide OLS regression specification with and without incentive variables. I assume that any interaction among the key variables should be zero because each variable was measured independently and no correlation exists. By looking at the estimated coefficients of each industry-specific variable jointed with the abnormal earnings, I test the hypothesis that none of the industry specific factors will explain the difference in stock prices. If the difference in stock prices shows no response to any interaction terms of four industry-specific variables, this supports the hypothesis that none of the industry specific factors will affect the difference in stock prices. Otherwise, this supports that at least one of the industry-specific factors significantly affects the difference in stock prices.
Table 6 shows the OLS specification, taking into account interaction terms of industry-specific variables and the abnormal earnings in the aggregate level. It shows that the CEO compensation interaction term and the credit rating interaction term are significantly different from 0 at the 5-percent level and at the 1-percent level, respectively. With incentive variables, only credit rating interaction term is statistically significant at 1-percent level. For the OLS specification without including incentive variables, 75 industries show significant effect on the stock price response. Total number of industries successfully regressed is 164, and 134 observations are statistically significant up to the 10-percent level. The effect of each industry-specific factor on the stock price is represented in Figure 2, Figure 3, Figure 4, Figure 5, and Figure 6, respectively. For the Herfindahl index interaction term, the less concentrated industry is more likely to have higher effect on the stock price as seen in Figure 2-1. As theory predicts, highly concentrated industries tend to have lower stock price response in earnings announcement. For the CEO compensation interaction term, Figure 3-1 provides that small compensation would be more related to higher response in the stock price. This result is also consistent with the theory that higher compensation CEOs have less incentive to manage earnings in the event of earnings announcement. Figure 4, 4-1 presents the effect of incentive variables on the stock price. The pattern is somewhat different from that in CEOs: the medium portion of discretionary compensation not the small portion of it represents higher response in the stock price. This means that the higher portion of CEO compensation could be used as discretionary, the more likely to show higher stock price response, except top compensation notch. For the credit rating interaction term, Figure 5 implies that neither a very good rating nor a bad rating has large effect in the stock price. Finally, the firm size interaction reveals that small firms tend to have larger stock price response to earnings announcements.
The principal result of this paper is that the asymmetric stock price response at both the aggregate and industry level has been detected and is empirically related to key strategic industry-specific factors (Herfindahl index, CEO compensation, credit rating and firm size). My results indicate that on average, the stock price responds more to negative abnormal earnings than to positive abnormal earnings. The difference in responses, although small, is statistically significant. Another interesting finding is that, the asymmetry in the stock price response is more related to the variance of the stock price change than to the magnitude of the stock price change itself (Figure 1). The effect of industry-specific variables on the stock price is consistent with what the previous theory predicts. For instance, the stock price has a larger effect in a relatively more concentrated industry. However, for the effect of the credit rating interaction on the stock price, further development of theory is needed. The fact that the variance of stock price response has a larger impact on the asymmetry in stock price response will naturally lead to time series econometric specification for future research. Furthermore, it seems worthwhile to control voluntary disclosure in this model.

This paper is an exploratory analysis of whether or not an organizations' gains from out-sourcing on-site clerical jobs have any impact on the clerical workers' compensation. For the purposes of this discussion, we consider a worker who works on site at company x, but paid by company y as "out-sourced." This analysis excludes the more classical definition of "out-sourcing" as a job sent overseas. The first part of this paper discusses whether economic gains from compensation drive firms' decisions to out-source. The second component examines where contracting firms locate and raises questions about the spatial mismatch. Finally, the third and primary sectino of the paper explores whether or not indirectly employed clerical workers receive lower compensation than do their directly employed counterparts. None of these analyses are conclusive, so the future research section sets an agenda for an inferential spatial analysis of the interaction between the demand and supply of clerical contractors, a model of spatial mismatch, and a longitudinal analysis of clerical workers, focusing on the importance of flexibility in the workers' decision to work indirectly. Ultimately, I hope to determine whether or not the longitudinal trend towards out-sourcing will negatively affect the low-skill female work force.
In the past decade firms increased their purchases of services significantly more than they increased their direct hires. From 1988 to 1997 "business services" grew by 5.8% per year. The rate of growth in business services increased at twice the rate of the rest of the economy over the past 2 decades. Business services is the larget category under the NAICS 54 to 56 categories. These include professional, scientific, and technical services, (legal advice, accounting, bookeeping, architecture, consulting, research, etc) management of companies and enterprises (establishments holding sufficient securities to administer and oversee a company, and administrative and support and waste management (office administration, human resources, clerical, security, cleaning, and waste disposal.) Personnel services, a subcategory of business services, is the largest employer in the cateogry. The second largest is computer services.(Clinton:1997) The fastest increasing sub-sub sector is the temporary help industry (largely clerical), which grew 11 percent annually from 1979 to 1995, five times more quickly than all other non-farm employment.(Autor:2000)
Contractors are gradually displacing their traditional direct-employment counterparts in some of the biggest sectors of the labor market. One-fifth of all wage and salary workers in the US are in administrative support jobs like clerical work.(Clinton:1997) This change is enough to transform the entire female low-skill labor market. Out-sourcing has already transformed the maintenance (janitorial) sector; by the late 1990's, 90% of all maintenance workers were subcontractors.(Clinton:1997) If this trend continues to diffuse, the effects could be enormous. If out-sourcing has different effects on compensation for low-skill and high-skill workers, there could also be an effect on overall inequality. Between 1973 and 2000 the average real income of the bottom 90% of Americans fell by 7% while the capital gains for the top one percent rose by 148%. (Piketty) Organizational theories suggest that high skill jobs might be outsourced to tap into intellectual economies of scale or because of variable demand for flexible services. If the factors that drive low skill outsourcing are different; namely if they are savings firms compensation costs, outsourcing could widen the ever-growing gap between high and low skill labor.
If organizations are increasingly hiring workers indirectly, it must somehow be more efficient to outsource. However, it is uncertain that these gains come from saved compensation costs. While this is intuitively the most logical motivation, there is surprisingly little evidence that indirect hires earn less than direct hires of comparable ability and more surprising, there is also little evidence that firms realize any economic gain beyond maintaining a flexible labor force.
John Benson's qualitative analysis of four Australian manufacturing firms suggested that in the long run, outsourcing does realize returns through reducing compensation costs. However, this does not cause any harm to the indirect employees who maintain the same earnings. All saving must therefore come from the firm being able to adjust the workforce as the workload fluctuates.(Benson:1999) In contrast, Young and MacNeil (studying two food processing companies) found that out-sourcing can incur unexpected management costs, negating any negligible benefit from cutting compensation(Young:2000)
Broader data anlyses also find mixed evidence. Analyzing the BLS's Industry Wages Survey, Abraham and Taylor found that higher wage companies were more likely to outsource their janitorial, machine maintenance, engineering, and accounting services.(Abraham:1996) Similarly, Gramm and Schnell found that organizations with higher wages are more likely to contract out.(Gramm:2001) In contrast Deavers found evidence that they are not. (Deavers:1997) Deavers argued that technological change, a focus on organizational competencies, and labor flexibility are the primary motives for outsourcing. Howeveeer Deavers provided little empirical evidence to support the claim. Davis and Uzzi also failed to find evidence that externalizing the labor force save organizations on health insurance, pensions, unemployment insurance, and supervisory costs, or wages.(Davis:1993) Mayal and Nelson found contradictory evidence; in their sample of 882 firms, they found that firms with higher benefit levels were much more likely to outsource to avoid these costs.(Mayall:1982) Mangum, Mayall, and Nelson found that firms use temporary and contingent workers to save money on the higher compensation associated with full time long-term employees. (Mangum:1985) Some of the variation in these findings is related to the different methods; some of these studies are qualitative, some estimate benefit levels based on the firm's interpolated likely payout. Most of these studies are cross sectional, and all focus on one of the two important building blocks of the argument: 1) does the firm outsource in response to economic pressure and 2) does the outsourced worker receive less compensation.
The problem with these studies is that they need to prove both to come close to proving that firms outsource to save money on compensation, and in the process, harm the same workers they would have otherwise hired directly. For the firm that is indifferent between indirectly and directly hiring a worker, the cost of an indirect employee (oversight search, and compensation) should equal the cost of a direct hire (compensation, search and oversight costs, and down time.) As such the firm could have incentives to outsource without reducing compensation. Similarly, compensation could decline for the outsourced worker without benefit to the employer, (for example if outsourcing increases the employer's search, training, or oversight costs, or if worker quality declines.) However, the literature generally assumes that if we can find evidence that firms with higher wages and benefits are more likely to outsource, the worker is worse off. Similarly, they attempt to show that if indirect employee receives less compensation than the direct employee, the workers are the same workers. Further, most of the literature focuses on a case study of one or two firms, failing to make generalizable arguments.
The spatial diffusion of outsourcing is also important in telling the story of workers' welfare. The firm might be more likely to outsource if contractors areavailable locally. Local contractors could reduce the firm's search cost and reduce the cost of oversight and implementation. There is a literature examining the role of space in the diffusion of various organizational practices. Knoke used a pairwise distance measure to predict the spread of municipal reform, while Hedstrom (1994) examined the spread of unions and Nyers found that riots are more likely to happen in locations close to where other riots have occurred. While many studies find that economic and social practices diffuse in a spatially correlated manner, it is difficult to measure the cause, since spatial correlation in the social world can happen through many different mechanisms from social networks to marketplace pressures.(Soule) Space isimportant from the worker's side as well. As a sector's employment moves to indirect employment in an individual's labor market, that individual is more likely to be indirectly employed. Thus, if outsourcing diffuses through the Northeast first, or through the suburbs, the Northeastern or suburban worker might suffer lower compensation or enjoy shorter unemployment spells or more flexibility. 1
Firms would not indirectly employ clerical workers if it were not in their economic interest to do so. The 1997 National Organizational Survey (NOS), a survey of 1,002 work establishments from June 1996 to June 1997 asked firms about their outsourcing practices for maintenance, clerical, accounting, IT, and core production. It used a stratified random sample from 15 million work establishments. (It was stratified based on organization size since the majority of work organizations are small.)
In the 1997 NOS sample outsourcing low-skill labor was common, but constituted a small proportion of all clerical employment in the sample. Fifty-one percent of the surveyed organizations had employees that were not on the payroll (primarily through subcontractors or temp agencies.) However, on average only .42% of the organizations' jobs were outsourced. At most,organizations contracted between 1 and 15% of their jobs. The organizations were most likely to contract out clerical (16% of organizations), accounting (12% of organizations), and IT positions (13% of organizations.) These results mirror the BLS samples reported by Clinton and Abraham, who found that more than one half of businesses contract for one or more types of business or administrative support.(Abraham:1988, Abraham:1990, Clinton:1997) Seventy-one percent of the sample (710 of 1,002) was for-profit organizations, 20% was public non-profit, and 8% was private non-profit. The median organization had between 100 and 150 employees, 87% offered health insurance, 26% had unions in the workplace, and about half believed their industry was "competitive."
The survey asked managers why they outsourced and found that the overwhelming reasons cited wither variable work demand and contractors special skills. Lowering costs was not a common response, and interestingly, few firms claimed they did it because others do. Obviously, these categories are artificial separations. A firm could not outsource if other firms were not doing the same. Similarly variable work demand and simplifying administrative tasks, and recruitment strategy are all just various forms of lowering costs-down times costs, low productivity costs, and search costs. These answers are likely to have errors since the individual answering this question was generally not the individual who chose whether or not to outsource a given position and respondents probably preferred not to answer positively to unappealing responses like "willing to do unsavory tasks" even if this was the case.
In the NOS dataset outsourcing was common among organizations, but not among employees.
First I ran two logistic regressions examining why firms outsource clerical janitorial positions.
where:
For-profit organizations are much more likely to outsource clerical positions than are larger firms. Those organizations offering medical benefits are less likely to outsource clerical positions. Health care benefits might be highly correlated with "high skill industries" since these industries are both more likely to offer benefits and less likely to contract out. But even that explanation is suspect; more likely there is so little variance in offering benefits that the results are unreliable. It would be preferable to measure the percent of employees actually receiving benefits. Organizations with seasonal work are more likely to outsource and oddly, those organizations that perceive themselves to be in a "competitive" environment are less likely to outsource.
The strongest results are those for "for-profit" status and the size of the organization. Both of these results are intuitive. Smaller organizations have less flexibility both in terms of revenue and in terms of shifting staff within the organization. Thus small firms must outsource to maintain flexibility that larger firms inherently have. There are multiple hypotheses why non-profits might be less likely to outsource-one of which is that they face a less economically competitive environment. Other reasons cross into the organizational theory literature.2
Table 3: Outsourcing clerical positions: logit
For this analysis I generated some continuous maps displaying the number of firms offering contract services in four cities across the U.S. by zipcode. I also conducted analyses, not displayed here, for the total revenue, payroll, and the number of employees in these firms. The firms are all firms falling under the NAICS code 56, Administrative and Support and Waste Management and Remediation Services. The majority of this category includes those firms I am interested in: administrative support, temporary help, employee leasing, etc. However, it also includes collection agencies, travel agencies, security services, janitorial and landscaping services, and waste management services. Thus it is a very imperfect measure of contractor accessibility. Some of these categories, like waste management services, vary almost entirely by population. However, others, like locksmiths, might distort the data. However, without access to the business census microdata, spatial analyses of firms are limited.
Kriging is an interpolation method that originated in mining. It is similar to linear regression, however the weights (coefficients) are re-calculated for each interpolated point and the variance-covariance matrix is based not on the covariance of variables, but on the variance of the single outcome variable based on the distance between all pairs of points. In essence it is a weighted average. (Wackernagel)
where d is distance and z is the variable being interpolated.
the weights are calculated by experimentally finding the variance between two points as a function of distance. For a single interpolated point,i, the weights on each observed point are calculated as the betas are calculated in an OLS regression, based on the variance-covariance matrix. The formula for variance is generated by calculating for every pair of points in the data set the distance between them on the x axis and the difference in their values on the y axis. The functional form of variance is often exponential or gaussian, declining as the distance between two points increases. V0 is the vector for the variance between the observed points and the interpolated point.
CoKriging, the second method used here, is simply an extension of kriging, drawing in another spatially correlated variable that is correlated with the variable of interest. 3 In my case, coKriging is an important extension because I used zipcodes. Incorporating population does not "control" for population in the traditional statistical sense. Rather, it uses data on the covariance between population and the number of firms to create a better estimate of the number of firms. While firm level data is by zip code, population level data is by "place." This is an important adjustment in my analysis because in dense cities the organizational correlation is distorted. For example, imagine that I am kriging the number of organizations in New York City. My dataset is bounded by Northern New Jersey, Westchester County, and Farfield County, Connecticut. In Manhattan some zip codes include only one building. Depending on that buildings zoning, it might have 50 or 100 firms in it. The adjacent building might be zoned residential and have zero firms. Thus, in the densest part of the city I will have a lot of variability -- thus I will show a lot of variability between those points that are close together. The pairs of data that are furthest apart are those on the periphery of my data set. Given the geographic distribution of organizations, the majority of these zip codes have zero organizations in them. Thus, the zip codes furthest apart are the most similar. Incorporating population takes care of this problem because it is positively correlated with the number of organizations. Thus, cokriging gives me more realistic contour maps. All analyses here used a ln transform for kriging and then transformed back for presentation. I used a ln transform because the data was heavily skewed to the right.4
It should be noted that none of this analysis is inferential. I will incorporate spatial regression and inferential statistics in future analyses. These maps are simple descriptions of the supply side of the market. The mean zip code has 12 contractors. However, establishments are strongly skewed with the median zip code only having 4 establishments and the maximum being 459 (in New York City.)In 1997 the average zip code had between 20 and 49 employees in the clerical services and waste management category. Again, this measure is skewed with some zip codes having up to 50,000 employees in the sector.
These maps show first, that the number of contractors varies by population within the city, as expected. In addition, we notice that Detroit has an overall higher number of contractors than do the other three cities -- but it also has a larger population (and CoKriging did not "control" for population. Initially Houston seems to have many contractors, but given its large MSA population, we see that we might rank the cities in terms of contractor availability as Houston, Seattle, and then Detroit and Tampa. Second, it is clear that these organizations prefer to settle in satellite cities like Bellevue and Palm Harbor rather than in the central business district. Results were consistent kriging with the number of employees, total revenue, and total payroll, using ordinary kriging, and kriging by square mile. Results were inconsistent when I kriged New York City since New York is so densely populated and zoned mixed business residential, that there was no spatial correlation and a much higher mean number of organizations.
Suburban clustering could have two effects. First, it might mean that firms in the satellite cities are more likely to use contractors. Second, it might mean that suburban workers are more likely to work at these firms. The location of the contracting organization is where the worker interviews and perhaps trains for their placement, but is not actually where they work. Thus, the worker only needs to go to the firm at the beginning of their employment spell.
Much of the spatial mismatch literature is plagued by the problem that workers simultaneously choose their place of employment and their workplace, an analysis of indirect employees avoids this problem. Who moves to a new city to be closer to a temp job? Thus, we can be certain that if there is an association between the existence of contractors in an employee's area and an employee's status as an indirect employee, the proximity of the employers is causally related to their indirect employment status.
The dispersion of outsourcing and organizations' motivations for outsourcing do not tell us anything about how these changes are affecting the outsourced worker. There are multiple positive and negative outcomes that could result from the geographic and organizational shifts. Workers in areas with indirect employment opportunities might suffer shorter unemployment spells as contractors decrease search costs and increase the match rate between firms and employees. Workers in these areas might enjoy more flexibility, choosing to work fewer weeks or hours. Indirect hires might be paid less assuming that they are willing to pay for more flexibility or if they are lower quality workers. The firms might pay more in compensation and contractors' fees if they value flexibility or lower search costs; but they will pay less if the workers are lower quality. While this preliminary analysis will not be able to parse out the reasons behind pay differential -- it will examine whether or not there is a pay differential between indirect and direct employees. Insofar as the total demand for clerical workers is not increasing, any pay differential could suggest an overall shift over time in clerical pay -- and a regional shift in clerical pay where outsourcing practices have diffused.
For this final analysis I analyzed compensation differentials for direct and indirectly employed clerical workers. I pooled data from the 1994 through 2000 March Current Population Survey (CPS), randomly selecting one observation for each respondent, and using a sub-sample of women reporting paid employment as secretaries, typists, receptionists, administrative support, and data entry workers. I split this data into two groups, those reporting that their employer's industry as a personnel supply services company, management services, or miscellaneous business services, and those reporting working in some other industry. There is some automatic error in this classification scheme since even the contractors do hire some direct clerical workers. The final sample of clerical workers under 30 included 710 indirectly employed clerical workers and 20,409 directly employed workers. Because I was unable to control for labor force attachment beyond hours and weeks worked, some analyses focus on female clerical workers under 30. All numbers presented are unweighted because of the narrow population of interest; however results using CPS person weights were almost identical. All wages are in 2000 dollars using the CPI (not the Urban CPI)
Indirectly and directly employed clerical workers are not identical groups -- in fact indirect hires seem to be a higher skill group. Indirect hires have significantly more education, are about three years younger(36 versus 39), and are much more likely to be single. Indirect hires are more urban; only 10% of the indirect hires live in non-metro areas and 32% are in central cities while 20% of the direct hires live in non-metro areas and only 23% are in the central city. Indirect hires are also about twice as likely to be black than direct hires.5
Education of female clerical workers, all ages
Marital status of female clerical workers, all ages
On average, the indirect employee earns less compensation than does the direct employee and is less likely to receive a pension.
One possible reason for the difference in compensation is labor force attachment. Since this is cross sectional data, I was unable to analyze how long teach type of worker remains in the clerical sector, though it is likely that the contracted worker might be more transient worker, moving onto some other career. Thus, they might accept lower wages for a short term position, just as a new college graduate accepts a position as an assistant in a prestigious firm, ultimately moving up to a better position in the firm. Without the promise of higher future wages, the career clerical worker may expect higher wages in the current period.
While I was not able to examine long term attachment to clerical work, I was able to determine their short-term labor force attachment. Indirectly employed clerical workers fewer weeks per year but the same number of hours per week. This seems to suggest that indirect employees are under-employed, not voluntarily seeking more flexibility. The contract worker who is pursuing a secondary career as an artist or the part time mother likely wants fewer hours regularly, to cover bills and to have free time with their children or secondary career. Intermittent full time work seems to speak to an under-employment problem.
Labor force attachment for clerical workers under 30 years old
Of course indirect and direct clerical employees are not exactly the same populations. Controlling for their individual characteristics indirect employment might not influence their compensation. Because of the minimal labor-force attachment data in the CPS, I ran a simple OLS regression on the subset of female clerical workers under thirty years old.Initial results suggest that indirect clerical workers earn lower hourly wages. However, using cross-sectional data, this data is no more than suggestive of some possible inequality.
Female Clerical Workers under 30, Compensated less? OLS: R-square = .11 Logit: LogLikelhiood is significant with a test statistics of 1348 and a pseudo R-square of .20. 6
These regressions show that, at least in a cross sectional analysis, young female clerical contractors earn 87 cents less per hour than their direct counterparts (or about 9 percent less.)They are also about a third as likely to be offered a pension, though the average worker in both groups is unlikely to be offered a pension. A similar analysis of all female clerical workers (relaxing the age constraint) shows that the indirect employee earns $1.80 less per hour than the direct employee and that the indirect employee only has 11% the chance of getting a pension as does the direct employee. Compared to other important factors, like firm size, education, and rural/ urban markets, only urbanity is more important than contractors status in determining compensation.
Of course, these results are far from conclusive. The CPS offers insufficient control for worker quality and labor force attachment. A future analyses should use panel data, controlling for the worker's work history, their tenure in the clerical workforce. Further, it would be interesting to consider a hazard analysis of indirect workers, testing the chance that they move to direct employment. Perhaps the indirect labor market works as a matching mechanism, bringing inexperienced workers into the marketplace and eventually matching them with firms. If this is the case, the worker accepts lower compensation in exchange for an ultimate placement with a direct placement.
Based on this cross sectional analysis, I cannot answer any of the above hypotheses. However, given the longitudinal trend in the out-sourcing market, these "positive-spin" stories seem unlikely. Assuming that there has not been a dramatic increase in the overall demand for secretaries (and given the innovations in office computing -- it seems likely that there might even be a decline)there has been a dramatic shift among secretaries towards indirect employment. Did clerical workers always have an unsatiated demand for a more flexible work schedule? for job matching? Rather, it seems more likely that the firm faces pressure to minimize variance among its office employees, particularly in fringe benefits. Hiring a worker indirectly can relax these pressures, allowing the firm to compensate low skill workers less, without facing legal, bureaucratic, and social pressures to offer less dispersed compensation.
This analysis also fails to consider the spatial mismatch question posed in the second part of this paper. Indirect employees are more likely to live in urban areas with the indirect employers located in satellite cities. Does the relative location of the firm and the contracted worker influence the worker's decision to work at that firm? This question can be pursued using two sorts of data sets, the first is a set of workers linked with their firms, with addresses of both. Also, given a data set including workers' addresses I could use the data used in section two to krig a "job acessibility" measure for each worker. This measure would simply be the spatially weighted average of the number of indirect employers divided by total employers in the worker's commuting area. The spatial weight would use a decay function, giving extra weight to those organizations closest to the worker and less weight to those at the perimeter of their commuting area (say forty miles out.) Access to both types of data is limited because of the confidential address data. I am currently working on two applications for access to data, one to the RDC to use the LEHD to work with firm-worker data. The second application is to work with my advisor's datasets on Chicago Health. (This survey includes extensive information about employment in Chicago and includes addresses.)
This analysis raises more questions than it answers. These questions fall in three overlapping categories: wage inequality, space and the labor market, and organizational practices diffusion.
The first set of questions this analysis creates relate to inequality. Clearly, we found that outsourced workers are compensated less. It is uncertain whether or not this differential is due to workers desiring flexibility or to some sort of selection. However, it seems likely that outsourcing release the firm from bureaucratic and social pressures to offer low-skill workers higher compensation, no par with their other workers. If this is the case, what does the trend toward outsourcing mean for low skill laborers? As more firms outsource will low skill laborers be less likely to earn a living wage, receive pensions and health coverage? If the answer to these questions is "yes" there are clear public policy concerns related to retirement benefits and public health insurance.
The second set of questions motivated by this analysis relate to space. First, why do contractors locate in satellite cities? Is it the lower overhead costs? Are they closer to the firms they contract with? To the workers they employ? Do these firms' locational decisions contribute to the spatial mismatch? If workers find permanent positions through these firms, will inner city workers be unable to take advantage of these firms because they interview and train in satellite cities? Are suburban firms more likely to outsource because they are closer to contractors? If inner city firms are less likely to hire contractors, this might negate the spatial mismatch problem.
All of the above questions rely on the question of how this organizational practice diffuses. Has it already diffused in the clerical market? If not, what is its equilibrium point? What types of organizations will maintain direct hires and where will they be located? If this phenomena has not played itself out, what will the low skill labor market look like when it does?
In this analysis we established that for profit organizations and organizations with variable workflows are significantly more likely to outsource their clerical workers, contracted clerical workers earn significantly less than their direct hire counterparts, and that the contractors are located disproportionately in satellite cities. Given the strong longitudinal trend towards outsourcing, it seems unlikely that the compensation differentials are entirely explained by workers' demands for flexibility. Furthermore, if the firm also desires flexibility, why does the contractors' fee primarily come out of the worker's paycheck and not from the firm? In conclusion, this analysis seems to provide suggestive evidence that this trend towards outsourcing low skill jobs like janitorial and clerical services might exacerbate the trend towards growing inequality.
I still have not attacked the research question that motivated this project, an analysis of whether or not the density of contractors encourages firms to outsource. (In other words, is there a spatial covariance between the decision to outsource and the number of contractors? The predictor of interest would be the percent of business in the region that is business services. Spatially weighted (by zip) economic controls will include the average wage rate, average unemployment, average education, etc. Individual level predictors will include classic controls like education, work experience, etc.
There might be positive outcomes associated with more employment through business services organizations. The most likely positive outcome is a more fluid labor market. This might be related to shorter unemployment spells for clerical workers. I think I'll probably propose a hazard model testing whether or not individuals in area with more outsourcing are more likley to have shorter unemployment spells.
1 The question of economic gains from lower benefits is more complicated than that of wages because it encompasses questions of tax law. US Code Title 26, subtitle A, Chapter1, Subchapter D, PartI, Subpart A, Section 401 a(4) states, "if the contributions or benefits provided under the plan do not discriminate in favor of highly compensated employees (within the meaning of section414(q)). For the purposes of this paragraph, there shall be excluded from consideration employees described in section 410(b)(3)(A) and (C)."
This section of the US code says that if an organization offers benefits to the only the top 20% of its paid employees, or only to those employees paid more than $50,000, their expenditures on pensions, health insurance, and life insurance will be taxed 25%. This law prevents discrimination in favor of the high skill worker but not against the low-skill worker. At the same time -- the threat of additional taxes combined with the administrative costs of managing multiple plans and economies of scale in bargaining for single plans, might encourage firms to provide consistent benefits to all employees. As such, it is easy to imagine that a firm might realize gains in benefit costs by directly employ their workers whom they wish to compensate with one level of benefits, and out-sourcing those who they do not wish to compensate with comparable benefits.

Traditional models of international assets typically assume that countries and currency areas are identical in size. The major contribution of the paper that I am going to discuss demonstrates analytically and empirically the implications of relaxing this assumption. The primary finding is that the differences in sizes of economies have important implications for international asset returns. More specifically, the author's model predicts that larger countries' bonds must pay lower excess returns in equilibrium. This is because the bonds of larger countries provide better insurance against shocks that affect a larger fraction of the world economy. The author interprets the results of his model to declare that uncovered interest parity (UIP) fails unless countries are identical in size. He conducts a set of regressions to test whether his model's predictions are empirically significant. I describe below the author's motivation, theoretical development, empirical strategy, and his results while providing a critical assessment of each of these parts at the same time. I focus in this paper on the intuition of the author's model and its pros and cons as well as providing my assessment of his empirical strategy.
The author's research question is motivated by some recent empirical findings that document the existence of significantly better hedging properties of large country assets. For instance, he cites: 1) Campbell, de Medeiros, and Viceira (2007), who find the Euro and the Dollar to be better hedges against the risk faced by a global equity investor than other currencies; and 2) Lustig and Verdelhan (2007), who suggest that portfolios of bonds denominated in low interest-rate currencies tend to be good hedges against US consumption risk. These citations do not necessarily justify the expectation that it is primarily the sizes of the economies which lead to these empirical regularities. One could possibly argue that there is an unobserved effect that is correlated both with country size and low interest rates, or some other omitted factor. To preempt such an argument and to further motivate his question, the author runs a couple of regressions with time fixed effects (although one would also like to see individual effects included) to show that there is a strong negative correlation between a country's share in total OECD output and interest rates. While this first pass at the data propels us to care about country size, the author's interpretation of these regressions as a significant departure from UIP is premature, mainly because the simple empirical approach here is unable to capture the assumptions of a UIP framework. Another deficiency with the motivation of the research question and the "descriptive" regressions is that none of the model's key arguments are communicated at this point to describe why we would see such a negative correlation between country size and interest rates. The motivation seems to arise solely due to an empirical observation for interest rates but with no theoretical inquiry about why earlier models have not cared about country size and we now should. As a result, the author cannot motivate well enough why we should care about this question theoretically as well as trying to explain the data better.
The author carries out two main exercises to study the effects of differences in country size on international asset returns. The first and major model of the paper is a standard Lucas-tree endowment economy with complete asset markets which includes non-tradable goods. In this respect, it is very much in the spirit of Backus and Smith (1993), Tesar (1993), and Stockman and Tesar (1995) -- the author only relaxes the assumption of identical country sizes and derives the implications for asset returns under this case. The second exercise is introducing monetary shocks to the previous model. The author departs from the complete asset markets assumption in this part to establish the desired relationship between country size and interest rates under a weaker set of conditions.
The introduction of non-traded goods in consumption has the primary role of allowing the consumer price index to differ across countries. Since the real exchange rate is defined as the ratio of two countries' consumption price indices, shocks to a country's endowment of the non-traded good is the driver behind the fluctuations in its exchange rate with the other country. The author skillfully makes use of this regularity to demonstrate how stochastic shocks to a country's endowment of the traded and non-traded goods affect its asset prices. More specifically, the author solves the competitive equilibrium problem for a representative household, where, taking prices as given, the household solves1:
where Q(ω) is the first period price of a state-contingent security that pays one unit of the traded good if state ω occurs in the second period, and W1(i) is the NPV of household i's endowments (net of transfers). Notice that the household exhibits constant relative risk aversion (CRRA) utility. The consumption and price indices are given by:
Before discussing how the endowment of non-tradable goods affects λT and how the author demonstrates the implications of country size differences, a few remarks are in place. After solving for the Euler equation of the household, the author presents Lemma 1 of his paper which relates the difference in the (log) expected returns between two arbitrary assets to the difference between the covariances of the payouts of these assets with λT. The lemma basically says that households prefer assets that pay off high whenever additional traded goods are sorely needed (i.e. marginal utility tradables consumption is high), which implies that the asset that has the higher covariance with λT must pay a lower return in equilibrium.
Appendix A shows that the return to an asset is found by summing up the prices of state-contingent securities from the households' FOCs, and the final result is found by using statistical identities that make use of the assumption that the payouts of the assets are log-normally distributed. While there is a nice intuition that arises out of this lemma, the author actually possesses the required findings to solve for asset return differentials directly from his model. He is able to do this since he solves for the marginal utility from tradable goods in the planner's problem, the equation for which is given in (14) on p.12. Then plugging this in to (10), the equation for the price of a state-contingent asset, he can solve for
With the exact same setup, Backus and Smith (1993) solve for the interest rate differential between two countries in Example 1 of their paper (p.306). This example not only demonstrates how to derive the interest rate differential directly from the model, but also shows that the relationship between the interest rate differential and non-tradable endowments crucially depends on the assumptions on functional form. Backus and Smith (1993) assume that α = 0 and γ = 0, thereby allowing for additive separability.3 Correspondingly, the state utility function form is the Cobb-Douglas case. Their only assumption on the planner's weights is that
The second primary identity that the author establishes shows why we should care about differences in country size. This identity is the closed-form solution for the marginal utility from tradable goods, λT, from the planner's problem. The planner's problem is cast as: max
How does this condition compare to earlier findings in the literature? In her paper focusing on a two-country version of the complete markets economy model with non-tradable goods, Tesar (1993) notes the importance of the assumptions on the functional form of utility, just like Backus and Smith (1993) demonstrated the implications of different forms. She shows that if utility is separable, then the realization of the endowments of the non-traded good does not affect the optimal allocation of the tradable good. If utility is not separable between traded and non-traded goods, however, the allocations of the traded good will depend on the endowment of the non-traded good. This is the case that the author implicitly assumes throughout his paper. Tesar (1993) shows (equation (6) in her paper) that consumption of the traded good in the home country is an increasing function of the home country's endowment of the non-traded good when the sign of the cross-derivative of utility is greater than zero. That is, if this cross-derivative is positive, the home country wishes to consume more of the traded good when its endowment of the non-traded good is large, because its marginal utility from tradable goods will be large. Now, contrast this with the author's findings above. While Tesar (1993) relies on a sufficiently high degree of complementarity between the tradable and the non-tradable goods, the author requires a high enough substitutability between the two. Indeed, focusing on the author's equilibrium levels of the consumption of the tradable good in the home country and λT, we see that a higher level of a country's endowment of the non-traded good means a lower level of its consumption of the tradable good and a lower value of λT, under Condition 1. To see the sharp contrast between the two papers more explicitly, note that under the identical isoelastic preferences and CRRA utility setup, the requirement on the parameters for Tesar's (1993) results is
It is thus highly important for the author to address the importance of the functional form of utility and the assumptions on the parameters of the model. Going back to Example 1 of Backus and Smith (1993), we see that the assumptions α = 0 and γ = 0, which allow for separable utility, do not agree with Condition 1 of the author, but with Tesar's (1993) condition. In the case of α = 0, the direction of the inequality will be determined by whether γ>1 or not. If γ>1, as the author assumes, then Tesar's (1993) condition is no longer satisfied.
After solving for the important variables in the model, the author solves for the prices of the traded and non-traded goods by calculating the Lagrange multipliers associated with the planner's problem, which I do not report here. The implications on the prices actually follow from the discussion about the relationship between consumption of tradables and the endowment of non-tradables from earlier on. Whenever the world average endowment of non-tradables is high, more tradables are delivered to the domestic economy, which diminishes the relative supply of the non-traded good within the country and hence makes it relatively more expensive. These implications are again dependent on Condition 1, which stand in contrast to the condition in Tesar (1993). The author finally presents the spread on international bonds, which depends on a country's price level since the country's risk-free bond pays in terms of the home consumption bundle. The intuition is well posed: when the domestic endowment of non-tradables is high, the home consumption bundle is relatively cheap and the ex-post payoff from the risk-free bond is relatively low. The author presents his Proposition 1:
, which directly reveals that there will be an interest rate differential unless countries are of the same size. The author argues that this points to the failure of the UIP, and this result is derived in a neat and analytical way under certain assumptions. We see this result independent of the fact that the equation above corrects for expected movements in the exchange rate, although it does not account for the presence of an exchange risk premium (I turn to this in greater detail below). However, the direction of the interest rate differential is again dependent on whether Condition 1 holds, which is contestable given the earlier literature and the indeterminant results surrounding what the value of γ should be. Taking this equation to the data might potentially return ambivalent results given that countries will differ in their respective degrees of risk aversion and elasticities of substitution. Indeed, some of the robustness checks that the author provides in his empirical results show no particular difference from zero in any direction.
The second exercise that the author carries out is to introduce monetary shocks to the model economy that is constructed previously. This extension is of secondary importance to the results of the paper, and seems to be constructed to support the analytical findings of the earlier model in a different setup. The value added from this extension, however, is to establish the point that Condition 1 is no longer needed to arrive at the author's conclusions. This is done by switching to an incomplete markets economy, where only a subset of households in each country is allowed to trade complete state-contingent assets in a Calvo-style formulation, and all households are subject to a cash in advance constraint. The results of the model will be very similar to the ones presented above, with the exception that home country assets will now be priced by the ratio of the marginal utility of "active" households, i.e. those household which can access complete markets.
The competitive equilibrium solution to the household's problem in the monetary extension case (not reported here) indicates that inflation at home country will lower the tradable and non-tradable consumption of the inactive households, thereby functioning as an inflation tax. After having established this result, the author solves the planner's problem in the case of incomplete markets for active households by adding the ratio of the active households in the population to the Lagrangian presented above. The new equilibrium solution for the marginal utility from tradables becomes:
, where the upper bar again denotes the world average. Whenever domestic inflation exceeds the world average, the non-traded good becomes relatively more abundant at home, leading to a decrease in its relative price and thus to a depreciation of the domestic currency in real terms. It follows from here that a larger country's risk-free bond is a better hedge against consumption risk, so it should pay lower returns in equilibrium. Notice that these conclusions are actually much stronger than the ones presented for the previous model. These results do not require any restrictions on the parameters of the model like Condition 1 does. The reason is that since inflation directly affects the amount of tradables available to active households, it must always lower marginal utility, giving the required condition to establish the author's results. Hence, he no longer needs additional conditions to be imposed on the model. This is surely a strong result, which comes at the cost of leaving the realm of complete markets, though.
The final piece of the author's theoretical work concerns the formation of a currency union. This additional step is not motivated theoretically, but from an empirical point of view to account for the fact that the bonds of most European countries started to be denominated in Euros from 1998 on. The author simply extends one of his earlier propositions to say that the formation of a currency union should lower the expected return on risk-free and nominal bonds as well as stock returns in the non-traded sector of all participating countries. This claim and earlier predictions about interest rate differentials are tested in the empirical part of the paper.
As noted before, instead of solving directly for the relationship between the interest rate differential and the stochastic discount factors, the author chooses to provide identities that are more amenable to econometric estimation. Specifically, the theoretical discussion of the paper produces four testable predictions: 1) bonds issued in the currencies of larger countries should pay lower expected returns; 2) the introduction of a currency union should lower expected returns on bonds within the union; 3) stocks in the non-traded sector of larger countries should pay lower expected returns than those of smaller countries; 4) the introduction of a currency union should lower expected returns on stocks in the non-traded sector of participating countries. The main interest is on the first prediction here, as it is the closest test of the UIP. The author uses quarterly data from OECD for the period 1980-2007 and presents his empirical results for several specifications and sets of right hand side controls. The main empirical findings support the author's predictions, although the econometric setup still has room for improvement. Instead of focusing on these minor econometric details, I will provide below a main discussion about the author's argument that UIP fails and argue why this implication is still immature. I describe below why his econometric strategy is not a complete test of his theory, thereby weakening his paper despite the strong theoretical results. I also provide a suggestion to the author that could either strengthen his case or tell him more about the interaction between country size and interest rate differentials once carried out.
Underlying the UIP concept is that under perfect capital mobility, interest-rate differences must be offset by expectations of exchange-rate movements. So for instance, the domestic interest rate can exceed the foreign interest rate only if the domestic currency is expected to depreciate at a rate equal to the interest-rate differential. This is in essence a no-arbitrage principle. The author argues that UIP fails unless the countries are of the same size, since larger countries tend to have lower risk-free interest rates. The theoretical derivation of this result is well-constructed and demonstrated (although still subject to my comments above and below). However, when it comes to estimating econometrically whether this effect is significant, the author reveals the weak side of his paper and does not seem to present convincingly whether a structural or reduced-form estimation method is relevant. Here are some of my concerns.
There are several reasons why the author's findings would come about even when capital is perfectly mobile within the framework of UIP. The first reason is that the rational expectations hypothesis might not apply in this context. A quick check for this would be to look at the serial correlation in the error terms of the regressions that the author runs. If serial correlation is detected, this could pose a serious problem to the assumption that the shocks that are realized this period are not correlated with any future shocks. Indeed, the variables under the author's focus, such as quarterly interest rates or exchange rates, are likely to follow time trends and the dataset that is available is essentially of a panel form. Given the time-series properties of these variables, it is highly likely that one will detect serial correlation in the error terms. Pooled OLS regressions that do not take such concerns into account as in this paper are likely to return inefficient estimates where standard errors are miscalculated5.
Another reason is the closely related issue of econometric implementation. The author notes (p. 23): "Since the model developed in this paper has only two time periods, I interpret the panel as a series of cross-sections and make the appropriate econometric adjustments." Firstly, it is not clear which econometric adjustments are carried out in order to be able to interpret the panel as a two-period system. The author uses quarterly data over the period 1980-2007 and compressing this relatively long panel into a two-period system has its drawbacks which are not mentioned. Second, rational expectations are imposed on the econometric estimation and the author rules out any concerns of autocorrelation to start with. Interpreting the panel as a two-period set of observations effectively prevents one from estimating HAC (heteroskedasticity and autocorrelation consistent) standard errors. Yet, testing for autocorrelation and correcting the standard errors for such a possible problem is what the author needs to argue that the interest rate differential comes from differences in country size as opposed to other possible factors. This brings me to my third point that this is not merely an econometric concern.
The third reason why the author's results might still be consistent with perfectly mobile capital in the UIP framework is the presence of exchange risk premia. While the author uses a constant relative risk aversion (CRRA) utility form to solve for household optimization, he bypasses the role that risk premia play in the determination of exchange rates, and correspondingly interest rate differentials. UIP holds under the assumption of no risk premia. If there exists exchange risk premia, then (part of) the cross-country difference in the interest rates can be attributed to the risk aversion of agents who want to be compensated for the exchange rate uncertainty they face. Hence, the author should be able to isolate the effect that differences in country size have on interest rate differentials on top of the portion that risk premia have in explaining these movements.
The current regressions included in the paper fail to account for the presence of risk premia. Hence, I would suggest the author to do a simple test of whether such premia exist. The author can easily carry out this test given the data set he has, and here is how to proceed6:
UIP implies that the interest differential between two countries equals the expected exchange rate change: i.e.
As a result, the lack of a theoretical argument about exchange rate uncertainty and exchange risk premia as well as an inconclusive econometric setup weaken the author's ambitious argument that UIP fails merely due to differences in country size. Testing for the presence of risk premia is relatively easy and certainly doable for the author. A more careful treatment of the data and econometric estimation can certainly help the author strengthen the implications of his theoretical model.
Overall, the author presents a well-written, clearly worked out, and a very intuitive piece of work especially in the theoretical parts of the paper. Results are demonstrated in a nice and clear way, and the implications of the model are well posed. However, the paper is weak on the side of highlighting its contribution and especially linking it to the previous literature on complete asset markets models with non-tradable goods. The author fails to identify how his results and theory are a step forward from previous papers, not providing any discussion about whether his results stand in contrast or complement earlier work on closely related topics. Moreover, the author does not check how functional forms or other parameter restrictions affect his model and only focuses on a single case. However, earlier literature suggests that these concerns are definitely in place. Another drawback of the paper is that the econometric estimation seems to be done only for expository purposes -- an econometric framework that aims to truly test the implications of the model is missing. More thorough work on the theoretical part that puts the author's findings in context with the literature and discusses its implications can be more telling than the regressions. However, improving the econometric framework and confirming the main model's predictions would also be a huge success.

There has been a large body of research regarding underreporting real outputs to the IRS to evade or to avoid corporate tax. There have been also large amount of research on the relationship between the overstatement of the reported output and management compensation. While we have plenty of works done for either of the falsifications, however, it is relatively in recent years that both falsifications are dealt as an agent's simultaneous decision problem under information asymmetry, or in principal-agent problem context.
Erickson, Hanlon, and Maydew (2002) examined the extent, if any, to which firms pay additional income taxes on allegedly fraudulent earnings. They found that firms are willing to sacrifice substantial cash to inflate their accounting earnings, which is not assumed in pure tax evasion problem. Desai and Dharmapala (2004) also analyzed the relationship between the compensation and tax sheltering, and concluded that the increases in incentive compensation tend to reduce the level of tax sheltering. Crocker and Slemrod (2003), adopting the optimal insurance model developed by Crocker and Morgan (1998), suggested the optimal contract for the CFO under information asymmetry between the CFO and shareholders and the possibility of falsification of taxable income report.
In many cases, both the overstatement of output to the market and understatement of taxable income to the IRS must be decided at the same time. In addition, and one incentive acts in the opposite direction to the other one. In this paper, adopting the implication of an embellishment under rational expectation from Kwon and Yeo (2005), I analyzed a relationship between the level of both reports -- tax and output -- under different marginal tax rate, intensity of compensation, and the level of output by calibration.
An agent (e.g. CEO) is risk-neutral and is assumed to have private information regarding the level of non-negative effort a and that of non-negative manipulation ma. The real output of the company y is determined by
(1)
where ε is an unobservable state noise such that E(ε) = 0.
After the real output is observed by the agent, she announces the reported output for her firm to the public. The announced output, which is public, is the sum of the extent of manipulation and real output as
Now, there is a risk-neutral principal (e.g. a representative shareholder) and he wants to make a contract with the agent. The distribution of the real output y is known to the principal, but he only knows the reported output. The support of y is assumed as
Finally, the company has to pay corporate income tax, which depends on the level of income. Note the reported income in this case is not necessarily the same as that released as output announcement. The company report different level of income yt = y + mt to IRS, and the corporate income tax rate t is determined as
where τ is the marginal corporate tax rate and r is a constant.
The timing of this model is as following: First, the agent chooses her level of effort a. Second, after the real output y is revealed, she decides the level of manipulation ma and mt. Third, the output is announced as ya, and separately report the income yt to the IRS, and, based on it, marginal tax rate t is determined. Fourth, stock return for the firm ys is determined by the market, based on the reported output ya minus the level of market conjecture on the level of manipulation μ and the income tax rate
The agent's expected utility is the following function of effort, manipulation, tax, and market-conjectured level of manipulation as:
(4)
Under risk neutral utility assumption we can rewrite this equation as
(5)
where c(a, ma, mt) means cost of effort and manipulation. We assume that c(a, ma, mt) can be separable into c(a), c(ma), c(mt),and c(ma-mt), which are convex (i.e. c' > 0 and c" > 0) and satisfy c'(0) = 0 and c'(∞) = ∞. These assumptions about cost functions of the agent imply that costs of putting effort or manipulation costs increase with respect to the size of overstatement. In addition, we assume that the wider the gap between the overstatement and the understatement is, the higher the cost is. For simplicity, we assume quadratic form of cost functions as
where km and kd are exogenous constants.
The principal's payoff Π(a, ma, mt | y) is based on the anticipated real output ys. Stock returns are also determined based on it. After the returns are determined, the agent gets paid proportional to them and shareholders will receive the rest of them as:
We solve any multi-stage game backward. At the end of the game, with the realized output y after choosing the optimal effort a*, the agent's problem is
In other words, the agent's problem at the final stage is to choose the optimal level of manipulation for the market and the IRS, respectively. The first order condition is
To simplify, let r = 0. It makes sense because the corporate tax rate is zero when the reported income level is the least. Now the equations (9') and (9") can be rewritten as
From Kwon and Yeo (2005), under rational expectation, ma* should be equal to μ*, and from the equation (2) and (3), we can derive dma = dya, and dmt = dyt. The equation (10') and (10") would be written as
Under assumption that the support of y is
With two unknown variables ma and mt, and two equations (14') and (14"), we can find the optimal levels of manipulations. Unfortunately, however, an analytical solution does not exist. I did calibration for different level of marginal tax rate τ, the strength of incentive β, and real output y to see the cross relationship between these factors and optimal levels of the understatement of taxable income and overstatement or output. Followings are three findings from the calibration.
First, the level of overstatement on output ma decreases with respect to marginal tax rate τ. Beyond one point of marginal tax rate, the announced output is even lower than the real output. On the contrary, the level of understatement of taxable income mt increases with respect to marginal tax rate.1 Second, the amount of falsification for both the understatement of taxable income and the overstatement of output gets higher with respect to the strength of incentive β. Third, as the real output y goes higher, the level of overstatement on output goes lower while the level of understatement on taxable income goes higher. Detailed results of calibration following are attached as tables and graphs.
This paper investigates the effects of the marginal tax rates and the strength of monetary incentives of CEOs, and the level of realized output on the two optimal levels of falsifications; the understatement of taxable income and overstatement of output in the context of principal-agent model. While the calibration results show very intuitive ones, several future works are worth pursuing. First, I need to investigate the effect of those factors on the optimal level of effort a, or the amount of labor to get better idea on the optimal progressivity of corporate income taxes. Second, stronger theoretical results are necessary. Even though analytical solution is not achievable, more rigorous calibration and comparative statics are needed to generalize the model. Third, detailed analysis of the calibration is necessary to derive policy implications. Especially, it is necessary to quantize the marginal changes in the level of falsification and that of effort with respect to the changes in the marginal tax rates, the strength of monetary incentives, and the true level of output, respectively. Finally, an empirical model to test the results from Desai and Dharmapala (2004) will be worthwhile. These future works would make this paper have more general and concrete results.

The primary concern of this paper is whether household consumption values constructed from wealth and income data in the Health and Retirement Study are accurate enough to be of use in estimating an Euler equation describing consumption in a model with uncertain lifetimes. This paper is a companion to Perry (2005) in which I actually attempt to estimate that equation.
People's beliefs about their own life-expectancy have not been extensively studied-mainly due to lack of data. It is not clear that people actually have consistent beliefs about their own chances of survival at any time. Even if they do, measuring them in a meaningful and convincing way is difficult.
The life-cycle hypothesis makes a simple prediction about the relationship between a person's perceived risk of death and their consumption: those who think they are less likely to die will have less consumption growth over time. Simply put, if you expect to live a long time, you will conserve your resources early in life in order to have enough later-this means earlier consumption will be lower than it would have been if you had thought your chances of survival were worse, ceteris paribus. In this way, a higher expected chance of survival should have the same effect as a higher interest rate or a lower degree of impatience.
The Health and Retirement Study (HRS) has elicited subjective life-expectation data from its respondents since the study's inception in 1992 (12 waves of the HRS have been completed-1992-2002, every two years). The questions are of the form "What is the percent chance that you will live to be 75 or more?" (the target age-75 in this case-can vary).
The HRS, however, does not elicit consumption data from respondents. Instead, it provides measurements of assets, income and capital gains. These can be used to deduce a consumption level for the time periods between survey interviews. This process leads to a large amount of measurement error, though, as the assets, income and capital gains are all measured with non-trivial measurement error to begin with.
While my primary goal is to study the relationship between consumption profiles and subjective survival beliefs, this study is a first step in which I test constructed HRS consumption data against three alternative models of consumption-specifically, models of the covariance structure of changes in log-consumption. I find that a model describing consumption as an individual-specific constant plus a time-varying random shock best describes the data. This can either be interpreted as showing that the consumption values have too much error to be of significant use for this application or as showing that that process describes real respondent consumption in this dataset. In either case, the result indicates that this dataset is probably not of much use in testing the primary relationship of interest.
In section one I describe the three models of consumption that I test the data against. In section two I describe the data and how I produced my values of consumption. Section three contains the results of fitting the data to the three models.
My method is to propose a model for the consumption data; use it to derive conditions on the covariance between the changes in log-consumption in different periods; and then use those conditions to fit the data to that model using generalized method of moments as described in the appendix of Abowd and Card (1989).
The first model I propose to test is a model in which an individual's log-consumption equals a person-specific constant plus a random shock that changes each period:
This implies that the change in log consumption between two periods is given by:
As stated in the introduction, this model can thought of as a model of pure measurement error, or alternatively as actually describing the process that describes real consumption. The only parameter value to be estimated is
The second model I test is a variation on the random-walk model of consumption proposed by Hall (1978). The variation is that I propose (for convenience) to test whether log-consumption follows a random walk. The proposed model is
I ignore the issue of whether there is an additional trend term as that will not be identified by the GMM approach I take. Equation (4) implies
Therefore, the covariance structure of changes in log consumption is:
Similar to the measurement error model, the only parameter to be estimated is
However, this model implies zero covariance for elements that will have non-zero covariance in the measurement error model.
The third model I test is based on the life-cycle implication stated in the introduction. The implication that consumption growth should rise with a rise in a person's mortality risk comes directly from the Euler equation of an agent maximizing the sum of additively separable utility over his lifetime (my formulation is borrowed from Kuehlwein, 1993):
Here, p is the probability of surviving to the next period, r is the interest rate and
is the rate of time-preference. Lowering p has the same effect as raising
or lowering r-it privileges current consumption over future consumption. In this way, because I have no measurement of
, the effect of survival expectations will not be separately identified from the effect of time-preference. I ignore the issue of the utility value of a bequest upon dying.
I assume a felicity function
, and take the logarithm of each side. Also, because my main concern is with life-span uncertainty, I assume that the income stream is known. This means that there should be no uncertainty about realized consumption in period t+1, given that the respondent survives to that period, so I dispense with the expectation operator:
Adding a term to account for measurement error in the change in log-consumption gives:
The only variables from (9) that I have measured variation in are consumption and subjective survival expectation. Therefore, I make the possibly unfounded assumptions that the difference between log rt and log
is distributed randomly in the population given log(pt), and that
is constant (or distributed randomly) throughout the population. This leaves the relationship that I examine:
This equation implies the covariance structure:
The GMM estimation of this model will fit values of both
, then this reduces to the random-walk model.
The HRS is a nationally representative panel study of persons over 50 in the United States. Beginning in 1992, respondents were interviewed every two years, covering health, finances, physical and mental capabilities, family structure and relationships and job history. A study called AHEAD (Assets and Health Dynamics of the Oldest Old) began in 1993 and focused on older respondents. In 1996, the AHEAD study merged with HRS. New cohorts were added to HRS in 1998 so that the survey would remain representative of those over 50. The last wave of data available for this analysis comes from interviews done in 2002. I employ all HRS waves, but I do not use AHEAD data that was taken prior to the merger with HRS.
Table 1 shows descriptive statistics of the population that I will use for this analysis. In each survey wave this population consists of all respondents who answered at least one subjective survival question in that wave and who were single throughout the time 1992-2002. P(75) and P(85) refer to the mean values of the probability responses to the subjective survival questions that ask about target ages of 75 and 85 respectively1. These statistics are meant simply to make it clear what the population of respondents is like in any particular year. They cannot be used to make accurate inferences about the evolution of households or singles in the HRS population over time because those respondents who have a valid answer to at least one subjective survival question are a highly non-random group. This is due to both self-selection (it takes a certain mental capacity to give a sensible answer to a probability question) and due to survey variation (exactly which sets of respondents have been asked which questions has varied over time in the HRS).
Each wave of the HRS contains detailed questions on household assets (both real and financial), household income (separate from capital gains), and capital gains. The survey does not contain any consistent measure of household consumption. In order to test the implication of survival expectations on consumption profiles, I use the HRS data on assets, income and capital gains to infer a measure of consumption for each respondent for each period between survey interviews.
The basis of the calculation is the relationship:
That is, consumption between two measured points in time equals whatever the household took in, in earned income and capital gains, minus the amount that their asset level grew during that period. It is ambiguous in the HRS whether respondents give pre-tax or post-tax income levels and so there is no way to account for income tax. I do, however subtract property taxes from inferred consumption.
First, I use the HRS income data to estimate household income over the period between survey interviews. I divide the study's constructed household income variable-which estimates total household income in the one-year period prior to the interview-by twelve to get an estimated monthly income and then multiply by the number of months between interviews. This procedure will add measurement error to the extent that actual household income during the period between interviews differs from income during the period just prior to the interview. Additionally, all financial variables in the HRS include imputed values which increase the level of measurement error, but also substantially increase the number of data points available. To exclude the imputed values from this analysis would entail dropping a majority of the available data since almost all respondents require imputation on at least some financial variables.
Second, I use the capital gains section of the survey to estimate capital gains between survey interviews. Respondents are asked whether they have put money in to or taken money out from their various assets. This information, combined with the asset values reported in the earlier and later waves, allows for inference of the respondent's capital gains over the period. This is straightforward except that housing capital gains are not well-measured for respondents who buy or sell a house during the period, so those respondents are dropped.
Finally, I calculate respondents' change in assets between the survey interviews by subtracting the later survey-interview household assets variable from the earlier survey-interview household assets variable. I do not include housing assets on the assumption that people-particularly retired people-do not generally monetize housing assets for the sake of consumption. Descriptively, this assumption is probably alright for the period 1992-2002, but may be less so now.
Adding income and capital gains and subtracting asset growth and property taxes and then deflating by the CPI-U yields the measure of consumption in 2002 dollars that is used to test the life-cycle prediction.
Using this strategy I have measures of consumption for the periods between the survey waves 1992 and 1994, 1994 and 1996, 1996 and 1998, 1998 and 2000 and 2000 and 2002. These five sets of consumption data can be used to calculate four cross-sections of log-consumption growth, the statistic of interest in the Euler equation. Table 2 shows summary statistics for the measures of consumption in 2002 dollars and log-consumption growth. Panel A shows consumption measured for all households consumption measured for households composed of singles. Panel B shows log-consumption growth measured for all households and for singles. The values in Panel A can be thought of as approximately 2-year levels of consumption for those households as that is approximately the time between survey interviews. Panel C, most relevant for this study, shows consumption and log-consumption growth for those singles who exist throughout the entire panel period and who have no negative consumption values-these are the respondents the model is estimated for. This group is different from the overall singles group in two major ways. First, consumption levels are much more steady through the period-perhaps because it is always the same group of people. Consumption for each approximately two-year period is always approximately $50-60K. Second, for both of the other groups (all households and all singles) consumption growth is negative for the first three periods and positive in the last. For this group of singles, consumption growth is negative in the first period and positive thereafter. I have no explanation for that difference.
Two elements of this table suggest large measurement error. First, in each year a large proportion of households have negative values for this measure of consumption-typically 11-15%. Because actual consumption cannot be negative, these cases are necessarily mis-measured. The proportion of negative cases is a lower bound on the proportion of mis-measured cases in each year. The large number of negative values also explains why the number of cases is significantly lower in Panel B and Panel C than Panel A-if there is a negative value in either the early period or the late period, then log-consumption growth cannot be measured.
Second is the fact that, for both the whole population and for singles, measured consumption drops substantially for the period 1998-2000 and then rises substantially for the period 2000-2002. This may be due to unreported capital gains appearing in the change in asset level. If a respondent had substantial capital gains in 1998-2000 (as many did), then did not report them as capital gains and did correctly report their total assets, this would result in measured consumption being biased downwards. The reverse is likely for the period 2000-2002. Poorly measured capital gains are probably not restricted to these time periods-they just show up strongly in these periods because asset values fluctuated substantially.
The HRS does provide a few variables that can be used to corroborate my deduced consumption values. In 1996 and 1998 the survey asked each household what their total spending-including all debt payments, utility bills, rent, transportation, entertainment, food, clothes and any other expenses-was in the previous month. Also, in 2002, the survey asked three food consumption questions: how much did the household spend in the past week on all food; how much did it spend having food delivered; and how much did it spend eating out. The left side of table 3 shows mean values for these measures and for deduced consumption values measured on a monthly basis for the same time period. The HRS survey levels of consumption are substantially lower in both cases. Also included are mean values of inferred consumption with negative values removed-this exacerbates the difference between the HRS measure and my measure. It is questionable how accurate a respondent is likely to be in making a fast estimate of monthly spending, so there is no guarantee that the HRS measure is very good. One possible, partial explanation for the large difference between the values reported by respondents and the calculated values is that I have not accounted for income tax. If respondents generally report pre-tax income, then my calculation will count their taxes as consumption. It seems likely that few respondents would include income tax in their response to the 1996 and 1998 HRS consumption question.
The right side of table 3 shows correlation coefficients and respective significance levels between the HRS measures and my inferred levels of consumption for the relevant time periods. For both HRS consumption measures, the correlation is substantially higher when the negative cases are removed from the deduced consumption numbers. This is unsurprising as those cases almost certainly represent particularly egregious cases of measurement error. Furthermore, it is encouraging that the inferred consumption shows such a high correlation with the HRS measures of consumption given the likely presence of substantial measurement error in both. Interestingly, of the food consumption measures in 2002, only the measurement of what a family spends eating at restaurants is significantly correlated with my inferred consumption measure. Also interesting, though I have not shown it, is that these measures of food consumption correlate very little with each other-again probably due to measurement error.
Table 3 shows that despite its weaknesses, my inference about household consumption do match up to a substantial degree with the limited information the HRS survey provides about actual household consumption.
In the Euler equation that provides the hypothesis tested, p represents the agent's subjective assessment of his probability of living to the next period. The HRS provides answers to questions of the form "What is the percent chance that you will live to be 75 or more?" These questions are asked twice in each survey wave with different target ages, although some respondents may only be asked once or not at all. From 1992 to 1998 respondents were asked the questions with 75 as a target age and then with 85 as a target age. In 2000 and 2002, the first question remains the same and the second question has a target age that varies from 80 to 100 in five year increments depending on the age of the respondent (the target for anyone under 70 was 80, for those 70-74 it was 85 and so on).
A response to one of these questions does not imply directly any particular value of the respondent's expected chance of living to any particular date other than the target age. In order to use the survey responses to calculate a value of p (in the life-cycle model the probability of living to the next period; in this analysis the probability of living through the next period of measured consumption) for each respondent, some assumptions are necessary. In the appendix, I describe an algorithm I use to produce values of p from the responses to the subjective survival questions in the survey.
The object I use to fit the three proposed models to the data is the covariance matrix of log-consumption growth. This is shown, along with the corresponding correlation matrix in table 4. Significance levels are shown for the correlation coefficients as well (a value of 1%, e.g., indicates that the correlation is very significant). As can be shown relatively easily, the pure measurement error model implies a correlation matrix with -1/2 on the elements one-removed from the diagonal in the correlation matrix and zero for the other off-diagonal elements. Prima facie, the correlation matrix of the data appears very close to that-with one-off diagonal elements close to -1/2 and highly significant and no other significant correlations.
The results of GMM estimation are presented in table 5. Results are shown both for optimal minimum-distance (OMD) estimation, using the inverse of the variance matrix of the vector of covariance elements as a weighting matrix, and for equal-weighted minimum distance (EWMD) estimation, using the identity matrix as a weighting matrix. In both cases, we cannot reject the measurement error model at standard significance levels and we can reject the other two models. Indeed, the random-walk model and the life-cycle model both fit the data very poorly-producing very high chi-square statistics. (Note in table 5, I use the term p-value to indicate the probability of observing a
There are two other results to note in table 5. First, the value of
that minimized the test statistic for the life-cycle model is zero. This reduces the life-cycle model to the random walk model-indeed the values for
are the same for each model. The models produce different
values in the EWMD case because although they minimize the same statistic, the test statistic for that minimization depends on the first derivative of the vector of covariance elements with respect to the parameter vector. In the life-cycle model this derivative depends on the covariances of p (the subjective survival probabilities). In the random walk model, this derivative is a vector of constants.
Second, OMD and EWMD produce very similar parameter values for the well-fitting model, but very different values for the poorly-fitting model. Abowd and Card note this issue and that is their justification for using both-it serves as another test of how well the model fits.
The primary implication of these results is that this data is unlikely to be of much use in estimating the life-cycle model that is really the object of interest. Because of the procedure used to produce the consumption data, I conjecture that the results of this study indicate a large amount of measurement error in the data, rather than a real consumption process for survey respondents.
As the main goal of this research program is to investigate whatever link may exist between people's stated survival beliefs and their decision-making, it will be necessary either to derive a testable result that does not rely on consumption data or to find a dataset that measures both survival expectations and consumption.

This research examines the intersection of race and gender in shaping opportunity to pursue careers in sciences, technology, engineering and mathematics (STEM) fields. The modest goals of gender and racial/ethnic parity have both been reached between the national undergraduate student population and the US population as a whole, but African Americans, Latinos, and American Indians remain underrepresented in graduate programs (See Fig. 1 and Fig. 2). Although the data show women now overrepresented in degree-granting institutions overall, aggregated data such as these mask both persistent disparities within particular disciplines in the academy, as well as the unique nature of inequality borne of intersecting dimensions of identity.
When one looks at the intersection of race, gender, and academic field, the underrepresentation story looks very different. In STEM fields -- and particularly within the disciplines of physics, chemistry, and engineering -- women of color are so underrepresented that many statistical compilations leave their data out entirely to protect their anonymity. There are sub-disciplines in which women of color have never earned a doctorate. One's equal access to careers in STEM fields neither amounts to nor ensures equality in those fields, but equality includes requires equitable access; thus, improving access is both a precondition to and part of the realization of deeper equality. The purposes of this research, then, are (1) to examine the ways that race and gender intersect to shape access to careers in STEM disciplines and (2) to consider the roles that two programs at the University of Michigan have in increasing this access -- the Undergraduate Research Opportunities Program (UROP) and Women in Science and Engineering (WISE).
Race is a socially constructed way of dividing the human population by phenotypes that has been politically reproduced since the colonial era. Race was created to justify social inequality, and became a marker of inequality, specifically, the subordination of those with darkerwho are not White skin to the domination of those who are Whiteith lighter skin. Race, or "the color line" (DuBois, 1903), has been the basis for both individual prejudice and governmentally sponsored subjugation, including denial of personhood, rights, citizenship, and countless other manifestations of freedom. DuBois called it "the color line," which is to say the separation of some from others on the basis of skin color.
Genders are the socially constructed meanings attached to the sexes, including the functional norms that dominate people's understanding of masculinity and femininity. In the last ten or so years, dialogue on gender has begun to open up to include not only traditionally female and male orientations toward self and the world, but also categories such as transgender (i.e., one's physical sex does not match gender identity/role), pangender (i.e., one cannot be labeled as female or male), and genderqueer (i.e., one takes a "both/and" approach to female and male) identities and roles. Like race, gender has been used as the basis for systematic domination of one portion of the population (in this case, males) over another (females).
From these definitions, it is clear that race and gender, like all socially constructed categories, are dynamic in meaning and laden with values. One's status -- and I believe it is fair to say, one's freedom -- is shaped by the salience of one's race, class, gender, national origin, sexuality, and religion in a particular temporal, geographic, and social context. More importantly that the ways that an individual's race, gender, and other dimensions of identity play out independently, however, are the ways that they intersect to shape experience on an individual level. Poor, uneducated Irish immigrants, for example, may initially have been excluded from the white racial category despite their light skin color and European origins, but within a few generations had assimilated the values of privilege and the discourses and behaviors of domination that permitted them to claim status within the hierarchy (Marable, 2000, p. 245).
The significance of the cell in which the female gender row and the African American race column intersect is in what it means for one's opportunities and experience not only to be Black and to be a woman (as separate dimensions of identity), but also to experience what statisticians would call the "interaction effects" that accrue from being a Black female. That is to say, The effect of identifying with both a subordinatedthe effect of these identity dimensions' intersectionrace and a subordinated gender is greater than the sum of the individual effects that identifying as Black or female has for one's experiences and opportunities. A theoretically robust description of the meaning of intersectionality for Black women is advanced by Patricia Hill Collinss (1998), who introduced the concept of intersectionality to modern social science thinking as a paradigm that can account for social reality's complexity (Collins, 1998).
If one's race is white or gender is male, inIntersectionality functions for some and in some contexts to counterbalance the oppression or discrimination one experienceds as a result of the otherone dimension of identity being subordinated (e.g., being female, but also being white). However, for or interseAfrican American women, intersectionality may functions as a double or "interlocking oppression" (Collins, 1998)., such as being African American and female.
In the context of higher education and the pursuit of scientific careers, to be a Black woman means that one is in a small minority whether or not gender or race is most salient. While African American men and women are both critically underrepresented in the sciences, (Pascarella, Wolniak, Pierson, & Flowers, 2004)Pascarella, Wolniak, Pierson, & Flowers (2004) found that gender was significantly more important in predicting graduate school aspirations for African American women than it was for Hispanic and White students at the end of their third year of college. Controlling for all other influences in their regression equation, African American women had just one tenth of the odds of planning to earn a graduate degree as their male peers.
Add to this structural disadvantage the history of Black women being viewed as objects, not agents, of knowledge (Collins, 1998, p. 97), and it becomes clear that it is not simply a matter of being in a minority among one's peers or colleagues in STEM fields, but of defying normative roles and resisting the force of history.
It is possible within the academy to think of one's discipline as a third dimension of identity with which race and gender intersect. Disciplines not only advance understanding of different types of knowledge, but they do so according to particular epistemologies and ontologies. In sociology, Patricia Hill Collins could advance a theory grounded in her experience as a Black woman due to the discipline's relatively expansive epistemology and recognition of multiple realities. Rhonda Dzakpasu, a Black female physicist at UM whom I know, is very unlikely to have opportunity to integrate her standpoint into the content of her scholarship because of the narrowness of physics' epistemology and emphasis on a single, measurable reality. Much like race and gender, then, disciplines present unique ways of being in, understanding, and valuing the world; thus, for scholars they have power to shape one's experience and view of the world.
Intersect best understood as an individual level theory than in explaining group experiences. Group analyses quickly become too complex to conceptualize, but "groups do not operate as individuals do" (Collins, 1998, p. 208); therefore, they are more difficult to study – another reason that intersectionality is best-suited for individual level analysis.
(Add to the identity matrix in higher education the axis of one's disciplinary identity -- means something different for Black women sociologists than chemical engineers)
It is possible within the academy to think of one's discipline as a third dimension of identity. Disciplines not only advance understanding of different types of knowledge, but they do so according to particular epistemologies, carry with them unique ontologies, and operate under disparate axiologies. In short, much like race and gender, disciplines present unique ways of being in, understanding, and valuing the world; thus, they have power to shape one's experience.
Taking individuals out of the state of nature and making them citizens of a civil society is the expressed purpose of the social contract, which was codified as a state sponsored arrangement through the Constitution. In this arrangement, citizens are inherently equal and inherently free, and the power of the government derives from their will. The individuals whom the Constitution tacitly empowers as free and equal citizens, however, are its authors and signatories: White, property-owning, males. While the gender and race-neutral language of "individuals" embedded in the Constitution gives the appearance of a gender and race-neutral social contract, "unwritten presuppositions about the proper role of women and the relations among the races limit the apparent universality of constitutional language" (Starr, 1992). This contradiction has had the effect of permitting de jure and de facto domination of women and people of color, such that "Modern contractual [domination] both denies and presupposes women's [and non-Whites'] freedom and could not operate without this presupposition" (Pateman, 1988).
Pateman and Mills argue that the creation of the social contract via the Constitution implicitly generated sexual and racial contracts that both formally excluded women and people of color from the social contract and established structures of domination. Mills quotes a 19th century French imperial theorist, who eloquently expresses this idea with regard to race: "The subject people are not and cannot become citizen in the democratic sense of the term...It is necessary, then, to accept as a point of departure the fact that there is a hierarchy of races and civilizations, and that we belong to the superior race and civilization" (quoted in Mills, 1997, p. 25). Add to the collectively imagined hierarchies of race and civilization a hierarchy of sex, and you have the grounds for white supremacy and patriarchy.
Under the sexual and racial contracts, systems of slavery are intrinsically justifiable and serve as logical (i.e., effective and efficient) mechanisms for maintaining patriarchal white supremacy. At this level, the significance of race and gender's intersection viz a viz the social contract becomes clear, for "The body itself...is the foundation for all other levels" (Mills, 1997) of domination. "Exactly what form subordination takes, to what use the body is put or what kind of access is granted, depends on whether a man or woman is constituted as a subordinate" (Pateman, 1988, p. 231). Clearly, black women have been "constituted as subordinates" and the nature of their bodily subjugation has historically been more dehumanizing than that suffered by white women or Black males. For example, female slaves endured physical, sexual, and psychological violation that no wife of a slaveholder would ever be forced to experience. Between the sexual contract's fundamental appropriation of the female body as property and the slave system's formal, lawful possession of Black person's body as property, the Black woman's body was doubly not her own. Even today, not only are Black women subjugated as women and subjugated as Black, but the two are mutually reinforcing. Denigrating a person on the basis of race permits an even deeper disrespect of her body as a woman.
Among scholars who have analyzed the underrepresentation of women and people of color (and, in a few instances, women of color) in science, technology, engineering, and math (STEM) disciplines, a clear distinction can be made between literature on the pipeline to careers in the sciences and critically-oriented literature that acknowledges the ways that women of color are systematically excluded and marginalized in the sciences. Research based on pipeline assumptions views the problem as one of inequitable access to resources and an inadequate supply of scholars from historically oppressed groups, whereas the small, critically-oriented literature views the problem as inherent in the pipeline itself and in the use of the pipeline metaphor. These latter scholars contend that the process of becoming a scientist was developed by and for white males, and that the power structure established in the sciences since the mid 19th century (i.e., in research universities) systematically excludes women and people of color and marginalizes even those who make it through the pipeline. This literature review will critically discuss extant literature on women of color in STEM fields, including an extended analysis of the role that undergraduate research programs play in providing aspiring scholars of color with early socialization to careers in the sciences.
Scholars have very rarely attended specifically to the underrepresentation and experiences of women of color in STEM fields (an assessment of the literature with which Clewell, 1991 and Orn, 2005 concur). Scholarship on female scholars of color in education and the social sciences is abundant in comparison, and from that body of work the entire theory of critical race feminism has emerged as an intellectual response to the experiences of double marginalization that women of color in the academy have endured. Indeed, it is from this work that intersectionality literature originated. In my review of the literature, however, only Orn Ong (2005) employs intersectionality and critical race feminism in exploring underrepresentation of women of color in STEM fields. Just as a body of scholarship exists about women of color in the academy, in general, and in education and social sciences, more specifically, there is a large literature examining the underrepresentation of women in STEM fields; however, the vast majority of this does not explicitly consider the added impact of race, or it does so only tangentially (such as in concert with other individual-level characteristics). Less research has been conducted on the underrepresentation of people of color in science than women of science and here, too, only a small fraction of it analyzes the ways that gender compounds race-based inequities. The paucity of literature on women of color in STEM fields is easily apprehended through a Venn diagram, where each of the circles represents a subject of study and the size of the overlapping areas and of circles themselves represent the relative size of the literature on those subjects:
Fig. 1: The Depicting the relative size of existing literature on the intersection of race and gender in STEM fields
Pipeline scholars have identified barriers to participation in the sciences for women and people of color as separate challenges; however, intersectional analyses are absent from this literature. (Oakes, 1990)Jeanne Oakes (1990) gives a prototypical assessment of this perspective. "Careers in science and technology result from students passing through a long educational 'pipeline.' Doing so successfully involves three critical factors: opportunities to learn science and mathematics, achievement in these subjects, and students' decisions to pursue them" (Oakes, 1990, p. vi). Women tend to leave during senior high school and college, she finds, and they do so because they choose not to pursue scientific careers. Students of color start out with greater interest in science during elementary school, but because of lower achievement, tend to be funneled into remedial tracks presenting limited opportunity for science related experiences (vii). That is, of the three factors that facilitate progress through the pipeline (i.e., opportunity, achievement, and decisions), she finds that women choose not to pursue science careers while people of color are impeded by a combination of achievement and opportunity. Oakes advises, therefore, that educators should intervene at the places we know these groups tend to leak from the STEM career pipeline. She suggests altering the way science and math are taught at the elementary level will spur girls to consider science, and providing additional science exposure and role models in and out of the classroom to retain students of color (viii).
(Clewell & Anderson, 1991)Clewell (1991) reviewed the literature published between 1959 and 1990 on young women of color in the sciences and mathematics between grades four and eight, and found four general barriers impeded potential female scientists of color: student attitudes, student achievement, student selection of courses, and students' career interests and aspirations. Her review showed the tendency of the literature to explain underrepresentation in terms of student deficits. In this same vein, but on the other end of the pipeline, (Baker, 1998) Baker (1998) analyzes persistence in science and engineering Ph.D. programs among women and students of color, finding that when he controlled for "ability" (questionably operationalized in terms of GRE scores and grade point averages), that sex and race differences in degree completion are significantly reduced. He does not indicate the effect for women of color, specifically, but does indicate that a gender gap persists even after controlling for GRE scores and GPA. His analysis conflates ability with test and academic performance, however, and uses measures shown to systematically vary by race and gender due to stereotype threat.
An alternative perspective shows that structural and cultural sources best explain attrition in STEM fields, not individual student adequacy or effort. This view sheds light on the external pressures on women and people of color that ultimately deter and exclude them from careers in the STEM disciplines. For example, (E. Seymour, 1992, 1995)(E. Seymour, 1992, 1995) actively debunks what she calls "attrition myth theories" such as Oakes', which explain students' leaving of STEM majors for the social sciences and humanities to students' lack of ability, effort, or application. The "widespread acceptance of this theory functions to allow schools and departments to focus on weeding out those least fit to survive, and to regard their leaving as a sort of 'natural selection' process" (Seymour, 1992, p. 237). Exploring reasons for switching from science to non-science majors, she finds that "switchers" are just as academically successful, hard-working, and capable of facing the difficult conceptual content of STEM courses, but they less frequently use "situational resources" to overcome the challenges also faced by non-switchers (Seymour, 1992, p. 232). In addition, the structural barriers she cites include the need to work long hours to pay tuition, high school preparation, length of the major, teaching quality, and approachability. Non-switching seniors cited coping strategies as a reason for their persistence in the major. Seymour's perspective is a clear improvement on the deficit-based "attrition myth theories," but it does not explore the impact of race as strongly as it does class and gender-based structural explanations; furthermore, it takes for granted that the pipeline metaphor best describes the reasons for women's underrepresentation, even as it posits alternative explanations for the pipeline's leaks.
A multidimensional critique of pipeline explanations for women's underrepresentation in sciences and engineering is advanced by (Xie & Shauman, 2003) Xie & Shauman (2003) in his widely acclaimed book, Women in Science. The pipeline framework, Xie contends, inherently limits the scope of how we study gender disparities by conceiving of becoming a scientist as an overly linear, stage-based process that "equates noncompliance with the normative career trajectory to 'leaking' or 'dropping out'" (p. 9). Its narrow conceptualization also prevents alternate career trajectories from being seen as legitimate and overlooks the role of one's family, assuming that one's educational and occupational attainment are independent of other major life events. In addition to problems with the ways that the pipeline metaphor unduly restricts conventional wisdom about career processes, most pipeline-oriented research suffers from methodological problems. His comprehensive analyses of the dynamic and multidimensional processes by which women enter the science and engineering labor force are guided by a life course theoretical framework, which is informed by a combination of structural allocation and self-selection theories. While Xie's (2003) research only tangentially studies the impact of race, it makes a critical contribution to the literature on access to STEM careers, the vast majority of which leaves the assumptions of pipeline logic unquestioned.
Orn (2005) also introduces fresh perspective to the research literature through a critical race feminist perspective. Moreover, she is the only scholar I could identify who acknowledges that intersectionality of race and gender in access to science. She finds intersecting identity has largely been ignored in the literature, and thus takes a consciously intersectional perspective in relating the findings of her longitudinal, qualitative study of female students of color in physics. Women of color in physics "sense that their belonging and competence in science are questioned because their bodies do not conform to prevalent images of the "ordinary" white male physicist" (p. 593). To persevere, her participants described two coping strategies: (1) gendered and racial passing, and (2) manipulating others' stereotypes of minority black women in the sciences by seeking to perform with superiority to their white and male peers. Lacking females of color with whom to identify in their own fields, these true pioneers in their discipline persist by avoiding and transcending stereotypes.
To address persisting disparities for women and students of color in the sciences -- particularly at the graduate school level and in the labor force -- a variety of interventions have been developed to retain students in the sciences at all levels. One prominent approach is involvement of college students in faculty-mentored, original research. Among other outcomes, undergraduate researchers have higher baccalaureate attainment rates (Nagda, et al, 1998), greater interest in science careers (Campbell and Skoog, 2004; (Campbell & Skoog, 2004; Kremer & Bringle, 1990)Kremer and Bringle, 1990; Russell, 2007), and higher graduate school enrollment rates (Bauer & Bennett, 2003; Hearn, 1987; Russell, Hancock, & McCullough, 2007)(Bauer and Bennett, 2003; Hearn, 1997; Russell, 2007). The Boyer Commission (1998) thus urged research universities to create opportunities for all students to engage in research as one of ten recommendations for improving undergraduate education. Comparison group analyses, however, show that undergraduate research has a significantly stronger effect on the retention, aspirations and post-baccalaureate choices of students of color and first generation students' than those of white students and students whose parents attained a four-year degree (Hathaway, et al., 2002; Ishiyama, 2002; Nagda, et al., 1998; Russell, 2007).
Interestingly, in their program design, activities, and objectives many undergraduate research programs tend to reflect characteristics of both pipeline logic and the need to fundamentally rethink and restructure the process by which women and people of color approach the sciences. Consistent with pipeline-minded scholarship that emphasizes shoring up individual students otherwise at risk for attrition from the sciences, programs provide support structures to encourage academic success and increase students' self-confidence, for example. However, they do so through early socialization as a research scientist and extended engagement with faculty sponsors who can open doors that might have otherwise been closed – a particularly powerful experience for students from populations that have historically been marginalized in the sciences. Some programs, such as the Meyerhoff Scholars Program and McNair Scholars Program, explicitly emphasize connecting undergraduates with scholarly communities of color and role modeling, with the goal of increasing faculty diversity across the country. Such programs affirm the pipeline principles of access and retention; however, they seek not only to plug the pipeline's leaks, but also to alter the entire process by which scholars pass through it and to prepare students for what to expect on the other end as a scholar of color in the academy.
Although numerous programs exist specifically for students underrepresented in their disciplines based on their race/ ethnicity, gender, and/or socioeconomic status, scholarship on undergraduate research has not approached students' experiences with an eye to intersecting identities. A mixed methods of how female graduate students in the sciences perceived the long-term impact of their undergraduate research experience (Campbell & Skoog, 2004) (Campbell & Skoog, 2004) emphasiz emphasized the mentoring role that the research experience provided -- both from their faculty mentor and graduate students working in their lab. Through their early experience with research, these women also gained self-confidence as a scientist, a solid understanding of the time demands a lab-based science career would require, and had their career aspirations influenced in the direction of scientific research. Of course, such a retrospective methodology among those who have persisted suffers from the drawbacks of selection bias; we should expect relatively positive outcomes of research to be reported when the sample does not include undergraduate research participants who did not persist in pursuing scientific careers after college.
Among the many types of undergraduate research that are proliferating, the Undergraduate Research Opportunities Program (UROP) here at the University of Michigan has a strong national reputation and scholarship on UROP is some of the undergraduate research literature's most methodologically rigorous. UROP was specifically developed to improve retention of students of color by "broker[ing] intellectual relationships between faculty and first-year and sophomore undergraduates through research partnerships" (Nagda, et al., 1998, p. 58). Since most undergraduate research programs serve upperclassmen, UROP's program design is unique, and a rigorous quantitative study confirms the program's success in meeting its primary objective: the attrition rate for African American participants in the 1993-1994 year was approximately half that of African American non-participants in a control group (Nagda, et al., 1997, p. 62). African American program participants demonstrate significantly higher retention outcomes than participants from other racial/ ethnic backgrounds. Although it has since opened participation to students of all racial/ethnic backgrounds, the program leadership retains a particular emphasis on the success of women and students of color in the sciences and has an emerging focus on facilitating graduate education and postgraduate research.
In1 another study of UROP outcomes, (Hathaway, Nagda, & Gregerman, 2002)Hathaway, Nagda, & Gregerman (2002) undertake an inquiry into whether undergraduate research participants and non research participants at the University of Michigan differ in their pursuit of graduate education, use of faculty recommendations for post-baccalaureate opportunities, and continued contact with faculty after graduation. In order to overcome the selection bias problem, researchers matched university alumni who had participated in the Undergraduate Research Opportunities Program (UROP) with non-participants based on major, race/ ethnicity, GPA, and graduation date (N=291). They were surprised to discover that, among non-participants, enough respondents indicated participation in non-UROP undergraduate research that they were able to construct a third comparison group (which they called other research students).
Through Chi squared analyses, they found that UROP and other research student alumni were significantly more likely to have pursued graduate education, to be involved in ongoing research, and to use faculty for recommendations. Overall, UROP and other research students were significantly more likely to pursue graduate education than students of color (African American and Latino) who had not participated in undergraduate research. UROP students were also most likely to pursue doctoral degrees -- including law and medicine. While White and Asian student were most likely to pursue doctoral education across the entire sample, UROP students of color were as likely to pursue doctoral education as White and Asian UROP students.
By offering a sub-environment in which students engage the research university's mission and scientists critical in carrying out that mission, undergraduate research can counteract two institutional factors that deter students from the sciences: lack of faculty contact and a lack of community with other students. Clearly, UROP participation has particular benefit to students of color over and above the benefit of research participation, in general, a finding that motivates the need for deeper understanding. While the article offers a more methodologically sophisticated analysis of research outcomes for students, like most scholarship in this literature it is retrospective. Thus, it cannot provide evidence to show whether or how specific structures of research influence students, nor does it explain how the research experience may stimulate students' consideration of specific graduate degrees and/or careers.
In general, literature on the impact of undergraduate research participation suffers from two methodological problems: selection bias and retrospective analysis. Selection bias is endemic in the literature, as most studies sample only from within a given research program, neglecting to compare learning outcomes with a matched comparison group of non-participants. This practice makes it difficult to ascertain which outcomes are indeed a function of research participation and which are correlates of the predispositions that lead students to participate in research in the first place. Another weakness is a reliance on retrospective analysis. By asking past research participants to reflect back on their experience we learn valuable information about its long term impact. Although such analysis does not necessarily weaken the validity of findings, it does prevent us from understanding how participants make meaning of the research experience while they are in it and how it is that research effects the beneficial outcomes that retrospective analyses have identified. In their methodological design, Hathaway, et al. (2002) overcomes the selection bias problem and is thus able to offer sound empirical support for the claims to research impact that many have posited without sufficient evidence. Applying this type of analysis to a set of longitudinal data would present an even stronger advance in the scholarship on the impact of undergraduate research.2
Like the two studies examining the impact of UROP participation on retention and graduate enrollment, (Barlow & Villarejo, 2004)Barlow & Villarejo (2004) likewise found that undergraduate research increased the likelihood that students of color majoring in a biological science would persist to graduation at University of California-Davis. Moreover, their participants were more likely to continue on to graduate education than graduates from the institution overall. The impact of undergraduate research experience on graduate school enrollment is well established for students in general, but particularly for students of color (Hathaway et al., 2002; Ishiyama, 2002; Russell et al., 2007; Elaine Seymour, Hunter, Laursen, & DeAntoni, 2004)(Hathaway et al., 2002; Ishiyama, 2002; Russell, Hancock, & McCullough, 2007; Elaine Seymour, Hunter, Laursen, & DeAntoni, 2004). However, in a large survey of 4500 undergraduate research participants and 3400 STEM degree recipients who had conducted undergraduate research, no significant differences were found by gender or race in program outcomes. "No formulaic combination of activities optimizes the undergraduate research opportunity, nor should providers structure their programs differently for unique racial/ ethnic minorities or women" (Russell, et al., 2007, p. 549). Rather, the duration of their research experience was most strongly correlated with reporting expectations of earning a Ph.D. Among STEM field researchers, 73% said that the research experience raised their awareness of graduate school and 68% reported an increased interest in a STEM career as a result of their undergraduate research opportunity (Russell, et al., 2007).
Although women of color have made great strides in their educational and occupational attainment in the last forty years, access to careers in STEM fields represents a major frontier for race and gender equity in education. While the pipeline metaphor dominates the literature, the most current research advances alternative explanations for disparities that may empower scholars and practitioners to innovative scholarship and programming that extends opportunity to women scientists, especially those of color. Among these innovations are undergraduate research programs that use concrete research experience to stimulate the educational and career aspirations of students. Of particular relevance to the current analysis are findings from multiple studies showing that students from historically marginalized backgrounds disproportionately benefit from these programs, and that participation almost entirely erases baccalaureate attainment and graduate school enrollment gaps for women of color who major in STEM fields. However, resolving undergraduate retention and graduate school enrollment disparities does not resolve the persistent disparities in graduate degree attainment rates and attainment of both tenure-track faculty positions and tenure, itself.
Based on my interests in the role undergraduate research plays in enhancing the opportunities of students of color and in access to STEM careers more specifically, I elected to conduct two interviews: one with Sandy Gregerman, director of Undergraduate Research Opportunities Program (UROP), and one with Cinda-Sue Davis, director of Women in Science and Engineering (WISE). WISE and UROP were founded explicitly to improve the retention and achievement of women and students of color, respectively, but over the years have opened participation to students regardless of their demographic background. UROP expanded its participation to White students in the late 1990's as undergraduate research became nationally recognized as a beneficial mode and context for learning, and WISE opened to men just last year as a result of Proposal 2. In both cases, though, the program directors continue to derive their passion for the programs from the impact they have in increasing access and achievement. Moreover, both programs are funded through a combination of state general funds and federal grant funds, permitting the continuance of some programming specifically for women and students of color. Both admitted at the outset of our interviews, however, that they have much less expertise regarding the intersection of race and gender as it pertains to STEM career access than with women and students of color, as separate populations. Although women of color come through both programs, little programmatic attention is paid to the meaning of this intersection for their unique experiences.
WISE was founded in the late 1970s when Davis was pursuing her Ph.D. at UM in Biochemistry. Informally run for years by women faculty, they petitioned the central administration in 1980 to have a formal program established. The university's provost at the time, Billy Frye, had two daughters, and Davis believes this was critical to his willingness to have the program established. Early activities emphasized a group of female faculty Davis called "survivors of the system" teaching the next generation of female students and young faculty the rules of the science game, assuming that if they knew these rules that they would succeed. After several years of working in the research faculty as she started a family, Davis took the position of WISE director in 1984 and led the program in efforts to augment their pipeline efforts with deeper structural transformations within the university. This work included engagement with curricular and pedagogical committees and bringing to light the inhospitable climate women faced in many STEM departments on campus.
UROP was created as an outgrowth of the Michigan Mandate. As the number of students of color increased and clear retention disparities were observed between white and non-white students, Psychology professor John Jonides looked to the research literature for academically oriented, non-remedial programming that might support students of color. Drawing on Tinto's theory of academic and social integration viz a viz student retention, they developed UROP. Although it was developed with an eye to retention and research showed its success in accomplishing this (Nagda, 1998), it became clear than an unintended outcome of the program was enhancing its participants' interest in graduate education (Hathaway, et al., 2002) and, because so many UROP projects have a biomedical focus, particularly in medical school.
Davis and Gregerman, who are both white, independently confirmed my assessment that the research literature is weak on the intersection of race and gender. I shared with both of them my assessment that the two primary perspectives in the literature about the access women of color have to STEM careers seem to be grounded in either pipeline or critical race feminist assumptions, and asked them which of these views seems to resonate with their professional experience. Davis grinned and answered, "It depends on the day."
Congruent with the pipeline and critical perspectives, but underacknowledged in the literature, they both emphasized the importance of having a sense of community within the discipline for women of color and, if not structural diversity is not present, a clear sense of inclusiveness in the closest groups with whom students work (e.g., one's research lab). Confirming this, Gregerman spoke about the specific majors to which women of color tend to gravitate, such as biopsychology for the structural diversity and to engineering for the strong community of color. Davis affirmed the strength of the community in engineering, which she says is most clearly manifested through active student organizations such as Movement of Underrepresented Sisters in Engineering and Science (MUSES), Society of Women Engineers, and the National Society of Black Engineers as well as the University's MEPO and WISE programs. MUSES takes a particular interest in reaching out to female middle school students of color through mentoring, tutoring, and campus tours. Activities like these meet the desires women of color have to reach back and for community engagement, even as they pursue highly technical, frequently impersonal, academic fields. Although they indirectly make critical social contributions, engineering and sciences disciplines do not market themselves like social work, education, and psychology as disciplines whose work benefits society, and Davis believes this public image problem deters women of color. Having opportunities through groups such as MUSES then, presents an opportunity to for women to stay involved in social change issues interpersonally, and not just through developing technologies that will improve people's lives. At the same time, MUSES offers a mutually supportive network that enhances their own academic success.
In the post-Proposal 2 environment, groups like MUSES are more important than ever, since the policy has removed important resources that were used in the past to encourage student success. Of course, given the language of Proposal 2 emphasizing race and gender, women of color are once again uniquely caught in the crossfire. Particularly devastating for their efforts to retain current students, Gregerman and Davis say, is the withdrawal of designated scholarships for women of color. However, Davis spoke at length about the new challenges of recruiting students of color having positive effects for her program. With the new university-wide focus on outreach and community engagement, which is work that WISE has been involved in since 1990, "Suddenly, we've become legitimized." Along with other university programs that have received minimal recognition for their decades of outreach to students in the K-12 system, WISE leadership is now at the forefront of the university's outreach strategies. Davis has been a primary consultant in the development of Tony England's Office of Engineering Outreach and Engagement (OE2), and both she and Gregerman are key players in the development of the M-STEM Academy, which will offer a first year living-learning community to students admitted to UM through OE2.
The University of Michigan goes back and forth with Georgia Tech as the institution enrolling the most women in STEM fields, and only HBCU's graduate more African American engineers than UM. Maintaining its national leadership in promoting STEM career access in the post-Proposal 2 environment has created a sense of urgency that Davis believes has and will stimulate new creativity and strategies. It has also generated the political will in the administration to use every mechanism at the university's disposal to foster access. "We're far from where we need to be," Davis concluded, but she and Gregerman remain optimistic about the university's capacity to maintain a real commitment to the success of women of color. Indeed, they remain in this work because they believe well structured learning environments can make a difference that transcends the forces of policy and history.
I am struck in considering Davis and Gregerman's comments by their implicit assumption that institutional commitment to access is the primary force by which said access is achieved. Institutional commitment does not ensure commitment at the level of individual departments/ majors or by individual faculty for their students' success. The institution plays a critical role in recruitment and retention, but individual students' primary contexts for learning and career advancement are within smaller organizations, such as research groups, departments, and disciplines. Thus, in addition to crafting and supporting co-curricular programs that bridge academic and student affairs such as UROP and WISE, the university has an obligation to vigorously cultivate inclusive cultures within departments. After all, what we want women of color to have access to in their careers as scientists and engineers is not simply the job of scientist or engineer. Fundamental among the institutional resources we must push for is access to the freedom to participate in these fields as respected equals. Although I do not know how this is to be achieved, in part because of my own standpoint as a white woman outside the STEM disciplines, there are two qualities to the sort of academic freedom I envision:
In higher education, we traditionally think of academic freedom as a cornerstone of both the academy's reason for being and its modus operandi. It is a powerful value that lures many scholars into academic careers. Thus, while we attend to the disparities by race and gender that continue to exist and work to eradicate them, an important part of that work must be to improve access to the most basic forms of academic freedom. I am convinced that it is no accomplishment for women and people of color to be technically included but marginalized as unequal participants. As a result of this course and this research, therefore, I intend to take my own study of educational inequality to a deeper level by studying the ways that research universities' organization of knowledge inequitably structures opportunity.

American higher education is becoming more diverse now than at any previous time (Zhao, Kuh & Carini, 2005). Behind this trend are international students who constitute an increasingly relevant and important source of diversity on college campuses. During the past decade, American colleges and universities have witnessed a steady increase in international student enrollments. From 1958 to 2005, the population of international students enrolled in U.S. higher education institutions increased from 43,000 to over 560,000 (Open Doors, 2005). Among that population, Asian students comprise the largest proportion, approximately 58 percent, of all international enrollments (Open Doors, 2005).
This increasing population of international students, especially from Asia, in the United States faces special challenges in terms of adaptation to a new living and learning environment at host universities and colleges (Perrucci & Hu, 1995). In addition to academic pressures, many international students tend to experience a variety of adjustment concerns, including language difficulties, insufficient financial resources, social integration, challenges in daily life tasks, homesickness, and role conflicts (Mallinckrodt & Leong, 1992). Interestingly, several studies suggest that students from Asia have more difficulty adjusting to life in the United States than international students from non-Asian countries (Abe, Talbot & Geelhoed, 1998).
These difficulties in adjustment among Asian students may stem from the greater differences in cultural norms, values, and languages between their home countries and the United States, which results in alienating Asian international students from the host society. In order to avoid potential social alienation, developing interpersonal relationships may provide a powerful coping resource to overcome barriers to adjustment in a new environment for these students (Mallinckrodt & Leong, 1992). In addition, based on the appreciation of cultural differences, supportive social interaction with diverse others has the potential to enable Asian international students to develop intercultural maturity by promoting positive changes in their sense of self and relationships with others (King & Baxter Magolda, 2005).
This paper aims to address the importance of international students' building social support networks with diverse others as a coping strategy for adapting to American life. In addition, this paper will examine how effective these social supports are in facilitating their identity development and intercultural maturity. To this end, this study reviews existing resources that identified major difficulties in adjustment encountered by international students while studying in the United States and the impact of such social interaction on their adjustment and further identity growth. Drawing from the review of the literature, this study then interviewed first-year Asian graduate students who were granted Fulbright scholarships to examine the role of social support in their adjustment, identity development, and intercultural maturity.
For most international students, the experiences of studying abroad can be an overwhelming personal and cultural transition. Many researchers have sought to identify specific adjustment problems that international students have experienced. English language proficiency was recognized as one of the major adjustment issues for international students (Mallinckrodt & Leong, 1992; Surdam & Collins, 1984). Studies have found that international students encountered significant problems in communicating with Americans in English especially in academic settings.
Poor language skills were found to be detrimental not only to academic process, but also for social interaction (Huntly, 1993). Research has demonstrated a high correlation between poor English language skills and a lack of interaction with American students and the surrounding community as a whole. By contrast, students who are fluent in English and have American friends tend to have fewer adjustment problems during their stay in the United States (Schram & Lauver, 1988; Surdam & Collins, 1984).
In addition to language challenges, international students were often faced with the need to adjust to a variety of other cultural and social challenges as well, which often entails considerable psychological stress. Fatima (2001) found that many international students experienced significant adjustment-related problems in immersion to a new culture, manifested as anxiety, frustration, loneliness, helplessness, distrust and hostility towards members of the host culture, disruption of one's identity, and loss of self-esteem. These are common symptoms of the early stages of cultural shock (Zhao et al., 2005).
Students from non-Western countries were significantly more likely to experience more of these acculturation stresses than those from Western countries (Surdam & Collins, 1984). Compared to White and Black international students, Asian students were less engaged in active and collaborative learning activities and were less satisfied with their campus environment (Zhao et al., 2005). In the same vein, Abe et al. (1998) found that students from Asian countries experienced more challenges in adjusting to college life than students from non-Asian countries.
Using the University Alienation Scale (UAS), Schram and Lauver (1988) studied the relationship between international students' adaptation and social alienation. The UAS, designed to measure alienation of students, included items assessing powerless, meaninglessness, and social estrangement as aspects of alienation. They found that students from Asia had the highest alienation scores, reflecting minimal social interaction with Americans and other international students. The concept of "cultural distance" (i.e. the degree of difference between home culture and host culture) has been suggested as having considerable explanatory power for these differences in adjustment (Parr, Bradley & Bingi, 1992).
Researchers agree that social support plays an important moderating role in protecting international students exposed to a new environment against the deleterious effects of acculturation stress (Cemalcilar, Falbo & Stapleton, 2005). Since international students tend to lack their familiar support system when they come to a new culture, developing a new social support system may fulfill their desire to achieve a sense of belonging. In particular, for students facing stressful transition to an unfamiliar culture, Mallinckrodt and Leong (1992) found that developing interpersonal relationships in a new environment provided a powerful coping resource to overcome barriers to adjustment. They found that social support from significant others was effective in buffering the impact of life stressors and promoted psychological adjustment. Schram and Lauver (1988) suggested that international students who have a strong social support system tend to adjust to college life in the United States more quickly and effectively than those who do not.
The patterns of seeking social support among international students have been categorized into three potential sources: host nationals, co-nationals, and other international students. Several studies have indicated that more frequent and closer interaction with host nationals is a predictor of successful social and cultural adjustment (Perrucci & Hu, 1995; Schram & Lauver, 1988; Surdam & Collins, 1984). Surdam and Collins (1984), who defined adaptation as the satisfaction of social and academic demands, found that spending more leisure time with Americans was significantly correlated with the adaptation of international students. A Similar study reported that international students who had more interaction with host nationals also felt that they had better cultural, academic, and social adjustment (Heikinheimo & Shute, 1986).
Schram and Lauver (1988) found that social interaction with Americans was the best predictor of alienation, while less interaction was predictive of alienation. Based on this result, they recommended developing an orientation program that would encourage international students to become acquainted with Americans and provide opportunities for such interactions.
Despite the benefits of social interaction with host-nationals, many international students experienced only superficial contact with Americans, and give up hope of establishing deep cross-cultural relationships (Mallinckrodt & Leong, 1992). As was discussed above, English language deficiencies represented a significant barrier to meaningful relationships with host nationals. In particular, some studies have shown that Asian students have reported language difficulties as their major concerns in contrast to students from other regions (Heikinheimo & Shute, 1986).
In addition to language differences, another potential barrier to cross-cultural relationships is perceived differences in core cultural values and communication styles (Lam, 1997). Asian students who were from collectivistic cultures, which highly value interdependent ties with their family and community, perceived American students to be more individualistic and competitive compared to their own culture (Perrucci & Hu, 1995). Furthermore, Asian international students struggled to communicate clearly with American that their indirect communication style may be perceived as an unwillingness to disclose their private lives in contrast to the direct style of Americans (Lam, 1997).
Lack of opportunity to interact with Americans, the level of receptiveness to foreigners displayed by members of the host culture, and American ignorance about their home cultures may also significantly impact international students' social interaction with host nationals (Abe et al., 1998). Particularly, perceived prejudice and discrimination were negatively related to international students' adjustment and acculturation (Yoon & Portman, 2004). To overcome these barriers, Heikinheimo and Shute (1986) recommended that international students need to be receptive to cross-cultural learning, endeavor to learn English, and develop common interests to facilitate interaction with host nationals.
One of the major criticisms of previous research is the implicit assumption that social contact with people from the host culture is the only way to avoid social isolation and the adjustment-related problems, thus ignoring the importance of social support from co-nationals and other international students (Perrucci & Hu, 1995). Considering that most of this research was conducted before mid-1990s' when relatively few international students attended the U.S. institutions, this assumption may have been valid previously due to difficulties in organizing co-national student groups or international student groups as a whole. Currently, however, the numbers of international students, especially from Asia, have grown enough to provide social, cultural and academic assistance independent of the host culture (Perrucci & Hu, 1995). Due to this fact, growing social interaction among international students without involvement from host nationals is also significant in understanding adjustment of international students.
Faced with the challenges of forming ties with host nationals, many international students rely heavily on co-national students for social support. Enclaves of co-nationals provide support in coping with the challenges of adjustment by providing a sense of belonging and psychological comfort (Schram & Lauver, 1988). Due to the likelihood of shared language and values, support from a co-national is qualitatively different from that offered by Americans (Rajput, 1999). Establishing strong relationship with others from a common cultural background can raise self-esteem and consequently affect the adjustment of international students positively (Myles & Cheng, 2003).
Despite its significance as source of support, frequent social interaction with co-national may insulate international students from opportunities to engage with host culture (Lam, 1997). The convenient opportunities to meet fellow nationals who share common language and cultural values may result in reduced social interaction with Americans, thus retarding adjustment to the new culture (Cemalcilar et al., 2005). Due to their greater difficulties in English proficiency and perceived cultural differences, Asian students tend to withdraw from social relationships with Americans and maintain insular mono-cultural social networks with co-nationals (Rajput, 1999).
With few students from their countries, support from enclaves of co-nationals becomes difficult to obtain. In this case, "shared foreignness," sharing the experience of being a stranger in a new culture, encourages these international students to build social ties with other international students (Lam, 1997). In a comparison study of friendship preferences between international students and American students, international students tended to be friend with other international students, over 40 percent of whom had no American close friends (Rajapaksa & Dundes, 2002). In addition, sharing similar cultural background and geographical regional proximity positively influenced the establishment of social relationships with other international students (Rajput, 1999).
The literature reviewed for this study suggest that cross-cultural adjustment of international students is enhanced when they experience relationships with co-nationals, or establish connections with host-nationals and other international students. Unfortunately, Asian students seem to have more difficulty establishing intimate social ties with host nationals. As a result, interaction with co-nationals or other international students often becomes their primary source of social support.
Engaging in multicultural networks with host-nationals and other international students from different cultures, in addition to mono-cultural relationships, enables international students to develop intercultural competences. By interacting with diverse others, international students may come to understand and accept cultural differences.
To promote intercultural maturity in college, King and Baxter Magolda (2005) proposed three comprehensive dimensions of intercultural maturity: cognitive, intrapersonal, and interpersonal maturity. Achievement of intercultural maturity in these three dimensions is demonstrated as follows: 1) cognitive maturity: complex understanding of cultural differences using multiple cultural frames; 2) intrapersonal maturity: the capacity to create an internal self that openly engages challenges to one's view and considers social identities in a global context; and 3) interpersonal maturity: the capacity to engage in meaningful, interdependent relationships with diverse others based on an appreciation of human differences (King & Baxter Magolda, 2005, p.576).
The third dimension, development of interpersonal relationships with diverse others, is especially relevant in the explanation of international students' patterns of social interaction. At the initial level of development, social relationships are grounded in one's primary social identity or affinity groups (King & Baxter Magolda, 2005). This level may be applied to explain international students' initial experiences in the United States when they had few social relationships with host nationals, thus relying primarily on social support from similar co-national groups adapting to a new environment.
At the intermediate level, there is a greater capacity to explore cultural differences and to interact effectively with others, acknowledging the legitimacy of multiple perspectives and realities. The highest level of the interpersonal dimension is achieved by engaging in intercultural interactions based on cultural understanding, enhancing one's identity and role as a member of society (King & Baxter Magolda, 2005). When international students, who move between different cultural perspectives, construct meaningful relationships with host nationals and other international students that are grounded in respect for cultural differences, this mature level of interpersonal competence enables them to promote intercultural maturity.
The interrelationships among the three dimensions of students' development lead to interpersonal competence facilitating cognitive and intrapersonal competence in a multicultural context (King & Baxter Magolda, 2005). Beyond dependent relationships with similar co-nationals, building interpersonal relationships with diverse others will lead to cognitive and identity development of international students. International students' development of interpersonal relationships can be examined through the lens of intercultural maturity model in relation to their cognitive and identity development.
In summary, the literature on international student adjustment, social relationships, and intercultural maturity suggest that interpersonal relationships have a significant impact on cross-cultural adjustment and development of intercultural maturity. Engaging in social support networks with diverse others plays a key role in their acculturation process and obtaining intercultural competence based on the appreciation of cultural differences. Although most literature seems to indicate that interpersonal relationships with host nationals have the most positive effect on cultural adjustment, interaction with co-nationals should not be ignored since they provide a sense of belonging and comfort. In addition, despite a lack of literature, social relationships with other international students may also be important sources of support in adjustment, and provide enough contexts for developing intercultural maturity.
Despite the positive impact of cross-cultural relationships, the literature reviewed in this study consistently suggested that students from Asia tended to encounter more difficulties in cross-cultural adjustment and developing interpersonal relationships, due to the greater perceived cultural distance between Asia and the United States compared to the other Western countries. This indication of particular cross-cultural challenges for Asian students motivated me, an international student from Asia, to identify whether this result is valid and thus applicable to explain the experiences of other Asian international students. Hence, this study attempts to investigate Asian international students' cross-cultural adjustment problems and interpersonal relationships in particular.
Based on the previous review of literature, two data-gathering methods were used: individual interviews and participant observation. During the Fulbright Enrichment Seminar held in Chicago from March 23rd to 26th, 2006, I conducted participatory observations to examine the process of developing social relationships of international students. Under the objective of Building Trust in Diverse Communities, the U.S. Fulbright recipients who had study abroad experiences and international Fulbright recipients from 67 countries participated in this seminar. The participation in this seminar provided a rare opportunity for me to explore the process of developing social relationships among this diverse population. In particular, two outreach programs, a public school visit and a home hospitality event, were designed to encourage international Fulbright recipients to develop meaningful relationships with Americans in Chicago.
The public school visit gave international Fulbright recipients an opportunity to visit Chicago area high schools in small groups to experience a U.S. public high school and speak with local students. This visit allowed international students to share information about their home country and culture with local students and their teachers through exchanging ideas, perspectives, and experiences. The home hospitality program was organized by Chicago supporters of the Fulbright program who invited international Fulbright recipients for a dinner in their homes. This program provided a unique experience for international Fulbright recipients to be involved in typical U.S. family lives.
In addition to participant observation in the seminar, data were collected via individual interviews with six first-year Asian international graduate students who enrolled at U.S. colleges and universities, all of whom were participants of the Fulbright Enrichment Seminar. Interviews were conducted in informal locations during the breaks at the seminar. The interviews included the following questions on the participants' patterns, preference, perceived differences of the relationships, influences on intercultural maturity and changes in identity:
Several themes emerged from the analysis of interviews and observation, including the process of adjustment to U.S. life, motivation to develop social relationships, patterns of building relationships, and influence of social support on identity growth and intercultural maturity.
Recalling their initial sojourn experiences in the United States, most of the interviewed participants expressed some degree of loneliness and social isolation due to leaving from their home culture, loss of familiar customs and behavior, and reduced contact with their family and friends. For them, loneliness was felt most acutely during the first few months of the stay. The fact that all participants were unmarried seemed to influence their feeling of loneliness the more.
Separation from their culture and society motivated the students to develop new social relationships in the host culture. Because they needed to receive both emotional and practical support to adjust to their new environment, such as asking for a ride to an airport or to go grocery shopping, all of the participants sought assistance from a caring, trusting, and reciprocal relationship with new people, regardless of their nationality. They all agreed that they could develop interpersonal relationships with people from different countries if they shared similar personal characteristics, experiences, values, and beliefs.
If they desired, all participants were able to establish some form of social relationships with Americans, co-nationals, and other international students. Unlike the literature that indicated particular challenges for Asian students in making American friends, most of them were not afraid of interacting with Americans. This contrasting tendency stems from their earlier living experiences in the United States before they came to attend graduate schools. Three of the participants had lived in the United States during middle school with their parents. Another two participants had benefited from undergraduate exchange programs which enabled them to study at U.S. institutions for a year. Their unusual earlier experiences in the United States contributed to their ability to communicate with Americans and experience less cultural conflict between their home country and the United States, which further facilitated the development of relationships with Americans. In this sense, these participants' experiences confirm the previous literature (Schram & Lauver, 1988; Surdam & Collins, 1984) that highlights the positive correlation between language fluency and interaction with Americans.
For the participants with strong English proficiency, the Fulbright Enrichment Seminar provided enriching opportunities for developing interpersonal relationships with Americans. Throughout the seminar, interaction with the U.S. Fulbright recipients seemed to be facilitated by their cross-cultural understanding. Due to their past experiences of being international students, the U.S. Fulbright recipients were open-minded to international students and displayed a particular interest in learning about their culture and language. Some U.S. Fulbright recipients who had stayed in Asian countries, such as Thailand and Japan, were quite familiar with Asian culture and religion, and enjoyed asking questions about the participants' countries. In this case, their acceptance of foreign culture seemed to become a strong factor to facilitate cross-cultural relationships.
Furthermore, the home hospitality event facilitated caring and lasting interpersonal relationships between Americans and international Fulbright recipients. By inviting international Fulbright recipients to their home, the American supporters took the initiative to interact with international students, which was critical in building relationships with them. One of the female participants reported that her American host, who was a New Age musician, gave her one of his recent albums as a present. Because he played New-Age music that stems from the Asian spirituality, he enjoyed discussing the Asian cultural values surrounding his music with her. His interest and understanding of Asian culture were likely to build the long and lasting interpersonal relationships between them.
In their visit to public high school, each participant gave an informative presentation about their home country and culture in the classrooms. When the participants showed maps and pictures brought from their own countries, American students and their teacher seemed to be intrigued by the apparent differences, such as the appearance of people, landscape, and geographical distance. During the presentations, however, it was notable to observe the movement of their focus from cultural differences to intercultural similarities between their home and the United States. For example, when one of the participants from India discussed the caste system which divides people into separate social classes, American students, most of whom were African Americans, seemed to have much sympathy for the issue of social stratification. Comparing the caste system to their experiences of racial discrimination in the United States, the American students and the participants shared the thought that all human beings should be valued equally regardless of their social class, race and ethnicity. By exchanging their ideas and perspectives with local students and their teacher, the participants seemed to be inspired by sharing common interest and similar concerns surrounding each country, rather than simply focusing on specific cultural differences.
Through these various trust-building activities, all participants seemed to enjoy developing caring relationships with Americans. The intercultural relationships between Americans and the participants stemmed from mutual interests as well as different, but similar experiences, along with respect for each other's culture. Indeed, these enriching opportunities for interaction with Americans, American acceptance and interest in foreign culture positively influenced establishing meaningful ties with Americans. These findings support the previous literature of Abe et al. (1998), indicating that opportunities for interaction, the level of receptiveness, and ignorance about foreign cultures significantly impact international students' social interaction with host nationals.
In addition to facilitating relationships with Americans, the seminar provided a diverse context to develop cross-cultural friendships among international Fulbright recipients from 67 different countries. In comparison to relationships with Americans, a "shared foreignness" seemed to connect the participants more strongly. Facing challenges in adjusting to a new life seemed to create a sense of connection and solidarity among them. In particular, having common concerns in academic achievement, relationships with their American peers and faculty enhanced the participants' supportive interaction by producing abundant topics to discuss. Some participants who pursued Doctoral degree shared their anxieties on passing qualifying exams and keeping up with American peers in their academic studies.
Most participants showed a slightly higher preference to engage with other international students from regions that were geographically close to their home country. For instance, some participants from southeastern Asia appeared to enjoy more communicating with students from neighboring nations than those from distant regions. Despite their comfort with intercultural diversities, the cultural familiarity surrounding the region still seemed to unconsciously attract them like a long lost friend. As noted by Rajput (1999), similar cultural background and geographical proximity was an important factor that promoted social interaction with other international students.
Similarly, common cultures and languages played a key role in developing interpersonal relationships with co-nationals. Living in a foreign country, co-national friendships provided important opportunities to maintain a sense of belonging and identification with their home culture. Some participants reported that support from co-nationals was particularly helpful when they initially came to the United States without any familiar resources and social relationships. For instance, they received substantial assistance from co-national friends when searching for a new residence and selecting courses. In this sense, their experiences confirm the observation of Schram and Lauver (1988), who noted that relationships with co-nationals support the adjustment process by providing a sense of belonging and comfort. In addition, during the seminar, the participants from the same countries were more likely to be close friends than those from different countries even if they met the first time at the seminar.
For the participants, a social network with co-nationals served as a secure base upon which they were able to build relationships with Americans and other international students. One of the participants reported that the presence of his co-national student organization in his school provided a sense of pride in his own country, which significantly increased his interpersonal competence when communicating with Americans and other internationals. In contrast with the previous literature that suggests potential insulation from the host culture, these participants' engagement with similar co-nationals further facilitated interaction with diverse others. In this sense, the social ties with co-national were not a barrier, but a critical foundation in developing positive relationships with the host culture for successful cross-cultural adjustment.
During their stay in the United States, the participants appeared to achieve considerable personal identity growth by engaging in multicultural interpersonal relationships. All participants reported enhanced self-efficacy in managing relationships with diverse others. This was boosted by their increased sense of self by mastering the intricacies of a different culture and language. One of the participants reported that she could raise a sense of intercultural competence by coping with everyday cross-cultural challenges and engaging with relationships with diverse people from diverse countries.
In addition, a greater sense of appreciation for their cultural heritage, as well as for the host culture led to more balanced perspectives of both cultures. This made possible for them to integrate multiple perspectives into their identity development. One of the participants from Cambodia reported that he once regarded the United States as a far superior society to his home country. His perspective was changed when he wore his traditional costume on the Halloween party, where his American friends showed much interest in his costume and culture. He felt like wearing the costume and representing his own country and culture, which contributed to developing his identity with balanced perspectives on his home country and the United States.
The most notable outcome from the interviewees' diverse interpersonal relationships was to realize that people from different cultures were more similar to them than different, as manifested by interaction with American Fulbright recipients and programs such as the public school visit and the home hospitality event during the seminar. For most participants, interaction with Americans and other internationals throughout the seminar enabled them to acknowledge fundamental universal human characteristics across cultures. Such transformative experiences would be an important sign of intercultural maturity. Indeed, their participation in the enrichment seminar significantly contributed to their personal growth in their identity and intercultural competence.
Observations of the Fulbright seminar and focused interviews with six Asian international graduate students provided insights into the key role of social support in cross-cultural adjustment and intercultural maturity. Despite the relatively small number of interviewees, each of them offered valuable perspectives for examining the interpersonal relationships with diverse others and subsequent changes in their identity and intercultural competence.
By being separated from their home countries, participants suffered from loss of familiar systems and social relationships. This motivated them to seek new social relationships with new people. Due to the interviewees' high English proficiency and earlier living experiences in the United States, they found it easy to build social relationships with Americans. In particular, their interaction with Americans was facilitated by a variety of multicultural trust-building programs during the seminar. On the basis of mutual understanding and interests, the participants developed caring relationships with Americans who showed sincere hospitability and acceptance toward them. Because Americans with multicultural interests are effective sources of support for international students, the findings of this study highlight the importance of providing opportunities for long-term contact with Americans, such as the Fulbright Enrichment seminar.
However, at the same time, the participants' earlier experiences in the United States and higher English proficiency separated them from the other Asian international students whose experiences and language skills may be different from them. Because the participants were not the general representatives of Asian international students in the United States, the opportunities for such interpersonal relationships with Americans may have disproportionately benefited these "extraordinary" students compared to the "ordinary" international students. This limits the findings of this study in generalizing to other Asian students whose experiences are similar to the students described in the literature.
The diverse members of the seminar from all over the world provided a rare opportunity for them to engage in friendships with other international students because the shared foreignness and common concerns created a sense of connection. Particularly, relationships with co-regional students were attractive in that they gave the participants a sense of cultural familiarity and comfort in a new environment. Seeking similarity encouraged them to develop interpersonal relationships with co-nationals, which was especially helpful during the initial adjustment period. For the participants, connection with co-nationals served as a secure ground in building relationships with culturally different others, which provided an impetus to overcome cross-cultural barriers and develop intercultural maturity.
Despite the limitation of the study, these sources of social support that benefited the participants in the United States significantly contributed to increasing understanding of their identity development and intercultural maturity in a diverse context. With better appreciation for different cultures, they could strengthen their sense of self with increased self-efficacy in maintaining relationships with diverse others. Beyond the cultural differences, they discovered commonalities across human cultures. This was the most encouraging finding of this study.

The design features of a speedometer are not trivial details. Everyday, millions of people rely on speedometer details when driving automobiles. A few small defects in the speedometer design could distract the driver enough time to cause an accident and potentially cost many people their lives. Consequently, studying the effects of speedometer design parameters on speedometer reading time and accuracy is very important. The design parameters examined in the experiment include character color, location on the instrument panel, character size (strokewidth and height), radius and numbering scale. Each parameter was examined in order to explore the possible combinations of design features such that the speedometer average reading time would be low as well as having high reading accuracy. These design parameters, however, may have a different effect on reading time and accuracy depending on the task condition. Two task conditions were examined in this experiment. The first speedometer task condition was to find the exact speed while the second was to verify if the vehicle was above or below the speed limit. Therefore, the two objectives of the experiment were:
 -- To find the relationship between speedometer design parameters, task condition and task performance (reading time and accuracy).
 -- To find which speedometer is best for each given task condition and why.
The experiment was conducted using 13 test participants. Appendix A displays the relevant information for each test participant. All the test subjects were volunteers and students of the University of Michigan. There were many individual differences between the test participants that were considered relevant to the experiment and will be analyzed in later sections of this report. These differences include participant vision, handedness, sex and viewing distance. All other factors were considered negligible. For example, test participant age was considered to be negligible due to the small range of values (18 to 21).
The experiment used a simple projection and timing system to collect the data. This system included the following equipment:
The DRCU, PSCU, and response boxes were all custom made at the University of Michigan. This system worked such that the software in the computer controlled, directly or indirectly, all the elements of the system. The software was called RT334 and written by Prof. Chuck Wooley of the University of Michigan. The system projected various images of speedometers for an exact determined amount of time while being able to store data that was collected at the response boxes. The images were projected on a large projection screen in the front of the room. Two TVs were used to show summary data to the test participants in between testing blocks. Figure 1 displays the system's electronic connections.
Figure 1 displays how the computer, directly or indirectly, controlled the entire system.
Four unique speedometers were used in the experiment. Table gives a description of the significant differences for each speedometer. It is important to note that the measurement values are for the size of the characteristics when projected on the screen, not the actual size.
One characteristic of speedometer 4 that is not apparent from Table 1 is that the first number on the speedometer after 0 mph was 15 mph. From then on a scale of 10 mph was used. All the speedometers were circular in shape. The pointing needles of the different speedometers had negligible differences. Figure 2 displays the instrument control panel layout, common to all the speedometers, shown on the projection screen (without negligible and non-effecting details).
The speedometer was found in either of the large, inner circles in Figure 1. The other circles represent other gauges for RPM, temperature, oil level, etc. Other test materials and equipment used in the experiment include a standard tape measure, chairs, and tables.
Each experiment began with the 13 test participants seated in two rows, each with their own response pad directly in front of them on a standard classroom table. The middle of the first row was 10 feet and 5 inches from the projection screen and had 8 participants. The second row, containing 5 participants, was 15 feet from the projection screen. The participants in the second row were sitting in slightly higher (approx. 6 inches) chairs than the first row. Figure 3 depicts the testing room layout.
Each number on figure 3 represents the respective test participant's seat. Both televisions projected identical information, which was easily visible for every test participant.
The experiment was run in blocks of trials. Each trial began when the shutter opened revealing an image of a speedometer. This is also when the clock started for the trial. The participants could then begin to enter a response by pushing a button on the response box in front of them. After pushing the button, a signal would be sent indirectly to the computer. The time after the shutter opened until the response button was pushed was stored in the computer for each participant. The shutter would then close, completing the exposure duration. The participant could enter a response after the shutter closed, but only until the maximum response time for that trial. Only the first response the test participant gave was recorded. If the participant had responded with an incorrect response (error), the time as well as the fact that it was incorrect was recorded. After the maximum response time, data collection ended and the projector advanced. If the participant gave no response, the maximum response time was recorded for that participant as well as the fact that the participant did not respond (miss). The time in between the end of data collection and the shutter opening again was called the intertrial interval and was common for all trials.
Nine blocks, each with either 16 or 32 trials were conducted. In all the blocks the speedometers read 50, 55, 60 or 65 mph. With blocks 1 through 6, each participant pressed one of the four buttons on the response box that had been assigned a respective speedometer reading (50, 55, 60 or 65 mph). In blocks 7 through 9, the participants pressed 1 of the 2 buttons that had been assigned "speeding" and "not speeding". The "speed limit" was considered to be 55 mph. In both protocols, the participants were instructed to assign one finger for each button. Also, before every block the test participants were given the instruction to "be as fast and as accurate as possible". The first two blocks were practice in order to familiarize the test participants with the routine and system. Block 7 was also practice because of the new response protocol. The practice blocks had one large difference from the testing blocks. In the practice blocks no speedometers were shown, only 3 boxes, each with a word in them. Only one box actually had a correct spelling of fifty, fifty-five, sixty or sixty-five. The one correctly spelled speed was viewed as the speedometer reading for the trial. This difference was to make sure that the test participants did not get any practice at looking at any speedometer before they were tested on it. Also, between each block, the test participants were shown their mean response time and error count of the previous block on the 2 televisions in the front of the room.
The speedometers were grouped into 2 groups. Speedometers 1 and 2 were group A and speedometers 3 and 4 were group B. Only one group was shown per block of trials. Within each block, the 2 speedometers of the group as well as the reading on them would randomly change from trial to trial. Table 2 displays each block with its respective parameters.
The first step in analyzing the data was look at the summary data of each block with respect to each speedometer. Table 3 displays each speedometer's average response time for each block in which the speedometer was shown.
Table 4 displays the average error percentage rate and average miss percentage rate for each speedometer.
Comparing blocks 3 and 5 (the blocks with prolonged speedometer exposure duration), patterns become apparent. Speedometer 2 had the lowest average response time and the lowest error and miss percentage rates. Speedometer 1 was very close in average response time, but not as close in error rate. Speedometers 3 and 4 seem to be stratified from speedometers 1 and 2 in response time. One possible explanation for these differences can be in the overall size of speedometers 1 and 2 versus 3 and 4. Speedometer set A averaged much larger character strokewidth, character height, and speedometer radius than speedometer set B, accounting for the large difference in task performances.
Comparing blocks 4 and 6 (the blocks with shortened speedometer exposure duration), similar patterns become apparent. Overall, the average response times were all reduced from blocks 3 and 5. Also, as with blocks 3 and 5, speedometer set A is stratified from speedometer set B in that it has much lower average response times. Overall, average error and miss percentage rates were much higher than in blocks 3 and 5 due to the shortened exposure time for the blocks. This shows that as response time decreased, error and miss percentage rate increased. The error rates for all the speedometers were about equal, however, the average miss percentage rates for set B were double that of set A. This can also be attributed to the overall size difference of the speedometer parameters.
When the task condition changed to whether or not the car was "speeding", the patterns remained the same. Speedometer set A had lower average response times than set B, as well as having lower average error rate percentages. Overall speedometer response times as well as error and miss percentage rates did decrease due to the lower number of response choices.
The overall size increase of many speedometer design parameters between speedometer set A to set B seem to have accounted for the overall lower average response times and lower average error and miss percentage rates. However, other design differences, such as location of the speedometer on the instrument display panel, did not have such influential effects on the data. Comparing speedometers 1 and 4 (speedometer on left side) to speedometers 2 and 3 (speedometer on right side) only one small pattern was seen. Speedometers 2 and 3 had, compared to their respective speedometer set, slightly lower average response times than the other speedometer in the set. Whether or not this small difference in average response time can be attributed to the location of the speedometers on the instrument display panel was not possible.
Other major differences in design features within speedometer set B also seemed to have little effect on the data. Speedometer 4 had a different character color, location, numbering scale and character height than speedometer 3. However, the differences did not produce much stratification in the data. Speedometer 4 had slightly lower average error and miss percentage rates than speedometer 3 while speedometer 3 had slightly lower average response times than speedometer 4. From prior established patterns, the speedometer with larger character height (speedometer 4 in this case) should have had lower average response times and error and miss percentage rates. Although speedometer 4 did have lower error and miss percentage rates, it did not have lower average response times. Therefore, it is possible that the combined differences in speedometer design between speedometers 3 and 4 caused speedometer 4 to be slightly less legible than speedometer 3.
Speedometers 1 and 2 differed in location and radius size. However, the speedometers differed only slightly in average response times and error and miss percentage rates. Speedometer 2 consistently had the lowest average response time as well as having the lower average error and miss percentage rates (except in one instance). The effect of location, as discussed earlier, was not clear. However, the larger radius of speedometer 2 could of caused the small differences in the data. This inference continues the pattern concerning the size of speedometer parameters.
It is necessary to examine the effects of the test participant differences in order to find any discrepancies in the data. The relevant differences examined were sex, handedness, visibility (defined as the need of correctional lenses vs. no need), and distance from the projection screen. The first step in evaluating the effect of the test participant differences was to compare how each of the 4 modeled differences affected the average response times and total incorrect (errors and misses) responses. The discrepancies in average response times for most of the test participant differences were very small (less than 0.060 s). The same result occurred for the average incorrect responses. The sample size of test participants was also small, making almost all inferences impossible. However, the one individual difference that did stick out as possibly meaningful was sex. The average response times for males were, on average, 0.111 s faster than for females. Also, the average total incorrect responses for the males was 6.3 less than for females. One explanation for this difference can be attributed to the test sequence. Due to the 2 televisions, all the participants could see each other's average response times after every block, creating a small competition in which males are commonly known to be more aggressive and competitive. This would explain the reason for the 0.111 s lower male average response time.
In order to find the best speedometer of the 4 examined, it is required to first define which task condition is being used. For the task condition of reading the exact speed (4 choices), speedometer 2 was clearly the optimal speedometer. It consistently had the lowest average response time and average miss percentage rate. It had the lowest average error rate percentage one of the two 4 choice blocks.
For the task condition of "speeding" versus "non-speeding" (2 choices), the decision was not as simple. Although speedometer 2 had the lowest average response time, it had 6 errors. Speedometer 1 had a slightly larger average response time with 4 errors. However, the task performance measurement that becomes more important in the selection criteria for this task condition should be the average response time. This is because the most important task while driving is to have your eyes on the road. Whether you are mistaken in your knowledge of speeding or not speeding is generally not as important to your safety as the time you look away from the road. Therefore, the optimal speedometer for this task condition was also speedometer 2.
The relationships between speedometer design parameters, task condition and task performance were not obvious or clear given the collected data. However, some inferences can be drawn. For example, the larger the character height, character strokewidth and radius, no matter what the task condition, the more the task performance improved. This was shown in the stratification between speedometer sets A and B for all task conditions in the task performance measurements. This pattern was also visible when speedometers 1 and 2 were compared based on their radius size. Conclusions about the effects of location, character color and numbering scale were not possible.
Using the relationships found above as well as the data collected, the optimal speedometers for each task condition were found. Using low response time and incorrect response rate as the criteria, speedometer 2 was selected the optimal speedometer for both task conditions. One interesting point was the difference in importance between response time and incorrect response rate. This was concluded due to the natural consequence of not having your eyes on the road. The safety issues concerning the amount of time your eyes are off the road are generally more severe than incorrect knowledge of whether you are speeding.
Revisions in the experiment could prove to be very helpful. For example, due to the poor choice of speedometers, it was very difficult to assign the causes of differences in task performance to attributes of the speedometers. Speedometers with similar characteristics and one large difference should have been chosen for the experiment such that the process of assigning the causes of differences in the data could have been clearer. Also, a larger sample size of test participants as well as testing blocks should have been used to increase confidence in the inferences made. With these changes, the experiment could be much more effective in answering the objectives stated.

Anthropometry is a technique of measuring the human body in terms of dimensions, proportions, and ratios. Within the field of ergonomics, anthropometric data is extremely important in product design. Specifically, characteristics such as body size, strength, height, weight and range of motion are all considered when a product needs to be tailored to a particular user group. If no data is obtained or measurements are inaccurate, products may be designed in such a way that they are either unsafe or unpractical, which most likely would lead to a decrease in their sales. As a result, it is in the best interest of all companies to consider the appropriate anthropometric measurements when developing new products.
This report examines some of the human characteristics that are important when designing a stress ball for college students. In particular, 3 questions were examined:
One group of 12 participants, all of whom are currently enrolled in Ergonomics Laboratory 334 Section 003, was used for this study. Due to their class enrollment, all participants were volunteers and not-paid for their time.
Making up this group of participants were 8 women and 4 men who ranged in age from 20 to 22. Of the 11 right-handed and 1 left-handed participants, none claimed to have any hand injuries.
The test stimuli in this study were the protocols, or instructions for collecting data, that were used at each of the measurement stations. Since each participant had their height, weight, comfortable grip strength and maximum grip strength measured in this study, 4 different protocols were used. Each of the 4 protocols, plus a general introduction to the study, was written by the experimenter and all protocols were tested so they were as methodologically rigorous as possible. Summaries of each of the 4 protocols and the study introduction can be found in the Test Activities and Their Sequence section of this report.
The test equipment for this study included a Technasonic Weight Talker II scale, used to measure the weight of each participant, a model 0955 Invicta Plastics Limited standing anthropometer, used to measure the height of each participant and a model 5001 Grip A Takei Physical Fitness Test that was used to measure both the comfortable and maximum grip strength of each participant. In addition, a pencil and paper were used to record all participant data. No test software was used in this study.
On October 25, 2005 from approximately 6:02 PM to 6:50 PM anthropometric measurements were taken on 12 participants in room G699 IOE at the University of Michigan Ann Arbor.
The study began with the experimenter giving a brief introduction of the study to each participant. After attaining the participant's first name and thanking them for their time, the participant was told the study was being conducted by IOE Balls Models, a company that produces stress balls. They were also told that by participating in the study, which consisted of taking 4 anthropometric measurements, they would be helping IOE Balls Models to design the most effective stress ball possible. Finally, after the participant was told the study was risk free, the participant was given the chance to express their questions and concerns to the experimenter. It is important to note that throughout this introduction process, the experiment maintained a positive demeanor.
The first station the participant visited was the weight station, which held the Technasonic scale. After the scale was turned on by the experimenter, the participant was asked to "remove their shoes and any excess clothing". Next, after the experimenter tapped the start button on the scale with their foot, the participant was instructed to "step on the scale and to remain standing up straight and as still as possible until the scale verbally acknowledged their weight". After the experimenter recorded the weight value, the participant was told they could "step off the scale and collect their personal items".
The second station the participant visited was the height station, which held the Invicta Plastics standing anthropometer. After the green measurement disk was placed at the maximum height of 207 cm by the experimenter and the participant had again taken off their shoes, the participant was asked to "stand with their feet on the green platform, as straight and as still as possible, with their back against the anthropometer". The experimenter then lowered the green measuring disk so it touched the top of the participants head and recorded the height value to the closest tenth of a centimeter. Upon completion of this task, the participant collected their personal items.
Next, the participant visited the grip strength station which was equipped with a Takei grip dynamometer to measure their comfortable and maximum grip strength. Comfortable grip was measured first by means of 3 trials. For each trial, after the experimenter had reset the dynamometer to a value of 0 by turning the center knob counterclockwise, the participant was told to grip the device with their "writing hand" to hold the dynamometer "downward and approximately 6 in away from the body ". Next, the participant was told to "slowly squeeze the device until slight pressure was felt in the palm of their hand and at that point to release their grip and hand the device to the experimenter". The experimenter then recorded the strength value to the nearest quarter of a kg and reset the device for the next trial. After the 3 comfortable grip strength trails were completed, the participant's maximum grip strength was measured. This was done by having the participant hold the device in the same manner, but instead instructing them to "exert full force on the device by squeezing their hands together (as if they were juicing a lemon)". After allowing the participant to exert force for 3 seconds, which was thought to allow enough time for build up to maximum force, the dynamometer was returned to the experimenter who recorded the maximum strength value to the nearest quarter of a kg.
Finally, before the participant left the study site, the participant was asked to provide their age and that value, along with the sex of the participant was recorded by the experimenter.
Representative height, weight, comfortable and maximum grip strength values for the participants in terms of means and standard deviations are shown in Table 1. Values are shown in this format, because if one knows the mean and standard deviation of a characteristic, they can calculate percentiles values by using the appropriate z-score, which is important in product design. Table 1 also displays the means and standard deviations for the 4 measured characteristics for men and women participants separately. It is important to break the population into men and women subgroups, because in this case, women made up 2/3 of all participants which could skew the overall participant population statistics.
As can be seen from the table, the total participant population has a mean height of 165.6 cm, a mean weight of 68 kg, a mean comfortable grip strength of 10.25 kg and a mean maximum grip strength of 31.25 kg.
In terms of the differences between men and women, the 2 groups differ in each of the measurement categories. Looking at the mean values, it can be seen that the men participants in the study are an average of 9.6 cm taller and .5 kg heavier than the women participants. These findings support the idea that the average man is both taller and heavier than the average woman. Men participants also had a comfortable grip strength 1.25 kg larger than the women and had a maximum grip strength 8.25 kg larger than the women. Again, this data coincides with the idea that the average man is stronger than the average woman.
In order to determine the relationships between the anthropometric measures that were examined, the coefficient of determination was calculated for each possible measurement pair (6 total pairs). The coefficient of determination, or an R squared value, is an indicator of whether or not there is a good relationship between two variables. An R squared value of 1 indicates a perfect relationship while an R squared value of 0 indicates no relationship. Table 2 shows the R squared values for each measurement pair based off the data from all 12 participants.
As can be seen from above, the anthropometric measures examined in this study have very weak relationships with each other. The 2 strongest relationships are those between height and maximum grip strength with an R squared value of .46 and weight and maximum grip strength with an R squared value of .49. Since an R squared value of 1 means that two variables have a perfect relationship, these two relationships are only mediocre. In addition, the 2 weakest relationships are those between comfortable grip strength and maximum grip strength with an R squared value of .06 and weight and comfortable grip strength with an R squared value of .05. It is important to note that for this analysis, since the number of participants was small, the R squared values for the men and women subgroups were not examined. Furthermore, since all participants fell into an age range of only 2 years, age was not considered when determining the relationships between anthropometric measures.
In order to determine recommended dimensions for the stress ball, maximum and comfortable grip strengths of the participant population must be considered as well as hand breadth (which is the distance across the palm of a hand). Since a stress ball should be designed to comfortably fit in the hand of the maximum number of users, the 5th percentile woman measurement will be used for hand breadth. Since this particular measurement was not obtained during the study, the measurement was obtained from the Eastman Kodak Company and was found to be 6.8 cm. With respect to grip strength, the stress ball should be designed so the 50th percentile individual can compress the ball completely and the 5th percentile woman can grip it comfortably. By using the mean and standard deviation values found above, as well as the percentile formula (percentile = mean + z*standard deviation), the 50th percentile value for maximum grip strength was found to be 31.25 kg and the 5th percentile woman comfortable grip strength was found to be 3.17 kg. Thus, in order to appeal the largest number of users, the stress ball should be designed so that it is 6.8 cm in diameter and made of a material that starts to compress at 3.17 kg of strength but does not completely compress until 31.25 kg of strength have been applied.
After collecting anthropometric data from 12 participants, the data was analyzed in order to determine representative height, weight, comfortable and maximum grip strength measurements for young adults. From the data attained the mean values of these 4 measurement categories were 165.6 cm, 68 kg, 10.25 kg and 31.25 kg, respectively. Since these values are based of a small population, they most likely would change dramatically if more participants were measured.
Since it is important in product design to know the measurement differences between men and women users, participant data was separated by sex and reanalyzed. As can be expected, on average the men participants were taller, weighed more, and had higher comfortable and maximum grip strengths than women participants. However, since the number of participants was low, the difference between the values was reasonably small. In regards to weight, for example, the average man participant weighed only .5 kg more than the average woman, which is likely to be much less than the true difference in weight between men and women over a larger population.
In order to determine what kind of relationships existed between the tested anthropometric measures, R squared values were calculated for each of the measurement pairs. Since the 6 obtained R squared values ranged from .05-.49, all well below the perfect R squared value of 1, it can be stated that no strong relationships existed between any of the 4 measures. Since this conclusion does not mirror ideal data where values such as height and weight would have a higher R squared value, the error can be attributed to the fact that the number of participants was small and the data was likely skewed. Thus, if a larger participant population was tested, more complex relationships between height, weight, comfortable grip strength and maximum grip strength would likely be discovered.
In order make recommendations for the dimensions of the stress ball, both comfortable and maximum grip strength of the participant population had to be considered as well as hand size. Since this study did not measure the palm size of the participants, this data was obtained from an outside source. Next, the percentiles for which the different stress ball dimensions would be designed for were logically determined, and in this case were 5th percentile women for hand breadth, 5th percentile women for comfortable grip strength and 50th percentile men/women for maximum grip strength. These percentile values were calculated using the percentile formula and the known population means and standard deviations. The obtained values lead to the recommendation that the stress ball that have a diameter of 6.8 cm and be made of a material that starts to compress at 3.17 kg of strength but does not completely compress until 31.25 kg of strength had been applied. It should be noted that the recommended dimensions would generate a stress ball that would be best suited for the study participants, but not necessarily for the larger population. In order to determine those dimensions, a larger scale analysis would need to be completed.
The results of this study might be challenged on the basis that the population tested was small and not random (all participants enrolled in the same course). Also, the fact that women and men were not equal in number could have greatly skewed the overall participant population statistics and thus the stress ball design dimensions. In addition, the protocols for each of the stations could have lead to error, since participants may have interpreted the instructions differently. Finally, errors in manually reading the standing anthropometer and grip dynamometer, experimenter inconsistencies between participants, the fact the scale was not calibrated or checked for accuracy and individual participant differences could have lead to error as well.
In conclusion, anthropometric measurements are an important factor to consider when designing a product. By knowing mean values for the body size, strength, height, weight and range of motion of a target user group, companies can tailor product dimensions to best fit the group. Unfortunately, if these measurements are not taken into account, products may become either unsafe or hard to use, which of which are undesirable to consumers. As a result, in order to keep their products safe and useful for the broadest range of users, it is important for companies to design products using appropriate anthropometric data measures.

Rubberland Inc. is a producer of rubber liquid. They have been getting negative feedback from their customers who use the liquid rubber for the production of molded parts. Rubberland's customers lose, on average, thirty minutes per day of production. This time is lost in their mold process because Rubberland's customers have to adjust the temperature settings for every new-shipped batch of liquid rubber material. On average Rubberland's customers, lose $930 for each hour of lost production.
Process Engineers, organized by Rubberland management, found that the failures from their customers internal mod inspection process are due to parts not curing properly, scorch, parts being under filled, mold flash and flowlines. They also collected a variety of sample data that included 6 input variables and 7 output variables. Our six sigma team plans to identify which output variables are most important and then identify the key input variables that are most significant. We will then analyze the significant variables to provide recommendations for the process. These recommendations will reduce the problem with Rubberland's customers, thus reducing the amount of time and money lost to adjusting the temperature setting.
To find the current state of the process, we created a Pareto Chart to identify major failures. We also conducted a process capability test and measured correlation between variables.
Negative feedback from customers is a big concern at Rubberland. Our team has thus obtained a summary of the failures from the customer's internal mold inspection process. These failures are displayed in a Pareto Chart in Figure 1 below. The concern with the greatest frequency is "Parts not Curing Properly," and it accounts for 70.1% of all failures.
After brainstorming potential issues of the process, our team evaluated the performance of the process by collecting sample data and recording several material variables and process settings. We then conducted process capability tests on each of the output variables (Tensile strength, Elongation %, Type B tear strength, T10 time, VMAX, and CIR). Below in table 1, the Defects Per Million (DPM) values for each of the output variables are displayed. Table 1 shows that the largest DPM is for CIR.
The pp for CIR is greater than the ppk, and ppk approximately equals cpk, so there is a mean problem and excessive common cause variation. The ppk and cpk values for Tears are all about equal with values of around 0.48. Because the ppk and cpk values are equal, this is most likely due to a standard deviation problem.
After measuring process capability, we measured the correlation between the input variables and the correlation between the output variables. Figure 2 below shows the correlation of the input variables. Variables with p-values less than 0.05 are considered significant. Three variables, supplier, catwt, and catdisp had correlated p-values less than 0.05 and these are highlighted in Figure 2.
Figure 2 show that the supplier and the catalyst weight are negatively correlated. In addition, the supplier and catalyst dispensed are also negatively correlated. Lastly, Figure 2 shows that catalyst weight and catalyst dispensed are positivity correlated with a value of .978.
After observing the current state of the process by performing creating a Pareto chart, performing a process capability analysis, and measuring the correlation of input variables, we performed further analysis to try to determine the reason for failures in the mold inspection process.
The first step of our analysis was to develop a cause and effect diagram. Figure 3 shows the factors that cause customer dissatisfaction.
The five categories shown in the cause and effect diagram are material, personal, environment, methods and machines. As seen in Figure 3, the categories with the most causes for customer dissatisfaction are Material, Personal, and Machines.
After analyzing causes for customer dissatisfaction, we examined the input and output variables that lead to this dissatisfaction. To identify output variables with the most problems and inputs with the most significance, we ran regression models. We ran 7 different models, with each model having the 6 input variables and one of the 7 output variables. For each regression model, if a term was not significant (i.e. its p-value was not less than 0.05), we omitted it from the model and re-ran the regression. For the tensile strength, Tear "B1", Tear "B2", and VMAX we did not get any significant values. The p-values and regression equations for CIR, T10 time, and Elongation % are listed in the Table 2 below.
In these equations, the supplier is a binary variable. Supplier equals 1 when from supplier A and 0 when from supplier B. The linear regression equation for CIR shows that the input variables, supplier and catalyst weight (catwt), affect the output variable CIR. Similarly, for T10, the supplier and catwt variables affect its output, and for Elong, the crosslinker variable affects its output. Even though Elong and T10 had significant input variables, they did not have a large DPM and therefore additional analysis was not conducted on them.
After determining what the key input variables for CIR were (supplier and catalyst weight), we then examined how they affected the output of CIR. To do this we created a box plot, with CIR as the Y variable, and supplier and catwt as X variables. The results are shown in Figure 4 below.
Figure 4 shows that as the catalyst weight increases so does the CIR. In addition, the box plot shows that at the same catalyst weights, supplier A has lower CIR values then B. To analyze these variable affects further, we created a fitted line plot for CIR vs. supplier and CIR vs. catalyst weight. These plots are shown in Figure 5.
When we ran a box plot for T10 as the Y variable, and supplier and catwt as X variables, we achieved the same results as for CIR. The T10 value increased for increasing values of catalyst weight, and values for Supplier B were slightly higher than for supplier A.
From section 2.2 Process Capability Analysis, we found that Tear "B1" and Tear "B2", which both measure the same rubber sample, have high, but different DPM . We then performed a paired t test to see if the different methods for measuring the tear strength differ. Tear "B1" and Tear "B2" both measure the same parts. The results are shown in Figure 6.
From Figure 6 we can conclude that the tear measurement methods are not statistically different. The p-value is greater the .05 and the 95% CI for the mean difference between the measurement methods contains 0. Lastly, from Figure 6 we were able to see that the standard deviations for tear strengths are high with a value of 40.59 for Tear "B1" and 40.34 for Tear "B2".
Currently, Rubberland measures 6 input variables and 7 output variables. From our process capability analysis, we concluded that the output variable with the most problems was CIR. We then ran a linear regression model to determine which of the 6 input variables significantly affected CIR. From that regression model, we found that supplier and catalyst weight were both significant.
Further analysis on these input variables showed that as catalyst weight increased, both CIR and T10 increased, and supplier B had larger values than supplier A. The specification limit for CIR is 140 +/-5. The catalyst weight is targeted at 0.2%, however if this weight increases above 0.21%, the CIR reaches values greater than 145 and out of the specification limit. This is shown in a contingency table in Figure 7 below.
Failures are caused if the CIR value increases over 145, and this occurs when the catalyst weight is 0.21% or above. From the correlation analysis conducted in section 2.3, we saw that the amount of catalyst dispensed (catdisp) was 97.8% correlated to catwt. Because the amount of catalyst that an operator dispenses affects the weight, Rubberland should thus focus on fixing the amount of catalyst dispensed. We ran a contingency table of CIR defects vs. catdisp and found that 31.8% of defects started occurring when the catdisp was 0.20%. We therefore recommend that Rubberland have operators dispense a catalyst between 0.16% and 0.2% only. If they dispense more than 0.20%, they should remove some of the catalyst or consider the part a defect and not give it to customers. We also recommend that Rubberland perform additional analysis to see how small the lower spec limit can be for catalysis dispensed.
Figure 8, below, is a contingency table of CIR defects vs. Suppliers. From this figure, you can see that all of the CIR defects occur when using supplier B. We therefore recommend that Rubberland reduce the amount of orders from supplier B and order more from supplier A instead.
From regression analysis for tear strength, we found that no input variables significantly affect the tear strength. From this result, we recommend further studies to see if the noise can be reduced so Rubberland can see what input variables affect the tear strength. In paired t-test analysis, we found that the two measurement methods, Tear "B1" and Tear "B2" are not statistically different, but both have high standard deviations for tear strengths measured. We recommend looking into methods to reduce the variance for tear strengths. We also suggest only using one measurement method because they are correlated and not statically different. This will save time, which will also save money.
The linear regression analysis for CIR showed that the only two variables that were significant were catwt and supplier. The other variables, mixtime, crosslinker, and inhibitor were not significant. We modeled fitted line plots for each of these non-significant variables against CIR, and we found that all three plots had R-Squared values of 1.4% or less. An example of mixtime vs. CIR is shown in Figure 9 below.
The low R-squared values mean that the values of the variables, mixtime, crosslinker, and inhibitor do not have an affect on the output of CIR for the given specification range. We thus recommend that Rubberland can increase the specification limits for mixtime, crosslinker, and inhibitor. Rubberland should conduct further analysis to see how CIR reacts out of the given current specification limits, and will then most likely be able to increase this range.
Rubberland currently measures 7 output variables. CIR, Tear B1, Tear B2, and VMAX all have large DPM, and thus Rubberland should still measure these variables and make sure they are within specification limits. The T10 output variable did not have large DPM and it is correlated to CIR with a value of .887, so therefore, Rubberland does not need to measure the output of T10 anymore. DPM was not high for Elong or Tensile strength either, so we also recommend that Rubberland not focus its output studies on these variables either.
For Rubberland's input measurement variables, we recommend not measuring the catalyst weight to save time and money. We recommend this because the catalyst weight has a correlation value of .978 so it is highly correlated to the catalyst dispense.
To maintain our improvement recommendations we have several suggestions. We suggest Rubberland to develop better training for the employees that are involved in the production process. To supplement the training we recommend creating work instructions.
For filling the catalyst in the jar, we recommend that they come up with a better device to control the catalyst dispense so the operators cannot add any more then 0.2%. With this control, if the operator tries adding more then 0.2%, the device will stop the dispensing of the catalyst and thus the part will not have a chance to be a defect.
In our analysis and improve section, we found that defects for CIR occurred when the material came from Supplier B. However, there were also times that when supplier B material was used, the part was not a defect. Due to capacity constraints at suppliers, it may not be possible to not order from supplier B at all, so we recommend that Rubberland and Supplier B conduct a study to figure out why some parts from supplier B are defects.
In our recommendation section, we recommended that Rubberland could reduce the amount of output variables they measure and increase some of the specification limits of the input variables. If Rubberland decides to do this, they should conduct follow up studies to make sure that their new specification limits and removal of certain measured output variables do not produce more defects.
Lastly, we recommend analysis for why there are so many wrong materials, which are shown in the cause and effect diagram in section 3.1.

The objective of this experiment was to observe the human-machine system and the work methods utilized in a simulated bagel retail setting in order to identify wasted motions and make a proposal for more efficient operations based on this analysis. In the observed operation, the employee (main test subject) would be taking orders, selecting the bagel, cutting the bagel, preparing it with cream cheese at the customer's request, and delivering it to the customer. In this simulation, currency was not directly exchanged, though the customers placed an imitation currency on the table when placing orders, which served functionally as a consent form for video taping the experiment.
Although the main components of this study were the actual motions utilized by each hand of the test subject, the results of these motions were to be analyzed in the context of the entire human-machine system in order to determine the effects of the motions. Thus, it is important to be aware of the structure of the human-machine system for this experiment.
In order to obtain a clear understanding of the human-machine system, it is important to break down the specific attributes of each of the main categories in the system for this particular experiment, which is performed in the following table (Table 1.)
Take the order from the customer, in which he / she requests one of 3 different types of bagel; retrieve this bagel; cut the bagel into 2 halves; ask customer if he / she would like cream cheese, and if so, whether prefer regular or strawberry; load this type of cream cheese on the butter knife and apply this cheese to bagel; put halves together and present to customer.
1.) Table 1 (right table on depiction), which was used for storing the bagels, was 34" high, had a rectangular surface (48" width x 30" length,) and was flush against the wall on the right.
2.) Table 2 (left table on depiction), which was used for cutting and applying cream cheese, was 29" high, had a rectangular surface (60" width x 18" length,) and was located 58" from the wall on the right.
3.) Three knives were used in the operation: cutting knife size was comparable to U.S. standard kitchen knife; two butter knives (1 for each type of cream cheese) were used, each one of standard U.S. kitchen knife size.
4.) Two containers of different types of cream cheese were utilized, each one of approx 3" in diameter, 2" in height.
5.) Paper sheets (approx 8" x 11") were utilized for presentation to customers; sheets were stored in box that dispensed them (one at a time.)
6.) Three grocery bags (12"w x 7"l x 14"h) containing three different kinds of bagels were located on Table 1, situated 5" from both x-axis table borders and 11.5" from both y-axis borders, with 1" separating each bag.
1.) Employee must fall within height range that allows for large enough work envelope to use appendages comfortably and without strain on table surfaces.
2.) Sufficient wrist strength to cut through multiple bagels (approx 2-3 per minute.)
3.) Sufficient dexterity to hold bagel and cut; hold cream cheese container and load cream cheese onto knife; hold bagel and spread cream cheese on bagel.
4.) Ability to hear orders and feedback from customer.
5.) Ability to see and distinguish types of bagels and cream cheese; see and determine the center of the bagel in order to make proper cut.
6.) Cognitive awareness to maintain quality, safety, productivity and cordiality amidst potential customer cue buildup and complaints (this skill may be more prevalent in employees with experience in food service industry.)
1.) Although there is some customer feedback designed into the operation (selection of bagel and cream cheese type), excessive feedback is a potential attribute that is unpredictable and may cause delays in the operations.
Bagels should be produced at the rate of at least 3 bagels / minute.
It is important to understand several aspects about Table 1 as it relates to this particular experiment. With regard to the human attributes, the test subject in this experiment was a male, 6'2" and 195 lbs, with some experience in food service industry. With regard to environmental factors, it is important to note that although in the long run, attributes such as temperature and humidity will be taken into account with regard to bagel storage and freshness, because this analysis has a smaller timescale scope, and is focused on the operations of bagel preparation, these attributes will not play a role in the analysis.
Below is a graphical representation of the human-machine system:
The test subject was instructed to take orders from the customers and perform the tasks described above. No specific goal-oriented instructions were given, as whether to focus primarily on delivering quality bagels with cream cheese spread evenly and plentifully, to deliver bagels as fast as possible, to perform all actions with as little risk for safety violations as possible, or whether to be courteous with customers to the point of striking up conversation. Therefore, it was assumed that the test subject would balance the goals of quality, productivity, safety, and cordiality evenly and as came natural.
Multiple cycles of this activity were observed, and six cycles of this activity were video recorded for further motion analysis.1
First, it is important to view the data recorded from the motion analysis. The first table (Table 2) below lists all of the specific elements performed by each hand. This table expounds upon the actual Therbligs themselves, providing a detailed description of the motions. The second table below (Table 3) is a more succinct listing of Table 1, with just the Therbligs listed for each hand, as well as the change in time for each motion rather than the running time on the y-axis.
Figure 3 represents a simultaneous motion chart for each hands' motions. Each different shaded segment of the bar graph coincides with the motions listed on the tables above (tables 2 and 3), and the larger segments represent motions that took a longer period of time.
After looking at the detailed data within each cycle, it is important to analyze the time segments for the major portions of the job task from all six cycles to determine data on time averages and variance. The following table lists time data for each of the major tasks for each of the cycles, as well as cumulative times for each cycle.
Based on these six cycles, the mean length of each cycle was 36.0 seconds and the standard deviation was 4.82 seconds, indicating that there was notable variation. Further, although the final two cycles showed some evidence of a learning curve, the remaining data reflected a somewhat random distribution of the results. It appears that the largest sources of variation were in the very first step, in which the test subject took the order, grabbed the paper, and positioned the bagel, and there were secondary sources of variation observed in the cutting and cheese-spreading steps.
On the following chart, the time that was spent on major tasks for the six cycles are listed in terms of time spent.
The following figure (Figure 5) analyzes this data in terms of maximums, minimums, and averages.
In order to propose a more efficient layout and operations procedure, it is important to isolate wasted motions. According to Nieble and Freivalds (2003, as cited in Armstrong, 3-2, Methods Analysis and Ergonomic Work Enhancements), non-value added elements for this operation included "position,"
Because the "position" step took place in order to increase the safety factor during the cutting process, this step should not be compromised in any way. In a similar fashion, the "hold" steps listed in the table were integral in presenting the bagel, and this fact combined with the fact that their sum only amounted to approximately 1-2 seconds, indicates that these are not where the revision efforts should be focused.
Where there was considerable potential for time saving was in the initial steps of selecting the bagel, the process of spreading the cream cheese, and the process of cutting the bagel. The greatest variance in the major steps analyzed in Table 4 and Figures 4 and 5 were noted in the first step of selecting the bagel, and was most likely due to the timing of the customer's request. Nonetheless, there was room for improvement for this step. If the bagels were taken out of their bags before the entire operation began and placed in containers that would fit on top of the Table 2, it would eliminate the need for a Table 1 and should reduce the time in this first step (prior to cutting the bagel) by approximately 2 seconds. In addition to the time saved by the physical turning around and selecting, the process should be easier on the employee, saving him / her kinetic energy and reducing head motion, thereby increasing mental focus.
By far, the longest time was spent loading the knife with cream cheese and spreading it on the bagel. It was noted that of the six cycles, three times there were two trips made to the cream cheese bin to load the knife. However, this extra trip did not necessarily increase the time spent, as one of the one-trip cycles still had a total loading / spreading time of 16 seconds, which was the second most amount of time recorded for this step. It can be argued that the subject spent abnormally more time loading the knife before the one-trip cycle than the two-trip cycles, and also that he may have spent more time spreading due to having less product with which to work. Nonetheless, it is suggested that steps be removed from this process to save time. It will still take 2.03 seconds to load the cheese, but rather than spend the average of 13.7 seconds to spread the cheese on the bagel, it is recommended that the cheese be spread on the bagel with one motion, which occurred in 0.6 seconds. This should allow for an average savings of 13.1 seconds for this step. Obviously, this will result in not much of a spread at all, and in order to maintain a high degree of quality and customer satisfaction, it is recommended that a plastic knife be served with the bagel in order that the customer can spread the cheese him / herself. It is estimated that it will take the employee an additional 1 second to obtain this item and give to the customer at the end of the transaction; thus net savings on this revision should be 12.1 seconds. It should be noted that such a significant time savings should offset the variable cost of the plastic knives.
The second longest major job task was cutting the bagel (7.83 seconds on average.) A simple way to reduce this time would be to use a sharper knife, which would eliminate the second run that the subject performed on every cycle to complete the cut. Thus, using the observed cycle as a model once again, the subject will still spend 2.30 seconds for his cut. This is a conservative estimate due to the fact that it takes into account 4 observed oscillations still within that first cut, some of which may be eliminated with the use of a sharper knife. The cut should be complete after this cut, saving a total of 4.31 seconds on this run. It should be noted that eliminating this step would also eliminate a fairly major safety hazard observed during the operation. In between the two cuts, the test subject used his hand holding the knife (right hand), while still holding the knife, to rotate the bagel. In the event that the proposed change does not go into effect, it is suggested that two steps be added in which he puts the knife down to rotate the bagel and then picks the knife back up when finished with the rotation. However, this change would not be necessary under the new operational format.
In sum, with the new layout and operations changes, a total of (2.0 + 12.1 + 4.3) = 18.4 seconds should be saved, thereby reducing the average total time in half from 36 seconds to 17.6 seconds. This change translates into a production change from less than 2 bagels per minute to nearly 3.5 bagels per minute. It also reduces safety hazards, makes the workplace layout more comfortable for the employee, and should maintain a high level of quality.

During presentations or work with a computer, it is often difficult to read characters on the display when other light sources such as a fluorescent lamp and sunlight are present in the room and thus affect the contrast of the display. However, when lights are completely eliminated (except the light from the display itself) by drawing a curtain or so, people sometimes frown in the direct light from the display, especially when the display character is bright itself over a dark background. A previous study showed that the increase in contrast threshold (%) was smallest when a background and surround were at equal luminance, and the contrast threshold increased rapidly when the surround became brighter, while it increased slightly when the surround became darker (Ireland et al, 1967). The illegibility caused by luminance discord not only disturbs visibility, but also distracts audiences' or users' attentions. To provide the best legibility for normal display viewers in order to achieve a successful presentation or undisturbed work, the best surround luminance was found out from an experiment for both displays with black characters on a white background (White BG) and white characters on a black background (Black BG). Also, outcomes were compared with the previous study to assess the value of experimental results.
Total 5 volunteers whose average age was 26.2 years (range 23 to 29) participated in the experiment. Four of them were males wearing glasses (myopia) whose visual acuity ranged from 0.9 to 1.2 with glasses on, and one female subject who wore no glasses had visual acuity 0.9. A laptop, AMILO D 7820, made by Fujitsu Siemens Computers, was used to present a letter "E" in the center of 9" height ×12" width (image size) LCD display, played by Microsoft PowerPoint version 10.0. Two PowerPoint files for Black BG and White BG were made with their first slides showing only background color in order to let subjects adapt their eyes between trials and while surrounding brightness was being set. From second slides, a letter "E" (Arial, not bold, not italic, no underlined) was placed in the center with its font size in an ascending order from 4 to 23 (total 21 slides each) and the slide number on the low right corner. Both files were programmed to automatically switch to the next slide after 5 seconds except for the first slides which were operated by a space bar. The experiment was conducted in a photo dark room in order to eliminate surround luminance variation. Luminance was measured by light meters, L-508 CINE 700M MASTER, made by SEKONIC. The display luminance was 0.3 ft-L for a black background, 1.0 ft-L for a black character and 21 ft-L for a white character/background. Surround luminance was 0 ft-L with no light except the LCD, 0.83 ft-L with a distant lamp on and white surround, 14 ft-L with a close lamp on and brown surround, 20 ft-L with the close lamp on and dark khaki surround, and 65 ft-L with the close lamp on and white surround. The lamps were aimed at the surround of the LCD. Subjects were seated with their eyes 12 ft distant from the computer display. The monitor was set vertical and its center was at the same height as the subjects' eyes as shown in Figure 1.
A subject was informed that the experiment was to examine a legibility of a display and it had two sessions (Black BG and White BG), 5 surrounding conditions for each session, and 3 trials for each surrounding condition, with 30 seconds rest between sessions. Three trials under surround luminance 0 ft-L were conducted first, followed by 0.83, 65, 14, and 20 ft-L for Black BG, and the opposite order (20 ft-L → ...→ 0 ft-L) for White BG. Before starting an each session, an instruction was given to subjects; "Say 'Yes.' only if you are absolutely sure that you exactly distinguish the displayed character." The slide number at which the subject said Yes was recorded before the slide show was stopped and the file was restarted in the slide show mode for next trial from the first slide. A post-experiment interview was done for each subject about difficulties of reading a character in various conditions.
The minimum font sizes for distinguishing a character "E "under various surround luminance conditions and Black BG/White BG are shown in Figure 2. Since different displays would have different characteristics, the minimum font sizes cannot be absolute. Minimum font sizes were consistently smaller for White BG than Black BG. For White BG, the minimum font size was smallest at the surround luminance of 20 ft-L where the surround was as bright as the background, which is consistent with the previous study (Ireland et al, 1967) even though a different luminance range and a different independent variable were used. For Black BG, however, the minimum font size was smallest when sufficient light (14-65 ft-L) existed, not when the background and surround were at equal luminance. The illegibility when both surround and background were dark (0-0.3 ft-L) can be explained by spherical aberration caused by a big pupil size and it was supported by the interview in which subjects reported that characters looked blurred and were hard to distinguish. It was left to be discovered whether a too bright surround (above 65 ft-L) degrades legibility for Black BG. The illegibility when the surround was much brighter than the background can be explained by diffraction or lack of light stimuli caused by a small pupil size. In conclusion, White BG is better than Black BG in terms of legibility, and White BG works better under the surround luminance as equal as background luminance. If Black BG is used, a proper amount of light should be provided for legibility.
Figure 2 Minimum required font size and surrounding luminance (Dot line: Black BG, Rigid line: White BG)

The worker who is 1.9 m tall and weighs 85 kg is standing with two legs and lifting the 20 kg box with two hands. Symmetric force distribution in two arms was assumed. It was assumed that the box weight was applied at the distal end of lower arm and hand link. The worker's posture is as shown in Figure 1, the feet flat on the floor.
It was supposed that the individual was flexed his shoulder to lift their upper arm to a horizontal position (Angle S = 0) as well as the elbow extension, as the shoulder flexes, so that it was horizontal in the end posture (Angle E = 0). The relationship of the two angles was 7:1 shoulder flexion/elbow extension. The equations used for shoulder and L5/S1 moment calculation are as follows. Resultant forces at elbow and shoulder did not change as elbow and shoulder were extended or flexed. The box mass of 20 kg was assumed.
Shoulder moment = (length of upper arm * resultant force at elbow +COM of upper arm * weight of upper arm) * cos(Angle S)+Elbow moment
Table 6 shows the values of shoulder and L5/S1 moment values calculated for 7 equally spaced intervals through the motion (i.e. at each 10 degrees of shoulder flexion), and the data is plotted in Figure 10. The box mass was assumed 29 kg. Note that the plots in Figure 10 show the absolute values of the moments, not resultant moments. (The resultant moments were all negative values of the same amount of corresponding value).
In calculation of moments and forces in section 1, the moment was discovered to change depending on the segment weights, segment lengths, the posture and the box weight. It was shown that not only the box weight but also the body weight affected a great amount of the force and moment for each joint and the compression and shear force at L5/S1. From the calculation of the resultant force and moment at L5/S1 in section 1, it was shown that the length and the weight of L5/S1 to shoulder resulted in the great moment at L5/S1. Also, from section 2, L5/S1 disc compression and shear forces, it was shown that the moment created by the upper body weight at L5/S1 is more than a half of that by the box, as follows.
As for posture, as shown in the calculation of moments of shoulder and L5/S1 for different angles of elbow / shoulder in section 6 (Moment of Shoulder and L5/S1 when being Flexed), the posture played an important role on the moment for each joint. It implies that the posture with extremities widely spread out to the horizontal direction might cause bigger moments at joints. In the same sense, moment is expected to be smaller for the posture with body segments closer to the center of the body in the horizontal direction. Not to mention, the moment for each joint would become smaller with the box closer to the body. The moment value will vary if the box weight is applied in different position on hands or lower arm. Therefore, if it was assumed that the worker was holding the object by embracing it in his or her arm rather than holding it with the tip of the lower arm and hand link, the value of the moments at joints would be smaller than those calculated in this report.
It was shown in Figure 10 that extremities and the box's larger deviation from the body caused greater moment created at joints. The shape of two curves in Figure 10 suggests a greater slop in changes of moments in the range of greater angle (-70 ~ -50º) than in the range of smaller angle (-20 ~ 0º), which approximately follows the cosine curve. It can be explained by considering the amount of the extremities' deviation from the body in the horizontal axis, which can be described with cosine functions of the angle. From the elbow moment equation used in calculation as follows, for example, the length, the center of mass, and the weight of lower arm and hand and the box weight were constant, while Angle E was the only variable, affecting elbow moment in the cosine function.
Elbow moment = (length of lower arm & hand * box weight / 2 + COM of lower arm & hand * weight of lower arm & hand) * cos(Angle E)
Overhead exertions (which are necessarily close to the body in the sagittal plane), however, are perceived as stressful, despite generating relatively lower moment values at the shoulder. It can be explained by physiological phenomena that the muscle force varies depending on the length of the muscle as well as the length of the moment arm. Even though loads handled close to the body typically produce lower joint moment values, as muscle length is not optimized, the force capacity that the muscle can produce is reduced. Since overhead exertions are likely to cause upper arm flexion and lower arm extension, their muscles are often in the situation where they are contracting or stretched, which is not in an optimized state. To produce the same amount of force (which was required with the optimized muscle length), it can be perceived as stressful. Muscle moment arm is expected to have the similar effect, too. As the moment arm becomes shorter, the greater force is required from the muscle, which can be stressful.
With the same position but different mass of the box, it was shown from Table 2 and Table 3 that, with a heavier box, the force and moment for each joint and the compression and shear force at L5/S1 were greater. It implies that lifting a heavy object can load excessive compression force which can lead to an injury. In this view, NIOSH established a recommended limit of the weight of the object lifted by a person, as called as action limit, in 1991. The action limit is 3400 N of the compression force at L5/S1. The maximum box mass not exceeding the NIOSH action limit was calculated in section 5 (Maximum Box Mass according to NIOSH). As shown in Figure 9, the maximum box mass depends on the length of the erector spinae moment arm when the posture and the body segments' weights and lengths were same. The longer the erector spinae moment arm, the greater the maximum acceptable box mass, which was predicted from the equation of F muscle as a function of E, as below.
To decrease F comp, F muscle is to be decreased. For F muscle to be decreased, from the following equation, E is to be increased.
With only 3 cm variation in the erector spinae moment arm, the maximum box mass varied from 4.7 to 15.8 kg. It emphasizes the importance of the effect of the length of moment arm.
Also, with different box mass, the center of mass of the body and the box was located differently. As shown in Table 4, the centers of mass of the body and the box for both 20 kg box and 30 kg box were located over the foot. However, within the foot, the center of mass was located nearer to the center of the foot with 20 kg box. Therefore, it can be said that the individual could achieve a balance to stand with the box for both cases. However, since the center of mass lied more toward the toe for the case of 30 kg box, it can be said that the balance of standing was less stable with 30 kg box (or more stable with 20 kg box). Even though not calculated in this report, it is possible that, with flexing upper arm and extending lower arm, the distance from the box and arms to the body increases so that the center of mass does not lie over the foot any more. This is more likely to happen with heavier box. From the center of mass in the vertical axis, the center of mass was lower with 30 kg box. It can imply that, in case when the individual was trapped by a low bump, it would be more stable with 30 kg box (heavier box). However, it cannot be true that the heavier the box is, the more stable from being trapped, when the height at which the box is held increases more than or to the point where increasing box mass only raises the vertical position of the center of mass.
Further analysis about the effect of the horizontal force component such as leaning or being supported would be interesting, because those horizontal force components would sometimes help reducing the moments at joints, and other times increase the moments at joints.
Also, in this report, analysis was done in only 2D scale. Thus, no z-axis component was considered. Based on the analysis done in this 2D scale, it is expected that, the greater the width of the body (length in z-axis), the greater the moments at joints, as the greater the deviation from the body, the greater the moments at joints.

Materiel Services Center (MSC) plays a pivotal role in the University of Michigan Hospitals and Health Centers' (UMHHC) mission to provide excellence in patient care by meeting the medical and surgical supply needs of the clinical staff. MSC manages a 9,000 square foot warehouse where an inventory value of approximately $1.35 million is maintained and represents an inventory turnover of 24 times per year.
Materiel Services Center's periodic automatic replenishment (PAR) stocking services are utilized by approximately 210 customers within University Hospital (UH), Taubman Center (TC), Comprehensive Cancer Center (CCC) and C. S. Mott Children's and Women's Hospitals. With the imminent closing of M-Stores and the opening of the Rachel Upjohn Building, Cardiovascular Center, and the new Mott Children's Hospital and Women's Hospital, Materiel Services Center would like to re-evaluate their PAR stocking formula to ensure adequate stocking levels. Every customer is stocked on a daily basis by a Materiel Services Center stockkeeper from the warehouse on Level B2 of UH. Despite being stocked daily, some units are placing a high number of calls to MSC for depleted items. MSC responds by creating a Materiel Service Requisition (MSR) or supplemental order. An evaluation of the PAR stocking levels has been requested to reduce supplemental order requests in an effort to improve MSC efficiency and customer service.
The objective of this project is to determine the root causes for MSC's supplemental orders from units that are PAR stocked daily and to make recommendations for reducing the number of supplemental orders and improving overall PAR levels.
Materiel Services Center currently employs sixty-nine stockkeepers and runners who are responsible for PAR stocking services and filling supplemental and custom requisition orders. All carts are PAR stocked daily during first or second shift. The supplemental is that supplemental orders process requires that orders are delivered within 60 minutes of being requested 24 hours a day. If an ordered is placed stat, it is delivered within 15 minutes. Based on supplemental order data from 11/1/05 to 10/31/06, MSC fills between 67 and 576 lines per day, a daily average of 172 lines.
Interviews were conducted with MSC management and staff to gain an understanding of the current state of their operations. Process flow maps of the supplemental order process and PAR stocking process were created and validated by Materiel Services Center's employees and management (Appendix B and C). Interviews were also conducted on the receiving end of the supplemental order process with select units to validate anecdotal stories as well as to understand potential causes for supplemental orders. A fishbone diagram illustrating these causes may be reviewed in Appendix D. The fishbone diagram led to the creation of a taxonomy of supplemental order root causes, which subsequently became the basis for further observational study and data analysis.
Supplemental data from M-Pathways' Financial Operational Data Store (FinODS) was analyzed over the time period November 1, 2005 to October 31, 2006. The conclusions drawn from the data stratification led to process studies of Materiel Services Center's customer service representatives (MSC CSR) and to meetings with MSC top supplemental order customers.
A statistical analysis of the current PAR level equation was conducted to determine if it provides a sufficient level of stock to meet the demands of each unit.
The top 10 MSR customers by lines remained consistently the same when data was evaluated on a quarterly basis. The graph below summarizes the top 10 MSR customers by lines. (Lines are defined as the number of times an item is ordered by a unit regardless of the item quantity.)
The graph below illustrates the top 25 items requested from MSC through supplemental orders by quantity and their corresponding total cost.
The following three graphs show the total number of lines shipped by month, day of week, and hour of day all for the same time period, 11/1/05 through 10/31/06. Lines by month showed only slight variation and supplemental orders peak on Tuesday. Approximately 55% of all supplemental orders are placed between the hours of 8:00 AM and 2:00 P.M.
Further analysis of two of Materiel Services Center's highest supplemental callers, UMH 5DS (a surgical intensive care unit) and Mott Pod B/C (a pediatric intensive care unit), yielded the following:
Only three months of data was used when examining supplemental orders for each unit. Interviews with nursing staff from these units revealed that any unusual item usage prior to this time frame resulted in lack of recollection for any anomalies in the data. An interesting distinction between UMH 5DS and Mott Pod B/C is that Mott Pod B/C employs an "equipment technician", whose job responsibilities include reviewing the stock levels of the nutrition room and supply room for any stockouts and calling MSC as supplemental items are needed.
Nurses from both units commented that they were not sure what some of these items were based on the descriptions listed above. Another shared comment between these two units was that many of their supplemental orders were due to their highly variant patient populations and that they were requesting items to meet a particular patient's needs that they do not normally stock in their unit.
After a review of the supplemental order data in FinODS, a report was created detailing where supplemental orders were delivered and which department was charged. As a result of the manual process used by MSC CSR, billing errors became evident and they have been quantified at approximately $50,000 or 30% in of the lines in the billing report over the 12 month period between 11/1/05 and 10/31/06. The delivery location ("ShipCust" in the raw data set) and the six digit department ID number billed ("DistType") are both manually entered into FinODS which allows for typos and other errors. Appendix E provides a detailed report of the billing that occurred during this time frame. Journal entries to correct these billing errors are time consuming and costly as well as another manual process. All further analysis using this data set assumed the delivery location ("ShipCust") to be the most accurate field, as advised by the warehouse manager in Materiel Services Center.
The PAR level equation was assessed to validate whether or not it is generating a sufficient stock level for each (unit, item) combination. The current equation M-Pathways uses to generate PAR is:
where Average is the average monthly demand, Divisor is a factor corresponding to the number of times per week that the unit is stocked, and the 1.4 constant creates a safety stock of 40%. The average monthly demand is calculated as the summation of daily quantity received for an item over a year divided by twelve. Daily quantity is defined as the total amount stocked by the stockkeeper and any additional volume brought to the unit via supplemental orders. However, credits issued to a unit are not taken into consideration in this calculation, which could result in slightly inflated PAR levels. This will be rectified in early 2007, when Materiel Picking Feedback (MPF) will be introduced into the PAR calculation software by MAIS. This update will take credits into account when calculating PAR level for a (unit, item) combination.
To reduce the likelihood of stockouts, the variance of the item's daily demand should be taken into account in the PAR level equation. However, the equation currently does not do so and sets all (unit, item) combination PAR levels to be 1.4μ, where μ is the item's average daily demand (Average/Divisor). This results in a different stockout probability for each (unit, item) combination that can be approximated by using supplemental order data. A proxy for the stockout probability
By examining the PAR stocking and supplemental order data, the assumption that daily demand is normally distributed with mean, μ. Under this assumption, the optimal stocking level for a (unit, item) combination should satisfy the equation:
where μ is the item's mean demand, σ is the standard deviation of item demand, and z(α) denotes the one-sided z-statistic of the standard normal distribution corresponding to the probability of not stocking out of the item, α.
Since both Equation 1 and 2 calculate an item's daily PAR level, they can be set equal to each other. Solving for σ yields
Since it is very hard to place an exact value on the cost of stocking out, a common alternative is to use a service level in its place. Therefore, MSC can choose a Type 1 service level, the probability of not stocking out of a (unit, item) combination, that they want to achieve. When setting their service level, MSC should consider the cost of the item versus the labor cost required to deliver the item if ordered as a supplement, the cost of obsolescence if the item is left sitting on the cart, and the opportunity cost of the space that each additional item would consume. This Type 1 service level is equal to α and can be substituted into Equation 4. For example, if MSC decided upon a 99% service level, the PAR Equation would look like
This new equation will increase the PAR level for those items that have stocked out the most, causing both the number of stockouts and supplemental orders to decrease. This calculation is shown in detail in Appendix F.
Fortunately, out of the over 22,000 (unit, item) combinations, less than 500 have a probability of stock-out of 4% or greater. Please refer to Appendix G for a sample of 534 (unit, item) combinations that were stocked out at least 14 times (or 3.84% of the time) during the three-month period selected. New PAR levels for these (unit, item) combinations have been calculated.
Through observational studies and interviews, several categories of reasons for supplemental orders were identified. A taxonomy consisting of these categories was created as illustrated in Table 1.
While a few of the "whys" are stochastic random variables and out of MSC control, many are within their capacity to influence. A preliminary study of the degree to which the stockkeppers accurately inventory and stock the units was undertaken using a randomly selected nursing unit and PAR stocking order data from FinODS. However, due to the limited resources available for this project, these potential causes were not investigated long enough for any sound conclusions to be drawn. Nevertheless, by reviewing the five 'w's -- who, what, when, where, -- and how of the PAR stocking process, MSC management can determine if this is truly a contributor to the supplemental order problem.
This reason looks at why a nurse cannot find the item on his/her cart, despite the item actually being there, or after the supplemental order arrives to their unit.
After interviewing several nursing units, the reoccurring theme among all of them was the difference in nomenclature between Materiel Services Center and Nursing. Attached to each cart is a PAR list containing each item's quantity and location. However, this document lists the items by what MSC calls the item and not how the item is referred to by nursing staff. For example, the item nursing staff refers to as a cotton swab is called an "applicator, cotton tip 6in nonsterile" by Materiel Services Center.
Additionally, nursing staff have confirmed calling for supplemental items without looking in the stock room first or after taking only a cursory look. Also, clerks do not always know who ordered a supplemental item and therefore it never reaches the person who requested the item. Consequently, this often generates further supplemental calls until the item is received by the person who ordered it.
This dimension addresses supplemental orders that are a result of problems related to the pre-determined item inventory level on a cart.
PAR levels are only recalculated on an as requested basis by the PAR Maintenance Staff in Materiel Services. As discussed in Section 3 above, the current equation used to calculate an item's PAR level is:
This category investigates why a nurse manager may not keep items stocked in MSC warehouse on their carts.
Although stocked by MSC, nurse managers are responsible for the items and quantities stocked on their carts. Nurse managers must take the initiative to adjust PAR levels due to an increase in an item's usage or to add a new item to their PAR list. Consequently, the "whys" of this dimension are generally outside MSC's span of influence.
Non-Stock Item: This classification focuses on why orders are placed for items that are not stocked in the MSC warehouse.
Supplemental calls for non-stock items are unavoidable as these low volume and/or high cost items are not kept in stock in the MSC warehouse. Unless the cost-benefit of these items rises to a justifiable level for adding them as regularly stocked warehouse items, this dimension does not require further investigation.
Other observed causes for preventable supplemental calls:
Utilizing the taxonomy framework and the quantitative analysis results, the following recommendations are made to MSC management in their efforts to reduce supplemental order volume.
Work with MAIS to implement a system which forces users to select from a predefined list of delivery locations and have the corresponding department ID automatically populate. This will eliminate the need for having the CSR manually enter department IDs and reduce the potential for error. An option to override will be made available for use when necessary.
Create a committee of nursing staff and MSC staff to produce a directory of items, alphabetized by the nomenclature used by nurses that can be merged with the current PAR list on the carts. This will allow nurses to easily locate items in the stockroom. Since each stockroom layout is unique, this change will be particularly beneficial for nursing staff that float between units.
Providing more details about each item will facilitate nurses ordering the correct item the first time.
Items often end up in the stockroom when a clerk receives a supplemental order and does not know who the intended recipient is.
This will reduce the number of unnecessary supplemental orders for stocked out items.
Establish communication between stockkeepers and units. Nurse managers (or equipment technicians) can notify their stockkeeper of any unusual needs due to their patient population that day. This will result in saving their staff time later as they will no longer need to place a supplemental order.
a. Generate a trigger to notify MSC when a supplemental order is placed more than three times for a non-PAR item.
b. Generate quarterly reports of supplemental orders to determine top customers. Review reports to detect trends and abnormalities and adjust PAR as necessary.
Work with Public Relations and Marketing Communications (PRMC) to develop a brochure to educate units on services offered by MSC (e.g. ability to substock to an alternative location or bill an alternative org code) and on how to use MSC's services more effectively (communication about unit needs, encouraging nurse managers to review and update their PAR levels when necessary).
After reviewing the current PAR level equation and supplemental and PAR stocking data, it is recommended that the PAR levels are to be revised on a quarterly basis, based on the previous quarter's data. This new equation takes into account the variance of demand as well as allowing MSC to adjust the probability of stockout based on item cost or other factors such as holding or obsolescence cost.

Portable music players have become widely popular within the past decade. The MP3 format (formally known as MPEG-1 Layer III) has been one of the primary catalysts of this development. It allows for the storage of far more songs in a smaller amount of disk space than the traditional "Red Book" standard used for uncompressed music files stored on compact disc, which defined the baseline of digital audio from its inception in 1980. MP3 offers varying levels of compression to reduce uncompressed music files to a much smaller size, often of a ratio of 20:1 or even higher. Such compression is necessary to fit a large library of songs into a portable device. MP3 is, however, a so-called lossy compression format in that some of the original audio information is irrevocably lost during compression, and cannot be recovered by any means. Thus, the proper implementation of the compression (encoding/decoding) process is essential to maintaining an acceptable level of playback quality.
The format is standardized only based on its file format and to a lesser extent the way in which MP3-encoded content is decoded. Many different encoders have been produced over the years that support the MP3 standard as it is written, although sometimes with very different operational approaches. Basic options in almost all encoders include bitrate (expressed in kbps or kilobits per second, which fixes the size of the resulting compressed file), input sampling frequency (most commonly 44.1kHz, consistent with the original Red Book standard that defined "CD-quality" sound), as well as options for monophonic or stereophonic sound.
Of course, with the proliferation of MP3 encoders and various options have come attempts to guage the quality of various encoder, bitrate, sampling frequency, and decoder options. These tests have been almost entirely based on subjective listening tests (such as that presented by Bouvigne 1998 and Amorim 2004), where the same audio track is encoded with several different options and encoders and presented to a trained listener. The audio track is then rated on several factors, mostly with respect to fidelity to the original recording.
There are several problems with such a rating system, as it ultimately relies on subjective perceptual judgments that may differ vastly from person to person. Also, there are limits on how many different option combinations that a human listener can tolerate at any one time. Listening preferences could easily change between testing sessions or even within the session. This could make the usability of a large factorial experiment, for example, quite limited. Also troubling is the potential for highly non-normal results, which could skew or make normal factorial analyses statistically unusable.
As a result of the limits of traditional listening-based tests, it was decided to form a testing situation that was compatible with designed experiment goals as well as rely on objective rather than subjective measures. The measure chosen for this experiment is based on the idea that a musical sample that has been encoded and then decoded should possess a similar frequency spectrum as the original. Two sounds that possess very similar frequency spectra over their durations sound very similar. Therefore, diversions from this ideal represent noise introduced by the encode/decode process. An example of such a relative spectrum is shown graphically in Figure 1.
Figure 1 shows a relative spectrum result between an encoded/decoded file and its original source. The relative spectrum is calculated as:
Spectrum 1 is from the original file and spectrum 2 is from a file that has been encoded and then decoded. The resulting scale is called a dBr scale by the Sigview software. In the example in Figure 1, spectrum 1 (from the original file) has a much higher representation of the higher frequencies above about 19 kHz. It appears the encoder or decoder dropped a lot of information at these high frequencies. This is not unexpected since most humans are quite insensitive to sound at such high frequency ranges. (The limitations of this sort of analysis are discussed later.) To form an overall quality metric based upon this graph, the RMS (root-mean-square) was taken, which gives an overall view of the deviations of the two files from each other. Each y value represents a very narrow part of the frequency spectrum.
These relative spectra analyses were carred out with the Sigview 1.95 signal analysis software. As a result of this analysis, a treatment combination can be compared with its original source material using a simple quantitative metric. This will be the quality metric used for the rest of this report.
Several controllable factors can be considered for the analysis for this experiment. They include the encoder, the decoder, the sampling rate, and the bitrate. They are shown in tabular form below in Table 1.
Uncontrollable factors should only occur if any of the software used does not perform the same operations on different files consistently or even if the same operation is performed differently on the same file at different times. This is assumed not to occur and no other uncontrollable factors should be present.
These factor choices were made based upon commonly available software and achievable settings within software. LAME and BladeENC are enthusiast-written MP3 encoders. MP3ENC is an earlier freely available demo version of the commercial encoder produced by Fraunhofer IIS (FHG), who holds the original MP3 patents. MAD and MPG123 are widely available free command-line MP3 decoders (i.e. they are not music players, they only decode the MP3 compression and output it to a file) All software used claimed compliance with the applicable ISO (International Organization for Standardization) directives for MP3. ISO publishes some reference software programming code for encoding and decoding, although some programs claim not to use any of the ISO code.
A general full factorial analysis was chosen because the encoder treatment levels are qualitative and contain more than two levels. Also, it is possible there might be non-linear responses for bitrate versus quality so checking three levels of this factor will be desirable and center points cannot be utilized to analyze this because there are qualitative factors. As a result, a streamlined 2k analysis is not possible. Relatively few experimental resources are needed to create and analyze these files, so there is little benefit in running a fractional factorial anyway. In total, to fully explore all treatments at all levels, 36 runs are needed per replicate.
To analyze different types of music (the inputs for this process), three different musical selections were chosen for each of three replicates, and the runs were then blocked on these replicates to account for the differences in conclusions that might result from each different music selection. In total, 108 individual runs were made. Minitab R14 was used to set up and analyze the results. Randomization of experiment execution order was deemed unnecessary since the order in which each treatment was created and analyzed would have no effect on the results of the signal analysis in Sigview. No part of the analysis could be in any way affected by the run order chosen since the runs are in effect totally independent of the order in which they are executed: there is no process "memory" at any point.
To begin, three 10-second audio samples from three different audio tracks were taken to represent general musical selections. The selected tracks were:
Each song selection was extracted from an original retail CD using Exact Audio Copy V0.95 beta 4 to copy the original 16-bit stereo 44.1kHz files from CD to a PC. These files then had the applicable 10-second portions mentioned above trimmed out of them with the audio editing package Audacity 1.2.3. Audacity was also used to create 32kHz sampling rate equivalents of these 10-second snippets. Generally, sampling rates are chosen to be roughly double or more the top end of human hearing (which is about 20kHz) to avoid signal aliasing but can be set lower to have a smaller file size. Monaural files were created from the original stereo in this experiment to avoid technical limitations with analyzing stereo files. This will only affect the bitrate settings recommended. A 32kbps mono bitrate file would be doubled to about 64kbps to achieve comparable quality results if it were in stereo for most encoders.
Each uncompressed file (either in its 32kHz or 44.1kHz sampling rate form) was then encoded using all combinations of encoder, sampling rate, and decoder. Specific command line options invoked for each encoder and decoder are shown in the Appendix. Sigview 1.95 then was used to create the frequency spectra and calculate the relative spectra discussed above, along with their RMS values.
Before proceeding with the analysis, it was decided to check to see if a data transformation might be appropriate for this data, as it was highly possible that the data was non-normal since sound intensity and frequency scales are based on logarithms. The results of the Box-Cox transformation performed on all 108 RMS readings are shown below in Figure 2.
This analysis recommends a log transformation of this data as the best fit, which was somewhat anticipated as mentioned above. The data was transformed to a natural log scale and then it was analyzed in its entirety. The resulting ANOVA table is shown below.
Prior to using the analysis above to come to any conclusions, the residuals were examined for possible departures from normality that might invalidate an ANOVA analysis. The following figure (Figure 3) shows the residuals versus the fitted values from the model. Some clustering of residuals is apparent on the right side of the figure, as well as a few rather large residuals at the top of graph, but nothing highly non-random appears. A normality plot of residuals confirmed that there is no reason to reject the normality assumption at a significance level of 0.05, with a P-value of 0.163 on the Anderson-Darling test. This analysis presents no reasons to reject the analysis on the basis of lack of normality.
Upon first inspection of the ANOVA table for this model, it is evident there are many significant terms. The main effects are plotted in Figure 4. It is important to note that lower values of the quality metric are better, as it indicates better fidelity to the original file. All of the main effects except for decoder have a statistically significant effect on the spectrum quality of the encoded/decoded music files with respect to the original files. Decoder type is the only main effect not to be significant in this analysis. Perhaps predictably, bitrate is the single most important factor (it is responsible for the largest portion of the sum of squares) in the measured quality. This makes sense as it fixes the size of the encoded files and thus also most directly controls the compression level, which will ultimately have large effects on the amount of fidelity of the encoded file as compared to the original. It also exhibits a non-linear response, with diminishing returns occurring as bitrates increase. The quality improvement going from 32kbps to 80kbps is several times larger than the similarly spaced 80kbps to 128kbps improvement.
Oddly enough, the lower sampling rate of 32kHz actually results in higher measured quality. This is understandable because the compression in 32kHz files will actually be lower as they contain less data to begin with, and thus need less compression to arrive at the predetermined size that the bitrate selection fixes. Note that fidelity measurements were taken with respect to original 32kHz or 44.1kHz files. If all fidelity measurements could have been compared to the original 44.1kHz files that would hold the 32kHz files to a higher standard, it is likely 44.1kHz encoded/decoded files would outperform 32kHz files. This was not possible, though, due to technical limitations in Sigview, where comparing two files with different sampling rates is not possible.
Finally, the best encoder by this analysis is actually BladeEnc. This is surprising because it is much older than LAME is now and also was not the "official" MP3 encoder from Fraunhofer (FHG), which actually performed worst of all! As expected, decoder type has no discernible effect on quality.
The two-way interaction plots are depicted in Figure 5 with significant interactions circled. All interactions not associated with the decoder are significant. For sampling rate vs. bitrate, it appears 44.1kHz sampling rate files exhibit far more linear total improvements in quality as bitrate increases compared to 32kHz files. 32kHz files experience more dramatic improvements in quality as bitrate increases.
For encoder vs. bitrate, it appears FHG's encoder actually performs more poorly at 128kbps than at 80kbps, causing a significant interaction to occur because the other encoders do not experience this anomalous behavior. The sampling rate vs. encoder interaction is barely significant (P=0.043) and no clear interaction pattern can be seen.
Perhaps most interesting, though, is the presence of a 3-way interaction between sampling rate, bitrate, and encoder. To analyze this, the two-way interaction plots for bitrate and encoder were stratified by sampling rate. These are illustrated below in Figures 6 and 7. At a bitrate of 32kbps, little difference is noticed in stratifying by sampling rate. Larger discrepancies are noted at the higher bitrates. Most notable is the fact that the FHG encoder at a sampling rate of 32kHz and a bitrate of 80kbps appears to outperform itself compared to 128kbps. This is unexpected as generally lower bitrates induce more compression, which lowers quality. This doesn't hold true for the FHG encoder at 32kHz. It is unclear what meaning this has, other than there might be a potential programming or algorithm error in the FHG encoder at 32kHz that produces higher than expected compression artifacts at 128kbps. The best explanation may well be that this particular encoder may not have anticipated being used in a rather unusual condition (32kHz source material is rarely, if ever, used for MP3s) and thus performed in an unexpected manner.
The blocking factor (on replicates: music clips) was significant with the P-value very close to zero in this analysis in the ANOVA table. Therefore, blocking on replicates was an appropriate experimental procedure to follow.
Finally, the unadjusted R-Squared value for this model is about 92%. This means that 92% of the differences in quality between these files could be explained by the four factors or the blocked input variable (music clip). 8% of the variation must stem from other sources not explained by this model. Overall, though, an R-Squared value of 92% means that this model is excellent and explains almost all of the variation in quality measured.
A few recommendations can be made on the analysis above. First, the highest bitrate possible should always be selected that is practical for the storage space available, although returns do diminish with increasing bitrates. Based on this analysis, a 160kbps stereo MP3 would be much higher in quality than a 64kbps stereo MP3 given any encoder, but as large of an improvement would probably not be noticed by increasing it to a 256kbps bitrate. Decoders appear to play little role in ultimate quality as long as they are compliant with the applicable standards that define their operation. 32kHz outperforms 44.1kHz on the surface, but this is tempered with the knowledge that the 32kHz encoded/decoded files were only compared with 32 kHz originals, not with the inherently higher quality 44.1kHz originals. The lower sampling rate may have simply allowed for less compression and thus less distortion as compared to the originals they were compared to. 32kHz is not widely used as a sample rate for audio and 44.1kHz should be used so as to avoid unnecessary file conversions. BladeEnc appears to be the top performer with respect to the quality metrics discussed here, but the author subjectively would not have chosen it, especially at the lowest bitrate, where several aural artifacts were more obvious than those from the other encoders. Objective measures are good, but they should be sure to be tied with a user's subjective experience because that will ultimately determine the success of a music compression format, not an objective abstract measure.
There are improvments that could be made to this line of research to remedy the limitations of this quality metric. The relative spectrum calculated in these analyses relied on an unweighted frequency curve. However, human hearing is not unweighted. In fact, it is much more sensitive in some areas than others as shown in Figure 8.
Human hearing most closely follows the (A) curve indicated above. It is clearly not weighted evenly along the 0 to approximately 20kHz scale used in this analysis. As shown in Figure 1, it can be reasonably concluded that sacrifices are made at frequency levels where the human ear is not as sensitive. Also, the quality level is limited because, even if it is not intentional, departures from the baseline in encoded/decoded files will be perceived more readily by listeners at certain frequency bands. Unfortunately, no analysis software found was able to apply weighting curves to the RMS calculations. It would be a valuable extension to the current analysis and one that may become available as signal analysis software continues to advance.
If such analysis could be totally automated, it might be desirable also to analyze many different blocked replicates to come to widely applicable conclusions. Alternatively, different musical styles or types could be compared in different experimental analyses to compare recommendations. More encoders could be examined, as well as implementing stereophonic processing in the analysis, which would allow for greater resemblance between this model and reality.

The goal of this project is to tune the parameters of a high-fidelity simulation of a HUMMWV military vehicle using experimental data from an actual HUMMWV. This model has been developed by RDECOM (at the Detroit Arsenal) to assist them in the design of future HUMMWV platforms.
Initial analysis was performed using an unreplicated full-factorial experimental design and Lenth's method to evaluate factor significance. Response surface methods were then used to determine the simulation parameter settings that resulted in a simulation that most closely matched the experimental data from the actual HUMMWV.
The model predicts the vertical acceleration of the vehicle center of mass in response to a vertical impulse displacement applied to all four wheels simultaneously. The difference between the simulation predictions and experimental data is quantified by an algorithm, AVASIM.
Four model parameters were chosen as factors, and a 24 full factorial experiment (without replication) was specified with factor levels at + − 10% nominal. The model is deterministic, precluding the use of replicates. Applying Lenth's method none of the factors were deemed significant. A response surface method was employed for further investigation of optimal parameter values, and data already obtained for the 24 factorial design was used for the required corner points. Additional center and axial points were obtained as required throughout the process. A line search revealed a region with curvature, and a second order model was used to identify a stationary point.
In general the model accuracy response is ill-behaved. It is suggested that the steepest ascent optimization scheme implemented in this study is notadequate-more sophisticated approaches are suggested. However, the steepest ascent results indicate that the accuracy of the model will be greatly improved by setting the model parameters to the stationary point values.
The objective of this pro ject is to use experimental data from an actual vehicle to tune model parameters such that the simulation accurately predicts actual vehicle response. In addition, it is desired to identify which parameters have a significant effect on model accuracy.
The system used for this pro ject is the High Mobility Multipurpose Wheeled Vehicle, or HUMMWV [1]. This vehicle is a mainstay of the Army fleet and is constantly being redesigned to better suit the needs of the Army. This constant redesigning requires that the Army have a high-fidelity simulation model of the vehicle to allow for powerful design techniques to be employed. The simulation model used for this pro ject, developed by RDECOM (at the Detroit Arsenal), is a high-fidelity three-dimensional dynamic model of a HUMMWV with full suspension characteristics. The model is currently configured to parameter values based on an older model vehicle. The Army wishes to parameterize the model to reflect the performance of the latest design in order to use the model in further design studies. In order to do this an actual HUMMWV was set up on a four-poster shaker table and instrumented to record various data. The input to the system is a vertical displacement in the shape of an impulse applied at all four wheels simultaneously. The vertical acceleration of the center of gravity of the vehicle was chosen as the output based on the test setup and input applied. The availability of this experimental data will facilitate the task of finding model parameter values that produce a more accurate simulation. In order to compare the output from the experimental setup with the output from the simulation model an algorithm called AVASIM was employed [5, 6, 7]. This algorithm generates a measure of model accuracy which varies from one to negative infinity, with one representing one hundred percent accuracy and zero representing a user-defined tolerance on the accuracy of the simulation output. This dictates a maximum-the-best optimization strategy.
Four parameters of the simulation model were identified and selected for this study based on discussions with Army personnel. These parameters were the front and rear spring rates and damping coefficients. Access to only one experimental data set was available for this project. Due to this fact and the deterministic nature of the output from a non-heuristic computational simulation, experimental replicates cannot be made. This prompted a full factorial experiment with four factors and no replications for the initial stage of this study. This experimental design was suitable to analyze both the effect of the parameters on model accuracy and the significance of each factor. Lenth's method for unreplicated experiments was employed initially to identify important factors within the simulation model. Unfortunately, this method indicated that none of the factors or interactions were significant.
After the failure of Lenth's method to identify important factors, it was decided to use a response surface methodology to tune the model parameters with respect to all four factors. The data obtained for the Lenth's method analysis were used in addition to new center points to evaluate the parameter main effects and curvature around the nominal parameter values. After finding an absence of curvature around the nominal values, a line search was performed in the direction of steepest ascent. A change in slope was detected, and a new nominal point was set where that change occurred. A full response surface analysis including the factorial, center and axial points was then used to identify a stationary point and possible optimum parameter value settings. The accuracy at the stationary point was markedly improved over the baseline accuracy.
Chapter 3 provides an overview of the AVASIM algorithm used to evaluate model accuracy, and introduces the design of experiment and response surface methods employed in this study. Chapter 4 presents the results of the investigation, and Chapter 5 summarizes the interpretation of the results and suggests opportunities for future work.
This chapter provides some detail on the AVASIM algorithm used to evaluate model accuracy, and provides an overview of the data analysis methods employed in this study.
The AVASIM algorithm evaluates model accuracy by calculating an Overall Performance Index for a model. To calculate the Overall Performance Index for a specific input and system configuration using AVASIM an output of interest must be first identified. The comparison is made between the model under investigation and the truth. In this pro ject the truth is obtained from experimental data on a real system. However, in other cases it might acquired from a full model. The full model is a more complicated model with enough complexity to provide the most accurate predictions of the systems behavior. For the output of interest, target points can be selected based on the use of the model (Figure 3.1). For example, in a model with a response similar to an ideal second order system, the engineer is usually interested in the overshoot, rise time, steady state value, etc. As many of these points as desired can be defined as targets, in both amplitude and time. For each target point tolerances are then defined.
These tolerances can be either absolute or relative (percentage, etc.) tolerances. Based on intuition and experience relative tolerances are the preferred type of tolerance. Absolute tolerances can be used if the numerical value of a target point is at or near zero (resulting in very large negative performance indices for relatively small errors), or there exists some other problem-specific reason to do so. After target points and tolerances are chosen a performance index at each point is evaluated. This is done using the following formulas:
It is important to note that a Target Performance Index of 1 at any point corresponds to 100% accuracy. It is also of importance to note that a Target Performance Index of 0 corresponds to the response of the reduced model being at the tolerance for that particular target point. If there are no target points defined, as is the case for this pro ject, this step can be skipped.
Once each Target Point Performance Index has been calculated a Response Performance Index is generated. This is done through the creation of a threshold model. This is simply amplitude scaled and time shifted version of the full model of the form:
The scaling factors a and b are chosen to be as large as possible while insuring that the tolerances at all target points are met. If no target points have been defined then a percentage tolerance can be defined directly for a and b. This leads to a maximum perturbed model that still has acceptable accuracy. Once this metamodel output has been generated, the residual sums between the reduced and the full model and the threshold and the full model are calculated using the following formulas:
After these values are obtained the Response Performance Index in obtained in a fashion similar to the Target Performance Indices using the following formula:
Finally, the Overall Performance Index for the reduced model for a specific input and system configuration is calculated as an average of the Target Point and Response Performance Indices using the following formula:
If there are no target points defined then this value is equal to the Response Performance Index. Once this value has been calculated there have been two proposed methods of assessing model validity called the liberal and conservative criterion. The conservative criterion states that all the individual Target Point Performance Indices (Jp,i ) and the Response Performance Index (Jr) must be positive in order for the model to have sufficient accuracy and therefore be valid. The liberal criteria states that it is only necessary for the Overall Performance Index (P I ) to be positive (meaning that some Target Point or Response Indices can be negative, or have insufficient accuracy) for the model to be valid. When there are no target points defined these criterion are equal.
The response of model accuracy evaluation tools is notoriously ill-behaved. The response surface is typically highly non-linear, sometimes discontinuous, multimodal, and usually numerically noisy. These properties pose substantial challenges with regard to finding an unconstrained optimum, discussed in the next section.
The nominal vehicle parameters are set to:
where A = front spring rate, B = rear spring rate, C = front damping rate, and D = rear damping rate. The factorial levels were set to + − 10% of these nominal values. AVASIM was used to evaluate model accuracy at all 24 factorial points. Since no replicates were available, Lenth's method (individual error rate approach) was selected to check factor significance. With Lenth's method the pseudo standard error (PSE) can be used to estimate the standard error [8]. The full factorial experiment also provides insight into the response of the model accuracy evaluation.
Response surface methods provide a generally efficient method for performing unconstrained optimization. The well known steepest ascent method [2, 8] uses the principal that the gradient of a function points in the direction of steepest ascent. A line search in this direction is performed until a maximum in the gradient direction is found. The gradient at that new point is calculated, and used as an updated line search direction. This process can be repeated until the gradient is determined to be the zero vector, at which point necessary conditions for optimality are satisfied (this point is called a stationary point xs ). It can be shown that the search directions for adjacent iterates are orthogonal. If the response function is highly elliptic the steepest ascent method can converge very slowly. Alternate methods such Newton's method or Quasi-Newton methods such as BFGS update are more efficient in this case.
Instead of iterating until the gradient is zero, a second order model can be fit to the data around the current point. This of course only makes sense if curvature is present in the region, which can be checked by either comparing the average value of factorial corner points to the average of center points, or calculating the p-value of the aggregate quadratic term. If curvature is present, a second order model is then fit to the data (requiring additional axial points). The stationary point of this second order model is then used to estimate the location of stationary point1 . If a second order model approximates the response well, this is a reasonable approach. Care should be exercised if the stationary point is determined to be far outside the range of sampled data.
The model accuracy response corresponding to the 24 full factorial points described in §3.2 are displayed in Table 4.1.
Table 4.1: 24 full factorial experiment results.
This data was used to calculate the main effects and interactions, and the PSE required for Lenth's method. The cutoff value for significance was found to be I ER5%,15 = 2.16, and the pseudo standard error was P S E = 0.457. Table 4.2 displays all of the effects and t test values for each effect. It was discovered that none of the factorial effects were significant according to Lenth's method.
Table 4.2: Lenth's method calculations.
The data gathered from the original 24 experiment was used to begin the response surface process for finding the optimum model parameter settings. Seven center points, generated by small (within 1%) perturbations around the nominal values, were used to perform a curvature check. MINITAB was used to fit a first order model (with an additional aggregate quadratic term) to the data, and the p-value for the aggregate quadratic term was found to be 0.971, indicating an exceedingly small presence of any curvature. It was decided to perform a line search in the steepest ascent direction. The scaled coefficients of the linear model provide the search direction:
Because steps of 2∆ would quickly bring the parameter values out of the feasible range, higher resolution 1∆ step sizes were used. The line search produced the following sequence of P I values:
A change of slope can be observed around the 4∆ response, and the investigation was directed to that point. The nominal values were updated to the 4∆ parameter values. Another 24 experiment was designed, centered at the new nominal values. The factorial points were set to + − 10% of the updated nominal values, and seven zero points were specified using small perturbations of the nominal values. Again MINITAB was used to fit a first order model (with an additional aggregate quadratic term) to the data. The p-value for the aggregate quadratic term in this case was found to be 0.004, indicating a strong presence of curvature. Since a stationary point may be near, a second order model (equation 4.1) was fit after eight axial points were specified and evaluated (using a value of α = 2.0).
The values of β0 , b, and B were found to be:
Setting the gradient of the second order model to zero, the stationary point was determined to be:
The values of P I at the stationary point x ∗ calculated with the second order model and the simulation were respectively:
The second order model was only roughly accurate in this case. To determine the nature of the stationary point the eigenvectors of the B matrix were found:
The B matrix is therefore indefinite, indicating a saddlepoint. However, note from the b vector that the effect from factor D is not very significant. If the factor D is ignored, the resulting 3 × 3 B matrix is negative definite, indicating a local maximum. Setting D to its nominal value and running the simulation again with the other factors at stationary point values, we find that P I = −11.1459, which is in fact better than the simulation response at the stationary point.
The accuracy of the model at the baseline parameter values and at the stationary point can be depicted graphically. Figure 4.1 illustrates how the dynamic time response predicted by the model using baseline parameter values roughly agrees with the experimental data. Figure 4.2 shows that the model with the stationary point parameter values predicts a dynamic time response that more closely follows the experimental data.
The AVASIM algorithm was used to quantify the accuracy of a computer simulation of a HUMMWV Army vehicle with respect to actual experimental data.
The objective of this study was to find parameter values that maximized this accuracy. A 24 experimental design without replication was used for initial analysis. This indicated a lack of curvature in the region around the baseline constraints, requiring a line search to find a region with curvature so that a stationary point and optimum parameter set could be found. A steepest ascent response surface method was employed, and a stationary point with marked improvement over the baseline performance.
The functional nature of P I merits some discussion. As mentioned earlier, it is known to be highly non-linear, sometimes discontinuous, and somewhat noisy. This makes response surface methods difficult to implement. Second order models do not fit the response well. This was demonstrated by the discrepancy between the second order response and the simulation response at the same parameter value settings presented in Chapter 4.
Other optimization approaches are better suited for this task. One suggested approach is to use a gradient-free algorithm, such as DIRECT2 [3] to find a region in the model parameter space likely to have the global optimum, followed up with a more sophisticated gradient-based algorithm, such as Sequential Quadratic Programming [4]. This approach is likely to find the global optimum more efficiently.

The entire course was centered around a single problem dealing with the manufacturing of printed circuit boards. Printed circuit boards are made up of many different types of components, including transistors, diodes, and capacitors, which are arranged in different numbers in different places on a substrate, or blank board, to form the final printed circuit board. The components are stored in sleeves in a pipe-organ setup above the area where the substrates are held, with a retrieval arm that moves between the sleeves to pick the correct component. Each of these different elements needs to be placed individually on the substrate by a pick-and-place machine. This process involves two steps: first, the correct element must be retrieved from the correct sleeve (corresponding to the type of component needed); second, the substrate needs to be moved below the insertion point so that the element is placed in the correct location.
This is a rather simplified version of the problem, however, because the setups can be torn down and reassigned, for a time cost of σ. We can immediately see that if σ is very large in comparison to the overall production time, then it will be in our best interest to put all the boards together in one setup and manufacture them according to the optimal setup for all boards. If σ is very small, or zero, then we will tear down the setup between every board and manufacture each board according to its own optimal setup. From here on out, we will denote an individual board's optimal setup cost by where b is the number of the particular board. The difficulty arises when σ is neither 0 nor exceedingly large, but somewhere in the middle. Now the problem becomes not how to order the components in the sleeves, since the optimal configuration is uniquely determined by the boards being produced together, but exactly which boards to group together into a cluster to be produced under a single setup. Since there are n ways to group n boards into clusters, or sets, of 1 or n-1,
Our first step to try and get a handle on the problem was to try and come up with a good heuristic solution. In this assignment, as in many of the others that involved implementation, the most difficult and time consuming piece was arriving at a suitable data structure. I could not figure out a way to store the clusters as vectors of the boards included, since the dimensions would change from cluster to cluster, so I settled on a system of storing the clusters as a 24-element vector with the first component corresponding to the number of the cluster that contained the first board. This could be arbitrarily set to one, but the random-generation code that I wrote was easier if it could just assign random numbers between 1 and t where t was the number of clusters into which you wanted to partition the boards. Looking back, much of the implementation would have been easier had I used the system of 24-element vectors of zeros and ones, where a one signifies that that board is in the cluster. But, using my rather clumsy data structures, I was still able to create a randomized loop which would generate 500,000 instances for clusters of size two to twenty-three, and then would save both the average cost across all instances, and the lowest cost along with the vector that generated that lowest cost.
The best answer we were able to generate from this method was 906,231, with an optimality gap of 2.22%. This optimality gap is misleading, however, because we were comparing it with the only lower bound we had -- manufacturing all boards alone according to their optimal setup with no tear-down cost -- which is clearly an infeasible solution. So in a little over two hours, through our random generation, we were able to get a solution barely 2% away from an impossibly good solution. In reality, our answer was only 0.23% off the actual optimal solution of 904,145. In nearly all industrial applications being 0.23% away from the optimal solution would be more than enough. However, though this worked fairly quickly for the particular data set we were given, we have absolutely no guarantee that it would work for other data sets, or larger data sets. On the other hand, we have seen that having a good feasible solution to start from can result in substantial decreases in the time required to solve the problem, so using a random-generation algorithm such as this might prove useful in obtaining good initial solutions. We also briefly discussed using genetic algorithms in combination with this randomized procedure to generate even better solutions. Unfortunately I do not know enough about genetic algorithms to actually be able to implement them in any consistent fashion, but from the little I do know, my guess is that we could have improved our final solution by breeding several near-optimal solutions together and looking at their offspring over several generations. Additionally, we talked several times about Dushant's local neighborhood search, and the potential it has to help generate good initial feasible solutions. I know even less about neighborhood searches than I do about genetic algorithms, but it seems like it might be an interesting area to look into, especially since outside of the realm of academia, many people are more interested in "good" solutions than in provably optimal solutions. However, being firmly ensconced in the realm of academia, a heuristic solution is not enough to satisfy our unceasing hunger for provable optimality. So we endeavored on.
In order to find a provably optimal solution we must first have a formulation. The first formulation we looked at was George Polak's model (the Polak model) from his published paper.
Since the original model was so difficult to understand, we were given the task of coming up with new models, in the hopes that we could come up with something a little easier to understand. The most intuitive way to construct a model would be to have if boards i and j are manufactured together. Unfortunately, there is no easy way to determine which boards are in a cluster, nor any way that we could come up with to sort them, given this definition of x. Oddly enough, this idea of defining whether two boards are paired together or not turns out to be a very effective branching strategy later on for the master problem, though in a different form. Setting that particular idea aside, Shital and I came up with three models, none of which held any great hope of yielding a solution. Our first model was a slight adjustment to the Polak model, using all the same variables.
This model presents a slightly more intuitive definition of where if boards k and m are in the same cluster. Unfortunately, in order to achieve this, we had to introduce a non-linearity into the model in the form of absolute value. So while it may be slightly more intuitive, the non-linearity excludes it from being truly useful in solving the problem. But thinking about this model led us to our second model which eliminates the issue of determining how many setups there are by assuming that we know there are S setups. Thus, this second model would need to be solved once for S=1,...,N, where N is the number of boards.
Here if cluster r contains board k, and 0 otherwise, returning us to the counterintuitive "0 means yes" definitions from the earlier Polak model. The first two constraints are the same as in the Polak model, and the third simply says that each board must be contained in exactly one cluster. The confusion arises from the fourth constraint; however, if we look more closely we can see that the constraint is only meaningful in the case when , i.e. boards k and m are in the same cluster, thus their setups must be the same. This is exactly what is implied by the second part of the constraint, since then it must be true that , and whether they both are zero or one is immaterial. Additionally, when the constraint drops out and becomes meaningless. Thus, this fourth constraint guarantees that all boards in the same cluster have the same setup. This model seems much more elegant and intuitive than the original model. Unfortunately, it needs to be solved N times (where N=number of boards). As we were running the random-generation code, we noticed that nearly all of the best solutions had between six and nine clusters. If this was a structure that was common across all data sets, a formulation such as this one would allow us to exploit that and only look at the solutions with the likely number of clusters. However, in order to be able to prove that a solution obtained in this manner is optimal, we must be able to show that the function of optimal solution value vs. number of clusters in the solution is convex, which we will address in the next section. The final model we tried took an entirely different view of the problem, setting a new variable if board b is manufactured according to setup s, and 0 otherwise.
Here,
Throughout the process of trying to come up with alternate formulations for this problem we realized just how difficult it can be to effectively model a problem that can be described in only a few sentences. Later, as we looked at modeling the subproblem, this was really hit home -- translating problems into math programs can be exceedingly difficult, even when the problem itself seems relatively easy to describe.
If we knew that the optimal value was convex in the number of clusters in the solution we could take formulations such as Model 2 above, and solve them for increasing numbers of clusters until we saw an increase in function value. If the function is convex, once you see an increase the function value will continue to increase, implying that our previous solution is, in fact, optimal. This has the potential to save a great deal of computation time if the optimal number of clusters is small relative to the number of boards. If the optimal number of clusters is close to the number of boards, then even if the function is convex, it will not save us a great deal of time since we will have to solve the problem for all, or nearly all values of S. Working with Shankara, we tried to find a counterexample using a set of only three boards with three components. Our flaw was assuming symmetric sleeve costs; we tried all possible combinations of boards and found that all of them (under the assumption of symmetric sleeve costs) were convex. Others in the class rejected the assumption of symmetric sleeve costs and thought they had found a counterexample, though it was later proved to be incorrect. Further, their counterexample was an extremely pathological case, with strongly asymmetrical sleeve costs. All the evidence we could find supported the conclusion that with symmetric sleeve costs, the function would indeed be convex; but we were unable to provide a proof, so it is all merely conjecture. Yet even if we could prove convexity, it would mean that we would need to solve an impossibly difficult problem slightly fewer times, but we still do not know how to solve the problem even for a given number of clusters.
Another dimension of the solution involves the integrality of the solution. Clearly we cannot have fractional amounts of clusters in an optimal solution; but solving the linear programming relaxation of the problem is vastly easier than solving the integer program. The question then becomes: If we solve Polak's model with the x variables relaxed, will the solution still be integer? If the constraint matrix is totally unimodular (TU) then yes, the solution to the LP relaxation will be integer. If the constraint matrix is not TU, then the solution may or may not be integer. So we set out to prove that the constraint matrix of the Polak model was TU. Unfortunately, our proof turned out to be flawed and we were unable to correct for the flaw, though it does seem that in most instances the constraint matrix will be totally unimodular. At the end of the course, looking at the solutions using the rank-cluster-price algorithm, most of them did turn out to be integer, especially for small problem instances, implying that we might well be correct that the formulation is indeed TU. Regardless, the solutions do turn out to be integral in many instances.
The integrality of the solution is related to the strength of the LP relaxation. If the LP relaxation is close to the convex hull of the integer program, then the solution will often be integer. Another way of conveying the idea of total unimodularity, is that a constraint matrix that is TU describes exactly the convex hull of the integer feasible region, and so all the extreme points will be integer. In fact, we do not particularly care if all the extreme points are integer, only if the extreme points near the optimal solution are integer. But we were unable to prove that even just the extreme points near the optimal solution were integer, though it might be a more efficient way to approach the proof. Regardless, the better your LP relaxation is, the fewer nodes there will be in your branch and bound tree once you start trying to solve the integer program. As we saw later, this can have a profound impact on the solvability of a problem.
Having nearly exhausted the store of knowledge in the class without additional information, we turned our attention to Dantzig-Wolfe decomposition, which motivates the technique known as column generation. The idea is that we can separate the problem into a set of subproblems which are then linked together in a "master problem." Typically these subproblems have some special structure, such as an assignment problem, that allows them to be solved quickly and easily. This technique is especially useful if you have a set of constraints that affects only a small group of variables, thus those variables and constraints can be separated out and put into subproblems, with "linking constraints" in the master problem which link all the subproblems together. These subproblems then generate a set of extreme points of the solution. And we know that the optimal solution can be expressed as a convex combination of the extreme points of the subproblems, so we can replace the variables in the master problem with these convex combinations of extreme points, and solve. If the subproblems are easily solved, this can prove to be a very powerful mechanism for solving large-scale problems. The key, however, is that the subproblems need to be easy to solve.
With the column generation technique, motivated by the Dantzig-Wolfe decomposition, in mind we set out to define a master problem and subproblem to which we could apply these ideas. We fairly quickly found our master problem:
Here, we change notation slightly where K is the set of all clusters being considered. if cluster k is in the optimal solution, and 0 otherwise. is the cost of cluster k including setup cost σ. is an index variable which equals one if board b is in cluster k. Thus the only constraint in our master problem is that each board must be contained in exactly one cluster, and we want the set of clusters with minimum cost. It seems simple enough, and the idea behind the subproblem is fairly simple. We even know what the subproblem is: to generate the most negative reduced cost column. Then we would take that column from the subproblem, enter it into the master problem, get new duals from the master problem and hand those off to the subproblem and repeat until our subproblem reports back that there are no negative reduced cost columns, showing that our current solution is optimal. Regrettably, this turns out to be much easier said than done.
The reduced cost of a column (cluster) is its true cost minus the dual associated with the boards included in that cluster. The true cost is the manufacturing cost of all the boards included in the cluster plus the setup cost, σ. Thus, we define variable if component c is retrieved from location l for board b in this particular cluster, and 0 otherwise. Additionally, we use if board b is included in the cluster, and if component c is assigned to location l. This results in the first version of the subproblem,
This model minimizes the reduced cost, subject to the following constraints: each component is assigned to exactly one location; each location is assigned exactly one component; if component c is not assigned to location l, then component c cannot be retrieved from location l for any board b; finally, if xb is one then each component must be retrieved from a location for board b. Together these constraints define a feasible column, and the problem will find the feasible column with the most negative reduced cost. Unfortunately, this problem would not yield an answer in less than 2 hours with either the x, or the y, or the x and y variables relaxed. When we relaxed all the variables, we ran into a different problem: the LP relaxation is weak. When you solve the LP relaxation with all variables (x, y, p) relaxed, the optimal solution is fractional. The reason the solution turns out to be fractional, while not immediately obvious, is because the model is splitting the components for the boards it wants into different sleeves and only paying for the cheapest one. And while it will solve to an integer solution with some dual values, we only got it to solve with made-up dual values, or the dual values from the near-optimal solution. Unfortunately, after only a few iterations of shuffling between the master and subproblem starting with the near-optimal solution, the duals become such that the subproblem will no longer solve to integrality. In fact, even with all the variables relaxed we could not get it to solve with the duals from solving the master problem with the identity matrix.
All these frustrations drove us to look at an entirely different model,
This model is exceedingly difficult to understand. In this model the xcl variables take the place of the ycl variables in the previous model, the yb's replace the xb's, and the is data simply taking the place of in the previous model. When M is large enough, and yb is one, implying that the board is in the cluster, the constraint says that
Unfortunately, as we have seen, introducing a big M into a model almost always results in a terrible LP relaxation. Here, the problem arises because the model has two constraints to determine the value of θb:
Despite all these problems, we were able to get a single instance of this version of the subproblem to solve with the duals from the identity matrix in just over twenty minutes. But returning to the big picture, this means it takes twenty minutes to solve one instance of the subproblem to optimality. In order to solve the root node of the master problem, we might need to solve the subproblem millions, or tens of millions, of times. And then, we have no guarantee that the solution to the root node of the master problem will be integer, so we might have to branch on the master problem, meaning we would have to solve potentially hundreds or thousands of versions of the master problem before we arrived at a truly integer solution to our overall problem. Given the complexity of the overall problem, a subproblem that takes twenty minutes to solve is simply worthless in the overall context. And since there is really no way to remove the big M from the model, we were forced to chuck this rather creative model, returning our focus to the seemingly less-promising Subproblem Model 1
Looking at Subproblem Model 1, we noticed that the variables were simply taking the place of xbycl, and serve only to eliminate the nonlinearity in the model. So what if we left the nonlinearity in the model? We end up with a new, nonlinear model:
This model boils down to an assignment problem, which anyone can understand quickly. Additionally, assignment problems typically solve very quickly. We used the MINOS package to solve this model, and the subproblem solved in mere seconds. Shuffling back and forth between the subproblem (in MINOS) and the master problem (in CPLEX) was regrettably too time consuming to really be able to see if the nonlinear model would yield an answer to the subproblem, however it does appear that it would be a good method to generate good columns quickly. To prove optimality in any of our other models, we need to show that there exist no more negative reduced cost columns. The worry is that this nonlinear model might return a positive reduced cost column when there were still negative reduced cost columns out there. But at the very least, we would only have to solve the long linear model a few times, possibly only once, to verify that the nonlinear model had given us all the available negative reduced cost columns. Unfortunately, before we can actually test this theory, we would need to be able to write a .run file which would allow us to switch between MINOS and CPLEX to solve the subproblem and master problem, respectively, without having to do it by hand. It would seem as though there would have to be a way to accomplish this, although we were unable to find anyone who knew how to tackle this particular issue. With a better implementation, this method would seem to hold great promise for solving this problem quickly. I think this is perhaps one of the most exciting avenues we ignored.
Since we could not find a way to get the nonlinear model to solve automatically, we were forced to return to our original model, Subproblem Model 1. We could not get Model 1 to solve with the duals from the identity matrix, so we tried solving it with the duals from the many iterations we did by hand using the nonlinear model, thinking that these duals might be "better" in some sense. Unfortunately, they did not yield a solution either. Then we had the brilliant idea of using the near-optimal solution from the first week as a starting solution for our master problem -- this eight column solution gave us duals that allowed the subproblem to solve lickety-split. This solution raised issues about the validity of the duals from a solution that did not form a basis. In order to form a basis, you need as many variables as you have constraints; here we had only eight variables, one for each column (or cluster) and twenty-four constraints, one for each board. What does this mean for the validity of the duals? After much thought, we decided that even though we do not have a full basis to start with, the duals are nonetheless meaningful, since we end up with exactly eight (the number of columns we started with in the initial solution) non-zero duals. All the other duals, corresponding to the extra variables that are needed to form a basis are simply set to zero. However, this does yield a strongly degenerate solution, where most of the variables in the solution are set to zero. This means that when we took those duals and plugged them into the subproblem the objective value stayed exactly the same over many iterations, because of the degeneracy. The algorithm was simply pivoting in different variables, which were all set to zero, so the objective function did not change. This is the definition of a degenerate pivot. Degeneracy kept coming up again and again throughout this course, and often drastically increased our computation time.
After a few iterations of switching back and forth between the master and subproblem, starting with the near-optimal solution, eventually resulted in duals that the subproblem could no longer solve quickly, at roughly the same point as it reached a full rank basis. We hypothesized that the speed of the subproblem was due to the reduced size of the problem -- with only eight non-zero duals, you really only have to consider eight boards, which makes the problem much faster. And indeed when we solved the master problem with the eight near-optimal columns plus the identity matrix, the subproblem took more than a few hours to solve, supporting our hypothesis that it was the zeros in the duals which made the near-optimal instance easier to solve. So how are we ever going to get this problem to solve? We need it to give us an integer solution, and we have already seen that the LP relaxation will always give us xb values close to ½, so eventually we're going to have to put it into a branch and bound tree. Perhaps looking at the branching strategy will give us some insight, we thought.
When we started thinking about branching, little did we know we would be spending weeks and weeks on it, and what a big deal it would turn out to be. Our first step in looking at branching was to observe what happened when we branched on different variables. We initially started branching on the xb variables, since they seemed to be the ones we ultimately cared the most about -- whether or not a board was included in a cluster. Branching on the x's changed the one particular value you set to be integer, but left all the other values fractional. As soon as you set the very last x to be integer then all the y's and p's would suddenly become integer, but you need to get all the way to the bottom of the tree before you seen any major changes, which could result in 224 (in our instance with 24 boards) branches before you get a solution. 224 is a large number, and not something you want to sort through every time you want to solve the subproblem (which might need to be solved millions of times to generate an answer to the root node of the master problem, which may or may not be integer). So we decided branching on the x's was probably a bad strategy, all things considered.
If branching on the x's will not work, then what about the y's? Setting a ycl variable to be one means that component c is assigned to sleeve l, which automatically sets all other y's in that row and column to be zero, since only one component can be assigned to each sleeve and vice versa. Additionally, you also set all pclb values containing that component or location, but not in conjunction with each other, to zero. If component c is assigned to sleeve l, c cannot be retrieved from any other location for any board; likewise no other component can be retrieved from location l for any board. So branching on the y's holds promise. Unfortunately, when we look at setting a y variable to zero, almost nothing happens; we see an infinitesimally small decrease in the objective function, nothing like what you see when you set y to be one. This is a potentially hazardous branching strategy, since it results in a strongly unbalanced tree. And here again we have the same problem that we need to completely enumerate the y values before we see the x values become integer. The objective jumps the most when setting y to be one, and the least when setting y to be zero, so perhaps there is no clear cut winner. If we have to completely enumerate the y's it will only be 216 instead of 224 branches, but 216 is still too many branches for a subproblem. But what about the p's?
Setting pclb=1 has an even greater impact than setting ycl=1, since pclb=1 then sets ycl=1 with all the concomitant effects, in addition to setting xb=1, and many other pclb's to zero. Surely this is the largest impact that any one variable can have on the problem. But there are 24*216 pclb variables, and again if we look at the other side of the tree, where we set pclb=0, there is practically no change in the objective, and no other variables are set to be integer. In the end, there is no clear winner -- all the branching strategies have their flaws, and the problem was starting to look completely hopeless. Since we were not getting anywhere with this approach we decided to go back and see if we could figure out why the optimal solution was fractional and if we could fix it in any way.
We had seen earlier that the reason the LP relaxation was fractional was because the model wanted to split the boards between several different columns and then only pay for the cheapest one. We noticed, as we were working through the branching strategies, that when you set ycl to be zero, meaning that you can no longer assign component c to sleeve l, it did not change the objective value, because the model simply moved all the components that were in sleeve l and moved them to the symmetric sleeve that had the same cost as sleeve l, changing nothing, really. This means that we were branching twice as many times as we really needed to. So how can we eliminate this waste? By creating a model that assigns two components to each sleeve, and only has half as many sleeves; then you can randomly pick which of the two components goes in each of the paired sleeves because they result in equivalent costs. This lead us to the next model.
This model differs from Subproblem Model 1 only in that the sum of ycl over all components for each location is two here, instead of one as in the previous model. Everything else remains the same. Since this problem has only half as many sleeves, the number of variables is reduced by just less than half, and results in roughly half as many branches as the original, the assumption was that it would solve faster. We ran a check to make sure that both models gave us the same answer when applied to a smaller, randomly generated data set, and indeed they did, showing that the models, in their integer forms, are equivalent. Hopefully the LP relaxation of the non-symmetric model will be stronger than that of the original. Unfortunately, even with the fewer number of variables, we were still unable to get the non-symmetric model to solve with any of the variables set to be integer using the duals from the identity matrix, implying that while it may be faster, it is still not fast enough. So again, we examined potential branching strategies. Here, instead of looking at what happened near the top of the tree following a breadth-first search, we decided to see what happened near the bottom of the tree and followed a depth-first search. We confirmed that indeed none of the x values became integer until all the y values had been set. More than that, we discovered that there was an order of magnitude jump between the next-to-last branch and the very last branch where all the values suddenly became integer. This is unfortunate because it means that pruning will be nearly impossible -- none of the nodes in the branch and bound tree will ever be above the upper bound of the incumbent solution until they are themselves integer -- implying that we are still facing the potential of full enumeration to solve the subproblem. We also tried branching on the p variables, but since every time we set a p to be one we were forcing the corresponding xb and ycl variables to be one as well, we were in effect randomly picking x's, which resulted in a very poor solution. Again we saw the problem of the imbalance in the tree -- setting whichever variable you choose to be one has a far greater impact that setting that same variable to zero, so the tree is much larger on the zero side. Eventually we decided that examining the branching strategy might not actually be fruitful, and the course came full circle as we turned our energies back towards heuristics.
Heuristics are generally not considered as elegant as traditional linear programming methods, but we were hoping that we could find a heuristic that would generate negative reduced cost columns quickly. This might be an effective strategy because we do not particularly care about finding the most negative reduced cost column; we simply need a single negative reduced cost column (or many if we want to put in multiple columns at a time). The first thing we noticed was that we can automatically throw out boards with negative or zero duals, since there is no way their contribution to the reduced cost can be negative, since we still need to pay for the manufacturing. We also know that including boards with larger duals will be more likely to yield a negative reduced cost.
The first heuristic we came up with started by sorting the duals in descending order. We automatically included the two boards with the largest duals, since we know any new column will have at least two boards (since we started with the identity matrix where each board is by itself). Then, in decreasing order of the duals, we examine each board. If adding the board to the previous cluster maintains a negative reduced cost column, we add it and move on to the next board. If the board makes the reduced cost greater than zero, then we do not include the board and output the previous cluster. This would seem to be a smart heuristic because it will almost always yield a negative reduced cost column, but it fails to take into account any measure of similarity between the boards. If boards 1 and 2 have very large duals but are entirely dissimilar, it might not be a good idea to put them together into a cluster. Additionally, we discovered that this heuristic generated very similar columns at every iteration, because the duals do not change much when you start with the identity matrix. We saw other situations where the duals changed much more rapidly, so there is a chance that this heuristic would work in different circumstances; however, it is not a good bet when starting from the identity.
Our second heuristic harkened back to the random-generation code of the very beginning. We simply generated random clusters, checked their reduced cost, and saved the smallest. If the random generation failed to produce a negative reduced cost column, then we applied our first heuristic. Though we consistently produced negative reduced cost columns, the objective value was decreasing by only a little more than 1% per iteration, so using this method would take a long time to reach a near-optimal solution.
The other idea motivating us to find good initial solutions was the thought that we could then use them in AMPL as better upper bounds, allowing CPLEX to prune the branch and bound tree faster. What we discovered with both of our heuristics was that AMPL generated an integer solution with a better objective value than that of our heuristics in less than a minute, so we were not really improving the pruning by giving it our heuristic upper bound. Perhaps had we been able to generate solutions with much lower objective values it would have made a difference, but given what we had, we made no impact at all. AMPL is much smarter than we give it credit for, it would appear.
Still fixating on the issue of solving the subproblem with the duals from the identity matrix, we decided to test whether or not it was a special property of the identity matrix, or simply the fact that we had a full rank basis. So several people tried multiple randomly generated sets of 24 clusters, and found that often the clusters did not provide a feasible solution. Adding the columns from the near-optimal solution so that a feasible solution existed, however, resulted in a problem that still took just as long to solve as the one using the duals from the identity matrix. Thus, we can firmly state that the faster speeds observed using the near-optimal solution stem not from the near-optimality of the solution, but the fact that the columns do not form a full-rank basis. This exercise did not really provide any profound insight, but at least confirmed that our suspicions were correct.
So now we've decided we cannot come up with a way to generate good upper bounds (or at least upper bounds that are better than those that AMPL arrives at within minutes), we cannot find any inspiration from branching strategies, and we are sure that this is our most promising model. It is at this point that we were all grateful that this was not our dissertation research. So what do you do?
Thankfully, we had our intrepid professor to help us along, and refocus our attention on pruning. Previously, we had agreed that any board with a negative or zero dual would not be included in an optimal solution, but perhaps there was a tighter bound we could use. If we define to be the cost of manufacturing board b according to the optimal setup for cluster C, we know that
Perhaps the most intuitive pruning technique is to look at the tree and see if adding a board to a particular cluster increases the reduced cost. If so, then you can prune that branch of the tree. But this is only a valid method of pruning if you can prove that there is no other combination of boards including the original cluster, the additional board and some subset of the remaining boards which will have a lower reduced cost than that of the original cluster. In other words, can we prove that if
So we have shown that we cannot prune the tree going down, but perhaps we can gain some insight about the future of a branch based on what is left to explore. If the reduced cost of a node is positive, and the lower (or best) bound on the contribution of all the boards left to examine does not outweigh the current reduced cost, then there is no way the branch will ever be negative and we can prune it. We define the potential of a node to be the lower bound on the reduced cost contribution of the remaining boards, or , where R is the set of all remaining boards. Since this is a lower bound on the contribution of the remaining boards, if the current reduced cost plus the potential is not negative, then we can prune the current node. This will hopefully save us some time when we near the bottom of the tree. We attempted to implement this pruning scheme, but unfortunately were stymied by our lack of object-oriented programming knowledge, and we could not figure out a way to store all the data of the tree in Matlab. Here again, formulating the data structures proved to be the most difficult part of the implementation, which is itself the most difficult part of testing the idea.
With this notion of potential in hand, we are now prepared to understand the final solution, though it probably would have taken us another semester (or at least another half a semester) to arrive at it on our own. The final solution involved a slightly modified version of this potential. The potential we defined above assumes each board is manufactured according to its own optimal setup, which we know is almost guaranteed not to happen in an actual solution. So how do you take that into account? We could say that we must account for the cost of manufacturing all the remaining boards together, i.e. if we had three boards left to examine, the potential would be
Why would the algorithm take so very long to find a solution that it already had to begin with? (All instances started with an initial set of columns consisting of the identity, the exhaustive set, all 2 board clusters, all 3 board clusters, all n-2 board clusters, and all n-3 board clusters.) The answer belongs to our old friend, degeneracy. In a situation where the optimal solution is the exhaustive set, you have n-1 degenerate variables with value zero and 1 variable (the exhaustive set) with value one. The algorithm will continue to find negative reduced cost clusters, because it thinks it can make the objective value better, except all those negative reduced cost columns enter into the basis with value zero, and so have no effect on the objective value. There are nearly as many combinations of n-1 boards as there are of n boards, and you can cycle through most of them without seeing any change in the objective. The problem is that you have to find those degenerate columns and pivot them in, otherwise you will never know that you are optimal, because there will still be negative reduced cost columns out there. So how can you avoid this seemingly endless degenerate pivoting? You can't, basically, without exploiting some sort of problem structure.
In this instance, we noticed that the cost of the exhaustive column is
So we have a solution, or more rather, a way to obtain a solution. Was that not the point of the class? Well, we have a way to find a solution to the root node of the master problem, but what if the root node is fractional? Then you have to branch (and bound) to get an integer solution. Surprisingly, branching within the confines of column generation turns out to be quite challenging. The traditional way to branch takes a fractional variable, x, and says it has to be less than or equal to
Branching in this problem turns out to be much simpler than one might initially think. If we have a cluster, say boards 2 and 3, that is fractional, instead of setting x2,3 to be zero or one, we can define the branching in terms of "togetherness." If we want to set x2,3 to one, another way of saying that is we always want boards 2 and 3 together, so whenever we add board 2, we automatically add board 3 in with it. Continuing in this fashion, we can create a togetherness web -- board 1 is with board 4, boards 2 and 3 are with board 7, board 5 is not with 4, etc. And this will eventually allow us to determine our integer solution without violating the needs of the subproblem.
It has been a long, crazy journey on the road to this solution. But in this journey as in most, what is most important is not the destination, but the path taken to arrive there and the experiences gained along the way. The two most important things I learned from this course were the importance of thinking flexibly about problems, and that implementation is a pain. We explored the depths of frustration, we mounted the pinnacles of success, and saw that research is truly an up and down process -- some of the things you try work out nicely, most of them will not; and you've got to learn to roll with the punches.
On the more practical side, this course really gave us a firm handle on what exactly column generation is and what sorts of problems we can adapt to fit it. Column generation is a massively powerful technique, yet elegant in its simplicity. I very much enjoyed getting my hands dirty with the nitty gritty of it all. We learned about degeneracy, and what a pain that can be. We learned about LP relaxations and how important a good formulation can be if you ever want to solve your problem. We saw that big M's are almost always bad news, which will definitely encourage me to look for other ways to model my problems from here on out. One of the things that will really help me in the future is the ability to look at a fractional solution, and think about why it is fractional, step-by-step. Because many of the problems you model as integer programs will, in fact, turn out to be fractional, and many of them will make no sense as to why they are fractional. Being able to analyze a problem in the detail we did in this course, I think will help me a great deal in my future research.
We also saw how to exploit problem structure, and what great advantages that can give you. From now on, I will know to always look for symmetry in my models, to see if there is a way I can reduce the number of variables, and thus the complexity of my problem. We learned about branching -- what had formerly seemed like a fairly straightforward process is now even more of a mystery. I can't say that I gained any deep insight into branching strategies, except that I more fully understand their vast complexity. I did learn, however, how to critically attack branching strategies, and the process one would go through to try and evaluate them, which I think will probably be useful in the future. We saw that using a programming language without good references can be frustrating in the extreme. The majority of my AMPL questions were answered not by the manual, but by other students in the department who had wrestled with these exact same questions.1 But perhaps most importantly, we now have a whole toolbox full of skills to use when we are attacking a problem that just will not budge -- you can take the dual of the problem and see if that gets you anywhere, you can try to reformulate it using entirely different variables, you can look to exploit problem structure, you can closely examine the branching strategy and see if you can gain any insight... Many of these approaches I never would have considered before taking this course. I feel like I am much better prepared to face the frustrations and difficulties that inevitably face researchers now. (Which is not to say I am prepared, per se, simply more prepared than I was previously.)
I really wish we had someone who knew how to write a .run file that would allow us to test the non-linear model. I would be very interested to see how it behaves and if, in fact, it would get us close enough to an optimal solution that then we could throw it at the symmetric model and prove optimality. There is no real way to know without writing that .run file, but it seems like it would be so simple, and the results so fascinating. I also feel like we had some interesting intuition about other branching strategies right there at the end that might have been interesting to explore. Not that it would have been particularly fruitful (given all the other complexities of the problem) but I would think with a little more work we might be able to prove the convexity of the optimal function; it definitely seems like it should be convex, and my dreams would be sweeter knowing that it was. Additionally, I would have liked to revisit the idea of genetic algorithms to see if we could concoct a heuristic using breeding that would get us close to the optimal solution. And while I am sure there could be multiple semesters taught on Dushant's local neighborhood search mechanisms, I would have enjoyed at least a little glimpse into that world. It also would have been interesting to look at the lagrangian relaxation -- I do not know much about lagrangian relaxations, except what I saw in 518, Intro to Integer Programming, but they seemed to be powerful tools to help solve integer programs with nasty constraints. I feel like I finally have a handle on degeneracy for the first time in my life, which is rather exciting. That said, I still would have enjoyed looking at different situations where we can exploit problem structure to avoid degeneracy, since I feel that is one of the more important skills I gained from this class. People have talked about Bender's decomposition, but I had never even heard of Bender's decomposition until this class. Dantzig-Wolfe decomposition blew my mind -- I hazard to guess what Bender's decomposition would have done to me.
And that, as they say, is all she wrote.

Efficiency measurements are widely used in various industries to benchmark performance, document operational improvements, and provide other managerial information (e.g.,, Arogyaswamy and Yasai-Ardekani 1997; Westphal et al. 1997). Conversely, water utilities exist in relatively non-competitive environments, with few quantifiable operational measurements being available to compel management efficiency. This paper describes a procedure by which the efficiency of water utilities can be assessed using data envelopment analysis (DEA), a procedure widely used to provide objective numerical efficiency rankings for comparable units.
Data envelopment analysis was first described in a landmark paper by Charnes et al. (1978), and has since experienced extensive development and growth to become a ubiquitous efficiency measurement method (Seiford 1996). DEA has been used for efficiency measurements in such industries as health care (e.g. Banker et al. 1986; Ozcan et al. 1992; Kontodimopoulos and Niakas 2005); insurance (Brockett et al. 1998); agriculture (Coelli 1995; Wadud and White 2000); food processing (Jayanthi et al. 1999); and many others. DEA has been used to measure the efficiency of engineered products (Bulla et al. 2000) and has been used to guide the selection of new technologies (Baker and Talluri 1997).
Of particular interest to our work in analyzing various operational challenges facing water utilities is related work analyzing units in sectors which similarly experience extensive regulatory presence and government control. For example, DEA has been used to examine changes and policies within public school systems (Bessent and Bessent 1980) and to examine the efficiency impact of government regulatory changes on units with different ownership models, such as banks (Bhattacharyya et al. 1997). In other relevant studies, researchers have used DEA to measure the relative efficiency of government-run publicly-owned forest management districts and to estimate the potential efficiency gains from different organizational alternatives (Kao and Yang 1992).
DEA has also been used to measure efficiencies of units within the municipal infrastructure sector. Bosch et al. (2000) used the procedure to measure the efficiency of municipal waste collection services, finding that services operating within competitive environments were more efficient than services operating within monopoly environments. Worthington and Dollery (2001) used the procedure to study the efficiency of 103 municipal waste collection units, comparing inefficiency drivers between urban and rural units and between units covering various geographical scales. These investigators estimated that inputs could be reduced by 65 percent while maintaining the same level of service if best practices were used by currently inefficient units.
A few DEA studies have in fact been performed on selected aspects of potable water supply systems. Akosa et al. (1995) reported on the DEA efficiency analysis of ten water and sewage infrastructures in Ghana, a low income, West African state. Pursuant to funding agency interests, the projects had six input variables (technical, financial, economic, institutional, social, and environmental) representing such things as community input, etc., and three output variables (reliability, utilization, and convenience) representing various levels of use. Despite the high ratio of variables measured to units compared, the analysis indicated that only one unit was fully efficient when all nine input and output variables were considered. Although a limited number of compared units were compared, the investigators were able to draw relevant inferences regarding future funding efforts to optimize benefits.
Variants on DEA-measured efficiencies have been used as input to regulatory pricing structures relevant to the work described here . The British Office of Water Services (OFWAT) regulates potable water price structures with the goal of balancing inflationary pressures and efficiency gains using DEA efficiency results as input to price evaluations. Thanassoulis, for example, has published a series of papers (2000a; 2000b; 2002) on the use of DEA calculated efficiency measures to guide the pricing structure of private water and wastewater utilities in Great Britain. In the first of these paper (2000a) a single input of operating expense was employed and five outputs were considered; i.e., number of connections, length of the mains with the distribution system, total water deliveries, measured and estimated water deliveries, and number of pipe bursts. Ten utilities combining potable water and wastewater units and 22 potable-water-only utilities were included in the study, In an earlier related study performed by OFWAT only the combined utilities (those providing both water and wastewater services) were used in defining the efficiency frontier, a fact which Thanassoulis in his paper demonstrates was a potentially a flawed premise because potable-water-only utilities tended to define the efficiency frontier. Aida et al. (1998) also performed water utility measurements using DEA to compare regional water utilities utilities, but did not evaluate the effects of ownership type. Lambert et al. (1993) used DEA to compare ownership type, but limited their study to public versus private utilities. Using a single output variable measuring total water production and four input variables measuring annual labor use, total energy use, financial value of material inputs, and total value of capital, Lambert et al. concluded that public utilities were more efficient overall. Anwandter and Ozuna (2002) concluded that neither decentralization nor the presence of an independent regulator provided any benefit in the efficiency of Mexican water utilities, and concluded that competition might have had a greater role in increasing efficiency.
None of the previous studies cited (Lambert et al. 1993; Aida et al. 1998; Thanassoulis 2000a) used non-discretionary type variables for connections, network length, or water delivery, thus tacitly assuming that the utility had some measure of control over these variables. The current study builds on these previous studies by using non-discretionary variable types for variables that are not controllable by the utilities, and by efficiency comparisons of utilities based on different water sources.
Three main objectives underlie the study reported here. The first objective was to compare the relative efficiencies of different water utility ownership types calculated using different measurement variables. Three main water utility ownership types were considered; public utilities owned by local or regional government, private not-for-profit utilities operated by non-governmental agencies, and private for-profit utilities operated by private enterprise. Ancillary water utilities, those utilities run as a side operation by a larger concern, were not evaluated because of their limited financial independence.
The second objective was to compare the efficiencies of groundwater source utilities to those of surface-water source utilities using various measurement variables. Although there is clearly a geographic constraint limiting unbounded selection of water source, unbiased efficiency data could be important for evaluation of future water resources and policy regarding development.
The final objective of this paper is to discuss variable selection by presenting a brief analysis of the measurement discernment of additional variables. In this section, we assessed differences in efficiency measurements for decision-making units analyzed with increasingly discrete measurements of output components. A detailed set of variables can discern a finer resolution of the efficiency measurement than can a sparse set of variables. However, a detailed set of variables also can artificially inflate the apparent efficiency of the decision-making units and can result in an incurred cost due to the need to measure each variable.
A series of DEA trials, each with unique input and output variables, were performed to identify variables significant to the efficiency measurement of each utility type. US EPA Community Water System Survey Data (EPA, 2002) was used as input into a DEA model to determine the relative efficiencies of water utilities. Comparisons between utility types were checked using the Wilcoxon-Mann-Whitney comparison of ranked efficiency measurements. Two comparisons were performed using the efficiency data obtained from each trial. The first comparison examined efficiency differences between water utility categories of different ownership type and water source. Three main utility ownership types are represented by the EPA Survey Data: public, private not-for-profit, and private for-profit, while water source was either ground water or surface water. Efficiency measurements varied depending on selection of input and output variables, but major trends and variables having a significant influence on efficiency rankings were identified. The second comparison was an inter-ownership evaluation of utility efficiency within each utility category type using different selections of input and output variables. The results of this evaluation were then used to identify significant variables for each utility category.
DEA provides a numerical non-arbitrary score of efficiency that can be employed to improve utility operations. Efficiency rankings can be used to guide utilities to improve operational efficiencies by providing operational targets and to identify best practices at highly efficient utilities. The DEA approach can also be used to identify treatment utilities that are efficient under their particular environmental conditions but which might not be considered efficient using traditional metrics, e.g. expenditure and treatment volume. The DEA procedure requires numerical measurement data for all appropriate input and output variables. Each utility is evaluated with respect to peer utilities using unique sets of measurement variables. The DEA approach defines an efficiency frontier consisting of all fully efficient utilities, and an efficiency score is calculated for all non-efficient utilities based on their relative distance from the efficiency frontier.
The efficiency score is a non-arbitrary value based on the relative amount of inputs and outputs respectively used and produced by each utility. The DEA procedure can be either input or output oriented. An input oriented DEA model assigns the most efficient DMUs an efficiency score of one, and assigns the less efficient DMUs an efficiency score between one and zero representing the fraction of their original input they could use to still produce as much output as their peer DMUs if they were as efficient as the most efficient DMUs. An output oriented DEA model assigns the most efficient DMUs an efficiency score of one and assigns all of the other DMUs efficiency scores greater than one representing the fraction increase in output they could achieve with the same input if they were as efficient as the most efficient DMUs. Mathematically, the DEA analysis is performed by optimizing a series of linear programming equations by varying the relative weights of the measurement variables. Additional background and methodology can be found in Cooper et al. (2000).
This efficiency analysis was performed using an input-oriented non-discretionary (non-controllable) output approach. This approach was dictated by the fundamental environment of utility service requirements. Water utilities are constrained to fulfill customer requirements. They do not have the option, for instance, to reduce the number of connections within their distribution system. Since it is not reasonable to quantify water utility efficiency based on improving output for a given input, the input-oriented approach was selected.
In some cases it was not clear whether a variable is an input or an output parameter. In these cases, it was considered an output variable if an increase in its value required more efficient management ability in order to maintain all other variables constant. For instance, if two systems were the same in all aspects except total number of connections, the system having greater connections must be more efficient.
The data used in this analysis was obtained from the US EPA Community Water System Survey Data (EPA, 2002). Due to blanks entries and the presence of illogical data a significant amount of review was performed to obtain a suitable data set. Water treatment systems were removed from the set of DMUs when relevant data was missing or nonsensical. For example, systems which had either zero water delivery reported or which left this field blank were removed from the analysis.
In all cases, DMUs with data which were not obviously correctable were deleted from the analysis, even if other components of the specific DMU contained data which might have been useful. The result of removing such DMUs is that the DEA analysis might produce a conservative estimate of the efficiency frontier. All water systems with non-water revenues were removed from the analysis. Almost uniformly, systems which had non-water revenuepresent did not report an average residential bill, indicating that their production of water was an ancillary activity. Water quality data was not available and thus was not used as an efficiency ranking.
The final set of 714 utilities was comprised of 549 public, 96 private not-for-profit, and 62 private for-profit utilities, with 7 utilities either not reporting ownership information or were reported as ancillary operations without clear ownership structure. The 714 utilities were comprised of 389 utilities that used a ground water source and 325 utilities which used a surface water source.
The DEA analysis was performed using DEA-Solver Pro, an Excel add-in (Saitech, Inc., 2004) on a Dell Inspiron 5000.
The water utilities were analyzed for efficiency differences based on ownership type and water source. The three ownership types, public, private not-for-profit, and private for-profit, and the two water sources, ground and surface, had their efficiencies calculated with DEA using several variable sets. The influence of each variable set on the efficiency ranking was then determined by comparing the relative efficiency of the utilities within each utility type to the relative efficiency of the utilities within the other utility types. Comparisons between utility types were checked using the Wilcoxon-Mann-Whitney (WMW) ranked-sum comparison of the efficiency measurements. The WMW comparison was used because the underlying distribution of DEA efficiency measurements is unknown and thus a non-parametric statistic was required.
Because we used the WMW rank-sum test to compare the efficiencies of the two populations, the negative t-statistic means that we can claim the first population was generally more efficient than the second population, while a positive t-statistic means that we can claim the first population was generally less efficient than the second population. The significance level of this claim is calculated from the t-statistic using the inverse of the standard normal distribution.
The utilities were analyzed with DEA using thirty eight different sets of output variables, as shown along the right side of Figure 1-Public Versus Private not-for-profit. The three other comparisons also had figures generated for analysis but were not included due to space limitations. The output variables were selected from four main categories: age and length of distribution system and various combinations of connections and treatment volume. The age and length categories were the average age of the distribution system, and the reported length of the distribution system, respectively. For connections, the categories were total number of connections (shown as " total" on the figures), a partial separation into residential and non-residential connections (shown as "res/non-res" on the figures), and then a complete separation into residential, industrial/commercial, agriculture, and other connections (shown as "RCAO" on the figures). For volume, the categories were total flow (shown as total on the figures), a partial separation into residential and non-residential flow (shown as "res/non-res" on the figures), and then a complete separation into residential, industrial/commercial, agriculture, unaccounted for water loss, and other flow demands (shown as "RCAOU" on the figures). Each DEA efficiency analysis was performed using the entire combined set of 714 utilities of all ownership categories and used the described selection of variables to calculate an efficiency score for each utility. For each efficiency analysis, the input variables were annual expenses and average 5-year capital investments.
After the DEA efficiency analysis was performed for the combined set of ownership categories, the WMW comparison t-statistic was calculated between the utilities of each of the three ownership sub-categories: private not-for-profit versus private for-profit; private for-profit versus public; and public versus private not-for-profit, and the two water source categories, ground water versus surface water. In order to perform each WMW comparison, the efficiency data for the combined set of utilities was separated into the relevant sub-categories, and the efficiency data for the utilities in one of the sub-categories was then compared against the efficiency data for the utilities in another one of the subcategories. A t-statistic for the comparison was then calculated.
Figure 1 shows the results of efficiency comparison for the public versus private not-for-profit ownership categories. Figures for the other comparisons: private not-for-profit versus private for-profit, private for-profit versus public, and ground water source versus surface water source are not shown. The WMW t-statistic for each set of variables was plotted from largest to smallest, with a description of the variables used for each efficiency analysis shown next to each t-statistic plot. The description of the four categories of variables used in the DEA analysis is shown as a series of columns along the right side of the plot.
The ranked efficiencies of public utilities versus the private not-for-profit utilities generally showed a moderate yet statistically significant advantage of the public utilities over the private not-for-profit utilities for the majority of cases, as shown in Fig. 1. Twelve of the variable sets used for efficiency analysis indicated that private not-for-profit utilities were more efficient, while twenty-six of the variable sets used for efficiency analysis indicated that the public utilities were more efficient.
Only two of the variable sets used for efficiency analysis that showed greater efficiency from the private not-for-profit utilities had more than 90 percent significance. Similar to the previous comparison of private for-profit utilities versus public utilities, the variable sets which showed the least public utility efficiency were either distribution network length and average pipeline age, or length and age combined with total water volume. Both showed a statistically significant (greater than 90 percent) efficiency advantage of the private not-for-profit utilities over the public utilities. It should be noted that the variable set of distribution network length and average pipeline age used to compare the public utilities against the private not-for-profit utilities resulted in the most significant t-statistic of any of the variable sets used for any of the utility comparisons, with essentially 100 percent significance.
In addition, the top eight variable sets showing the greatest efficiency of the private not-for-profit utilities all used distribution network length and average pipeline age as measurement variables. In contrast, none of the top eight variable sets which indicated the greatest efficiency of the public utilities used average network length as a measurement variable and only three of the eight used average distribution network age as a measurement variable.
Eleven of the variable sets used for efficiency analysis which showed greater efficiency from the public utilities had more than 90 percent significance. All but one of these variable sets measured some combination of both network connections and water volume delivery. More than half of these cases used the most discrete measurement possible of both network connections and water volume delivery in the form of either residential, industrial/commercial, agriculture, andother connections, or residential, industrial/commercial, agriculture, unaccounted for water loss, and other flow demands. By comparison, none of the variable sets which showed greater efficiency from the private not-for-profit utilities used either connection data or water volume data at this level of detail. Thus, it appears that public water utilities are more efficient than private not-for-profit utilities at serving a variety of customer types, and are not as efficient at serving a single customer type.
The ranked efficiencies of private not-for-profit utilities versus the private for-profit utilities showed a moderate yet statistically significant advantage of the private not-for-profit utilities over the private for-profit utilities for almost all cases. Only six sets of output variables showed greater efficiency of the private for-profit utilities, with the largest difference in efficiency being when using residential and non-residential connections and residential and nonresidential treatment volume as output variables. The comparison when using the most pro-private for-profit utility variable set had a t-statistic of 0.39, indicating only a 30.6 percent chance of true difference between utilities measured using these variables, which is not a statistically significant difference. None of the variable sets which indicated that private for-profit utilities were more efficient used distribution system length as an output variable. This implies that the systems with the greatest distribution system length were the private not-for-profit systems and that they would use less input, in the form of capital investment and yearly expenses, to manage a distribution system of any particular length.
By comparison, 32 variable sets showed that the private not-for-profit utilities are more efficient than the private for-profit water utilities. The largest t-statistic showing greater private not-for-profit efficiency was -4.40, and resulted from using distribution network length and average pipeline age as output variables. This result is fairly uninteresting since water utilities are typically valued by the quantity of water they produce, measured by flow volume, and the number of customers they serve, measured by number of connections. However, the next highest t-statistic, -2.12, resulted from using distribution network length, average pipeline age, and total treatment volume as output variables, and was statistically significant with a 96.6 percent probability of difference. Three more comparisons also demonstrated that the private not-for-profit utilities are more efficient than the private for-profit water utilities with greater than 90 percent significance.
The ranked efficiencies of private for-profit utilities versus public utilities showed a strong, statistically significant advantage of the public utilities over the private for-profit utilities for all but two cases. The only variable sets used for efficiency analysis which resulted in private for-profit utilities being more efficient than public utilities were when using distribution network length and average pipeline age, either alone or combined with total water volume, with 99.6 and 67.6 percent significance respectively. Every variable set which used some measure of connections within the distribution system to measure efficiency resulted in the public utilities being evaluated as more efficient than the private for-profit utilities. There were 25 variable sets used to measure efficiency which indicated greater efficiency of public utilities over private for-profit utilities with greater then 90 percent significance. None of the top ten variable sets which showed the greatest public utility efficiency used distribution system length as a measurement variable. However, 13 of the data sets which used distribution system length still showed greater public utility efficiency with significance greater than 90 percent. It appears that using distribution system length as a measurement variable tends to moderately decrease the efficiency of private for-profit water utilities compared to public water utilities, but the effect isn't enough to completely overcome the efficiency effects of the rest of the variables. The use of completely separated connection or flow variables tended to push the efficiencies towards the public utilities.
The ranked efficiencies of ground water utilities versus surface water utilities generally showed a slight, yet statistically significant advantage of the ground water utilities over the surface water utilities for the majority of cases, as shown in Fig. 4. Overall, there were twelve variable sets that demonstrated an efficiency advantage to surface water utilities while there were twenty-six variables sets that showed efficiency advantage to ground water utilities. However, eight of the variable sets that showed a surface water utility advantage and nine of the variable sets that showed a ground water utility advantage had less than 50 percent significance, indicating that there was a greater than 50-50 chance that these data sets were identical.
Only one variable set used for efficiency measurement that demonstrated an efficiency advantage to surface water utilities had greater than 90 percent significance, while nine variables sets that showed efficiency advantage to ground water utilities had greater than 90 percent significance.
The only statistically significant variable set which indicated surface water utility efficiency advantage was when total water delivery was used as the sole measurement variable. Adding additional measurement variables which would account for either distribution system length, or numbers or types of customers, all caused a reduction in surface water utility efficiency and an increase in ground water utility efficiency. This implies that surface water utilities are most efficient when they have a few, high-volume, customers such as irrigation or large industrial demands.
However, another data trend indicates potentially contrasting results, and that is when the water delivery category was fully broken down into the component variables of residential, commercial/industrial, agricultural, other, and unaccounted for water deliveries. DEA efficiency measurements which used these variables tended to show a moderate trend towards ground water utilities compared to surface water utilities, with seven of the measurements indicating the superior efficiency of surface water utilities, and only three indicting the efficiency of ground water utilities. This result does not indicate a strong trend, since none of the results were at high levels of significance, but does indicate a mild trend towards the efficiency of surface water utilities over ground water utilities when delivering a lot of water to a wide variety of customers.
Since tracking and maintaining information variables entails a cost, it is reasonable to discuss variable selection criteria. The essential question is: What variables reasonably add new information? As part of this study, we briefly investigated the additional information provided as a result of additional measurement variables by using an informational surrogate which measured the informational spread in efficiency ranking obtained when using additional variables. Informational spread γ was defined as
(1)
where a and b are the utility rankings using two sets of measurement variables A and B. Spread approximates the difference in informational content measured by two variable sets because it measures the absolute difference in efficiency ranking due to the change in measurement variables. Large changes in efficiency ranking between variable sets imply a large informational difference between the variable sets and would result in a large calculated spread. Correspondingly, small or random changes in efficiency ranking between variable sets imply a small informational difference between the variable sets and would result in a small calculated spread.
For this investigation, the information spread between different measurement variables was determined for a series of variable sets that were kept constant for all but one measurement category. Each variable set used for efficiency measurement consisted of distribution network length, average pipeline age, and variables from both the connection and volume categories. All the variables measuring length, age, and one of the two remaining categories were kept constant, and the base efficiency scores for the utilities were determined by excluding the remaining measurement category. The remaining variable category then was increased to a single value representing the total value for that category, then to a partial separation into the residential and non-residential values for that category, and then to a complete separation for that category. The informational spread was calculated between the basic case, which did not include the varying category, and between the more advanced cases, which did include the varying category. The informational spread was plotted against the number of new variables added for each case, as shown in Fig. 2. The legend shows the number of variables in the basic case, while the x-axis shows the number of additional variables added for each analysis. There is clearly a trend towards marginal returns as the variable categories are broken down into more discrete measurement. The only outlier is the line representing the increase in volume measurement when length, age, and RCAO connections were kept constant. This plot reveals the decreasing marginal return on information gained from using a more discrete measurement of any particular data category.
The work reported here reveals a distinct efficiency advantage of public utilities over private for-profit utilities. Every analysis that used some measure of number of connections within a distribution system resulted in public utilities being evaluated as more efficient than private for-profit utilities. In comparing public utilities to private not-for-profit utilities, a much more moderate yet statistically significant efficiency advantage was evident for the former in the majority of cases studied. None of the variable set cases that showed greater efficiency for private not-for-profit utilities used either connection data or water volume data in full detail. The results also indicate that while public water utilities are more efficient than private not-for-profit utilities for serving a variety of customer types, the latter are more efficient for serving a single customer type. Private not-for-profit utilities were found to have a statistically significant efficiency advantage over private for-profit utilities for almost all selections of management variables, particularly for managing larger distribution networks.
Comparisons of ground water source utilities versus surface water source utilities generally showed a slight, yet statistically significant, efficiency advantage for the former in the majority of cases studied. Utilities employing surface-water sources are most efficient when they serve a few, high-volume demand consumers, such as irrigation or large industrial systems, while ground-water source utilities tend to be more efficient when delivering large volumes of water to a wide variety of different types of consumers..
Finally, informational spread behavior as a function of measurement variables employed indicates decreasing marginal returns on information gained from using more discrete measurements of any particular data category.
The authors wish to acknowledge Lawrence M. Seiford, Chairman of the Department of Industrial and Operations Engineering, Warren Sutton, doctoral candidate in the Department of Industrial and Operations Engineering, and Jill Ostrowski, undergraduate in the Department of Civil and Environmental Engineering, for their critical appraisal and evaluation of the manuscript.

Using data collected straight from test subjects can be extremely helpful in formulating theories about second language acquisition. My goals in this research is to analyse the speech of a student of German. I plan to do more in depth into certain aspects of his language skills as well as determine his overall level of speaking fluency as based on the definition given on page 14 of our MELAB booklet.
My subject, who wishes to be called Max, is a 24-year-old program manager for an international company whose first language is English. He began learning the German language by himself at the age of 20 through reading and listening to cassette tapes. During the next two semesters of college after a summer of this self-teaching, he enrolled in a total of 18 credit hours of German and advanced rapidly in his language skills. Max was exposed to a variety of teaching methods at that time.
Feeling that he was then ready to gain some practical experience in a country where the language was spoken, he enrolled in an exchange program and spent a year in Munich studying at the university there and taking specially-designed classes through his program. He was in an immersion-style setting involving classes only taught in German but with students who all spoke the same first language and could therefore help each other with difficult aspects of grammar and pronunciation.
Today, Max no longer uses his German skills often and admits that they may have grown a bit rusty, although he does still speak with a perfect German accent. He still enjoys speaking the language and had a high personal motivation to learn it in the first place because language learning is a favorite pasttime of his.
I have rented an audio cassette player from LS&A Media to record the verbal interraction between myself and Max. Other resources include parts of the readings discussed throughout the semester, as well as the MELAB booklet and handouts given out in class. I will also make use of the transcript of my recorded interview with Max for my analysis of his language skills.
One of the most important parts of the German language that does not exist in English is the presence of grammatical gender. As is the case with many other languages, nouns in German have had a gender arbitrarily assigned to them. This is one of the hardest aspects of German to learn because the monolingual English speaker has no experience with such things; and the English speaker with some contact with other languages will often also have difficulties because the genders of nouns may differ be different in German (i.e. el puente in Spanish, and die Brücke in German). I have chosen to take a more in-depth look into Max's use of indefinite articles since they came up more frequently than the definite articles and are similar in form and equally difficult to master. To make my data more easily intelligible, here is a chart of the indefinite articles of German:
For my analysis, I have taken several random samples from the pages of the transcript of my interview to look at the accuracy rate in Max's speech. These examples are representative of a general trend that I have found in his grammar. While Max does seem to have a basic grasp of the indefinite article, the more complicated and less-used forms still seem to allude him. Nominative tenses are correct most of the time, while accusative is correct some of the time, and dative and genitive are often incorrect. This leads me to believe that Max has memorized the basic (nominative) genders of the nouns but does not know how to decline them well. A glance at the definite and possessive articles in the transcript confirms this.
Verb placement is often difficult for second language learners because they must learn a new set of rules. In German, the verbs often occur in two different places in the sentence, with the modal verb before or after the subject (depending on whether it follows another item or not) or even at the end of a sentence (if the sentence begins with certain specific prepositions). To make this easier, all verbs will be in bold.
This data shows that while Max does see that there is a possibility in German for a more varied verb placement, he does not know how and where to impliment it. All the sentences in the example were chosen because they should have the standard SVOV order. Some of them do and some don't, reflecting Max's evident confusion with this aspect of the grammar.
Another aspect of the verbs in German is that the past perfect can be conjugated using either to have or to be. For example, he has come in English would be er ist gekommen (he is come) in German. Verbs using the to be form usually pertain to motion or movement from one place or position to another.
Here we see that Max is having problems distinguishing the to be verbs from the to have verbs. This probably is taken from his first language because English does not allow for to be verbs. Max rarely uses the to be form, even with the German verbs that call for it.
Using page 14 of the MELAB booklet given out in class, I would rank Max as a good speaker, but with a 3-, meaning that he is much closer to being a marginal/fair speaker than an excellent speaker. I chose this ranking because Max understood everything I asked him and could produce utterances easily and without accent. The reason he is not an excellent speaker is that he seems to have not yet grasped many of the aspects of the difficult grammar of German. He also sometimes had trouble finding the right word and often resorted to code-switching during our interview. I would suggest a more in-depth study of articles and sentence structure for Max, as well as more time spent in Germany. Being exposed directly to the language would help his German advance, and I am convinced that in time he would speak almost perfect German.

Over the past 20 years or so, hedging has become an increasingly well-researched aspect of academic writing. As with any identifiable aspect of academic writing, much of the research on hedging attempts to define it, theoretically and functionally. Because of the negative treatment hedging has received in the past, many studies (e.g. Skelton 1988a and 1988b; Myers 1996; Channell 1990; Banks 1998; Hyland 1994 and 1998) aim to validate the presence and legitimacy of hedges in academic writing. Other research (e.g. Hyland 1994 and 1998) has offered advice on how best to teach hedging in an EAP context. Research has been undertaken on the pragmatics of hedging and its link to politeness, its social implications, and how it affects the negotiation of meaning between writer and reader (e.g. R. Lakoff 1972; Myers 1996; Salager-Meyer 1994). Several contrastive rhetoric studies have looked at hedging in different cultures (eg Martìn-Martìn & Burgess 2004) and the possible linguistic transfer that may result from attempts to hedge in the L2 (e.g. Clyne 1991; Hinkel 1997). Some attention has been paid to the strength and presence of hedging and the variations thereof in certain genres (such as the IMRD pattern for research papers) (Salager-Meyer 1994; Martìn-Martìn & Burgess 2004; Banks 1994b). However, one issue that has not received as much attention in the literature as it perhaps warrants is that of multiple hedging (using more than one hedge in a given statement, such as in "this may suggest..." or "this could perhaps be..."). The fact that multiple hedging does indeed occur is evident from a look at almost any piece of academic writing, and it has not been entirely ignored in the research. Many studies mention it in passing; however, few devote any significant amount of space or time to its study. What this lack of focused attention leaves unclear is just how often multiple hedging occurs, whether or not it is considered acceptable (and if so how many hedges must be used before multiple hedging becomes overhedging), and what, if any, factors, such as level of education, native vs. non-native speaker status, etc, may affect the strength or amount of a given writer's use of hedges. In this paper, I will look specifically at multiple hedging as a phenomenon of academic writing. I will start by providing an extensive review of the literature on hedging, focusing first on how hedging has been defined, and gathering from these different definitions a working definition to apply to my own research; and focusing secondly on how the notions of overhedging and underhedging have been addressed in the literature, in order to see if multiple hedging has received any sort of value judgment by the academic community. After this literature review, I will turn to my own research: a look at multiple hedging in the Hyland Corpus of academic text (which attempts to answer the question of how often multiple hedging occurs) and a survey which attempts to assess the evaluation of hedging expressions of various levels of strength by different academic groups (this survey attempts to address the acceptability of multiple hedging and the factors that may affect the strength of a writer's hedging expressions). I conclude by summarizing the results of my research and exploring how these results may be useful to the academic community.
George Lakoff introduced the term hedging in 1972 as a way to refer to "words whose job it is to make things more or less fuzzy." Lakoff's introduction of the term did not spark much research, however, and in 1988 the subject was still "scarcely touched on in ELT" (Skelton, 1988b, p. 98). But since then, hedges have been written about relatively frequently in the literature, with a significant amount of attention paid to both hedges in speech and hedges in writing. Unsurprisingly, given the slippery nature of pragmalinguistic phenomena in general, an agreed-upon description of what linguistic devices count as hedges -- and more significantly what do not -- does not emerge from the substantial literature on this topic. What do exist are varied definitions of hedging, which of course result in correspondingly varied taxonomies. While the definitions given by most researchers are not always so different as to be mutually exclusive, and while all the taxonomies given have at least some corresponding items, it is nevertheless necessary to clarify which (if any) researcher's definition and taxonomy I will be using and to justify why I have chosen it. I will not necessarily be trying to construct the most sophisticated definition, but rather to give the concept sufficiently clear boundaries so that it can be operationalized in analytic practice. That is, I am by no means trying to construct my own definition of hedging; I wish simply to find a clear working definition to use for my research -- research which, in part, involves the location and identification of hedging devices in academic text.
First, it is helpful to give an overview of how hedging has been looked at in the literature and what controversies or disagreements exist when it comes to finding a precise definition. George Lakoff's (1972) description of hedges, which, as I've mentioned, was the first to be introduced in the field of linguistics, as "words whose job it is to make things more or less fuzzy," is -- though of course an important and useful one sparking years of research -- itself problematic. One problem lies with Lakoff's use of the expression "more or less." All other research has considered hedges to be words which make things more fuzzy -- not less fuzzy. Making things less fuzzy is now thought to be a different linguistic concept, with different pragmatic and linguistic properties; Hyland (eg 2004) calls these items designed to make words less fuzzy "boosters." If we disregard this part of Lakoff's definition, we are left thinking of hedges as words which make things more fuzzy. This may appear to be a simple and straightforward definition on first glance, but with a bit of thought it becomes clear that the word fuzzy as it relates to hedging is, well, fuzzy. What is meant by Lakoff's little adjective and the linguistic phenomenon it attempts to describe has been a hot topic of debate in the research literature on hedging.
There is of course a certain level of correspondence in the research on hedging: all authors seem to be in agreement that written hedges express an attitude of uncertainty on the part of the author and/or a degree of non-commitment to the truth of a given proposition. However, the agreement stops with attempts at further, more precise definitions and to an even larger extent with attempts to come up with a definitive group of linguistic expressions to be considered as hedges. This can be seen by a quick examination of some of the principal research. The relatively early study by Prince et al. (1982) classifies hedges into two different groups: approximators which show uncertainty/non-commitment "within the propositional content proper" and shields, which show uncertainty/non-commitment "in the relationship between the propositional content and the speaker" (p. 86). While Prince et al. focus on spoken academic discourse, research in academic writing discusses similar types of hedges (see for example Salager-Meyer 1994).
Skelton (1998b), who prefers to call hedges "comments" (to avoid the "pejorative connotation" of the word "hedge"), presents three categories into which comments fall: Type one comments indicate tentativeness or levels of probability and include copulas other than be; modal auxiliaries; adjectivals and adverbials introduced by It is, This is, There is, or which are sentence or clause-initial and immediately followed by a comma; and lexical verbs. While Skelton focuses by far the most attention on Type 1 comments, he also discusses Type 2 comments which "are concerned with the way adjectivals and adverbials in general function in academic text," and Type 3 comments, which deal with the "association between commentative language and stylistic markedness" (p. 102).
In some contrast, a few authors focus on hedging's pragmatic function and its link to politeness, such as R. Lakoff (1972) and Myers (1989), who argues that hedges are a social phenomenon and are perhaps best understood as politeness strategies. Myers's taxonomy includes modal conditional verbs and modifiers and, perhaps a bit too broadly, "any device suggesting alternatives [...] -- anything but a statement with a form of "to be" that such and such is the case" (p. 13). Salager-Meyer (1994), who adopts a functional approach in her attempt to define hedging, deals with the concept of modesty (related to politeness) and believes that research on hedging should give more emphasis to the fact that hedges are "the product of a mental attitude" (p. 152). She adopts for her research a definition that "goes beyond [...] mere association with speculation" and entails "purposive fuzziness and vagueness (threat-minimizing strategy); that which reflects the authors' modesty for their achievements and avoidance of personal involvement; and that related to the impossibility or unwillingness of reaching absolute accuracy and of quantifying all the phenomena under observation" (p. 153). Her corresponding taxonomy includes five categories: shields, which include modal verbs expressing possibility, "semi-auxiliaries" (or copulas other than be), probability adverbs and adjectives, and epistemic lexical verbs; approximators; words which "express the author's personal doubt and direct involvement"; emotionally charged intensifiers; and compound hedges (p. 154-155).
Hyland (1994, 1998) focuses on hedging's link to epistemic modality, defined by Lyons as "any utterance in which the speaker explicitly qualifies his commitment to the truth of the proposition expressed by the sentence he utters" (cited in Hyland 1994, p. 240). Hyland also acknowledges hedging's social function saying that it's "a central means of gaining communal adherence to knowledge claims" (p. 241). In other words, hedging is a way for scholars to situate themselves within their discourse communities. According to Hyland, hedging "reflect[s] a relation between a writer and readers, not only the degree of probability of a statement" (p. 241). Hyland's taxonomy of hedging devices is quite extensive (especially in Hyland 1998) and includes modal auxiliaries, adjectival, adverbial, and nominal modal expressions, modal lexical verbs, IF-clauses, question forms, passivization, impersonal phrases, and time reference (1994, p. 240). Hyland (1994) also clarifies that "Many instances of hedging take unpredictable forms, for example by referring to the uncertain status of information, the limitations of a model, or the absence of knowledge" (p. 243). Hyland's taxonomy, then, does not just include specific linguistic forms, but a range of possible expressions.
Peter Crompton (1997), who believes that the lack of agreement among researchers when it comes to precisely defining hedging as a linguistic phenomenon is problematic, proposes a "narrower" definition of hedging: "A hedge is an item of language which a speaker uses to explicitly qualify his/her lack of commitment to the truth of a proposition he/she utters" (p. 281). In order to determine whether or not a given proposition involves the use of hedging, Crompton proposes the following diagnostic question: "Can the proposition be restated in such a way that it is not changed but that the author's commitment to it is greater than at present? If "yes," then the proposition is hedged" (p. 282). Crompton includes the following in his taxonomy of hedges: copulas other than be, epistemic modals, clauses and adverbials relating to the probability of the subsequent proposition being true, "reported propositions where the author(s) can be taken to be responsible for any tentativeness in the verbal group, or non-use of factive reporting verbs," and "reported proposition[s] that a hypothesized entity X exists and the author(s) can be taken to be responsible for making the hypothesis" (p. 284).
Overall, we can see that, although the theoretical standpoint behind each may vary to a certain extent, the fundamental base of these researchers' definitions of hedging is essentially the same: all involve expressing degrees of uncertainty (I believe that both uncertainty used intentionally as a way to avoid responsibility and uncertainty expressed because more precise figures are unavailable require hedging devices, though some researchers would disagree); and all the definitions I examined give some attention to hedging's pragmatic and/or social functions. It is the terminology and the taxonomies of hedging expressions which differ to such an extent that I must pick and choose among them in order to do my own research. Because my work involves the use of a corpus and, more specifically, searching for instances of hedging within the corpus, it is helpful to come up with a list of which linguistic expressions I will be considering in my search. In an attempt to be as accurate as possible and to remain fairly neutral, I have chosen to accept as hedges those linguistic items listed in Crompton (1997) as discussed by two or more researchers (p. 280). These include copulas other than be, certain lexical verbs used in certain contexts, modal verbs, and probability adverbs and adjectives. It is also necessary to define more specifically which modal verbs and lexical verbs I will be using: I will consider only very "weak" lexical verbs (like suggest), so that I may be certain of their hedging content (for more discussion of this issue see Johns 2001), and only modal verbs which express epistemic uncertainty. Some further explanation is necessary here: One is able to say that something "can be done" -- meaning that it has been done, or is factually able to be done; in this case can is not a hedging device. On the other hand, one is able to say that something "may be relevant," in which case, may is expressing epistemic uncertainty on the part of the speaker and is therefore a hedging device. The same distinction applies to probability adverbs and adjectives: the expression "possible to" cannot be used to hedge, as it expresses factual ability; the expression "possible that" can be used to hedge, as it has epistemic content.
In addition to these more or less agreed upon linguistic categories, I will include several other items. The first of these is approximators, as described by Salager-Meyer (1994). I do so because, although Crompton claims Salager-Meyer is the only author to include this item in her taxonomy of hedging, many of the articles I've looked at (specifically those on vagueness in academic writing) also considered approximators as hedges. Also, approximators fit with the basic theoretical standpoint I'm adopting: the relatively agreed-upon idea that hedging is used to express degrees of (avoidable or non-avoidable) uncertainty. Next, I will include items which are semantically related to (but belong to a different word class than) the lexical verbs I consider as hedges (eg: implication, suggestion, etc). Crompton states that Hyland is the only researcher to do so, but I see no reason why I should not include these related words, as they have the same semantic content as the lexical verbs (and it is the semantic content of a word that creates a hedged effect, not that word's part of speech). Using the same reasoning, I will include words related to the copulas other than be (such as appearance and seemingly) and nouns of probability, when appropriate (such as possibility when clearly used in an epistemic sense). I will not include any other items mentioned by only one researcher, such as IF-clauses (Hyland, 1994, 1998) and "expressions of the authors' personal doubt and direct involvement" (Salager-Meyer, 1994).
I would also like to point out that I believe Crompton (1997) is correct when he says that "to count all uses of certain linguistic tokens as hedges, is to run the risk of misrepresenting the discourse" (p. 279). For this reason, I will attempt to consider not only the lexical items I take to be hedges, but their placement and meaning within their respective propositions, in order to verify that these expressions are indeed being used as hedges. By doing this, I hope to avoid some of the "misrepresent[ation of] the discourse" that Crompton speaks of. I of course acknowledge that introspection is not always a reliable form of data analysis, and is quite difficult when working with the large amount of data yielded by corpus research (especially research using Hyland's sizeable corpus of 240 academic research articles) but I believe it is better than somewhat blindly considering only surface forms without taking into account any of their semantic context.
The hedging categories, with some obvious linguistic realizations, are therefore as follows:
From here, I will move on to a discussion of how multiple hedging has been looked at in the literature and how these explorations by other researchers relate to the current research.
Views on hedging as a phenomenon of academic writing have undergone a dramatic shift within the past 15 to 20 years. Skelton (1988b) shows that at the point in time of his article, hedges were quite strongly looked down upon, the term hedging carrying "unfortunately pejorative connotations in ordinary language" (p. 98). In fact, the idea that hedges should be used -- not avoided -- is presented by Skelton as an almost radical idea. (Today, at least among academic researchers "in the know," this idea is accepted without question, perhaps even taken for granted.) Over time, more and more articles justifying the legitimacy of hedging in academic writing have been published and today the term hedging does not carry as strong of a "pejorative connotation." Unfortunately, all is not as well as it should be. To this day, many style guides still advise strongly against hedging; the widely used Strunk and White (1979), for example, calls qualifiers (another words for hedges) "the leeches that infest the pond of prose, sucking the blood of words" (p. 73). This comment could perhaps be appropriate in talking about business or technical writing, which aims for clarity, precision, and conciseness, but it hardly seems appropriate for academic genres, where hedges often have an essential role (for more examples of this inappropriate type of negative treatment, see Hyland 1994 and 1998). And, according to Hyland, at least as of 1998, many ESP/EAP textbooks have failed to give an adequate amount of attention to hedging (see Hyland 1994 and 1998). If our view of what constitutes good academic writing are based on authentic texts and the discourse practices involved with those texts rather than prescriptive rules determined by some (often unknown or unnamed) authority's idealized version of a language (and I quite think they should be), then we cannot ignore or deny the legitimacy of hedging's presence, whether in speech or writing. Indeed, the importance of hedging has been demonstrated, with increasing frequency over the last 20 years or so. The most extensive of these works devoted to demonstrating the validity of hedging and to defining some of its characteristics is Ken Hyland's book Hedging in Scientific Research Articles (1998). Other works include Banks (1998), Myers (1996), Hyland (1994), Skelton (1988a), and the previously mentioned Skelton (1988b).
It will be useful at this point to see where multiple hedging fits in through a further examination of some of the justifications of hedging provided in the literature. Hyland (1994) focuses on arguing that the strong presence of hedging in academic writing requires an equivalently strong amount of attention in EAP textbooks -- an amount of attention which, in Hyland's view, is rarely found. By this point in time, hedging has found its place in the research literature, but according to Hyland, not much has been done to place the research done into practice. Hyland stresses that teaching materials should be revised to place an appropriate amount of emphasis on hedging. In his view, the problem students face is that they underhedge; he does not mention overhedging as a potential problem. One might feel here that Hyland is perhaps overstating the need for focus on hedging in EAP textbooks. While hedging does occur very frequently and should, of course, be given some attention in any well-developed set of language teaching materials, it is possible that hedging does not receive more focus because it is not extraordinarily difficult for students to accomplish some base level of hedging, like the use of modal and lexical verbs, for example. It's likely that a relatively short amount of coverage would suffice. This does not take away, however, from Hyland's point that the negative attitude towards hedging readily encountered in the past is problematic and that EAP materials should work to counter any negative associations with this useful phenomenon. Also, his point that a lack of attention may lead to underhedging is potentially valid and should be taken into consideration.
Other justifications are provided by authors' work on vagueness (which is not quite the same as hedging, though definitely related, as it can be used to moderate levels of certainty). David Banks (1998) writes about the importance of vagueness, focusing on quantification in scientific journal articles. Here, the focus is on dispelling the myth that scientific writing should strive for total objectivity and should avoid vagueness. According to Banks, a certain level of vagueness is not only acceptable in scientific discourse, it is necessary. He implies that previous assumptions which do not consider vagueness as an essential part of scientific writing should be reconfigured, stating that "it is of upmost importance that we do not allow our understanding of vague quantification in the scientific journal article to remain vague" (p. 26). He does not, however, mention the use of multiple expressions of vagueness or being overly vague.
Myers (1996) also discusses the legitimacy of vagueness, arguing that vagueness can be used strategically in academic writing. Instead of teachers telling students and editors telling authors that vagueness is to be avoided at all costs, an attempt should be made to understand how vagueness can work as a useful technique that extends the scope of a text through time and space, thus giving the writer a larger role in the reader's interpretation. Myers points out that there is such a thing as being too vague, but it's difficult to pin down. As he says "Vagueness is appropriate in some contexts, but writers have to understand how and why they are using it." (p. 12) He claims that people, especially teachers, need to take time to better define the notion of vagueness, allowing room for its intentional and appropriate use: "The question to ask in teaching is not whether an expression is vague but: For whom is it vague? Vague as opposed to what other expression? To what end is it vague?" (p. 12).
Channell (1990) also argues in favor of vagueness, saying that "the view that the precision of science and scientific language depends upon the precision of its terms is certainly very plausible, but it is none the less, I believe, a mere prejudice" (p. 95). While she does not specifically deal with multiple hedging, or over/underhedging, Channell stresses the need for a clear understanding of using vagueness appropriately to be imparted to students: "Clearly, prospective writers [...] need to develop a [...] sensitivity to the conventions relating to precision versus vagueness" (p. 117). In her view, what is most important is emphasizing to students that "vagueness is appropriate in certain circumstances" (p. 117).
It seems that these articles, especially Hyland (1994) and Banks (1998), are so intent on arguing against the contention that hedging is to be avoided (an understandable position since, as we have seen, hedging still needs a lot of arguing for) that overhedging isn't really mentioned. John Skelton (1988a), who suggests that hedges should be broken down into two distinct classes, comments and propositions, also argues for the general importance of hedging to academic discourse. However, unlike the previously discussed authors, Skelton is quick to point out that, although a lack of sufficient attention given to hedging can certainly lead to underhedging, it may, just as importantly, lead to overhedging as well. Undergraduate students in the sciences may underhedge because they are told to be "objective and propositional," resulting in overconfident-sounding claims like "The jump in the reading was enormous and a great surprise" (p. 40). On the other end of the spectrum, the "contract of inexactitude" that hedges (or for Skelton "comments") attempt to create between writer and reader can fail to be upheld by a writer who presents a large number of comments in a given chunk of text. He cites the following example from the textbook Reading and Thinking in English (British Council 1980):
"Before we consider what mechanisms could possibly underlie the metal-bending effect, we must first help ourselves by starting with a brief summary of what it is that we shall have to try to explain. The multitude of the accounts make it clear that we are dealing with a genuine effect which can happen sometimes as a result of direct contact with a subject, and sometimes without it. The main action in the case of direct contact appears to be that of gentle stoking by the fingers of one hand. The length of time taken to cause an appreciable bend seems to vary, but it is normally less than thirty minutes and more than two or three; moreover, for a particular subject it can vary considerably from one day to the next." (cited in Skelton 1988a, p. 40).
It's interesting to note here that not only are a large number of hedging expressions used, but the presentation of the data is also quite inconclusive and unhelpful. One can also see that the writer uses hedging devices when they are not at all necessary, for instance by saying that the length of time "seems to vary" and then giving direct evidence that it does indeed vary. Skelton clarifies that this overhedging is not as apparent or as off-putting to the reader as underhedging but is still problematic as it will tend to give the reader the impression that the writing is too vague. Skelton cites a "typical student response" to the previously quoted passage: "It's all up in the air, no facts, real...it's all maybe this, maybe that, he never gets down to it. It's just vague" (p. 41). As far as how much and what kinds of uncertainty are acceptable/appropriate, Skelton can only say that it is an area needing future research.
While the previously discussed articles have suggested that the lack of sufficient attention given to hedging in English style guides and EAP textbooks (and the assumption that scientific prose should be completely objective) mainly leads to underhedging, various contrastive rhetoric based studies (eg Clyne, 1991; Hinkel, 1997) suggest that NNSs writing in English have a tendency to hedge more than NS writers and that this may create an impression of excessive indirectness for NS readers. Clyne (1991), for example, shows that German-speaking scholars hedge more when writing in English than do English-speaking scholars. He claims that this is likely due to both the strong presence of hedging in German and a lack of comfort when writing in a foreign language. According to Clyne, the abundance of hedges in German authors' written English is not wrong, per se, but it can be problematic, for German-speaking author and English-speaking reader, because it does not meet the "expectations of discourse" of academic written English. (Whether or not NNSs should be expected to conform to the discourse expectations of academic written English is another issue -- undoubtedly important, but beyond the scope of this paper).
Hinkel (1997) also provides evidence supporting the idea that NNSs L1 hedging habits may interfere with their use of hedges in English. Her article posits that NSs and NNSs use different types of hedges at different rates, so that, while NNs use possibility hedges more than NNSs, and hedged performative verbs are used at the same rate by both groups, performative, lexical, and quality hedges are used more by NNSs. Hinkel seems to be in agreement with the consensus in saying that "what represents appropriate levels of indirectness in written and academic discourse is not always clear," (p. 363) also mentioning that "when students are instructed that English writing is expected to be direct, they often produce expository pieces so open and frank that they can be perceived as inappropriate" (p. 363). Hinkel's research highlights the bind that many NNSs writing in English doubtless find themselves in: on the one hand, the hedging practices for NNSs whose native languages are indirect and vague may influence these writers to transfer these qualities onto their English prose, resulting in a piece that might strike the reader as overhedged; on the other hand, these same students may underhedge because of attempts to change made by teachers to lessen the degree of these students' indirectness -- that is, precisely because they are told not to overhedge.
Level of education may also have an effect on strength and frequency of hedging, though there is no conclusive evidence. Shaw (2000) for example discusses a group of dissertation writers who tended to hedge claims that were already hedged (in other words, to use multiple hedges). For example, Shaw points out that the dissertation writers may be more likely to use phrases such as "suggesting that X may be Y" rather than "suggesting that X is Y" (p. 52). Although Shaw does not explicitly say so, this is perhaps worth remarking on because it may indicate that writers at different levels in their education may hedge with a different level of strength and frequency. In this case, the dissertation writers may be hedging more than their colleagues who have already attained a PhD. But Shaw doesn't say whether or not more experienced writers have the same tendency to add more hedges to already hedged claims, so it is difficult to tell if this is significant. It is also interesting to note here that, as mentioned above, Skelton (1988a) shows that undergraduate students in the sciences may tend to make overconfident-sounding claims. Suggestions have been made, then, though it remains to be seen how valid they are, that lower standing on the academic totem-pole can result in both underhedging and multiple/stronger levels of hedging (perhaps overhedging as well, though the hedging that Shaw discusses is not talked of as problematic). But again, one cannot draw conclusive claims from this somewhat sparse evidence.
The situation seems quite problematic, thus, for NNSs writing academic English and/or for inexperienced writers of academic English: on the one hand these not-yet-expert (but, for the most part, not-quite-novice) writers are expected to express uncertainty appropriately, without seeming overconfident or unable to see the validity of opposing evidence; on the other, they are expected to do so without going overboard and hedging all of their claims into obscurity. Exactly where this narrow margin of correct amount/intensity of hedging is found is difficult to determine. In fact, it seems that not even experienced linguists and discourse analysts seem to be able to pin it down precisely -- let alone relatively inexperienced writers (this is not a fault on the part of the experts: language itself does not take very well to matters of precision). In other words, the level of uncertainty a writer should adopt towards his or her expressions is just that: uncertain.
Some authors have attempted to delve into this subject by looking directly at levels of uncertainty in academic text. Françoise Salager-Meyer (1994) devotes some attention to the level of hedging considered acceptable in academic discourse (more specifically medical discourse, though her results can most likely be generalized, at least to other disciplines with a tendency to follow the IMRD pattern for writing research papers), claiming that "the choice of expression of tentativeness and flexibility is dictated by the general structure of the discourse" (p. 149). In other words, the level of acceptable hedging depends on the area of the paper and the type of publication. The results of her study indicate that the Discussion section is the most heavily hedged section of a paper and the Introduction section second most heavily hedged (the Methods and Results section tend to use much less hedging). She also discusses multiple hedging, which she refers to as "compound hedges." Though she does not explicitly discuss the question of whether or not compound hedges are acceptable, her treatment of the subject indicates that she believes they are (they are certainly used in the research papers and case reports that she uses for her study). In fact, she considers compound hedges as a category of hedging, that is as a type of hedge, and places them alongside other types of hedges (such as shields, approximators, and emotionally-charged intensifiers). Why she chooses to define these compound hedges as a separate category of hedging, despite the fact that compound hedges are made up of other categories, like shields and approximators, and have no distinguishing semantic features of their own, is unclear. Compound hedges follow the general trend she establishes, occurring most frequently in the Discussion section, relatively frequently in the Introduction section, and not very frequently in the Methods and Results sections. The reason for the strong presence of compound hedges in the Discussion section is, according to Salager-Meyer, due to the fact that, "It is in this last section of research papers that writers speculate, argue, contrast, and extrapolate from the described results, and at the same time avoid stating results too conclusively"(p. 163). The stronger need to demonstrate levels of uncertainty which occurs in the Discussion section calls for a greater amount of hedges. A strong level of uncertainty, thus, seems to be acceptable and even expected in research papers, but, Salager-Meyer points out, popularization articles and texts-books are expected to avoid any strong levels of hedging (she cites an example: " "this suggests the possibility" is replaced by "they discovered" " (p. 165)). Evidence supporting Salager-Meyer's proposal that the strongest and most frequent use of hedging appears in the Introduction and Conclusion/Discussion section is also validated by research done Martìn-Martìn and Burgess (2004) in their study on academic criticism in research article abstracts and by David Banks's (1994b) book chapter "Some Hedges."
In addition to Skelton (1988a) and Salager-Meyer (1994), several other researchers have dealt directly with multiple hedging. Clyne (1991), for example, directly focuses on multiple hedging, which he refers to most often as "double hedging" (also mentioning "triple hedging" once or twice). Clyne looks at the use of hedging in a corpus of 52 texts, a compilation of English texts written by both English-speaking and German-speaking authors and of German texts by Germans. As mentioned previously, the German-speaking authors hedge more than the English-speaking authors, both when writing in English and when writing in German, and this trend is the same for the use of multiple hedges. In fact, the results of Clyne's data indicate that the use of multiple hedging is a great deal lower for English-speaking authors. This is surprising, seeing as, for example, multiple hedges occurred so frequently in Salager-Meyer's (1994) study that she designated them as a distinct category of hedges and that David Banks (1994a and 1994b), as will be discussed in more detail later, sees double hedges so frequently that he is able to determine their collocation patterns -- certainly such an uncommon phenomenon as Clyne makes double hedging out to be wouldn't have its own set of collocations . One might guess that the large difference between use of hedging for German-speaking and English-speaking scholars results from extremely large amounts of hedges in German authors' work, but this is not the case, according to Clyne; hedging devices in texts by English speakers occurred an average of only 6.25 times (with the German authors' texts at a much higher average of 24 instances). Clyne does not clarify what percentage of the texts is made up of hedges, but as he is examining published works, including articles and conference papers, it is reasonable to assume that the percentage rate of hedging expressions for English-speaking authors would come out to be quite low, especially in comparison to the percentages given elsewhere: Salager-Meyer (1994) gave hedging expressions an occurrence rate of 13% "with respect to the total number of running words in each division" for the Discussion section of research papers (p. 155-157). Clyne also claims that multiple hedges of greater than two hedging expressions ("triple hedges", etc) were completely absent from the texts by English speakers, though not from the texts by German speakers, which is a bit difficult to believe and, if true, is probably a result of the small size of the corpus. Hyland's large corpus corpus of 240 research articles provides a significant number of triple hedges, such as "This might seem to help explain why it is that (2) is true but (2*) is not" and "While teachers might be able to suggest a number of strategies that students may find helpful..." (depending on one's definition of hedging, these examples could contain even more than three hedges). However, while Clyne's figures on hedging in the texts of English speakers may be a bit hard to accept (given the conflicting results of other studies), his point that German speakers hedge more frequently and more strongly than English speakers is well-taken. We have already seen that hedging may not be adequately explained to non-native English speakers, so it is completely understandable that German authors attempt to transfer their knowledge of hedging in German (which is strong and frequent, according to Clyne) into English, and perhaps in doing so they end up overhedging their claims. The idea of linguistic transfer leading to overhedging (or at least a greater occurrence of multiple hedges) is also demonstrated by Hinkel (1997), as discussed previously.
Perhaps the most direct address of multiple hedges occurs in David Banks' (1994b) book Writ in Water: Aspects of the Scientific Journal Article and in his article "Hedges and How to Trim Them" (1994a). Chapter 8 of Banks (1994b), "Some Hedges" discusses certain multiple-hedge collocations that may be noted. (These collocations are also discussed in Banks 1994a). Banks lists the following a typical combinations of hedging devices: lexical verbs linked to modal auxiliaries by the use of the modal in a subordinate clause following the lexical verb; a lexical verb and an adverb; a verb plus complement reinforcement; a modal auxiliary and an adverb; a periphrastic modal and an adverb; a complex noun group; and, most commonly, a lexical verb with hedging content combined with a modal auxiliary. In "Hedges and How to Trim Them" (1994), the results of Banks's corpus study of eleven scientific articles results in "58 sentences which possess more than one hedging device" (p. 587). He calls these multiple hedges "fertilized." He also speaks of "trimmed" hedges, which are de-emphasized hedges or combinations of hedges and "boosters" (as defined in Hyland 2004). According to his research, fertilized hedges are much more common and occur in many more varieties than do trimmed hedges. He sums up by saying "A hedged style has become a requirement of scientific journals" (591). One can assume that Banks' descriptive study of multiple-hedge collocations carries with it the implication that using double hedges is perfectly acceptable, and this last citation seems to affirm the idea. Indeed, most of the research examined here which mentions double or multiple hedging in one way or another (except that done by Clyne) carries a similar implication; however, neither Banks nor any of the other researchers make any judgment as to how much hedging is acceptable in a given proposition.
Now that we have considered the treatment of multiple hedging in the research literature, let us move to a brief overview of some textbooks used for teaching academic writing, which will not only give examples of the different extents to which hedging is covered in textbooks (which is explained in much more detail by Hyland 1994) but will also give an indication of whether or not textbooks devote time or focus to multiple hedging and the question of its acceptability or to underhedging and overhedging as potential problem areas for students. Writing in the Sciences (1998) gives a comparatively large amount of attention to hedges which it refers to as "qualifiers." The book places emphasis on the importance of careful conclusions and acknowledging uncertainty giving examples of verbs, adverbs and adverbial phrases, and modal auxiliaries which can be used for this purpose and providing several practice exercises. It mentions underhedging as a problem but not overhedging. There is also no mention of multiple hedging mentioned.
Another text, Responses to ESP (1997), does briefly mention hedging, though it does not give as much attention to the phenomenon as does Writing in the Sciences. In the section "Some features of scientific English" Alastair Sharp provides a list of "some of the features generally recognized as being important in scientific discourse" one of which is modals. Modals are mentioned as essential for hedging (which is very briefly explained as "the modification of language to limit the strength of a knowledge claim") but there is no mention of other types of hedges. A short exercise is given, but there is no mention of underhedging or overhedging.
Perhaps the best overall coverage of hedging that I found occurs in Academic Writing for Graduate Students by Swales and Feak (2004). Unit 4 of the book, which focuses on data commentary, has a section explaining qualification and strength of claim (which includes hedging devices, though the term hedging is not directly used). The book clarifies that it is problematic to describe rather than comment, but it is also problematic to "draw unjustified conclusions," and therefore it is important to find the right strength of claim, to be cautious in expressing a claim, and to know what expressions to use. Words which can be used to express probability, distance, and generalization are given and discussed, and there is also a section on "weaker verbs" (examples are given). The book also mentions that these expressions can be combined (multiple hedging) -- that is, multiple hedging is an acceptable and useful strategy -- but it should not be overdone, or no claim will be made at all, as in "It could be concluded that some evidence seems to suggest that at least certain villagers might not have traded their pottery with others outside the community" (p.129). It is the only textbook of those that I've examined which mentions multiple hedging (as an acceptable strategy) and discusses both overhedging and underhedging.
But again the question arises: where do we draw the line between being "confidently uncertain" (Skelton 1988b, cited in Swales and Feak) and overdoing it? So far, we have examined the treatment of multiple hedging and its link to overhedging and underhedging in research literature and EAP textbooks, yet we have found no concrete answers to these questions. This brings me at last to my research, which attempts to provide some sort of answer to this question of how often multiple hedging is done and to examine the judgments of different groups within the academic world on how seldom or frequently hedging should occur. To get an idea of how often multiple hedging is done, I've looked at select expressions in the Hyland Corpus, which contains 240 research articles coming from 8 different academic fields. To get an idea of the level of hedging people consider appropriate and to see if level of education or native/non-native speaker distinction affects attitudes towards hedging (and, one can assume, therefore use of hedging) I've conducted a survey of 3 different academic groups at the University of Michigan: teachers of academic writing, NS undergraduate students, and NNS graduate students. It is now appropriate to explain these in more detail and provide the results of my research.
To get an idea of the frequency with which multiple hedging occurs, I turned to the Hyland Corpus. Since examining every proposition of each of the 240 articles in the corpus for the presence of hedging devices is not a realistic goal, it was necessary for me to choose certain expressions and examine their overall frequency and the presence of hedging expressions in their immediate context using the Wordsmith concordance feature. First, in order to get an indication of how frequently hedged expressions occur in the environment of at least one other hedge, I examined the modal verb might and the probability adverb perhaps. I chose these two expressions because, unlike some of their counterparts, they are almost always used as hedges and thus should give a good starting ground to the present research. The results follow. A Wordsmith search yielded a total of 868 entries for the word might. I examined these 868 and deleted the irrelevant entries: there are some transcripts of speech in the corpus, which I chose not to include in my analysis as I am attempting to focus on academic writing; there a number of instances in which the given hedging expression occurs within a citation (I chose not to include these in my analysis because they don't really contribute to the overall strength of hedging in the claim); and finally, it seems that a Wordsmith search turns up a few repeated examples, which I tried to be aware of and exclude. After deleting these entries I was left with 810 examples, of which I examined the surrounding context (attempting to read at least the sentence the word occurred in), searching for other hedging expressions. My criteria as to what linguistic devices I included as hedges are defined earlier in the paper. It is also worth mentioning at this point that, although there has been some suggestion that hedging devices can carry over from one expression to another (Skelton 1988b), to avoid inflating the results of my data, I included only those hedges which fell in the same sentence as the word might. I was able to find another expression of hedging in the same sentence as the word might for 328 of the 810 might entries; this yields a percentage rate of 40.49%. I followed the same criteria in my examination of the perhaps entries, and found that 103 of the 282 relevant examples contained an instance of another hedging expression within the same sentence as the word perhaps -- a percentage of 36.52. In other words, around 40% of the hedged might and perhaps expressions were in fact instances of double hedging -- a much, much larger figure than that given by Clyne (1991). This is a substantial figure, examining over 1000 corpus entries. Even if I had been using a very broad definition of hedging, which I have been clear to point out I did not in fact use, 40% is a significantly large enough amount to warrant the claim that, not only is double hedging acceptable, it is quite common. The results of the might and perhaps search give us the beginning of an awareness of the rate of occurrence of double-hedges among already hedged expressions, but what of stronger types of hedges? How common or rare are triple hedges, for example? In an attempt to answer these question, I chose to look directly at an expression of double hedging -- the "may + hedging lexical verb" feature mentioned as one double-hedge collocation by Banks (1994a and 1994b) to see both how often this collocation appears in the total number of may entries, and to see if it tells us anything about triple hedging. The results of my findings are as follows. A Wordsmith search produces a total number of 2256 entries for the word "may"; after narrowing down these entries so that I was left only with the entries where may was followed by a weak lexical verb, I was left with 87 entries. This puts the percentage of may entries taking the "may + weak lexical verb" form at approximately 4%. It is important to emphasize here that this percentage is in no way meant to suggest the overall percentage of double or multiple hedging in the may entries; many of the entries deleted in this specific analysis contained other hedging devices than weak lexical verbs and thus, though they provided examples of multiply-hedged constructions, were not included in the present results. The overall percentage of may expressions which are doubly or multiply-hedged is therefore much higher than 4% -- likely somewhere closer to 40%, as suggested by the might and perhaps entries results. It is also important to point out here that I considered only a rather narrow set of verbs to be "weak" and therefore hedges. That is, my decisions regarding whether or not to consider a lexical verb as "weak," were quite a bit more selective than other researcher's have been, and only included verbs with a very weak level of assertion. For example, I did not include indicate -- the hedging status of which has been debated (Johns 2001) or other more "questionable" verbs. Had I included every non-factive verb encountered in the data (as Crompton1997) does in his taxonomy of hedging), I would have had a much higher figure of between 300 and 400 entries (13.3 % of all may entries for 300 entries) fitting the "may + hedging lexical verb" structure. As it is, though, I selected only 87 "may +lexical verb" entries, and of these 87, 26 entries -- 29.9% -- had an additional hedging device in the same sentence. Again, this is quite a significant number. The data set is perhaps a bit too small to allow for any definite conclusions, but this preliminary analysis places the rate of triple hedging at around 30% of all double-hedged expressions. It would be interesting to examine as well the occurrence of even larger congregations of hedging expressions (i.e. quadruple hedging) to see where this relatively strong occurrence rate drops off). These results show that, despite the narrow criteria I assumed in looking at the data, the presence of multiple hedges in academic text is quite strong; double hedging may occur as commonly as in 40% of hedged expressions, and triple hedging seems to occur at around a rate of 30% of double hedged expressions.
Next, let us turn to the survey. For this, an authentic academic article (Martìn-Martìn & Burgess 2004) that makes use of hedges was chosen and each of the hedges in the first 7 paragraphs of the Discussion/Conclusion section was highlighted and dropped into a multiple-choice format where hedges of greater and lower strength framed the original. Survey respondents were then asked to choose which of the multiple-choice expressions seemed most appropriate to the sentence, without knowing what expression actually occurred in the original. There were a total of 29 multiple-choice sets, a double-hedged choice was given in 17 of these 29 sets. There were 4 double-hedge expressions in this section of the original article and all 4 were presented as options in the relevant multiple-choice set. There were three groups of respondents (all of which work or study at the University of Michigan): four undergraduate students in the school of LS&A, all of which are native speakers of English; nine non-native English speaker graduate students from various fields; and eight teachers of academic writing at UofM's English Language Institute (a center for ESL teaching, testing, and research). In considering the completed surveys, I attempted to assess several elements: one is how likely the respondents were to choose double-hedged expressions as an appropriate choice (and to compare these choices to the double-hedged expressions in the original; another is the level of conformity among the respondents' answers (in other words, were the respondents' answers likely to match up? Or were the respondents at least likely to consistently choose a weak rather than a strong expression or a strong expression rather than a weak expression for a given multiple-choice set); another is whether or not any of the groups had a strong tendency to consistently choose stronger or weaker expressions than the original expression (this could indicate a tendency to underhedge or overhedge); and, of course, I did my best to note any other interesting trends in the data. The results from each group are given, followed by a comparison of the different groups' results.
We start with an examination of the NS undergraduate group. The four undergrads picked a total of 5 double-hedged expressions (several of which were picked by more than one person for a total of 11) as an appropriate choice; three of the respondents chose 2 double-hedges and the other respondent chose 5, giving a group average of 2.75 double-hedges chosen as acceptable by each person. Of the 11 chosen as acceptable, only 2 were not double hedges in the original. In other words, the undergraduates usually chose double-hedged expressions where a double-hedge appeared in the original. However, only 3 of the 4 originally double-hedged expressions resulted in a double-hedged choice by the undergrads. Specifically, the double hedge "may imply" which appeared in the original was never chosen, and a stronger expression always selected instead. It is interesting to note as well that the undergrads had a strong tendency to correspondingly select expressions that were either weaker/same or stronger/same than the original. In other words, for only 3 of the 29 multiple choice selections was there disagreement over whether the appropriate expression should be stronger or weaker than the original. In addition to the 3 expressions over which there was conflict, 12 of the expressions were chosen as weaker or the same, 12 were chosen as stronger or the same, and for 2 of the expressions, only the exact same expression was chosen as appropriate by the undergrads. Although the undergraduate sample may be a bit too small to offer definite conclusions, one can note some interesting features of this data. One is that, overall, the undergrads seem to have a sense of how much hedging is appropriate in a given clause, with many of their answers matching the original expression and most others differing from the original only by one or two degrees of strength. Another is that, given their equal tendency to choose a stronger or weaker expression, they seem unlikely to suffer from a tendency to underhedge or overhedge. Indeed, if anything, the undergraduates may have a tendency not necessarily to underhedge, but perhaps to avoid the use of double-hedges, as their average number of double-hedged expressions chosen was a bit lower than the number of double-hedged expressions in the original. However, the general results of the study show that the undergraduates have a fairly good grasp on hedging techniques (at least in recognition, one can't conclude in regards to production from the results of this survey) and the fact that they have chosen a few double-hedge expressions here and there shows that, though they may not always believe it the best option, they consider double-hedging the appropriate choice in some cases.
Next, let's turn to the NNS graduate students. It is worth mentioning that all of the graduate students in this group are presently taking an ELI class to help with their dissertation writing, indicating that they have either a need for or an interest in ameliorating their EAP skills. The nine graduate students picked 4 double-hedged expressions as the most appropriate choice (several which were chosen more than once for a total of 15), with an average of 1.666 double-hedges selected as appropriate by each person. Only 2 double-hedged expressions which were not double hedges in the original were chosen as the most appropriate. Just as with the undergraduate group, the majority of the double-hedge selections were double-hedges in the original article as well. However, 2 of the 4 expressions which were double-hedges in the original were never chosen by the grad students; instead, a stronger selection was always chosen in its place: "may imply," which was also never chosen by the undergrad group, and "would seem." Perhaps the modal complexity of the expression "would seem" was a bit difficult to interpret for the NNSs, leading them to instead choose a more straightforward option like "seems." The grad students also evidenced a tendency to agree on whether an expression, if it differed at all from its original, should be weaker or stronger than the original. In 6 of the 29 expressions there was some disagreement -- at least one person chose a stronger option while at least one other chose a weaker option, but the other 23 expressions showed agreement. For 13 of these, the grad students chose the same expression or a stronger one, for 9 of these the grad students chose a weaker expression, and for 1 of these, every grad student chose the same expression as that in the original.
Let us turn finally to the ELI writing teachers, eight of whom responded to the survey. 12 different double-hedges were picked as appropriate by the teachers (as with the other groups, several of these expressions were picked by more than one person for a total of 32 double-hedge expressions selected). In the case of the teachers, all 4 of the originally double-hedged expressions were selected as appropriate by at least one person -- though the groups of teachers conformed with the other groups in that "may imply" was chosen very infrequently -- only twice. The other 3 originally double-hedged expressions appeared quite often among the double-hedge selections, and in addition to these 4 originally double-hedged expressions, 6 other double-hedges were chosen by at least one person (and, interestingly, in many cases by more than one person). The average number of double-hedge expressions per person for the teachers is 4.125 -- much higher than both other groups and very close to the original number of double-hedges. It is interesting to note here, however, that while the average fell at around 4 double-hedged expressions, only a couple of teachers' choices fell in range of this number. In other words, while some teachers picked 6, 7, or as many as 8 double-hedged expressions as appropriate choices, others chose only 2 double-hedges as good options, and in two cases, only 1 double-hedge was selected. This indicates that personal style perhaps plays a role in hedging, though to what extent, and whether this is a feature only of EAP teachers and not EAP students is not clear. Moving on, the teachers also, unsurprisingly, showed a strong likelihood to agree, in the case of difference from the original, on whether an expression should be stronger than the original expression or weaker than it. For 5 of the multiple choice sets there was some disagreement, but the remainder showed consensus: in 9 sets, the appropriate expression was chosen as stronger than or the same as the original, in 13 cases, a weaker (or the same) expression was chosen, and in 2 cases, only the original expression was chosen as the appropriate option. This falls in interesting contrast to the ELI students, whose results were the exact opposite: 9 were chosen as weaker or the same and 13 as stronger or the same.
In sum, the average number of double-hedged expressions selected as appropriate choices was highest for the teachers, a bit lower for the undergraduate native speakers, and lowest for the ELI students. No group had a significant tendency to choose stronger expressions rather than weaker expressions or the original (which could indicate a tendency to underhedge) or vice versa (which could indicate a tendency to overhedge) -- though the small differences pointed out above may have some significance; for example, if we can consider the tendency to choose expressions of a certain level of strength as evidence of use, it is possible that ELI students use stronger language/less hedging in their writing than ELI teachers. It is not clear if this stronger use of language would be a problem, but it would not hurt to make the ELI students aware of this tendency and to clarify that multiple hedges are a useful and acceptable strategy. We have seen also that each group has a tendency to follow the same patterns (all going strong/same or weak/same the majority of the time); the ELI students have the strongest level of disagreement, but they are closely followed by the ELI teachers, so it's probably not a result of their lack of knowledge of the language (the disagreement among ELI teachers may also be a result of stylistic preferences, as discussed earlier in connection with the average number of double-hedges chosen by the ELI teachers). Assuming that teachers have authority on the subject, (which, it must be emphasized, is indeed an assumption) the fact that the ELI teachers chose double hedges more than any other group may indicate that both groups of students are not using double hedges as much as they should, or at least could. Also, the ELI teachers' double-hedge selection average is the closest of the three groups' averages to the actual number of double hedges in the original article -- a bit of evidence for their perhaps greater understanding of the phenomenon, though one can not draw conclusions on rights to authority from this little bit of evidence. Although not many options were given, triple hedges were never chosen by any of the groups, indicating that they're not terribly common (recall that the Hyland data showed they were certainly acceptable -- that is, they occurred, but that they occur significantly less frequently than double hedges. While the undergrads and ELI students would likely benefit from a clearer understanding of hedging practices, it is not my intention here to give the impression that the situation is grave and serious for these students. First of all, although the student respondents examined here tended to select multiple-hedges as appropriate options at a lower rate than the teachers and at a lower rate than that which occurred in the actual article, the rate of difference was not so large as to be alarming, and, although I'd like to think that the results of the survey give some indication of actual use in the respondents' own writing, this does not necessarily mean the students will be perceived as underhedgers. Secondly, although the rhetorical culture of the US has tendency to put a lot of the onus, in the case the communicative burden, onto the writer, we should remember that many readers make allowances and would likely not be overly disturbed by a bit too much indirectness or a bit too much directness; as long as students aren't making unwarranted dangerous claims in their writing or hedging so much they make no claim at all, no great damage has been done.
In this section we have examined the frequency of double-hedging among hedged expressions and the presence of triple hedging among double-hedged expressions through a corpus search, and we have looked at attitudes towards multiple hedging by both students (native and non-native English speakers) and teachers of EAP and have made some inferences about levels of multiple-hedge use among these groups.
We have established quite strongly that, no matter what beliefs were held in the past, and no matter what disapproving attitudes remain today, hedging is a common, legitimate, and useful rhetorical strategy. We have also established that multiple hedging, though it can be overdone, occurs quite frequently and is a useful technique for expressing strong degrees of uncertainty. Hyland's corpus of academic research articles has shown us that hedged expressions occur in the presence of another hedge up to 40% of the time, and that these double hedges are in fact triple hedges somewhere around 30% of the time. We have also seen that, although students may use multiple hedging less, double-hedges are considered acceptable options by students and teachers of academic writing. Further research could attempt to assess students writing, looking to see if multiple hedging does indeed occur less than in the writing of so-called experts; research into the influence of stylisitic preference on the use of hedging could also be beneficial. Pedagogical theories and their practical counterparts, textbooks, should take research such as the present into account, and more textbooks would do good to follow in line with Academic Writing for Graduate Students by mentioning multiple hedging as a useful and appropriate option. In the end, it seems that, despite what some researchers may think and despite any lingering pejorative connotations associated with hedging, hedged expressions, including multiply-hedged ones, are, as Banks (1994) says "a requirement of scientific journals" and an important technique for any scholar of EAP to have up her sleeve. As it may be difficult for less experienced writers to consistently select an appropriate level of hedging (due to the nuances of lexical selection), teachers and EAP teaching materials must strive to make it as clear to students as possible where this comfortable middle ground is found. It is my hope that studies like the current one will help to turn this goal into actuality.

Speech and writing have usually been seen as two totally different media; the differences between them can be significant in some languages, such as in Chinese and Arabic, whereas in other languages, such as English, the writing reflects speech somewhat more closely. However, even in English, the style, syntax, and wording of the written language are vastly different from the actual spoken language. The distinctions have been eroded by the new-age technology that gives people the power of communication at their fingertips. E-mail, for instance, takes aspects of both spoken and written language to form a new genre; in this new genre, "proper" grammar or punctuation is not enforced for purposes of expediency. Even further melding of speech and writing occur as a result of the new phenomenon of instant messaging (IM). In this paper, I explore the similarities and differences between IM and face-to-face conversations, using data gathered primarily from my IM conversations with members of my singing group that is part of an IM group.
The main instant messaging program that is used among college students today is America Online Instant Messaging (AOL IM, or AIM). According to studies by Thurlow & McKay, 2003), "70% use [the internet] for instant messaging. These statistics are higher for older teenagers at...83%." I started using this program in 1999 simply because many of my friends were using it at the time to chat. "Recent AOL research on 7,000 young people shows that the internet has in fact become the primary communication tool for young Americans." (Thurlow & McKay, 2003) It is different from most other instant messaging programs because it allows you to place your buddy's "screen name"s (usually creative identities to be used online, such as HockeyChik03 or BaBygUrL701) on your buddy list. It is safer and more intimate than most other chatting or IM programs because only those who know the exact screen name can IM that person. When a member on your buddy list is online and available to talk, the screen name will appear on the list and you can IM the person simply by double clicking their screen name and entering text into the IM window. Messages that are already entered will move up on the screen and new IMs will appear at the bottom. IMs are instantaneous -- they take less than a second to be sent to the other person, so conversations move as fast as one can type. Furthermore, you can have multiple IM conversations at once by having more than one IM window on your computer screen.
The data used in this study is taken from IM conversations with members of Gimble A Cappella. Gimble is a student-run a cappella singing group that began in 1997 as a part of Arts Chorale. Currently there are 17 members, ranging from freshmen to graduate students from various fields of study; each of these members were selected into the group through a music audition that demonstrates their musical abilities in various areas. I have been in Gimble for five semesters, the last two of which I have been the music director of the group. About 15 members of the group use AIM regularly (though all 17 members do have screen names) and these are the people with whom I talk most frequently via IM, mostly because we have similar interests.
Below is a sample IM text from a conversation that took place about two months ago:
The names on the left that appear before each IM line are the screen names, Mallee01 and SunE13. A conversation this size takes approximately five minutes, which can be longer or shorter depending on how much of a pause there was between IMs.
Instant messaging contains aspects of both writing and speech. It is similar to writing in that the discourse has to be written and read in order to be understood, and therefore places a certain importance on spelling and punctuation, while eliminating the focus on pronunciation. All the intonation, emphasis, and mood must be portrayed solely through the written text. However, instant messaging also resembles speech in that the syntax of the utterances found in IMs are usually incomplete sentences, while it is enriched with exclamations, expressions, and even laughter. Furthermore, IM takes on the reciprocal characteristic of face-to-face conversations; Cook (1989) states that "discourse is reciprocal when there is at least a potential for interaction....The prototype of reciprocal discourse is face-to-face conversation." Because IM is mainly used for social interaction and is carried on by reading and responding to each other's message, the level of reciprocity is high. As is the case with face-to-face conversations, what one says on IM depends entirely on the way the person on the other end responds; this leads to a complex system of back-channeling, as we will discuss later in the paper.
Face-to-face and IM conversations both take on the use of subject-initial ellipses, in which the subject of the clause is elided, but understood to be there:
In the first clause the subject I is elided, and in the second clause the impersonal it was left out. This type of construction is typical of speech and is a frequent syntactic element in instant messaging. Similarly, contractions such as "gonna", "wanna", and "kinda"that are common in speech also appear often in IM. This shows how instant messaging closely reflects the elements and style of spoken language.
Although instant messaging greatly resembles face-to-face conversation, due to the special text-based manner of conversation and the growing need for expediency in the age of technology, differences do arise. For instance, IMs tend to have shorter conversational turns -- in fact, almost a third of the turns are one-word responses. IM conversations look to express things in the simplest ways possible to minimize typing; if a message is going to be a long, complicated utterance, the person doing the messaging (or IMer) will break the utterance into parts. Consider these two samples from two different conversations:
Both of these IMers used multiple lines1 to separate what would have been a long strain of words. This is often done in order to simplify the utterance and make it easy to read and process; typing a long paragraph may turn the other person off from reading the message. Also, by cutting the utterance into parts, it keeps the interest of the reader because he/she knows that another message is on its way.
Face-to-face (F2F) conversations differ from IM in that the turns can be dominated by certain individuals, whereas turns and flooring in IMs are fairly equal most of the time. Example 1 is a perfect example of this; the first part of the conversation is Mallee01 discussing her plans, while SunE13 listens and sympathizes: "ahh" and "is that why you can't make it to the gig tomorrow?" In the second half, they reverse roles so that SunE13 is now complaining about her schoolwork, while Mallee04 supports by prompting, "are they in hard classes?" and "those are always time consuming." Neither person dominates the conversation, but they take turns speaking and listening in an equal, complementary way.
Often times in casual speech, many of the articulated sounds and syllables (particularly those that are most frequently used) are shortened or combined for speed and ease of pronunciation. For instance, the utterance "I'm going to class" may be shortened from [aim goiŋ tu klæs] to [aŋgoinə klæs]; most native English-speakers would have no trouble understanding the latter articulation of the utterance. Similarly, in instant messaging, frequently occurring phrases or words are often abbreviated or take on acronyms, and they are mutually understood by those who are familiar with instant messaging. Some are fairly obvious and easy to decipher:
Some are a little more difficult:
Others take a considerable amount of imagination:
Generally, these abbreviations and acronyms arise to save time in typing everything out; rarely will you see anyone writing out "be right back" when the simple "brb" can be used. Expert users of IM can usually tell if the person to whom they are talking is new to IM when he/she types out phrases and words that are typically abbreviated, or if he/she hyperextends by shortening a word that is usually not abbreviated (or using the wrong abbreviation), such as "ty "for "thank you".
Younger IM users tend to utilize more abbreviations in order to be "hip" or unique:
r (are), 4 (for), b (be), y (why), 2 (to/too), wuz (was), sry (sorry), wut (what), thanx
In fact, some of these are not much shorter in abbreviation form, but rather they are alternate spellings of the original word to make the statement look more interesting. By college, these types of abbreviations are considered to be too childish, flighty, and superficial; they are generally avoided and used only in mocking or joking.
An interesting phenomenon that occurs exclusively in instant messaging is the ability to have overlapping conversations, or talk about two different topics at once with the same person. Consider the following IM conversation:
Due to the fact that while one person is typing, the other person can't see what he/she is saying (until the message is sent), often times the conversation topics will overlap. Lines 1, 3, 4, 5, 7, and 9 discuss having Friday classes, and lines 6, 8, 10, 11, 12, and 13 discuss breaks between classes; one can see the two clearly distinct topics by reading the lines in the two groups mentioned above. Although overlapping conversation topics such as this can technically occur in speech, it is a much more common trend in instant messaging as a result of the way the timing of IM conversations take place: a few seconds to read the other person's message, a few seconds to type a response, and a second to send the message. Given that in IM these steps do not happen all at once, double conversations can easily occur.
In instant messaging there is a difficult task in establishing a balance so that "speech" can be converted in writing form and still retain the essence of conversation. Without being able to see or hear each other, the IMers are not able to express or discern the sense of intonation, emphasis, or mood behind the utterances, which play a significant role in F2F conversation. However, to overcome this possible communication block, text-based manners of expression have manifested themselves in IM. For instance, repetition of letters signify the elongation of the sound:
Capitalization is used for emphasis, or to show yelling:
The use of ellipses indicate pauses, unfinished thought, uncertainty, or keeping a statement open-ended (usually a sense of drifting-off when used at the end of an IM):
The asterisk indicates a self-correction of a mistake and is comparable to the act of going back and correcting oneself in speech.
Also in response to the impersonal nature of communicating via internet, users of instant messaging often try to express their individuality and style by the creative use of fonts, sizes, and color in the text. One person may use 12-point bubbly font with a yellow background, while some may use 10-point green script with a gray background; these choices usually reflect the creativity and personality of the user. Still further expression of individuality can be shown through the buddy profiles, buddy icons, and away messages. Buddy profiles are basically there to provide others with personal information, but many people use it for other creative purposes; profiles can contain telephone numbers, famous quotes, song lyrics, links to websites, shout outs to friends, inside jokes, or even daily updates about one's life in general-a heart-to-heart of the day, perhaps. Buddy icons are small pictures that appear on the bottom left corner of the IM window that can be used to individualize oneself. Away messages are selected when the IMer has to leave the computer but wants to stay signed on so that he/she can still receive messages. Away messages range from simple ("at class, leave a message") to elaborate ("SO much to do today, class 10-1, work 1-5, meeting 5-6, dinner 6-7, then out to study for the rest of my life...kill me NOW"). With the use of fonts, sizes, color, as well as buddy profiles, icons, and away messages, IM users can express their different style and personality without voice or physical appearance.
Also as a consequence of not being able to see or hear each other, it is often hard to tell whether the person is listening to what the other is saying. In speech, back-channels, such as "yeah" and "uh-huh" is often used to signify that one is listening while the other person is telling his/her story. Similarly, a text-based system of back-channeling has developed in IM; not only does it let the other person know that you are listening, but it also ensures that you get a message back. There are six major types of back-channeling that occurs in instant messaging. The first method is through the use of smilicons, small faces that can be used within an IM conversation that convey expressions and emotions. There are 16 standard smilicons available to use on AOL IM: the smile, frown, laughing, crying, embarrassed, innocent, foot-in-mouth, lips-are-sealed, surprise, kissing, money mouth, sticking-out tongue, cool, wink, angry, and undecided. Usually, a ☺ is used to respond to something pleasant, and a ☹ for something sad or disappointing. The smilicons can be used in isolation to indicate that you're listening:
Here, the frown indicated to SunE13 that Nirone83 is listening and sympathizes her, and prompted SunE13 to continue speaking.
The second method of back-channeling is through laughter, as is also common in speech. The standard laughs are "haha", "hehe", or "lol", which occur in almost every IM conversation. There are also some alternate types of laughs, such as "muahaha" (evil laugh) and "hee hee hee" (giddy laugh) that convey slightly different feelings of laughter3. Expressions of laughter are not necessarily in response to something humorous, but rather as a friendly comment indicating light-heartedness:
Other common classes of back-channels used in IM are affirmative responses (such as yeah, yup, good, yes, sure, sweet, cool, yay, and okay), sympathetic responses (such as oh man, awwww, dang, that sucks, and that's craziness), and neutral responses (such as ahhh, oh, and hmm). These types of back-channels reflect the content and mood of what has just been said, which indicates to the speaker that the listener is paying attention.
The last type of back-channeling used in instant messaging are questions that are used to prompt the speaker to keep talking, such as "why's that?" or "what did she want?" It encourages the speaker to elaborate on what he/she has just said with the knowledge that the other person is listening and is interested in what he/she has to say.
Instant messaging has become more than just another mode of communication -- it is one of the primary ways that college-age students in the United States keep in touch with one another. Due to the ease and efficiency of communicating with multiple people with the tips of your fingers, more and more college students use IM for social interaction. Through frequent use, instant messaging has become a genre of its own, an offspring of writing and speech with its initial roots in e-mail; users of IM have found creative ways to get past not being to see or hear each other in order to emulate the complexity and personal nature of face-to-face conversation, but retain the speed and accessibility of instant internet communication.

No one would question that there are fundamental differences between spoken and written language. Many features, including formality and spontaneity, are inherently different between these two types of discourse, although, of course, a text's features depend heavily on the context in which it is found and other factors. On the other hand, certain similarities exist between the two types of communication, and some features of one modality may parallel those of the other in function.
One phenomenon that helps illustrate this point is the concept of repairs and reformulations in speech. When one speaks, "there is no going back and changing or restructuring our words as there is in writing" (Cook 1989: 115). However, although a previous utterance cannot be erased from history, further elaboration of it can often alter the way it is interpreted. This is where repairs and reformulations enter the picture, providing speakers with the ability to change what he or she just said, at least to a certain extent. Use of these methods can be helpful in many situations, ranging from the frivolous (a joke being misunderstood among friends) to the serious (a statement being misinterpreted in court). The cases under consideration in this analysis fall somewhere in-between: the academic discourse that constitutes MICASE.
Locating previous research on a similar theme was difficult. Bruti (2004), however, has conducted similar analyses of reformulations across a set of biology textbooks, called the Pavia biology corpus. She examined this corpus for instances of the phrases "namely", e.g., "and that is". Her methods of gathering data and making observations with respect to semantic equivalence, expansions versus reductions, and the like have helped shape the foundations of this paper. Of course, the fact that Bruti analyzed written sources makes a comparison with MICASE a bit questionable, but this paper is not going to take much of her data into account, only her techniques.
In beginning my research, I (with assistance) brainstormed phrases that one might use when he or she wanted to fix a speech mistake, clarify an idea, or rephrase an ambiguous utterance. In so doing, several examples came to mind, often as part of a set of phrases with slight variation (i.e. "that's what I meant,"
Next, the apparently relevant results were sifted through and any data that did not involve repair were excluded. An example of one of these misleading utterances was the use of "meant" as "intended but foiled," as seen in the excerpt "I meant to bring a book to be able to read you this uh this big long quote." These and other similar instances of the phrases were removed from consideration. Due to the large amount of potential data, any utterances that I judged as "questionable" were eliminated, so some repairs were most likely thrown out because they were not obvious at first glance. The final data set was composed of 387 samples of reformulation.
I then divided these instances of repair words (which appear in contexts which I judge to involve repairs) into four categories, or types. These can be summed up in the following table, setting A as the person who utters the repair word (usually referred to hereafter as" the speaker") and B as a fellow conversant or passive listener. The types vary in their combinations of three parameters: who uses the repair word, who makes the mistake in speech, and who corrects the mistake.
Type 1 repairs involve a speaker making a mistake and then correcting him or herself using a repair word. Type 2 is where a speaker makes a mistake and a listener corrects him or her, and the speaker agrees with the reformulation using a repair word. Type 3 occurs when a speaker corrects a listener's mistake using a repair word, and type 4 involves a speaker asking a listener for reformulation of a mistake he or she made, using a repair word in the request. For the purposes of this paper, types 1 and 2 were considered self-repairs, as the person using the repair word is the one who had originally made the mistake. (Of course, the "mistake" does not have to be a factual error; it is simply a statement that is perceived as less than ideal by someone in the situation.)
Lastly, I looked at the data and, for each phrase, I separated the data into instances of expansion and reduction. This has been done only for the self-repairs, and only those classified as type 1. The amount of change between the original utterance and the reformulation was noted, especially with respect to any discrepancies in length.
The total instances of each repair phrase found in MICASE are shown in the following table. These are broken down according to the type of reformulation found in each instance.
Type 1 repairs can be further broken down into those that are made immediately and those that are delayed, being brought up later in the conversation. However, only three instances of delayed self-repairs were found in the data (two using "meant "and one using "misspoke"), so separating them in the data seems a bit superfluous.
Following are the specifics of each phrase: if the reformulation is an expansion or a reduction, how changed the reformulation is from the original statement, and any further comments that are warranted by the data. Illustrative examples are given for each of the three contexts using each phrase, if applicable.
This phrase is used mainly in the context of expansions, as when explaining a concept or giving examples that illustrate a point ("it is a call to personal and social transformation. in other words we can imagine and reconstruct various ways of creating sustainable relationships with each other, between partners between partners and children at different stages of our lives"). In second place for the amount of data found are reductions, reformulations that state the point succinctly ("men and women, have and they should have equal roles in both family care and work and public involvement, in other words both men and women should be, full human persons"). The remainder of the data could not easily be classified into either of these categories, as the reformulation contained about the same amount of information as the original statement ("the labor movement which rose in the late nineteenth century in this country championed the middle-class white family ideal. in other words they tried to raise the male wage to the level, where the wife could become a full-time housewife").
Reductions are the most often used reformulations with this phrase, and one particularly common type is to specify the concept after explaining it ("he's throwing, kind of an, anchor off. notice that she bounces up and down. pulling the boat in, sways. no. kay so that's what i mean by, boat-on-dry-land schtick"). Expansions are relatively common as well, where the speaker elaborates a term he or she has brought up ("what clinicians call body angst, or i like this term better, bad body fever, alright, and i_ what i mean by that is a continuous internal dialogue with the self, about what's wrong"). The type with the fewest representations is the same-length reformulations ("sometimes i refer to that as, vertically-dominated region of the rack and what i mean by that is the vertical motor, is the critical motor there").
Half of the instances of this phrase are found in the context of expansions, occurring especially often when defining technical terms ("they're called clades, and clades is another way of call -- of saying monophyletic group"). Also quite common in this group were reformulations of about the same length, when just rephrasing a concept using different words ("one way of putting it is that all the laws, of physics... work equally well, in any inertial frame of reference. and, another way of putting it is, the concept of absolute motion, so-called, whether that you're definitely moving or not"). Reductions appeared in the lowest proportion, mainly summing up a long initial utterance ("with anybody at least in a public situation i mean that, you sort of go out of your way to be polite and to not offend, um, and, and i suppose another way you can get at the same phenomenon is the ques-is in the notion of being honest").
This phrase is most often found in the context of expansions, to explain an idea in more detail ("so when i said they were free i quote they were free relative to going out into the world and looking at each sensor value"). Close behind are the reductions, summing up the previous statement in a few words ("this sort of misinterpretation of the Bible doesn't, occur in our world. yeah. i just quote specifically that. oh okay"). Less frequent in these instances, although still found, are reformulations of similar length and structure ("well i mean there's, three choices. that's what i quote. no three-part i quote to_ just, three choices").
Expansions are the most common types of statements following this phrase, as it is often used to detail an important concept ("uh and yet single women, uh that is to say w-uh, women who were never married or widows"). Next in rank are the reformulations that are about the same length as the original utterance, like simple restatements ("is there a time when independent judgment, and that includes technology, age of technology and science. that is to say, a point when, authority rests in something that is new something that is devised by individuals"). The two reductions involve a summary of what has just been explained ("the attitude that seems more modern to us than we would find in other thirteenth century thinkers that is to say, um, the, the, the utilitarian rule").
The three types of reformulations all have about the same prevalence when used with this phrase. Reductions are used when the original statement can be summed up in a very succinct way ("the concept of the predicate, Namely unmarried, is already in the concept of bachelor"). Next, utterances of the same length appear when the idea is just being put forth in similar words ("so in the c -- does this happen here where you have, basically two options, of characteristics, Namely, two different actions, of a person"). The last group of reformulations using this phrase involves expansions, where more detailed phrases are used in the second position of the utterance ("we've got some kind of, thing that needs explanation here Namely the fact that, every time we think of things we think of them in terms of time").
Expansions are found most often for this phrase, as would be expected, as it is often used to give examples or illustrations of an idea ("you're going from a sound here that's labial i.e. involving articulators that are in the front of the articulatory apparatus"). Reductions come about when the definition of a word is given before the word itself ("so trade, calculated in terms of a multipurpose medium of exchange and standard value, i.e., money"). The lone instance of a similar-length reformulation appears when the speaker substitutes another phrase with the same meaning, in this case a name ("you know outside the little cottage, that he shares in Berkeley with Alva Goldbrook i.e. Allen Ginsberg").
The reformulations using this phrase tend to have quite long, detailed follow-up utterances as the second part, and these are often not given immediately after the original utterance ("at this point, uh let me clarify at this point most of the te -- most of the texts a vast majority of the texts brought from India"). Most of the examples have a second part that follows quite far behind the original utterance, making it difficult to judge for certain what kind of reformulation it is. In addition, many of these examples are rephrasing an overall concept, although instead of summarizing them, the reformulations are also composed of detailed statements.
The corpus' only examples of this phrase are in the context of expansion, pinpointing the details of an unclear concept ("so all these circles represent goods which can be either, you know, a task or a resource. um, on this end, I have these box so uh you might another way uh another way to rephrase that might be, that, tasks are, resources that are supplied by other, agents"). If there had been more examples, same-length reformulations would very likely also have been found, since the word demands a fairly equivalent or more-detailed explanation in the second half.
This phrase was only found in contexts where the reformulation was an expansion of the original statement ("um, i'm now looking at injury prevention um, more specifically as a component of child advocacy"). By definition, this phrase would require the use of a more detailed reformulation, so the prevalence of expansions is no surprise. Reformulations of a similar length might be possible using this phrase, but reductions would probably appear awkward and poorly-worded.
The one example for this word came in a type 4 utterance, where a speaker asked a fellow conversant to reformulate his or her statement ("yeah how would you say that? in a nifty little paraphrase like three words. five words. six maybe."). The speaker is obviously asking for a reduction of the original phrase, and one would guess that this would be how the word would be used in a type 1 utterance.
The one instance of this phrase is used as an expansion, and the speaker uses it a bit strangely ("no it's it's wrong to look at it as the elites, spreading it to the masses that was a, a misspoke, on my part. it's that these we -- tended to be mass movements, with new kinds of leadership, that hadn't been seen, in these parts of West Africa before"). Considering that this is just one example, and the usage of it is not grammatically correct, it is not a particularly convincing argument for the context of the phrase. It is likely that this word can be used for expansions and same-length reformulations, though not quite as much for reductions, since it implies an actual error in the original utterance, and this would probably require a longer explanation for the sake of clarification.
The following table and chart illustrate the totals calculated across all twelve phrases in terms of expansions, reductions, and same-length reformulations.
Of all 359 instances of self-repair (type 1 reformulations), expansions made up more than half of the total. This shows a very clear tendency for people to follow announcements of self-repair with an elaboration of the original utterance, providing more rather than less information in the new statement.
This pattern does not seem particularly surprising, given speakers' intuitions on the concept. Giving an explicit announcement that a concept will be fixed implies that the repair will be easier to understand and thus, most likely, more detailed and informational than the original statement. Of course, when a statement can be better-clarified using only a few words, this will be chosen over elaboration. The last option, of using a reformulation of similar length and content, can be used with most of the repair phrases as well, usually when the original utterance has to be only slightly modified. All three of these choices occur in the vicinity of repair words, and they are all represented quite well in the data, though the proportions vary by the individual phrase chosen for a specific context.
Still, even though the results are not exactly surprising, the numbers are quite striking. Perhaps academic contexts tend to stress details at the expense of concise ideas, meaning that patterns of self-repair in different types of corpora would be fundamentally different. This question, however, would require much further research.

Most of the current research being conducted surrounding the newly formed Nicaraguan Sign Language has been focused on the role of children in its creation and development. Proponents of creolization theories which emphasize the importance of the innate capacities of children in language creation and acquisition have cited the events of the language's formation as evidence in support of their theories. This paper, however, will examine the role of adult linguistic input in the initial formation and subsequent development of the Creole known as Nicaraguan Sign Language, in order to determine to what extent (if any) adult input is necessary for a Creole to form. We will review several positions held by scholars supporting primarily child-driven creolization theories, and also several theories stressing the importance of adults in Creole formation. We will then analyze data specific to the case of Nicaraguan Sign Language to compare the actual results of this natural experiment with the competing theories about the nature and origins of Creole languages.
The main questions being asked in this study ask: Are only children necessary for the initial formation of a Creole, or do adults also play a role? Do adults play an innovative role in the subsequent development of the Creole? And, what sources of input are required in order for a grammatically complex Creole to form as opposed to a more simplified Pidgin system of communication? We hypothesize that although children are the driving force in the initial formation of the Creole, adults are necessary for the development of the grammar into increasingly more complex forms. After a preliminary review of the data, it also seems evident that Nicaraguan Sign Language is not one homogenous system, but rather three manifestations of related but distinct communication systems resulting from the same social context (Kegl, Senghas, & Coppola 1999). Therefore, it seems prudent to expect that each of the three forms of Nicaraguan Sign Language will have formed from a different set of input. In other words, the amount of adult vs. child input required for each of the three versions of the language may vary.
There are several terms relevant to the discussion of Nicaraguan Sign Language data that should be clarified. Idioma de Señas de Nicaragua (ISN) refers to the full Creole nativized by the second generation of children, and continuously restructured by subsequent generations. ISN is the only one of the three manifestations of Nicaraguan Sign Language that can be considered a structurally complete Creole. Lenguaje de Señas de Nicaragua (LSN) refers to the signed Pidgin created spontaneously by the first generation of children in the community. Pidgin de Señas de Nicaragua (PSN) is the term used to refer to the more traditional Pidgin used for communication purposes between deaf and hearing individuals. PSN uses a mixture of spoken Spanish lip movements, signs borrowed from ISN or LSN, and common Nicaraguan gestures.
In addition to the Nicaraguan Sign Language-specific terminology, there are several general terms that must be defined. When we use the term Creole in this paper, we will be using the following definition: A full, natural language resulting from the social contact of two or more distinct languages; expanded from a Pidgin. A Pidgin is defined as: A contact language lacking in structural complexity and lexical variety. And finally, because this paper will focus on the role of adult input in the creolization of a language, it is necessary to define the parameters of the term adult : Any person old enough to have passed the sensitive period for first language acquisition (beyond adolescence) or any native user of a mature, historically established and structurally complex language.
The structure of the paper will include an overview of the history of Nicaraguan Sign Language beginning with its origin in 1979, followed by a review of the literature supporting child-driven creolization theories, contrasted with literature in favor of the necessity of an adult role in Creole formation. Following the literature review will be the analyses of texts and data from research on Nicaraguan Sign Language specifically. Finally we will compare the analysis to our initial hypothesis and present our conclusions to the problems outlined above, along with a discussion of the possible linguistic and social importance of those conclusions.
The deaf population in Nicaragua before 1979 consisted of individuals with no access to the spoken Spanish language of their surrounding society, and no network of communication among the deaf members of the population. There was no Deaf community to speak of, but rather a number of unconnected people separated from society by a strong cultural bias toward those with physical disability. The deaf of Nicaragua were generally thought to be cognitively deficient, unteachable in a school setting, and for the most part incapable of caring for themselves. As a result of their isolation from both the spoken language around them, and lack of exposure to any signed languages, the Nicaraguan deaf went through life never learning any language at all. The year 1979 would bring about a cataclysmic change, however, after the Sandinista revolution, the government created an initiative to educate the deaf of their country and opened two schools in the capital city Managua. Approximately 50 deaf children from distinct parts of the country were brought together in the first serious attempt in Deaf education in Nicaragua (Glovin 1998).
The newly created school was poorly advised to use a finger-spelling method in order to communicate with the children (Osborne 1999). Not having had any previous experience with language, the children had no concept of words or grammar, let alone sufficient knowledge of Spanish orthography to successfully communicate by spelling. Initially the children were forbidden from using their hands to communicate with each other or with the teacher in the classroom, but on the buses and during free periods the children began to compare the gestures that they used with their families in order to communicate among themselves. The teachers realized that although they were still unable to speak with their students, the children were somehow communicating with each other by use of rapid and unintelligible hand signals. Mystified and eager to understand what was happening in the school, the government recruited an American linguist, Judy Kegl, to decode the hand signals and explain how the children had been able to spontaneously create their own complex system of communication (Osborne 1999).
Kegl discovered upon her arrival that the signs being used by the then-adolescent first generation of Deaf school-children were more than just gestures, they were part of a complicated system of communication, later named Lenguaje de Señas de Nicaragua (LSN). Even more startling were her observations of the younger generation of children who had entered the school for the Deaf a few years after the first group. These young children communicated with their hands at a much more rapid pace and with a fluidity that the first group of teenagers lacked. The environment of the school in Managua was a mixing pot of languages, each child bringing his or her distinct system of home signs together into a micro society with a need for inter-communication. The result was a Creole, the Idioma de Señas de Nicaragua (ISN). The children had developed a fully complex and grammatically complete sign language, and were using it with the ease of native speakers (Kegl, Senghas, & Coppola 1999).
Kegl and other linguists have been studying the newly formed Nicaraguan Sign Language in an attempt to understand the cognitive processes at work in the creation and acquisition of language. Much of the work has focused on the important role of children and their innate capacity to learn and, in the case of ISN, create language (Bickerton 1984, Goldin-Meadow & Mylander 1990, Roberts 2000). However, there are existing theories about language acquisition and creolization that emphasize the effects of adults, along with the mature languages they use, on the formation of a language in language contact situations (Lefebvre 1998, Singler 1988, Mühlhäusler 1997).
The most well-known and oft-cited proponent of the theory that children are the driving force for Creole formation is Derek Bickerton, with his controversial Language Bioprogram Hypothesis (LBH). This hypothesis states that children are born with specific innate structures in place for learning language. When a child's language-learning program is activated, it uses the lexical information from his surroundings to fill in the structures already programmed in his brain, which according to Bickerton explains the grammatical similarities in Creoles of varying origins. Most relevant to our current study is the claim that creolization occurs within a single generation (Bickerton 1984). In a 1981 publication Bickerton states that "the period at which [a pidgin can creolize] will be decided, not by any internal development in the pidgin, but by the communicative needs of children." For him, therefore, the process of Creolization results not from any construction of language by the community as a whole, but rather relies solely on the requirements of the children to communicate, thereby activating their Language Bioprogram.
Another author who argues that children play the most important role in Creole formation is S. J. Roberts, and although she disagrees with some of Bickerton's interpretations of his data, they both agree that children drive the creolization process. Referring to Hawaiian Creole English: "Older children learned [Pidgin English (PE)] at school or from neighborhood friends, they brought it into the home where younger siblings began to learn it, children then began to speak to their parents in PE while being addressed in the [Ancestral Language (AL)], and eventually parents began to speak with their children in both AL and PE" (Roberts 2000). According to Roberts' interpretation then, the parents were still speaking their ancestral language while the children began the process of language mixing in school. The children then began influencing the adults and eventually the changes spread to the older generations.
A third scholar, Goldin-Meadow, emphasizes the ability of children to regularize and make more complex the input which they receive, producing a language far more developed than that which was presented to them originally: "children can produce language output which exceeds language input, and...children have the ability to organize the pieces of language they receive to produce a linguistic system which is governed by rules not used by the adults in their environment" (1990). Goldin-Meadow, along with Roberts and Bickerton would interpret the events leading to the creation of Nicaraguan Sign Language as evidence supporting the importance of child innovation in the process of creolization.
On the opposite side of the debate are authors who contend that although children may play a part in Creole formation, adults are a necessary and influential component of the process. Lefebvre could actually be considered to argue that adults provide the sole input required to form a Creole. She describes her interpretation of the creolization process: "the creators of a creole language, adult native speakers of the substratum languages, use the properties of their native lexicons, the parametric values and semantic interpretation rules of their native grammars in creating the creole" (1998). In other words, a Creole is formed by taking the rules from each substratum language's grammar and combining them. The process according to Lefebvre is the opposite of Roberts' interpretation, beginning with the mixing of the different adult language grammars which may then be extended and nativized to include children, thereby transmitting the Creole to subsequent generations.
Singler also argues that adults are the creators of Creoles. If one accepts the concept of a Creole as a language with substrate and superstrate influences, as is commonly done in reference to plantation Creoles and other "classic" Creole situations, then by definition there must be a strong adult influence. The substratum language itself is a developed adult model from which the Creole is derived, or at least heavily influenced: "substrate influence, by its very nature, assumes an important adult-language influence" (1988). Of course, this assumption is based on the idea that all Creoles do in fact have a substrate influence. In the case of Nicaraguan Sign Language, however, it is somewhat more difficult to define what the substrate languages are, given that in most cases the home sign systems that the children used were no more complex than a Pidgin, certainly not complete languages themselves, although they would be the best equivalent of a person's native or ancestral language in terms of the elements of a "normal" language contact scenario.
Another proponent of the importance of adults in language change, Mühlhäusler, draws his conclusions based mainly on research about Tok Pisin, a well-researched expanded Pidgin. He acknowledges that the distinction between a Creole and an expanded Pidgin is minimal, and that aside from their historical origins, the complexity in grammar and usage in each classification is comparable. On the topic of adult input in creolization, or the formation of an expanded Pidgin, Mühlhäusler remarks that these complex products of language contact "illustrate the capacity of adults to drastically restructure existing linguistic systems" (1997). Although the supporters of adult input do not categorically deny the influence of children on the process of language change, they stress the importance of an adult model and indicate that adults are in fact capable of significantly changing a language's grammar.
The case of Nicaraguan Sign Language is a nearly ideal natural experiment for testing the competing theories outlined above most closely correlates with the actual events of ISN's formation. We will be using several recent studies specific to Nicaraguan Sign Language development and their data in order to pinpoint what kinds of adult input affected the development of the Creole, and to what extent.
Adult input refers to any of a number of possible sources of input, not to a single readily identifiable source, and not always the same sources in every scenario. So to begin with, it is important to define the parameters of adult input within the Nicaraguan Sign Language context. One study deals not with Nicaraguan Sign Language in particular, but which sheds light on the situation outlines the different possible sources of adult input to a deaf child in a similarly language-deprived environment. In this study by Goldin-Meadow and Mylander (1984), it is suggested that adult input into early gesture systems may have been derived from the following sources: imitations of a hearing adult's immediately preceding gestures, a gesture model provided by the mother from which the deaf child could induce regularities, or thirdly, maternal responses from gestural communications by the child. It was concluded, however, that none of these situations actually shape the development of the children's language, which means that parental input was likely not a factor in the formation of the signed systems in Nicaragua.
Pidgin de Señas de Nicaragua undoubtedly received input from both the hearing and deaf populations. Spanish-speakers influenced PSN by adding a lip movement to the gestural repertoire, a characteristic that is not present in ISN or LSN. The deaf PSN users were also familiar with a more heavily sign-based system (as opposed to iconic gestures), and so some of the arbitrary signs filtered through to the Pidgin (Kegl, Senghas, & Coppola 1999). In the same article, Kegl, et al discuss the interactions between LSN signers with their instructors in the setting of a vocational school. "In the vocational school, the teachers themselves used a mixture of signing and speaking with the students (PSN). The students, in turn, used PSN with their teachers and other hearing staff members" (Kegl, Senghas, Coppola 1999). The communications between adult LSN signers and adult Spanish-speakers constitutes a definite source of adult input influencing LSN, and possibly filtering to ISN learners who use an evolved version of LSN as their input model.
Finally, according to the definition being used in this investigation, all of the LSN and ISN signers who have grown beyond their sensitive period for language acquisition use a crystallized version of the signed language. Their linguistic models are considered to be adult input even if by age they would be considered an adolescent. We will discuss in more depth the effect of adult input from each successive generation of ISN speakers in section 3.3, but here it can simply be said that the older generations of ISN signers are indeed an important source of adult input in the development of the Creole.
The initial contact between the first group of children to come together had a different set of adult influences from the generations that followed, because there was no coherent linguistic model for them to access, apart from the gestures and other movements that accompany speech with semi-regularity. "We believe that in many cases paralinguistic, non-manual gestures used with unconscious regularity by hearing Nicaraguans gesturing to Deaf signers or even to each other at home or in these school contexts may inadvertently contribute to what eventually become grammatical building blocks of LSN and ISN" (Kegl, Senghas, Coppola 1999). Beyond these paralinguistic gestures and rudimentary home signs used by individuals with their families, the initial adult input prior to the formation of LSN consisted of commonly understood motions such as pointing or movements meant to symbolically represent the motion or object itself. None of this input included a means to refer to events outside of the present concrete space of the speaker.
The formation of ISN resulted from a much more coherent language model than the original children were able to access. LSN had already formed by the time the second generation of children entered the school in Managua. The new children were able to use LSN as their main source of adult input, regularizing and making more complex grammar rules than their older peers. The result was, of course, a more complex language, and it seems that even though the LSN model was incomplete, it was sufficient to activate whatever mechanism children possess for native language acquisition. However, even ISN appears to have gone through a relatively gradual creolization process: "Although the first cohort, as children, had access to multiple sources of input (e.g., the interlanguage formed by older students in the early 1980s, family homesign gesture systems, and the gestures that accompany spoken Spanish), the language did not develop its full complexity by 1983" (Senghas & Coppola 2001). A partially formed adult model (LSN) was key for the initial creation of ISN, but the language would undergo further development in the years ahead.
A large amount of research has focused on the changes in ISN's grammar from one generation to the next. Led by Ann Senghas and colleagues, these investigations have revealed the startling speed at which the newly formed Creole is evolving. Investigates the changes in grammar of INS through its first few generations. One recent study by Senghas (2003) separates the participants into three cohorts, depending on the chronological year in which each person entered the ISN community. All of the participants learned the language before the critical or sensitive period for language acquisition, and speak it with a native proficiency. Interestingly, the oldest and youngest cohorts seem to be unaware that their grammars differ; rather, they view the others' version of the grammar rule as incomplete or incorrect. The middle cohort is conscious that there is a difference in the grammars of the generation immediately preceding them and following. Senghas concludes that even after the initial crystallization of the Creole's grammar with the earliest cohort (now the oldest members of the community) the subsequent generations of children entering the community reanalyzed the system used by the older children and made further adjustments to its grammar. All crystallized versions of ISN (spoken by anyone beyond the critical period) affect the formation of the language in each generation that follows.
A similar study by Senghas and Coppola (2001) examines the usage of spatial modulations in two groups of participants, divided according to the year in which they were first exposed to Nicaraguan Sign Language. The results show that the second group (the younger of the two) produced more spatial modulations than the first, indicating that "sequences of child learners are creating Nicaraguan Sign Language" (Senghas & Coppola 2001). The younger members of the group not only learn the grammar from the older members, but they actually make modifications and add layers of complexity that surpass the original model.
As explained in Kegl, Senghas, & Coppola (1999), there are actually three distinct manifestations of Nicaraguan Sign Language, each of which has its own possible set of adult influences. The most complex form of the language, Idioma de Señas de Nicaragua, is a natively spoken Creole which is continually adjusted and added to by each new generation of children that enter the ISN community.
The evidence implies that adult input was necessary for the formation of ISN. The Creole was not in fact fully formed by the first generation of children creating the language, but rather a result of the second generation building upon the adult model provided by their older peers (Senghas & Coppola 2001). Although the grammatical changes in the language took place during childhood, the model is transmitted by adults for future development and stabilization. ISN required both an adult model (LSN) and children's innovation to create and advance the grammar (Senghas 2003).
The Lengua de Señas de Nicaragua (LSN) is a less complex but still systematic form of the language, created by the first generation of children to come together. This form of the language, although not as structurally complex or fluidly spoken as the previously mentioned form, shows the immense creative capabilities of children; the group of children with no experience in any language, and without any coherent model from which to draw, was able to combine each individual's basic system of home signs, used primarily to communicate simple ideas with their families, into the communally accepted system of communication that served as a template for the following generations to build upon. Children seemed to be the principal creative force shaping the new language in its earliest native form. Therefore, aside from a possible lexical influence based on gestures which accompany the hearing population's speech, the formation of LSN appears to have occurred without any noticeable degree of adult input.
The simplest form of Nicaraguan Sign Language, Pidgin de Señas de Nicaragua (PSN), actually developed in parallel to ISN and LSN. ISN and LSN did not evolve directly from the corresponding Pidgin, but rather has developed as a simple means of communication between the Nicaraguan Deaf and their hearing counterparts. PSN contains many more gestural elements than the more complex sign languages used within the Deaf community (LSN and ISN), and it also links many of its manual signs with a mouthed version of the Spanish equivalent. Of the three manifestations of Nicaraguan Sign Language, PSN has the most obvious ties to the surrounding spoken language, and is the most susceptible to adult influence, simply because it is intended to be understood by adult hearing Spanish-speakers. The influence from children on the development of PSN is negligible, and therefore the adult input is the driving force behind its creation.
Although the formation of the three forms of Nicaraguan Sign Language each required a different degree of adult and child influence, we must revisit the main problem that this paper attempts to answer: what is the role of adult input in the formation and development in a Creole ? There are some interesting implications from the different processes required by the two Pidgins of Nicaraguan Sign Language (LSN and PSN), but the answer to our principal question lies with ISN, as the only true Creole to come out of the language contact scenario in Nicaragua. LSN is more structurally complex than the more traditional Pidgin, PSN, and represents the extent to which Bickerton's Bioprogram Hypothesis has some validity, but it lacks the same degree of complexity that full, native languages possess. Therefore, although a relatively complex Pidgin may form within the first generation of children, both children and adults are necessary in order for a true Creole language to form.
It seems that our language acquisition abilities have been evolutionarily programmed to allow us to acquire a socially constructed language from our parents. Nature also limits the extent to which a child can create a language anew, a practical restriction which prevents the spontaneous creation of a new language by countless children born every day. It is possible for a group of children to create their own system of communication similar in complexity to LSN, but it is not possible for them to natively learn a unique and original language without input from the society around them. Perhaps this is nature's way of ensuring the transmission of language through the generations, so that grandparents can communicate with their grandchildren, and so that stories may be passed down from generation to generation.
The findings of this study indicate that language does not and cannot exist in a vacuum. Although children have an extraordinary ability to acquire language and make innovative changes, that ability does not extend to the creation of an entirely new language. Rather, a language can only be fully realized if there is a stable communicative need within a social setting in which there is a constant presence of child innovation. This investigation has focused on the importance of adult input in the formation of a Creole, but it has never been doubted that children provide the creative spark that motivates the language change. It is the balance between adult and child input that allows for not only language structure, but verbal style, culture, humor, in short every aspect of a budding society.

The Michigan Corpus of Academic Speech in English (MICASE) is a compilation of speech spoke by both native and non-native speakers on the University of Michigan campus. The MICASE project was created with the intention of accumulating a corpus of the natural speech produced in an academic setting. It considers a number of different factors that allow for a diverse body of texts. These factors include gender, academic status, language status (native vs. non-native speakers), academic division, discourse mode and speech event type. These categories play an integral role when analyzing this data. English as a Second Language (ESL) instructors can use this information to identify common speech patterns that occur in academic speech in an attempt to better understand how and why common grammar structures exist and are used.
We used MICASE to assist us in our study of the use of the word so , our main focus being on anaphoric so ; as in the sentence "I don't think so ." Some researchers such as Susan Hunston and John Sinclair believe that anaphoric so must be a part of a local grammar. According to linguist John Swales "the heavy restrictions on anaphoric "so" suggest that it may constitute "a local grammar", i.e. a distinctive sub-grammar that is best treated separately from the language as a whole." This leads us to ask the following questions: Where does it come from; what are its uses; when is it used, and why is it unique to English? In this paper we will try to answer these questions based on the results found in the MICASE database.
There are many ways a speaker can use the word so . We will look at the following uses:
Deictic so is used to indicate degree, which usually modifies an adjective or manner. Some examples found in MICASE include:
When deictic so is to be used it is often accompanied by an indexing act indicating the size (or degree) to that of which the speaker is referring. Huddleston examples:
One of the most common forms of the word so in modern-day English is as a discourse marker. We've found that as stated in Discourse Analysis, "Traditional descriptions of their use do not give the full picture of the deployment of words like so and because in discourse"(204). The most common use so as a discourse marker results in a feeling of casualness amongst speakers. The word so is most often found at the beginning of a sentence when being used as a discourse marker. Of the many examples we have found of discourse marker so , here are a few:
Another interesting use of discourse marker so can be found in Discourse Analysis. "One conventional way of signaling the beginning of narrative chunks is with so "
The focus of our paper is on the use of anaphoric so . Some of the many examples of anaphoric so include:
Pro-clause complement with finite antecedent: I think so.
Examples from Huddleston:
Some examples found in the MICASE data include:
Using the word so often expresses a positive sentiment while the word not is used in the negative pro-form. Examples from Huddleston:
In the pro-predicative form, so is used as a predicative complement. The antecedent is usually and AdjP but other categories are possible. (ii. is an examples with a NP antecedent.)
MICASE Example:
In certain cases, so can be used to express surprise or emotion as a reaction to a previous statement. It is important to note, that in the absence of an auxiliary, this so would have to include a supportive do.
According to Huddleston and Pellum, the anaphoric so cannot be placed into a specific word category. The pro-predicative form in 63. can be an AdjP, NP, or PP. A pro-clausal so can be considered a pronoun or a 'noun clause'. so does differ from other NP's in that it cannot functions as a subject.
As previously stated, there are many different ways in which so can be used. Many of these uses cannot be placed into the anaphoric, discourse marker or deictic categories. Because of this we have created this other examples section where we will explain some of the other types of so .
The use of as and so indicate a relationship between the main and subordinate clauses. Huddlston examples:
When so is used as a connective adjunct it marks reason or consequence. Our resource also says that as with many other connective adjuncts, there is an anaphoric component in the meaning "for this reason, as a result of this."
Our final use of so deals with idiomatic expressions. It is often combined to form expressions referring to measurement.
This use of so can be glossed as "or something like that". Another common idiomatic expression is " "
We initiated our research by determining the total number of occurrences of so in MICASE. The database produced 16000 results, from which we took a sample of 500 entries. Of these 500 entries, 4% (20 entries) were Deictic, 79% (395 entries) were discourse markers, .6% (3 entries) were Anaphoric so , and 16.4% (82 entries) were other uses of so . From here we further investigated the use of anaphoric so ; the verbs to which it can be applied and the frequency that these verbs occur in.
We obtained a list of the verbs that take anaphoric so from The Cambridge Grammar of the English Language text by Huddleston and Pullum. The following is a list of the verbs we looked at: imagine, know, believe, seem, appear, afraid, suspect, suggest, think, say, hope, suppose, guess, assume, presume, fear, reckon, tell, gather, and trust. These verbs are all what one may call "mental state" verbs and some are used more frequently than others. We wondered if all mental state verbs could take an anaphoric so but found that many couldn't. For example:
While it perfectly acceptable to say: "A: Do you like this class?" it is not acceptable to say "B: *I like so."
The above examples prove that anaphoric so occurs in an extremely restricted environment and can only be used with some " mental state" verbs. We looked up the following verbs from the Huddleston text to find their frequency in MICASE.
Our results show that the verb "think" is the most commonly used " mental state" verb, followed by "guess", "say" , "believe", "hope", and "seem". Surprisingly, other common verbs that take anaphoric so, such as "imagine", "suspect", and "suppose" did not produce examples in MICASE. We attribute this to the fact that their uses are less common on campus, whereas they might be more common in other contexts. While there were a number of verbs used with so in present tense, only "think" and "say" were used in the past and conditional tenses. Some specific MICASE examples that demonstrated the use of "mental state" verbs are:
While not included on the chart, we also researched other verb tenses but they produced no further examples of anaphoric so . Because of its high occurrence in the MICASE database, we chose to take a closer look at the way in which the verb "think" is used.
We found that there were 152 instances of anaphoric " think so" in MICASE. We thought it would be interesting to compare the number of instances of negated and non-negated think so to see if this grammar structure is more restricted to a positive or negative use. We hypothesized that there would a higher number of non-negated think so, in the MICASE database, but surprisingly found that there was not a significant difference between the two (80 non-negated and 64 negated).
After looking at negation we decided it would be equally important to look at speaker position to see whether or not anaphoric so was more likely to occur with first or second person, singular or plural. Out of 152 examples, we found that the 1st person singular was the most predominantly used form (92.1%), while 2nd person singular was second with only 4.6%. The rest include 1st person plural at 1.3% and ellipsis use at 2.%. There were no examples of the 2nd person plural.
After looking at each of these characteristics separately (negation and speaker position), we decided to combine them to see the frequency of usage. In figure 1.3 one can see that negated and non-negated usage of think so occur almost equally. In figure 1.5 we see a similar pattern in which the negated and non-negated forms of 1st person singular are almost equal in number. While most entries of 1st person singular and plural never occur in form of a question, the majority of 2nd person singular occur in this form. Looking more closely at the chart it is evident that 2nd person singular in the non-negated form occurs twice as much as in the negated form. We interpreted this to occur because the use of negated "think so", as in "you don't think so?" can be used as a polite way to respond to disagreement. There are no occurrences of 2nd person plural in either chart and in that the numbers of ellipsis are equal.
After looking at the grammatical instances in which anaphoric so occurs we thought it necessary to examine whether it tends to occur in certain contexts more often than others and whether or not native speaker status influences its usage. There are many different divisions of speech events in MICASE where usage of anaphoric think so can be found.
We thought that since anaphoric so is often associated with casual speech and perhaps discussions that would spark uncertainty, that it would most likely occur in events where there is more group dialogue and therefore more relaxed speech. This is in contrast to events where authoritative figures are present and when formal speech is expected. For this reason, we think that think so would be most likely associated with speech events such as discussion sections, labs, small lectures, and study groups. The following graph shows the break-down of the occurrences of anaphoric so as they occur in the various speech events.
In looking at the chart one can see that anaphoric so does indeed occur most frequently in categories where there is more casual speech amongst peers in highly interactive settings. With thirty-eight occurrences, the use of think so in the study groups (SGR) by far exceeds all other speech events. Small lectures (LES) places second with 23 occurrences. The laboratory (LAB) and seminar (SEM) settings have similar numbers (15 and 12 respectively). Going down the chart, the event types become more formal and centralized, and because of this the usage of anaphoric so greatly diminishes.
As a result of MICASE being a corpus of the speech at the University of Michigan there are many factors that can attribute to the usage of anaphoric so such as age, speaker status and language background.
Looking at the charts below one can easily see that age and speaker status are closely related. There is a trend between these two charts because people in senior positions of the university tend to be older in age. Therefore, it is presumed that formal language is spoken more often by these speakers in these contexts.
Since so has such a restricted use, we wondered how prevalent it would be in the academic speech of non-native English speakers on campus in the previously mentioned speech events.
To gain insight into how anaphoric so translates into other languages we consulted native and non-native speakers of Spanish, French, German and Hindi. When asked how to translate "I think so" into Spanish, our consultant replied with "Yo Pienso." ("I think"). This shows that initially he did not know how to incorporate anaphoric so into his L1. We later asked how he would translate "Creo que sí" into English, he replied with "I think so." Our consultant for Hindi is from New Dehli, India but has learned Hindi and English concurrently throughout his whole life. Upon asking how he would say "I think so" he struggled to find even a semi-accurate equivalent in Hindi. While he does use anaphoric so in his everyday speech in English, he has never had to apply it in Hindi. These observations combined with other glosses we collected show that while there are structures that express the same idea, there is no anaphoric so in other languages. Here a just a few examples of the structure used for anaphoric so. Glosses are provided in the parentheses.
From here we decided to look at the language background of the speakers who use anaphoric so . We assumed that MICASE would support our hypothesis that non-native speakers would be least likely to use it in their academic speech. Using MICASE would provide us with the most accurate answer for this question. As Ann Mauranen stated, "Because MICASE aims at depicting how English is used on a campus, instead of providing a model of correct or ideal usage, it includes, among other things, a good deal of English spoken by people who would not fit into the category of speakers known as native speakers. In other words, the data has not been subjected to any linguo-ethnic cleansing, but rather it seeks to reflect the reality of English spoken in the characteristic activities of a major American research university" (166). The speaker status in MICASE is divided into four categories: Native Speakers (NS) which are speakers of North American English; Native Speaker Other (NSO) which are native speakers of non-American English; Near Native Speakers (NRN) which are non-native speakers who consider English as their current dominant language and who appear to have native-like fluency and grammatical proficiency; and finally, Non-Native Speakers (NNS) which are non-native speakers of English other than near-native speakers (MICASEweb).
Out of the 152 instances of think so in the database, we found 140 entries from Native Speakers, 7 entries from Non-Native Speakers, 4 entries from Near Native Speakers, and 1 entry from Native Speakers Other. This overwhelmingly confirms the hypothesis that of the speakers cited in the MICASE database, anaphoric so is restricted to Native Speakers of American English. Just as with our small sample of consultants, this data shows the absence of anaphoric so on a larger scale, in international English and in other languages.
Throughout our research we have more clearly defined the grammatical uses of so and more specifically the use of anaphoric so . We found that there are specific mental state verbs that can take this form of so and some like think and say are more commonly use than others. The MICASE data shows that it was only these two verbs that were able to take tenses other than the present. In our sample of 500 occurrences of so , only two of these examples were anaphoric, 95% or 152 of 160 entries of think so were anaphoric. Because the verb think produced the highest occurrence of anaphoric so , we chose to look at it more closely. When considering negation and speaker position of this verb, it was evident that negation did not play a significant role in the usage of anaphoric so ; however it was most commonly found that speakers used anaphoric so in the first person. Over all, the situations in which this structure occurs are those where there is a high level of interaction between speakers in a more informal environment. There appears to be a strong correlation between age and status in the university in regards to frequency of usage of anaphoric so . This is predictable because professional figures tend to be older in age and find themselves in more situations where they occupy formal roles. By far, our most significant find was the difference in usage or anaphoric so between native and non-native speakers of American English. The data shows that 92.1% of the anaphoric think so was produced by Native Speakers of American English, while Non-Native Speakers produced this so only 4.6% of the time. In conclusion, the MICASE data that we collected further proves that anaphoric so should be considered a local grammar because of its heavily restricted use both grammatically and contextually. We feel that in the future, anaphoric so will most likely become more common as the spread of American English takes place throughout the world. If anaphoric so becomes more frequently used, it will be necessary to develop second-language teaching/learning materials that deal with this localized grammar.

The task of recognizing textual entailment is a difficult one that can be approached in a number of ways. The first PASCAL Challenge for Recognizing Textual Entailment (RTE1) received submissions that applied various semantic, syntactic, and statistical methods towards performing this task (Dagan et al. 2005), all of which were based on sound theoretical footing, but none of which achieved eye-catching results.
Within the framework of the RTE1 challenge, a dumb baseline of 50% could be achieved by simply labeling all entailments as true (or as false): in comparison, the highest accuracy achieved by any RTE1 submission was 58.6%. The modest algorithm outlined in this paper achieves an accuracy of 53.8% on the same test set.
The fact that this modest approach outperforms 5 of the 16 original RTE1 submissions (and performs comparably to several of the others), most of which involve much more complicated systems, is indicative of the current plight of RTE. As with any discipline in its early stages, simple systems can offer initial strong results, while the more intricate systems will require further development before their potential becomes apparent (Bayer et al. 2005). (See my other paper for the promising approaches for RTE)
Despite the fact that this paper illustrates that the BLEU algorithm can be adjusted to perform better on RTE, I do not think this is an area that warrants further work. I can imagine certain potential applications for the BLEU algorithm within RTE (e.g. as a metric for comparing the hypothesis H to atomic propositions generated by a RTE system), but the algorithm is not useful on its own as anything other than a baseline.
Before going too far, it will be helpful to briefly outline what exactly is meant by textual entailment. For a more thorough discussion of the textual entailment task and definition, see (Dagan et al. 2005, sections 1-2).
In a nutshell, textual entailment is loosely defined to hold if the meaning of a hypothesis text H can be inferred by an average human reader (using only his knowledge of English and some general world knowledge) given a text T. For example, below are two T-H pairs. In the first, entailment holds (the entailment is judged to be true) while in the second, entailment does not hold (it is judged false)
Many entailments are mere paraphrases,1 while some are more involved and require the application of world knowledge. Entailment is said to be applicable to a number of different fields within NLP (see section 3.2 for a list of the subfields recognized by RTE1), where it is desirable to know whether the information in a hypothesis can be derived from a given source text.
The fact that so many entailments are paraphrases, repeating some or all of T in H, is the main reason that the BLEU algorithm works for RTE. As will be seen, n-grams cannot hope to capture every case of entailment, but the BLEU algorithm can tailored to suit the RTE task.
The BLEU algorithm was created by (Papineni et al. 2002) as a method for judging the performance of machine translation systems. In this use, BLEU compares the output of a MT system (called the test or hypothesis) to one or more human-generated translations (the reference). The score of the system translation is based on the number of n-grams (with values of n that typically cover the range from 1-4) appearing in the test that also appear in the reference, modified by a brevity factor that penalizes the test for being shorter than the reference (on the fair assumption that any two translations should be roughly equivalent in length).
The scoring algorithm goes something as follows:
1. For each i up to N, calculate a score si that is the ratio of the count of i-grams co-appearing in both reference and test (ctest,ref) and the count of i-grams appearing in the test (ctest):
2. Average the values of si. This is accomplished with a weighted geometric mean; the weight wi is typically kept constant for all i (wi=1/N for all i).
3. Calculate the brevity penalty. If the length of the test (t) is greater than the length of the reference (r), then there is no penalty (b=1). Otherwise, the penalty is logarithmically derived from the ratio of the two lengths:
4. Finally, calculate the overall score as the mean of all scores multiplied by the brevity penalty.
Alternatively, all together:
The scoring algorithm described above is that found in Papineni's Perl implementation of BLEU, which was used for the BLEU evaluations used in this study and served as the model for the modified BLEU algorithm described below.
Application of the BLEU algorithm to the RTE task makes sense on the level that an entailed hypothesis will very likely (though not necessarily) contain many of the same words that appear in the source text. Thus, the basic core of the algorithm, matching the co-occurrence of n-grams, remains valid.
However, the relationship between text and hypothesis in RTE is not the same as the relationship between test and reference in MT. The most important difference is that while in MT both test and reference are expected to convey the same information, in RTE the hypothesis is only expected to contain a subset of the information contained in the text. While it is true that in some cases of entailment H will contain roughly the same information as T, it is counter to the definition of entailment that H could contain more information than is stated (explicitly or implicitly) in T.
There are several consequences that derive from this fundamental difference. The first consequence is that it is clear which of T and H should be considered the text and which the reference in the BLEU framework. Clearly, the hypothesis should be considered the test (the equivalent of the candidate translation), since we want to count the number of n-grams in H that also appear in T, and not vice-versa.
A further consequence of this difference is that there is no longer a motivation for the brevity penalty. Entailment hypotheses are very often shorter than the source text, by virtue of the fact that they contain only a subset of information in the source text. Thus, directly penalizing the hypothesis for being shorter than the text is not productive in RTE.
It is a simple matter to eliminate the brevity penalty in the BLEU algorithm, but there is actually a second penalty against brief or truncated hypotheses hidden in the scoring algorithm. This arises from the use of a weighted geometric mean to average the n-gram scores. Although stated earlier in log terms, the formula for calculating sn can be equivalently stated as:
This formulation makes it is easier to see that if si is null for any value of i, then the entire score will also be null. This is extremely harsh in the RTE task because often, due to the fact that the hypothesis is a highly summarized or truncated version of the source text, there will be no n-gram overlap for higher values of n. For example, 63% of the entailment pairs in the RTE1 development set had no n-gram overlap for n=4.2
To rectify this problem (clearly we don't want 63% of our data to have a null score), there are two options: we could use a lower value of n, or we could change the averaging function. It is not desirable to reduce the value of n: 37% is still a significant number of entailments that make use of the 4-gram overlap, and it is likely that these longer phrases represent the algorithm's best hope for capturing syntactic features. Besides, even at n=2, a significant number (15%) of the RTE1 development set would receive a null score. It is preferable to have a continuum of gradated scores than to break the data into essentially null and non-null categories.
The obvious solution is to use a linear, rather than geometric mean. In fact, Papineni et al. state that this same averaging method yielded good results during the development of the BLEU algorithm, but was later discarded because it did not account for the exponential decay in n-gram overlap for increasing values of n. This is less of a concern in RTE, where the main objective of this algorithm is to measure word overlap. Thus, we can use a linear weighted average such as:
In fact, this average score will act as the overall score in our modified algorithm, since there is no brevity factor. The values for si and wi are calculated as above.
The evaluation for the performance of the algorithms was the same as that defined for the RTE1 challenge. Accuracy was measured as the fraction of correctly labeled true or false entailments as produced by the system (i.e. the percentage of judgments that are correct). A second measure, a confidence-weighted score (cws), was computed. This measure weights judgments based on their relative ranking as follows:
Although other measures for evaluation, such as precision, recall, and f, have been recognized as potentially insightful for RTE (Dagan et al. 2005), they were not included in this project because they were not part of the RTE1 challenge.
The RTE1 challenge consisted of a development dataset, containing 283 true entailments and 284 false entailments (567 total entailments), and a test set containing 400 true and 400 false entailments (800 total entailments). In addition, the RTE2 development set consists of 400 yes and 400 no entailments (800 total entailments; they decided to change the terminology, but yes/no denotes the same thing that true/false denoted in RTE1). Thus, the dumbest baseline for all of these systems, choosing always either true or false, is 50%.
The examples for the RTE1 datasets were subdivided into categories: these categories correspond to different areas of NLP and the entailments are representative of the type of texts that arise in that area. The categories in RTE1 were: information retrieval (IR); comparable documents (CD); reading comprehension (RC); question answering (QA); information extraction (IE); machine translation (MT); and paraphrase acquisition (PP). Most systems showed slight differences in performance between these categories.
The first run used the unmodified BLEU algorithm to assign a score to each text-hypothesis pair in the RTE1 development set. For this run and all others, N was chosen to be 4, which is also the default value in the Perl implementation. After running the algorithm on all T-H pairs, a cutoff score scut was determined such that all entailments with a score equal to or lower than scut were judged false, and all entailments with a score higher than scut were judged true.
As mentioned above, the unmodified BLEU algorithm assigns a high number of zero scores on the RTE dataset. In fact, zero is an attractive cutoff point; as figure 1 above helps illustrate, the optimal accuracy on the development set was found to occur at two points: scut = 0.002 and scut = 0.132. Both of these cutoffs give accuracies of 53.8% (299/567). The lower value was chosen, as it is unclear what is represented by the second peak; a cutoff score of zero, on the other hand, has a certain aesthetic appeal.
At this cutoff, the unmodified BLEU algorithm correctly identified 253 false entailments and 46 true entailments, for an overall accuracy of 53.8% (299/567). The difference in number of correct false and correct true judgments is simply due to the fact that the majority of hypotheses received a zero score, thus falling below the cutoff score.
Applying this same cutoff value to the RTE1 test data, the unmodified BLEU algorithm correctly identified 253 false entailments and 163 true entailments, for an overall accuracy of 52.0% (416/800).
Both figures 1 and 2 illustrate that the unmodified BLEU algorithm achieves its peak accuracy with a cutoff of zero, perhaps because this very low score is most likely to capture the false entailments with very low correlation to their source texts. However, it is difficult to explain why the algorithm seems to have an alternative peak at higher cutoff scores: this may be due to the algorithm capturing the very highly correlated true entailments (perhaps from the CD or MT tasks). The distributions also seem to indicate a difference in the nature of the development and the test data sets: the potential peak cutoff at 0.12 in the development data would have an awful performance on the test set.
It is probably wise not to read too much into these distributions, as this system is performing at a level only slightly higher than random guessing. The best way to think about the unmodified BLEU algorithm as a RTE system is that it is a binary function: a zero score predicts a false entailment, while a nonzero score predicts a true entailment.
Since it was specifically altered to give fewer null scores, the modified BLEU algorithm provided a distinctly different distribution of scores than the original BLEU algorithm. It also achieved higher accuracy for the RTE1 development and test data
As before, we chose the optimal cutoff score scut based on the performance of the algorithm on the RTE1 development set. The ideal value was determined to be scut = 0.221; this corresponds to the maximum accuracy, as can be seen in figure 3. At this value of scut, the modified BLEU algorithm achieved 57.8% accuracy (328/567), correctly identifying 131 false entailments and 197 true entailments.
The system did not fare as well on the test data: using the previously determined cutoff score, the modified BLEU algorithm managed only an accuracy of 53.8% (430/800), correctly identifying 160 false entailments and 270 true entailments.
As a side note, if the algorithm were allowed to re-train based on the test data, the best possible accuracy would be 55.3%, using a scut of 0.1333; see figure 4. This may be an argument in favor of lowering the cutoff score for future applications.
Considering that there seems to have been a significant difference in the make-up of the two datasets in RTE1, it is worthwhile to investigate the performance of these two systems on a new dataset: the development set for the Second Recognizing Textual Entailment challenge.
The RTE2 dataset is slightly different in format consists of the categories IE, IR and QA from above, as well as text summarization (SUM). The RTE2 data were chosen "to provide more 'realistic' text-hypothesis examples, based mostly on outputs of actual systems" (Bar-Haim 2005), and thus one may expect that the datasets will be different in some ways from the RTE1 datasets.
Using the same cutoff score of scut = 0.221, the modified BLEU algorithm achieved an accuracy of 60.4% (483/800) on the RTE2 development data set. Thus, whatever changes the developers made to the datasets seem to favor the n-gram approach.
Likewise, the unmodified BLEU algorithm was tested on the RTE2 development set, achieving 56.0% accuracy (448/800) using a cutoff score of zero.
The RTE2 results at least partially validate the choice of cutoff score for the modified BLEU algorithm assigned by the RTE1 development data (0.221). The revised cutoff score based only on the RTE2 development data would be 0.2580, yielding an accuracy of 61.4%, only 1% higher than the accuracy achieved with the original cutoff score. Looking over the results of the RTE1 development and test data combined with the RTE2 development data, it seems best to keep the cutoff score near 0.22.
At this point it seems appropriate to bring up the work of Pérez & Alfonseca from the first RTE challenge. Their submission consisted of an implementation of the BLEU algorithm, but I have been unable to replicate their exact results. From their pseudo-code it would seem that they implemented a variation of the BLEU algorithm that used a weighted linear mean to average the scores. However, they do not mention that they have modified the BLEU algorithm from the version proposed by Papineni et al., nor do they give any of the details of their implementation.
I experimented with a modified version of the algorithm that included the brevity factor but used the linear rather than geometric mean, hoping to match their results. I was unable to come up with the cutoff scores they reported, although I managed to obtain loosely similar accuracy values.
Below I summarize the results of Pérez & Alfonseca's BLEU implementation, my unmodified BLEU implementation, and the modified BLEU implementation. The table contains the accuracy values for these three systems, as well as the best system in RTE1 for comparison:
While the table above illustrates that the modified BLEU algorithm proposed in this study outperforms the unmodified BLEU algorithm and the BLEU variant implemented by Pérez & Alfonseca, it also points out that none of the BLEU-based systems achieve accuracies close to the best system in RTE1, and this gap is not likely to be closed. In the next section, I will discuss the role of BLEU as a baseline in RTE and look at some other promising approaches to the RTE task.
(Bayer et al. 2005), in their submission to the RTE1 challenge, rightly point out that RTE is a difficult task and that until the complex systems are able to get their many components working well together the simple systems will outperform them. They warn against trying to "climb a tree to get to the moon" (quoting Dagan). In this sense, the BLEU algorithm is a tree; it gets us part of the way towards the solution, but inevitably leads to a dead end. It may have a role within a larger system, but the future of RTE is such that a simplistic n-gram approach will not be successful on its own.
The BLEU algorithm is not meant to be anything more than a baseline for RTE, thus it is not productive to spend much time pointing out its deficiencies. An example or two will suffice to show why an n-gram model will always fail on certain types of entailment pairs.
For example, consider the case where the entire hypothesis H appears as a clause in the text T. The BLEU algorithm will assign this a score of 1, since every possible n-gram in H also appears in T. However, this does not ensure entailment. Consider the pair:
Our first reaction might be to question the somewhat arbitrary relationship we came up with earlier, treating H as the test and T as the reference in the BLEU algorithm. However, changing this assignment accomplishes nothing, since the labels T and H can similarly be reversed in the above example to yield another false entailment that achieves a BLEU score of 1.
While problems such as this clearly show that BLEU is deficient as a stand-alone algorithm for RTE, it has potential applications as a moderate baseline and possibly as a final stage in more complex systems. One could imagine, for instance, a system that generates atomic propositions from T and uses the BLEU algorithm to compare these propositions to H.
Reading through the proceedings of the first RTE challenge workshop, it is clear that there are several interesting approaches being taken towards the RTE task. Below I list a few of my favorites, giving only a general sketch of how they work.
I group these together because even though there are some basic differences, they share a similar concept. Both approaches use dependency-tree structures to represent the T and H sentences (or clauses), and then use some procedures to calculate the difference between T and H in terms of the cost required to transform one tree/graph into the other.
Minimum tree-edit distance algorithms use the basic transformations of inserting, removing, and substituting nodes within the trees to find the shortest edit path that transforms the H tree into the T tree (or vice versa). It would be interesting to see a minimum tree-edit distance algorithm for RTE that could incorporate syntactic concepts into its cost calculations: e.g. inserting a "be" verb node into apposition structures to transform them into sentences. This would be a big project in itself! However, I think that there is a lot of room for innovation in this approach and it may offer significant progress in the future. See (Kouylekov & Magnini, 2005), (Raina et al. 2005), and others.
This approach treats entailment as translation, drawing on concepts from the field of MT. T-H pairs in the training set are aligned using software such as Giza++, and "translate" the text into the entailment. (Bayer et al. 2005)'s System 2 is exactly this type of system. Even though they describe it as a "tree" in their analogy above, it achieved the best performance in the RTE1 challenge!
While these systems may fail to directly address the underlying semantic and syntactic principles that define entailment, there is certainly room for improvement, especially in the view of those who hope to merge syntactic considerations with the statistical methods that have such success in MT. See (Bayer et al. 2005), (Glickman et al. 2005), and others.
This is an interesting approach that basically predicts the entailment hypothesis from the source text. A sentence contains several atomic propositions that each could generate an independent sentence; these atomic propositions are compared to the hypothesis to see if any of them matches. See (Akhmatova, 2005).
This is a concept (not really an approach) that is an essential feature of any effective RTE system. There are many types of relatedness between words (synonymy, antynomy, hypernymy, etc.) and a RTE system must be able to use these relationships effectively when performing various comparisons and transformations between T and H. See (Budanitsky et al. 2001) and practically any of the RTE1 submissions.
In this paper I hope to have shown that the BLEU algorithm is more effective in dealing with the RTE task when it has undergone certain modifications, namely eliminating the brevity penalty and adjusting the scoring mechanism to use a weighted linear, rather than geometric, mean. The modifications to the system led to a 2-4% increase in accuracy over all test sets when compared to the unmodified BLEU algorithm.
Despite these modifications, the BLEU algorithm remains nothing more than a baseline for recognizing textual entailment, because it lacks the room to grow and accommodate further improvements. This is not to say that it is useless for the future of RTE; it may potentially serve as a baseline for evaluating other systems and components in RTE, and it may itself act as a component in a robust RTE system.
This study made use of the Perl implementation of the BLEU algorithm by Kishore Papineni (version 4/12/2002).

Vlach has a phonemic system of five vowels. However, when I was building a sound inventory in a previous project involving this language, I recorded a wide variation in the realization of these five vowels. In this project, I will use the Praat software package to perform a close analysis of the nature of some of these variations.
In keeping with the intended scope of this project, I have decided to record the following words (Vlach in IPA transcription, followed by English glosses):
These words were chosen to allow me to compare certain vowel sounds appearing in certain environments. Below I outline the vowels and environments that will be analyzed as well as the comparisons I will make.
To obtain a statically representative set of data, each of the above words was spoken ten times by a native speaker and recorded into a digital wave file. To standardize the speaker's pronunciation as much as possible, each word was recorded within a frame sentence and the list order was randomized.
After recording, the sound files were analyzed using the Praat software. Each of the fifty tokens was isolated and its duration measured.1 The frequencies of the first three formants were measured at three points separated by equally spaced intervals throughout the duration of the vowel: at 1/4, 1/2, and 3/4 of the duration. The formant measurements were obtained automatically by Praat; only in situations where Praat's formant values were clearly incongruent with the spectrogram did I measure the formant frequencies myself. In these cases, the measurement was made by subjectively locating the central area of the formant on the spectrogram in Praat.
The analysis provided ten examples of each of the five vowels being investigated, and for each example, three measurements (corresponding to the 1/4, 1/2, and offset times) of each of the first three formants. To approximate the vowel quality of each token, these three measurements of each formant were averaged together. These values are reflected in figure 1 below, which plots F1 and F2 for each token on a traditional vowel chart.
To give an approximate value for the vowel quality of each of the five vowels being investigated, the values for the three formants were averaged over the ten tokens relating to each vowel. The mean values for F1, F2, and F3 for each vowel (in Hz), as well as the standard deviation for each formant, are listed in the table below along with the mean value (in milliseconds) and standard deviation for the vowel duration.
n.b. In the tables below, all frequencies are in Hz and all times are in seconds.
The tables below outline the average values for F1, F2, and F3 at each of the recording intervals, the overall average values for F1, F2, and F3 with standard deviation, and the average vowel duration with standard deviation, for each of the five vowels.
The vowel chart below plots the F1 and F2 (averaged over 1/4, 1/2, and 3/4) for all fifty tokens:
Fig 1. Vowel chart based on average values for all vowels being discussed.
The vowel chart above clearly shows a similarity between the two realizations of [i], and noticable distinctions between the three environments of [] and []. Below I will discuss the specific data and how it relates to each of the three comparisons I am making.
Based on my transcription of these words, I expected to find that the [] in [] was a lax version of the [] in []: not quite as high and not quite as front. In fact, the analysis showed virtually the opposite result. While there is an audible difference in the sounds, it is not the same as the difference I had supposed. Consulting the tables above, the differences between the vowels are: F1 is consistently lower in [] (avg. 358 Hz) than in [] (avg. 321 Hz); and the vowel duration in [] is significantly shorter (avg. 67 ms) than in [] (avg. 93 ms). However, the vowel chart also shows a fair amount of overlap in the F1/F2 values, suggesting that the difference between the two vowel realizations is minimal.
One cause of these differences is the extreme lack of stress on the [] in []. Because the second syllable is stressed, the initial syllable in the word is barely pronounced at all; resulting in the noticeable shortness of the vowel duration (by far the shortest out of all five vowels). The lack of stress is also quite evident in the spectrogram, where the contrast between the light first syllable and the dark second syllable was striking. I think that the shortness the main reason why I transcribed the word as I did. While the final syllable of [] is similarly not stressed, it has a significantly longer duration because it occurs at the end of the word.
It is possible to suggest an explanation for the difference in the two vowels in terms of the target theory of coarticulation. The final [i] in [] shows a clear, if not especially pronounced, tendency for F1 to gradually rise over the duration of the vowel, corresponding to a lowering of the tongue. This can occur because at the end of the word there are no remaining target positions for the speaker to aim at. This shift in F1 may simply be due to a general relaxing of the mouth at the end of the word, or it may be a controlled movement toward the "ideal" location for this phoneme.
The [] in [] on the other hand, shows much less of a shift in F1, and overall a much higher F1. Both syllables examined here begin with an [l], which has assumes a high tongue position before the vowel. However, in [] the speaker anticipates another high tongue position consonant, [s] ([p] has no target for tongue position), so the tongue is not allowed to relax to a lower point in the mouth as much as in [].
The overall result here is that both vowels are clearly the same phoneme and are in fact realized in a very similar manner. The major observable differences (shorter duration and higher tongue position in in x) could be attributed to the overall relaxation of the mouth that occurs at the end of the word in [] It would be good to see measurements of this phoneme in more different environments to judge the overall range of realizations of this phoneme. It may be that there are tense/lax allophones existing in other environments, or it may be that the only changes that appear in the realization of this vowel are due to the surrounding phones.
Consulting the vowel chart above, it is clear that the [] in [] forms a cluster distinct from that of the [] in [], the [] being lower (avg. F1 683 Hz) and more back (avg. F2 1368Hz) than the [] (avg. F1 607 Hz, F2 1570 Hz). These measurements explain why I transcribed the two vowels differently when recording them, even though they should both be realizations of the final /a/ of a feminine noun or vowel.
A closer analysis of the data can help explain why this vowel is realized so differently in the two positions. Consulting the table above, it is clear that while the [] in [] maintains fairly constant values for its formants over the duration of the vowel, the [] in [] changes drastically from onset to offset. F1 and F2 both experience a rise/fall of over 200 Hz over the duration of the vowel. The resulting effect is that the [] is actually gradually becoming more like the [] over its duration. This is best illustrated by a vowel chart, where we can compare the formants of the two vowels at onset and again at offset:
Fig 2. Formant values for [] in [] and [] in [] at ¼ duration (left) and ¾ duration (right)
The chart above shows the ten values for each vowel at onset, and the ten values at offset. The change is dramatic; while the two vowels are clearly distinct at onset, the [] has shifted to overlap with the region of the [] (which remains stable) by offset.
The conclusion must be that the speaker has the same target for both vowels, but the environment of [] causes the vowel to initially be produced higher and more front/centrally than the actual target. This is consistent with the fact that the vowel is preceded by a velar stop, which we would expect to produce much sharper transitions than the bilabial stop in x. Because the vowel occurs word-finally, there are no future anticipated targets and the speaker is able to fully reach the intended target of the vowel, which is represented by the offset values in both environments.
Based on the previous analysis, it would be reasonable to expect that the first [] in [] would have very similar initial values to the [] in [] , but that the vowel would never fully reach its low, back target values due to the following []. In fact, this is very similar to what is observed, but the difference between the two vowels is much larger than expected.
Referring back again to fig. 1, the vowel chart, it is clear that the first [] in [] is a vowel that is distinct both from the [] in [] and the [] in []. Also, as expected based on the transcription, this vowel is more similar to the [] than the []. However, the values of this medial [] are significantly different than the word-final [] in [] . Even the onset values, which should be very similar since both vowels are produced immediately after a [] sound, are rather different; the medial [] has average onset values (F1 389 Hz, F2 1855 Hz) that indicate a much higher position than the final [] (F1 481 Hz, F2 1738 Hz). Furthermore, the formant frequencies of the medial [] never even approach the final values of the final []; the highest F1 recorded for the [] in [] is around 500 Hz, compared to the final value of 700 Hz for the word-final [].
While the formant values of the first [] in [] do vary over the duration of the vowel (showing an overall increase in F1 of around 100Hz, and a decrease in F2 of around 300 Hz), the vowel is generally stable. The vowel duration is short relative to the word-final [] , which may partially explain why the values do not change more.
The question of whether this vowel is the same phoneme as the [] in [] (and thus also the same phoneme as the [] in x) is difficult to answer based on this data. The difference between the formant values of this [] (in [] ) and the other [] (in []) are significant, and would support the argument that they are in fact different phonemes. It is difficult to believe that the entire difference in formant values could be due to the surrounding consonants. However, to fully answer this question, one would need to analyze the features of word-medial [] sounds, which do occur in Vlach, and also study the distribution of the environments in which these sounds occur as compared to word medial [] sounds.
The best conclusion seems to be that the final [] in [] and the [] in [] are one phoneme, with the same target values (indicated by the offset formant values of those two vowels, indicating a low, back vowel), but the first [] in [] is a different phoneme (a more central, mid vowel). The confusion arises from the preceding [] in [], which causes the low, back vowel to have more mid, central formants at the onset of the vowel. This makes the vowel sound overall more like the mid, central vowel that appears in the first syllable of [].
The results above show that the surrounding consonants do play a role in the realization of vowels in Vlach, as do the stress of the syllable and the position (e.g. word-final) of the syllable containing the vowel. While the variations that appeared in this analysis were generally able to be explained by concepts like the target model of speech production and the locus theory of vowel transitions, larger issues such as what vowels may or may not be the same phoneme, or allophones, or entirely different phonemes, cannot be answered with such a small data set. To answer these larger questions would require both acoustic analysis of more vowels in a number of similar and contrasting environments, as well as a paper analysis of the distribution of the phones. In particular, the [] / [] question could be further investigated by doing a similar analysis to the one above incorporating word-medial [] (which does appear in my word list), ideally in the same or similar environment as word-medial []. This could help explain whether my transcriptions are over-detailed, using two symbols for the same phoneme, or if these sounds are in fact distinct in Vlach.
Below is the randomized wordlist used for the recording sessions. Before the session, the native Vlach speaker and I went over the short wordlist so that he knew which words he would be producing. We also agreed on the frame sentence below:
I prompted the speaker before each sentence with the English word (e.g. "chicken") which he then would translate and pronounce in the frame sentence, (e.g. [] The wordlist was visible to him to read along with the IPA transcriptions as we progressed.
The wordlist consists of each of the five words repeated ten times, randomized / shuffled by hand. Below is the wordlist.

In this project, I comp are affricates to stops and fricatives. In the analysis, the Vlach affricate [] is compared to the stop [] and the fricative [], and the affricate [] is compared to the stop [] and the affricate []. The comparison is based primarily on the duration of the consonants: since an affricate consists of a stop followed by a fricative, the first part of the affricate will have durations that can be compared to the stop consonant (e.g. stop closure duration and stop aspiration duration, if any), and the second half of the affricate will have a duration of frication that could be compared to the duration of frication in the fricative consonant.
I expect to find that the total duration of the affricate consonant is less than the sum of the durations of the stop consonant and the fricative consonant, because phonemically they are all consonants (i.e. the affricate is itself a phoneme, not simply an adjunction of a stop phoneme and a fricative phoneme). I suspect that the overall duration of the affricate will be similar to the overall duration of the stop or the fricative; thus I expect to find that the duration measurements of the independent consonants (stop closure duration, stop aspiration duration, and frication duration) will all be shortened.
A second measurement will be made of the peak frication frequency. This should correspond to the place of articulation of the fricative. Thus I expect to find that the frication frequency for the affricate [] is the same frequency as in [], and similarly for [] and [].
The main part of the experiment will consist of looking at these stops, fricatives, and affricates in word-medial position in two-syllable words. In all cases, the consonant in question will be the beginning of the second syllable, which is unstressed, so that a controlled comparison can be made. As a secondary analysis, these results will be compared to stops, fricatives, and affricates in word-initial, stressed position. I do not expect to find any major differences between the word-initial and word-medial position occurrences, except for perhaps a slight lengthening due to stress. If lengthening occurs, this should be the same for all word-initial consonants; thus the comparison between word-initial stops, fricatives, and affricates should be valid.
The data was acquired by recording the speaker with a solid-state digital recorder (a Marantz PMD660) and a non-condenser microphone in a soundproof recording studio. All recordings were made directly to wave files on a Compact Flash memory card, and then analyzed with the Praat software.
The full wordlist is given in the appendix. It consists of the following words:
Words were chosen to have two syllables: the first beginning with a stop and the second beginning with a stop, fricative, or affricate for the word-medial measurements; and the first beginning with a stop, fricative, or affricate and the second beginning with a stop for the word-initial measurements. Vowel quality was controlled as much as possible while using real Vlach words. Similarly, the consonants were controlled as much as possible, with the notable exception of [], which contains a consonant cluster at the beginning of the second syllable. No suitable two-syllable word could be found at the time that contained the [] word-initially and a single stop at the start of the second consonant.
The careful choice of words was made to facilitate the use of relative duration measurements; in these measurements the duration of the consonant is compared to the duration of the entire word, thus helping to reduce the effects of speech rate (which can have a significant effect on the absolute duration of a word or any part of the word). In this project, both absolute and relative duration values are used.
For each token, the relevant measurements were made from the list below; non-relevant measurements (e.g. frication frequency of a stop) were not recorded. The possible measurements were:
Accordingly, each stop had three relevant measurements (word, closure, and aspiration durations); each fricative had three relevant measurements (word and frication duration, and frication frequency); and each affricate had all five relevant measurements. Each word was recorded ten times: with three fricatives at three measurements each (90 measurements), three affricates with five measurements each (150 measurements) and two stops with three measurements each (60 measurements), this gave roughly 300 individual measurements.
From the above measurements, the total duration of the consonant was calculated (frication duration alone for fricatives; closure + aspiration duration for stops; closure + aspiration + frication duration for affricates). This total duration was divided by the word duration to give the relative duration of the consonant.
Frication frequency was determined by a script1 using Praat's formant-finding capabilities; measurements were taken at two points equally spaced over the fricative duration and then averaged together.
Word duration was taken to begin at the release of the initial stop closure2 and end when the final consonant ended. The first measurement point is rather objective; there is nearly always a clear release point for the initial stop. However, determining the end of the vowel can be subjective, so certain criteria were adopted to control for this. The end of the final vowel was taken to be the point where the first formant and the higher formants begin to disappear: the second (and sometimes third and fourth) formants have a tendency to linger longer into the final aspiration that occurs as the speaker releases excess air after the word.
Frication duration was rather easily measured as starting with the onset of wide bandwidth frication noise, and ending with the disappearance of the same noise. This occasionally left a period of low-amplitude noise that followed the frication but preceded the clear onset of the following vowel, but since this was consistently not included in the measurement, the criterion seems sound.
Stop closure duration is rather easy to measure in word-medial position and does not need to be described much here. See the results section below for a discussion of measuring word-initial stop closures.
Stop aspiration duration was taken to be the period occurring after the visible release burst of the stop but prior to a clear onset of the ensuing vowel formants. This was readily observable in the stop consonant, but not very observable in the affricates. Several affricates had a period of low-amplitude noise following the apparent stop closure release but preceding the onset of high-amplitude, wide-bandwidth noise that signaled the onset of the frication. When this appeared, it was measured as stop aspiration. However, in several instances this phenomenon was not observable; in these cases the stop aspiration was taken to be zero.
The first portion of this section will deal with the results obtained from the word-medial instances of these consonants. Word-initial consonants are discussed further below. Results follow for the key measurements made: consonant duration and fricative frequency.
As expected, the frication frequency remained constant between affricates and fricatives. The table below highlights this similarity:
As the table illustrates, the peak frequency of a fricative consonant or the fricative portion of an affricate is an important measurement that can distinguish between different fricatives (e.g. between an alveolar and a post-alveolar fricative/affricate), yet is consistent between fricatives and affricates with the same place of articulation.
In the results, affricates showed a reduction in closure time relative to stops, and a reduction in frication time relative to fricatives, with a slight overall increase in total duration of the consonant (with a corresponding increase in the relative duration of the consonant). The table below lists these differences.
Below is a graph comparing the durations of the five consonants. The total duration is broken into pre-release (i.e. the closure duration) and post-release (the combined aspiration and frication durations):
The figure illustrates the fact that overall, consonant durations are roughly the same, with affricates being slightly longer than stops or fricatives. The absolute values are generally comparable with the relative values.
Note the significant reduction in closure duration when comparing a normal stop (97 ms) to an affricate (46ms or 51ms). The duration time of the stop closure in an affricate is roughly 50% that of a normal stop.
There is also a reduction in duration of frication noise, although it is not as drastic as the closure duration reduction. For the alveolar fricative/affricate, there is a reduction in frication duration from 139ms to 98ms (70%), and for the post-alveolar from 135ms to 89ms (66%).
The total duration of the affricate is longer than that of either the fricative or the stop, though not as long as the sum of the two. Because the durations of the individual components are reduced, as described above, the total duration of the affricate is only slightly longer than the duration of a fricative or stop by itself. The alveolar affricate has a total duration of 162ms, 17% longer than the alveolar fricative (139ms), and 37% longer than the alveolar stop (118ms). Similarly, the post-alveolar affricate has a total duration of 145ms, 7% longer than the post-alveolar fricative (135ms) and 23% than the stop (118ms).
Similar calculations can be made to compare the relative duration of the consonant, rather than the absolute duration. The relative duration of the alveolar affricate (38% of total word length) is 31% longer than the relative duration of the alveolar fricative (29% of total word length). Likewise, the relative duration of the post-alveolar affricate (36% of total word length) is 9% longer than the post-alveolar fricative (33% of total word length). The alveolar affricate has a 17% longer duration than the stop, and the post-alveolar affricate has a 10% longer duration than the stop.
One interesting and complicating point is the presence of aspiration after consonants and in affricates. There was a clear, though short, period of aspiration following the release burst of the stop [t]. Furthermore, there sometimes appeared to be a similar, short period of low-amplitude noise following the stop closure in the affricates and preceding the high-amplitude frication noise. However, I am unsure whether to call this aspiration or whether it is simply a brief period of time that is needed to build up enough airflow for frication noise to occur at high amplitudes. Regardless, I measured this time period when it was visible, and it does not play a major role in the analysis a) because it is so small and b) because the overall duration incorporates this duration and so is not affected. When it does appear, it is preceded by a visible burst corresponding to the release of the stop closure. However, in some instances the affricate shows neither a clear stop release nor the ensuing period of low-amplitude noise. Below are spectrograms of the affricate [] as it was produced in two different recordings of the same word:
As stated above, I do not think it is appropriate to call this aspiration (as it is labeled in the figure), but rather it seems that sometimes the burst is strong, and sometimes is weak or practically nonexistent. Thus, it does not seem that the burst following the stop release is an essential component of the affricate -- this could be something to study in a perception experiment.
A final point worth discussing is the marked difference in affricate aspiration duration between [] and [] that is obvious in table 2 above. The previous paragraph described the different realizations of [] that were observed, with either a clear burst or no burst. However, the affricate [] was consistently produced with a sort of double burst, which was measured as a stop closure, followed by aspiration, followed by frication. The figure below illustrates this phenomenon:
As the figure above illustrates, there are in fact two visible bursts: the first is taken to be the release of the stop closure, and the second is taken to be the beginning of the frication noise. This phenomenon was consistently produced for the [] affricate but not for the [] affricate.
A word of caution: the data for the word-initial consonants are much less reliable than that for the word-medial consonants. The main reason for this is that the speaker often paused before pronouncing the word, which introduces a significant amount of uncertainty into the measurement of the duration of the initial stop closure. Furthermore, since the initial stop closure was not included in the word duration measurements (see the methods section above), this complicated the matter of calculating relative durations of the initial stop closure.
Due to this difficulty, a partial analysis that ignores the effects of stop closure duration, focusing solely on frication, is given below. In this analysis, I use the same criteria as for word-medial consonants, measuring the word duration as beginning at the point where the initial stop closure is released. The frication data, unaffected by the initial stop closure, is sound. At the end of this section, an analysis including the stop closure data is given, although with the caveat that the data may be corrupted.
Because only the post-alveolar fricative/affricate was measured word-initially, a comparative analysis of alveolar frication to post-alveolar frication can not be carried out. However, the results show a strong correlation between fricative and affricate frequency, and are comparable to the results observed in section 1 above:
Compared to the results obtained for word-medial consonants, the post-alveolar fricatives in word-initial position have a slightly higher mean frequency (by about 150Hz). The standard deviation is also significantly higher than in section 1; since the standard deviation is greater than the difference in mean frequency between word-initial and word-medial fricatives/affricates, not too much should be made of this observation. It may be that the increase in frequency is due to the difference in stressed/unstressed positions, but such a hypothesis would need further data to bear it out (incorporating more than one fricative/affricate and looking at stressed/unstressed positions both word-initially and word-medially).
The table below illustrates the absolute duration of the frication only in the two consonants. Relative values are not included for two reasons: first, relative values were not calculated in section 1 (relative values were calculated only for total consonant duration in the affricate, not for the frication duration independently), so there would be no basis for comparison. Also, the two target words used here were [] and []; the consonant cluster in the second word can be expected to have a longer duration than the single stop in the first word, potentially corrupting any empirical value of the relative duration measurements.
The result here is clearly comparable the result observed in section 1: the frication duration is decreased in the affricate compared to the fricative. Here, the duration of frication in the affricate (85ms) is reduced to 65% that of the affricate (130ms). Recall that the post-alveolar frication duration was reduced similarly to 66% (from 135ms to 89ms) in the word-medial position. Thus it would seem that word position and stress have little effect on the duration of frication in post-alveolar fricatives and affricates.
For the reasons mentioned above, this data should be considered unreliable. However, it is included here for completeness. One set of data with an especially egregious pause was removed prior to calculation of these values, but even for the data which are presented here, problems remain. Below is a table illustrating the stop closure duration and aspiration duration in word-initial stops and affricates:
Below is a figure that illustrates the differences in duration of the three consonants:
This figure shows a similar trend as figure 1(the duration values for word-medial consonants), although here the stop closure measurements must be considered unreliable, resulting in an overall longer length for the [t] values. Notice, however, that the stop aspiration duration is comparable to that observed in results section 1 above; this is not significant for the analysis, but it is encouraging to see consistency where we expect to see it, even if the stop closure measurements are corrupted.
Next, consider the total duration (and total relative duration) measurements. Similarly to the previous section, all of the consonants have a similar duration, and the fricative is slightly longer than the affricate (140ms rather than 130ms). This 8% increase is very comparable to the 9% increase seen in results section 1 for the word-medial consonant. Unexpected is the fact that the total stop duration (150ms) is 7% longer than the total affricate duration (140ms). Quite the opposite result was seen above, where the same comparison showed the affricate duration 23% longer than the stop duration. I suspect that this has to do with the excessively long stop closure of the word-initial stops, described elsewhere in this paper. However, I cannot explain why this lengthening should apply to the stop consonant but not to the affricate. The stop closure duration here word-initially is 127ms, compared to 97ms for the same duration word-medially, a 31% increase. However, the stop closure duration in the affricate word-initially is 55ms, compared to 51ms in the word-medial measurements, only an 8% increase. This is somewhat puzzling, but I suspect it is at least partly due to the inconsistent inter-word pausing that the speaker displayed.
Looking beyond these inconsistencies, the word-initial analysis is quite similar to the word-medial analysis. Further recordings, with special emphasis on avoiding the inter-word pauses, could help reveal to what extent, if any, the lengthening of the stop closure in the stop consonant is a valid feature of word-initial, stressed consonants and not simply due to pauses between words. This might also help explain why this lengthening occurs only for the initial stops, and not the initial affricates.
I would identify three major points of difficulty in this project. One has to do with the recording, one with the measurements, and one with the analysis.
First of all, I feel that the recordings made for this project were less than perfect, although, with the exception of the word-initial stops, the results of the experiment seem to be fairly sound. Nevertheless, during the recording session, there were several points where the speaker stumbled over his words and mispronounced the target words (e.g. saying [] for [] in multiple instances). I attribute this to many factors, including the repetitive nature of the word list and the high degree of similarity in the target words. However, I also think that the printed list I used to prompt the speaker was flawed: I should have used a phonemic transcription rather than a phonetic transcription, in order to mask the underlying nature of the affricates. Furthermore, I used the random number function in Excel to randomize my wordlist; I feel that this was actually worse than a pseudo-random wordlist. It seemed that there were too many instances of the same word being repeated twice or even three times sequentially, and these were exactly the places where many of the speaker's mistakes occurred. Furthermore, the list concentrated some words heavily near the end and others near the beginning of the list; thus, the words were not uniformly distributed over the recording session and factors such as fatigue may have played a role in the results. In future projects I will use a pseudo-random or locally-randomized wordlist.
A second complication had to do with measuring word-initial stop closures. This was discussed above, and I think the best solution is to have a frame sentence that will highlight the distinction between the previous sound and the stop closure (in fact, this was not a problem with my frame sentence), and also to encourage the speaker to say the entire frame sentence at a constant pace. The speaker in this experiment had a tendency to pause before and after the target words so as to highlight it; this resulted in exceptionally long silence before the target word which made it difficult to measure the initial stop closure. Even with a constant pacing throughout the sentence, though, there will still be a slight additional pause between words.
The final complication also had to do with the issue of word-initial stops. The point of release of the word-initial stop closure was chosen as a very identifiable point to measure the beginning of the word, specifically to avoid the problem of the pre-release closure and inter-word pauses. However, this led to difficulties when trying to measure the relative duration of word-initial stop closures: if the stop closure is not considered part of the word duration, then the relative duration calculations are meaningless (in one instance, the relative duration of the closure was over 100%: it was longer than the duration of the entire word! This was of course due to the speaker's pause before pronouncing the target word).
These last two difficulties could have been anticipated ahead of time, and in fact they were: my original conception of the experiment did not include the word-initial consonants for exactly the reasons above. However, I decided to add them to give an additional dimension to the analysis and to have more tokens to include in the data. In fact, I am not sure this was necessary and ultimately it may have added unnecessary confusion to the analysis.
The data in this experiment overall bore out the hypothesis regarding affricates, stops and fricatives. As hypothesized, the affricate was seen to consist of elements of the stop consonant (specifically, a stop closure followed by a release burst) and elements of a fricative consonant, and the duration of these elements was noticeably decreased in the affricate consonant, giving the affricate a total duration roughly equivalent to the total duration of the stop and fricative consonants.
The peak frication frequency in each affricate (alveolar and post-alveolar) was statistically identical to the peak frication frequency in the corresponding fricatives. There was a slight, nearly insignificant increase in fricative frequency of the word-initial, stressed instances of [] and [] ([s] and [ts] were not measured word-initially) compared to the word-medial, unstressed instances. The increased frequency may be due to some aspect of stressing in Vlach, but this needs further analysis, as outlined above in the results section.
Another area that warrants further analysis is the nature of the stop release in these affricates. It was seen that the stop release burst for [] is sometimes very pronounced and sometimes completely invisible. In most cases, the release was prominent, and it would be worth investigating whether the burst is preferred or necessary for perception, as well as what factors contribute to its production (fatigue may be a factor, since it seems that most of these non-burst affricates occurred near the end of the recording session). Furthermore, the double burst phenomenon in the [ts] affricate is interesting in itself, and could also be investigated as a perceptual experiment.

For this project, I will look at measurements of pitch, intensity, duration and vowel quality to analyze stress in Meglan Vlach, which I hypothesize is similar to English stress. Thus, I expect to find that pitch, intensity, and duration of a syllable is increased when the syllable is stressed, and reduced when the syllable is not stressed. It is also likely that vowel quality will be reduced (towards a mid-central vowel) in the unstressed syllables. By measuring the values mentioned above, I hope to confirm that pitch, intensity, duration, and vowel quality are indeed factors in stress in Vlach, and furthermore I hope to quantify these effects to some extent.
Similar to past experiments, the native speaker produced words from a randomized wordlist, which consisted of six instances of each of the following four words:
As can be seen, these four disyllabic words constitute two minimal pairs which differ only in the placement of stress. (Such minimal pairs occur often due to the conjugation of Vlach verbs, which results in pairs such as [] / [], distinguishing present from past tense.) The full wordlist, and the frame sentence in which the words were pronounced, are included in the appendix. To help avoid confusion (due to the similarity of the words in the list), the stressed syllable was underlined and the English gloss was printed alongside each Vlach word. Also, I used a masking guide so that the speaker only saw one line from the wordlist at a time, limiting the distraction of other words in the list.
The words were recorded using a solid-state digital recorder (Marantz PMD660) and a non-condenser microphone in a soundproof recording studio. All recordings were made directly to wave files on a Compact Flash memory card, and then analyzed with the Praat software. A Praat script (included in the appendix) was used to gather the various measurements, as described below.
All files were annotated to denote the start and end of each syllable as well as the start and end of the vowel portion of the syllable. The full syllable was used to measure syllable duration, while the vowel portion was used to measure intensity, pitch, and formant frequency. This was necessary to eliminate any artificial reduction in intensity or pitch due the non-sonorant portions of the syllable (e.g. stop closures). Values for pitch, intensity, and formant frequency were all extracted using Praat built-in functions. See the script for more details.
The built-in functions in Praat were effective for extracting most measurements, but the data needed a minor amount of hand-correction after execution of the script. This included manually gathering some pitch measurements which could not be measured automatically by Praat (perhaps due to the low pitch level in the second syllable), and correcting for some halving in the pitch measurements (several unstressed second-syllable measurements were recorded at roughly 40Hz when they clearly should have been at 80Hz).
The sections below summarize the results obtained for the four features hypothesized to be related to stress: duration, intensity, pitch, and formant frequency.
There was a visible trend relating stress to syllable duration lengthening. The data for the four syllables are summarized in the table below:
The data clearly show that stressed syllables are roughly 10-30% longer than their unstressed counterparts. Interestingly, the magnitude of the lengthening effect varies between word-pairs: the [] pairs show a 35% lengthening effect for both syllables, while the [] pairs show a roughly 10% effect, which is slightly more pronounced for the second syllable than for the first.
Intensity measurements also showed a positive correlation to stress. The data are summarized in the table below:
These measurements show that stressed syllables have a significant increase in amplitude compared to unstressed syllables. For all cases, the increase in dB was around 10%; the word-final syllables [] and [] showed a somewhat more significant level of amplification (16% and 12%) compared to the word-initial syllables (both 10%). This may be related to the fact that word-final syllables naturally have an overall lower amplitude than word-initial syllables, and stressing the final syllable compensates for this effect as well as raising the intensity of the syllable.
The table below summarizes the results for peak pitch. Mean pitch was also measured, but these values closely resemble the peak values and do not offer additional insight.
Clearly, pitch is a significant factor in marking stress, with marked increases for all stressed syllables. The effect is more pronounced on the second syllable (38% and 46% increases) than on the first syllable (19% and 20% increases). This effect can be attributed to the significantly lower pitch of unstressed second syllables compared to unstressed first syllables (note that first and second syllable pitch is roughly comparable in the stressed cases). Thus a possible explanation may be that the speaker naturally has a decrease in pitch throughout the production of the word, but this natural effect is countered when the second syllable is stressed, in a similar manner as was seen in the intensity values.
The four tables below illustrate the effects of stress on the values of the formant frequencies in the four syllables. The trend is to reduce the vowel quality towards a more centralized sound in unstressed syllables; thus the frequency will either increase or decrease depending on the quality of the vowel (a high vowel will be lowered, whereas a low vowel will be raised). This is reflected in the positive and negative values for percent change in the tables below:
The tables above show that each vowel has a slightly different quality when produced in stressed versus unstressed positions. The vowel [] is the most constant, changing its formant frequencies by only a few percent. The vowel [u] in the [] syllable shows the most regular and noticeable change, and the vowel [] in the syllables [] and [] also shows a significant difference between stressed and unstressed realizations. While the tables above show the quantity of the changes in terms of percentages, the figure below helps illustrate the quality of these changes, which all have the effect of minimizing the vowel quality:
Fig 1: Formant chart showing stressed and unstressed vowels.
In the figure above, all instances of the recorded vowels are plotted, and these instances are grouped together with ellipses to illustrate the centralizing effect. The effect is seen most clearly on the [] vowel in the [] / [] pair: the stressed, tense realization forms a distinct group than the lax, unstressed, and centralized realization (with one outlying instance).
The [i] vowels have a less clear distinction between stressed and unstressed realizations: while the unstressed instances tend to be less front than the stressed instances, there is a high degree of overlap between the two groups.
Finally, the [x] vowels show an apparent distinction between stressed and unstressed realizations, but there is a certain amount of inconsistency in their production. While the centralized variant has a significantly lower F1 in most cases, a few examples of the unstressed [] can be seen residing well within the group defined by the stressed instances. Additionally, one instance of the stressed [] lies within the group defined by the unstressed instances of the vowel. Thus, while there is a visible trend to reduce the vowel quality in unstressed syllables, this trend is not always followed, and may not be a strict condition for stress.
The results of this experiment strongly support the hypothesis that stress in Vlach is produced as a conglomerate of effects that include increasing the stress, intensity, and duration of the syllable, and reducing the quality of the vowel towards mid-central values. The strongest of these effects is pitch; values for peak pitch in stressed syllables were between 20-45% higher than values for unstressed syllables. Duration values also showed significant changes: the stressed syllables [] and [] were 35% longer than their unstressed counterparts, and the stressed syllables [] and [] were 9% and 13% longer, respectively, than their unstressed counterparts. Intensity also increased, roughly 10%, for stressed syllables.
The effects on vowels were much less consistent, but clearly vowel reduction does occur for unstressed syllables both in initial and final position. The initial [] in the [] / [] pair showed the least amount of reduction, while the final [] in the [] / [] pair, and all instances of the [] vowels showed clear contrasts between reduced (unstressed) and non-reduced (stressed) variants. However, the [] vowels showed several examples of crossover between groups, which suggests that the vowel quality is not a restrictive feature of stress production; stress can still be produced even if the vowel quality is not changed in the usual way.

This experiment builds upon my previous project, which described the various factors involved in the production of stress in Vlach. This previous project indicated that stressed syllables in Vlach were distinguished by a combination of factors including vowel quality, intensity, pitch and duration of the syllable. For this project, I performed a perceptual experiment that aimed to determine whether it was possible to shift the perceived stress in a disyllabic word by modifying the pitch alone and controlling for the other factors of vowel quality, intensity, and duration. The hypothesis was that by starting with a sample that was neutral in terms of these three factors, the pitch on each syllable could be adjusted to determine which syllable was perceived to be stressed. However, the experiment failed, with the subject identifying only 15 of 290 tokens as having word-initial stress. To understand why this failure occurred, one must consider two factors: the experimental design and the nature of the sole participant in the experiment. This paper is an attempt to find the weakness in this experiment so that future experiments might be carried out with better success.
The motivation behind this experiment was to quantify the role of various factors involved in the perception of stress in disyllabic words in Vlach. Since it appears that at least four factors (pitch, intensity, duration, and vowel quality) are involved in the production of stress, all four of these factors must be somehow accounted for in the experiment. This experiment focuses on the role of pitch, and it was conceived as if it were to be one part of a four-part experiment: the other three parts of this experiment, if carried out, would focus on the three other factors involved in stress.
A major obstacle lies in the task of measuring one of these factors (or variables) in isolation from the others. One of the assumptions of this experiment, which may be flawed, is that is possible to measure the effects of changes in one variable in isolation from the other variables. The assumption is that there exists (in principle) a token that is neutral in terms of stress for all variables. This neutral token, if it were heard by a native speaker, would be perceived as ambiguous. Furthermore, we might expect that if it were repeated several times and the speaker forced to judge whether it had first-syllable stress or second-syllable stress, the speaker's judgments would be evenly distributed between the two choices.
The first step in the design of this experiment, then, was to find a token that was neutral for all stress variables. The second step was to modify this neutral token to create a number of tokens with gradated values of pitch for both the first and second syllables. Finally, the modified tokens, along with several control tokens, were played to a native speaker in a forced-choice identification experiment in which the participant had to identify whether the stress was on the first or second syllable of the word.
For reasons described below, one word was chosen to be used in the experiment, a combination of the words []" he enters", and []" he entered". The first and second syllables were modified over a range of five pitch values, which combined to produce twenty-five tokens with modified pitch. Additionally, unmodified tokens of [] and [] were included as controls. Two additional control tokens were included: one was the neutral token with its original pitch contour; the other was the neutral token with a flat pitch contour. These twenty-nine tokens were repeated ten times each to obtain 290 total responses.
Since it is difficult to modify formant frequencies in Praat, the vowel quality differences between stressed and unstressed syllables could pose a major problem to the design of an experiment such as this one. However, it had been observed in my previous project that the vowel quality does not necessarily change significantly between stressed and unstressed syllables in certain words. Most notably, the final vowel of the word [] was produced either as a low-back vowel or sometimes as a slightly more mid-central vowel, with no apparent effect on the stress. Thus, it was possible to find instances of the minimal pair [] / [] where the vowel quality was independent of stress.
Below is a chart showing the vowel formants from the first and second syllables of these words, as recorded in the previous project on Vlach stress. While there is a visible difference between the vowel quality of unstressed and stressed vowels (both [] and []), there is overlap between the groups. The arrows indicate the values for two tokens identified as having relatively ambiguous vowel quality: token 09 (an instance of []) and token 16 (an instance of []) from the past project.
Due to their neutral vowel quality, these tokens were used as the basis for generating the neutral token that formed the basis of this experiment. (Both token 09 and token 16 were included unmodified as control tokens in this experiment, and both were correctly identified every time. This confirms that they are acceptable examples of both first-syllable and second-syllable stress, despite the lack of any significant vowel reduction. This would seem to justify their use in this experiment.)
Intensity is a significant factor in stress production (and presumably perception), and in order to create a neutral token, the intensity of the two syllables must be relatively equivalent. However, it turns out that it is very difficult to increase the intensity of an unstressed syllable to the level of a stressed syllable because it is so quiet to begin with that the amplification introduces a significant amount of noise.
Therefore, the decision was made to create an artificial token consisting of the stressed first syllable of token 16 and the stressed second syllable of token 09. This would solve the amplification problem and would not introduce any vowel quality problems, as can be seen in the figure above both tokens have very similar formant values for both syllables. Also, this artificial token should include less inherent bias than a token that was produced as an actual instance of first-syllable or second-syllable stress.
The final variable to control for is duration, which was modified using Praat's duration manipulation feature. The duration values were chosen to be midway between the stressed and unstressed duration for the syllable in question. Thus, using tokens 09 and 16 as references, the first-syllable duration was modified to be equal to 185ms (midway between 160ms and 211ms), and the final syllable was left unmodified at 261ms, as the difference between the two final-syllable durations (278ms and 261ms) was deemed to be statistically negligible.1
The duration modifications may have been a weak point in the experiment, as they were based only on the two tokens mentioned above, not on the whole of the data from Project 4 which shows the full range and average values of the durations of these syllables. If the full Project 4 dataset had been used to calculate mean midpoint values for duration, the first-syllable duration would have been 205ms and the second-syllable duration would have been 278ms.
After visually inspecting the pitch contours of the first-syllable-stressed token and the second-syllable-stressed token, the following design was chosen to modify the pitch contours. The first syllable would have fixed start and endpoints, with a midpoint that would vary over a range of frequencies. The second syllable would have a fixed endpoint and a start point that would vary over a range of frequencies. This design is summarized in the figure below:
As can be seen from figure 2, the design produced 25 tokens representing the various combinations of each syllables 5 different pitch values. Values 2 and 4 (and b and d) were based on the actual pitch values in tokens 09 and 16, with additional values added below, above, and between these values to create a gradient. Thus, modified token 2d is very similar in pitch to token 09 and modified token 4b is very similar in pitch to token 16. Thus, one would expect to see similar judgments for those pairs of unmodified / modified tokens.
The two figures below demonstrate the similarity of the pitch contours of these tokens:
This design expects token 2d to be identified as first-syllable stress by comparison to token 16, and for token 4b to be identified as second-syllable stress by comparison to token 09. The other tokens between 1a and 5b would be expected to receive judgments based on, perhaps, the relative pitch of the first and second syllables.
The results for this experiment were indicative of some type of experimental failure. Of 290 tokens, only 15 (including all 10 instances of the control token 16) were identified as having first-syllable stress. All others were identified as having second-syllable stress.
Of the remaining 5, these included the first instances of 4b, 5b, 5c and 3b, as well as a late instance of 2a (likely an error). Tokens 4b, 5b, 5c, and 3b are all tokens that would have been hypothesized to be judged as first-syllable stress by the experimental design. Thus, the experiment seemed to be working as planned up to a point. However, once token 16 was played to the listener, he never again identified any token other than 16 (and the late instance of 2a) as first-syllable stress.
Perhaps the most insightful result is that the unmodified base token, which was presumed to be neutral in terms of stress, was consistently identified as having second-syllable stress. This is a strong indicator that the "neutral" token on which all of the modified tokens were based was not, in fact, neutral.
Some of the comments made by the experimental participant may also be insightful. During the experiment he made some comments: first, he asked how he should respond if the word had stress on both syllables; later, he indicated that he had decided to just always choose the second-syllable option in the ambiguous cases. After the experiment had ended, he indicated his suspicion that some of the recordings had been made by splicing a stressed first syllable and a stressed second syllable, although in a later comment he revealed that he was not certain whether this was the case or whether he had in fact produced the doubly-stressed token himself in the previous recording session.
In his critique of the experiment, he stated that although the various tokens were "moving the stress" from the second syllable to the first, they never fully did so in his opinion. This indicates that the pitch alterations did have some effect on his perception of stress, but that the changes were not enough to fully shift the stress to another syllable. Thus, we should not assume that pitch is irrelevant to his perception of stress.
However, it may be that since the tokens were still not fully natural, he sought another indicator of stress on which to base his decision. In his own words, this indicator was the "length" of the final vowel, by which I presume he means duration rather than vowel quality, although I am not entirely sure. This certainly is a result of using the stressed second-syllable in the creation of the neutral token, and it may suggest that the second syllable duration needed to be shortened more. It is unclear why the second-syllable was perceived as more stressed than the first, since both recordings were in the stressed position.
It is also possible that by "length" the participant meant vowel quality. This is a possible interpretation, since the final vowel was of the non-reduced variety. However, the vowel quality was very similar to that of the [] control token, where it did not seem to affect the perception of stress. Nevertheless, it is possible that the participant chose to focus on vowel quality in the ambiguous, modified tokens, and that in token 16 the vowel quality could be neglected due to the presence of other indicators of stress.
The participant also had questions about whether the experiment recorded his response time to answer the question. This, along with his other observations, indicates that he was actively trying to see through the purpose and design of the experiment, and the results must be considered at least somewhat distorted as a result of this.
To be entirely honest, this experiment may have had design flaws that arose from its hasty preparation. The experiment could have been more robust by including modifications of other factors than pitch, but this was considered too complicated for one project. The flaws of this experiment could be corrected for in future experiments, but this would require more time and probably more than one speaker.
A naturalness judgment option was not included in this experiment, but one might have been helpful. From the participant's comments, it seems likely that all of the modified tokens would have been identified as unnatural to some extent.
One problem with the experiment might be that the intensity of both syllables was left unchanged. While this was done based on the assumption that it is only relative intensity, and not absolute intensity, that matters for stress perception, it would have been a much better idea to reduce the intensity of both syllables to a more neutral value. It may be that the high intensity of the second syllable is what cued the listener to place the stress on the second syllable. Although the first syllable had similar intensity, the intensity may have been perceived as higher for the second syllable by virtue of the fact that the latter part of a word will tend to have lower intensity than the first when no other factors are present.
One conclusion that can be drawn about the perception of stress in Vlach is that there are many factors involved, and modifying only one of these factors results in an unclear stress pattern. In the absence of other factors (i.e. if the control tokens had been absent) it may be that stress alone is enough to shift the stress of a word. This seems to be supported by the very early responses in the experiment, but cannot be confirmed due to the disturbed results for the majority of the experiment.
While one might conclude that this experiment shows that pitch is not a significant factor in the perception of stress in Vlach, I am not too quick to agree with that conclusion. In my opinion, the experiment was flawed and I would like to see the results of using these same tokens, without (or in a separate session from) the control tokens, and using Vlach speakers other than the one in this experiment.
Each token was surrounded by a target sentence, which was the same as in the previous project:
In the list of responses below, the sequence number, prompted token, and response are listed. A response of 1 indicates first-syllable stress, a response of 2 second-syllable stress. The stimulus "unmod" represents the neutral token with its original pitch contour, and "flat" is the neutral token with a flat pitch contour.

Statements concerning the stress pattern of Hawaiian date back to the mid 1800's, when missionaries began documenting the language in order to translate religious texts. Since that time, the conventional wisdom about stress placement was that main stress falls on the penult, with secondary stress iterating every second syllable from the main stress up to the beginning of the word; additionally, any heavy syllables always receive stress. This notion is essentially what appears in Newbrand (1951), the only comprehensive study of Hawaiian phonology.
However, many words of five or greater syllables defy this pattern. In the case of words with five short syllables, two stress patterns are observed, one that follows the description above (main stress on penult, secondary on the second syllable from the beginning), the other with secondary stress on the initial syllable. Schütz (1981) was the first to treat this inconsistency in detail, and provides a descriptive system that works. However, the system he proposes entails that stress placement is irregular, and must be specified for each entry in the lexicon.
Given that stress is non-contrastive in Hawaiian, the stress pattern would be expected to behave with some sort of regularity. This paper seeks to find this pattern, recasting the forms formerly regarded as "irregular" as regular, with deviations from the pattern falling out from an analysis of the morphological/prosodic structure of Hawaiian words. A system is developed which correctly predicts the stress pattern of Hawaiian in a vast majority of cases, using the theoretical framework of Optimality Theory (Prince and Smolensky, 1993/2004; see Kager, 1999, for overview). This paper begins with a description of the phenomena in question, followed by a discussion of the way in which the morphological structure interacts with the prosodic structure. Finally, an OT analysis will be given, using a standard constraint set with little modification.
The description given above, that stress iterates on every other syllable starting with the penult works for words with four or fewer short syllables, and such descriptions are given in much of the older literature on Hawaiian as well as in treatments of related Polynesian languages (Hayes, 1995, Blevins, 1994). Furthermore, heavy syllables are taken to be always accented. For Schütz (1981), a syllable is counted as heavy when it has two mora. Bimoraic syllables include those with long vowels and diphthongs (both long and short diphthongs are understood to have two morae). Diphthongs are any sequence of two vowels with a fall in sonority, plus iu, a so-called "even" diphthong. Schütz' characterization of the moraic status of Hawaiian segments is adopted straightforwardly here. Also, because consonants are disallowed in coda position, there is no "length-by-position" effect in Hawaiian.
Given these two generalizations, that accent iterates on every other syllable starting with the penult and the necessity of stress marking on heavy syllables, stress accent as in (1) is predicted and, in fact, observed.1
a. two light syllables
b. one heavy syllable
c. two syllables, one short diphthong
d. two syllables, one long diphthong
e. three light syllables
f. four light syllables
g. six light syllables
Although the examples in (1) do not exhaust the predictive power of the system described above, they serve to demonstrate the general patterns. The analysis given here assumes that feet are universally bimoraic trochees, without giving recourse to trimoraic feet (cp. Schütz' 1999 analysis of Fijian). Instead of allowing degenerate feet, the grammar of Hawaiian will simply not foot an isolated syllable (e.g. ka(náka) ), instead of applying epenthesis, truncation, or other processes. As seen in (1g), the system as described also accounts for longer forms with an even number of light syllables.
However, the system breaks down in words with five light syllables. The description of this type of stress pattern follows that of Schütz (1981), who also describes similar patterns in Fijian (1999) and Tongan (2001). Under the pattern assumed previous to Schütz' account of Hawaiian, exceptions to the rules are abundant. Five syllable words obeying the pattern are also observed, as shown in (2).
a. "irregular" stress pattern
b. "regular" stress pattern
Clearly, the challenge offered by these data is to account for the "irregular" forms, while maintaining a system that correctly predicts the "regular" forms. As it stands, the data in (2) present an outright contradiction. However, this situation may be salvageable given a different perspective on the regular pattern of stress and an understanding of the way in which morphology interacts with the prosodic structure of the word.
Informally speaking, suppose that the regular pattern of stress placement in Hawaiian is bidirectional, with main stress assigned from the right and secondary stress assigned iteratively from the left.2 Such systems are not uncommon among the languages of the world (e.g. Garawa, Piro, Indonesian; see Kager (1999), ch. 4, for analysis). Now, the words formerly characterized as irregular may be redefined as regular. Given the strict requirements concerning foot structure and stress on heavy syllables, all of the data in (1) are still accounted for.
Now, of course, the data in (2b) go unexplained. A solution presents itself given certain assumptions about how morphology interacts with prosodic structure. Only in certain morphological environments, to be explained below, will the stress pattern in (2b) arise; furthermore, even words with the "regular" stress pattern sometimes have their stress not due to the default process, but also because of morphological structure. In effect, the system proposed here makes a clear prediction concerning words which have five light syllables; the stress pattern will always be as in (2a) whenever the word is composed of a single stem and nothing else.
In the account given below, a "word" or "grammatical word" is defined as a structure composed of minimally one stem, which may optionally contain multiple stems plus affixes and reduplicated structure. Each word in this sense has one and only one main stress. Note that this definition is roughly equivalent to the traditional conception of "word." A prosodic word is defined as a freestanding stress domain, which may or may not have main stress, and may or may not be contained in a larger structure.
The proposal above, that Hawaiian has a bidirectional stress system, applies to all prosodic words, with the assumption that prosodic word structure may be recursive, such that the maximal prosodic word may dominate multiple, word-internal prosodic words.3 Compound words offer the clearest example of this type of structure, although reduplication and affixes will be examined in light of this proposal as well. While an account of stress patterns based in part on morphological structure is promising, care must be taken not to construct a circular argument of the type which Schütz warns against, such that "accent defines the word." Because there is no good etymological dictionary of Hawaiian, claims about internal structure must be made with caution, although there are many clear-cut cases.
With these considerations in mind, two examples below demonstrate how a system that treats each member of a compound as it's own prosodic word simplifies an account of stress. In (3), two five-syllable words with different stress patterns are given.
Although the examples in (3) differ in their stress patterns, their phonetic realization is expected given the assumptions about recursive prosodic structure detailed above. One outstanding problem concerns the placement of main stress versus secondary stress; this issue will be treated below.
Because Hawaiian grammar strictly enforces a foot structure of bimoraic trochees, stress that is assigned from the left edge of the word will go unrealized in a prosodic word of three syllables or less. In each of the cases in (3), stress is assigned from the right in all word-internal prosodic words. However, as is expected, only the rightmost accent in each grammatical word get main stress, even though right edges are always marked by a foot when possible. Put another way, the right edge of the entire word gets main stress, although the right edge of every prosodic word is footed. Although this system may seem complex, such a pattern naturally falls out of standard constraints within Optimality Theory, as shown below. Note that the two processes that mark right edges -- one that foots right edges and the other that assigns main stress to right edges -- guarantee that the right edge of a prosodic word will be footed whenever possible (i.e. whenever the penultimate syllable is not heavy).
In addition to compounds, affixes play an important role in the prosodic structure. Consider the following examples, in which reduplicated structure is treated as a prefix.
The system of stress assignment outlined will account for the varied data above given certain assumptions about the prosodic status of affixes. In the case of suffixes, such as -na in (4c), the affix appears to combine with the stem to form a prosodic word; therefore, main stress falls on the penult despite the presence of the suffix. Prefixes, including reduplicated structure, are treated differently. These combine to form a stress domain. In a case such as (4d), both "prefixes," the prefix -u and the reduplicated material, form an internal prosodic word. The same generalization holds for (4a), in which the reduplicated 'eke is treated as an internal prosodic word, although a prefix in the traditional sense is absent. In the case of (4b), the reduplicated syllable 'a may be treated as it's own prosodic word that is too small to be footed, or as adjoined to the maximal prosodic word; the difference may have import when issues of minimal words are concerned, but the distinction will not be explored here, where the later case (adjoined to the maximal prosodic word) is assumed.
The examples in (4) will, then, have the structures depicted in (4').
Having outlined the way in which morphological and prosodic structure interact, a formal account follows that strives to explain the multiple processes at play, while maintaining empirical coverage of the data.
The rest of this paper shows that the data above is quite amenable to the framework of Optimality Theory (OT). After examining a set of constraints that model the cases in which the structure of the prosodic word is not recursive, examples such as those in (4) will be considered. Since the constraint set postulated within OT is considered to be universal, it is a positive result that the following account only employs constraints that are standard in the literature. All of the constraints considered below are discussed in Prince and Smolensky (1993/2004) and McCarthy and Prince (1993), although many of these have antecedents in previous literature (see references above for overview). Only the domain of application of one constraint will have to be altered from the usual form in order to account for recursive prosodic words.
The first constraints to be discussed account for stress patterns in non-recursive prosodic words that contain light or heavy syllables. The important observations, noted above, are the following: a bimoraic foot structure that is strictly enforced (with no truncation or epenthesis), trochaic foot structure, right edge main stress, heavy syllables always stressed, and secondary stress iterating from the left.
The first constraints necessary for this account are those which enforce bimoraic foot structure and left-headedness, as well as a constraint that ensures the rightmost syllable gets main stress. The standard constraints for this account are defined below.
Additionally, a further constraint is needed to ensure that non-parsing of syllables is minimal.
Since Hawaiian always enforces a trochaic structure, I take RH-TYPE-T to be undominated. Also, the data in (7) show that a constraint such as *CLASH, which militates against consecutive syllables with stress, is very low ranked. Therefore, these constraints will be omitted from the tableaux below, for ease of exposition.
Given that Hawaiian is well documented to strictly enforce bimoraic trochees, while also marking the right edge of the PrWd with a foot, the ranking in (7) is expected.
The following tableaux bear this prediction out.
No ranking argument can be constructed between PASRE-SYL and ALL-WD-RIGHT. Furthermore, it is well attested that the final foot in any word gets main stress, so RIGHTMOST may be assumed to be undominated as well. A final constraint that may be assumed to be undominated appears in (9), since heavy syllables are always stressed. With the constraints discussed above accounted for, the grammar now has the ranking shown in (10), again omitting RH-TYPE-T and *CLASH.
While the grammar in (10) correctly predicts the stress rhythm in any word with less than three syllables, a further constraint must be introduced to account for longer forms. The alignment constraint in (11) serves this purpose; note that this is a gradient constraint, for which any foot removed from the left edge will occur a violation equal to the number of syllables that separate it from the left edge.
This constraint determines the direction of footing in any long prosodic word, while ALL-WD-R(IGTH) only forces one foot to appear on the right edge. ALL-FT-L is dominated by ALL-WD-R, as (12) demonstrates.
Before continuing with longer prosodic words with recursive structure, some words are in order concerning the current state of the grammar, which appears in (13).
Although the interaction between these constraints and candidate forms is completely mechanic, the constraints themselves achieve targets that are "natural" and common among the world's languages. As has been said before, some of these constraints simply impose a foot system based on the bimoraic trochee. The high ranking of these constraints ensures that this foot structure is never violated for any reason. The constraints WSP and RMOST give prominence to heavy syllables and (in a trochaic system) the penult, both common properties among the world's languages. Finally, the alignment constraints enforce the demarcative and rhythmic properties of stress. While ALL-FT-L enforces a rhythmic system of stress, iterating from the left edge of a prosodic word, it is not so important in this grammar as ALL-WD-R, which demarks the right edge of the prosodic word. The tableaux in (14) show that these constraints, with the ranking assigned in (13), select the correct candidate in the cases of pùlelehúa (butterfly) and 'ài:na (land).
The constraints as they have been defined so far allow the grammar to globally evaluate grammatical words with recursive prosodic word structure with almost no change. Since each prosodic word in a recursive structure has exactly the same status, and since the alignment constraints refer to prosodic words as their category of application, the grammar will select the candidate with the observed foot structure as optimal.
The only problem concerns the assignment of main stress. Up to this point, RMOST has ensured that the rightmost foot in the prosodic words gets main stress. Since the grammar will now be faced with words that have multiple prosodic words, something must be changed in order to assign the correct main stress. The domain of application of RMOST will have to be changed to either apply to the "maximal" prosodic word, looking at the morphological structure top down, or will have to apply to the grammatical word. Although the choice may be trivial, the second option will be pursued here. Now, instead of RMOST, the grammar will have RMOST', which is defined as in (15). Note that the addition of RMOST' is used merely as a label for the familiar constraint for the sake of convenience; the constraint has not changed, only the domain of application.
With our modified constraint set, observed candidates are selected as optimal even for long words with complex morphology, when the assumptions concerning the relationship between morphological structure and prosodic structure are adopted. The following tableaux demonstrate this for some representative examples. In each case, the assumed morphological analysis appears above the tableaux; within the tableaux, vertical lines and spaces separate prosodic words that occur within grammatical words.
The tableaux above and in (14) show that the grammar proposed in (13) accounts for a wide range of data that have superficially different stress patterns.4 This result shows that a fuller account of Hawaiian stress can be made when taking the morphological structure of words under consideration. At the same time, the proposal above does not define morphological structure in terms of observed stress patterns, but instead relies upon fairly straightforward analyses of word structure. The critical assumption, that prefixes (including reduplicated structure) and compounds create their own prosodic words within a maximal prosodic word seems not too radical either, when one considers that grammars often use stress to mark morphological boundaries. In effect, the grammar of Hawaiian is one that emphasizes the demarcative and quantity-sensitive properties as opposed to the rhythmic property of stress.

In this project, I explored German fricatives, a sub-class of obstruents, as produced by a native German speaker from Chemnitz, Germany. This paper discusses three main topics regarding the data collected from the speaker. First, I talk about what I expected to find based on a sampling of the literature regarding fricatives in German, including what phonemes the language includes and my data in comparison to those expectations. Second, I discuss important allophonic alternation of palatal and velar fricatives, which are represented in my data as predicted. Last, I discuss a phonemic/allophonic relationship that the literature was not coherent about, initial /r/, and the phonetic realizations of that /r/ in my data.
The literature on German is conclusive about seven distinct fricative phonemes: labiodental /f/ and /v/, alveolar /s/ and /z/, post-alveolar /ß/ and /Ω/, and glottal /h/. Though each of these is considered distinctive and not allophonic, some sounds occur only in certain environments. Namely, there is a fortition rule in German such that voiced obstruents lose voicing when in final position; in other words, the phonemic opposition is neutralized in this phonetic environment (Benware 1986: 22-27, 64-65). Thus, no voiced sounds will occur word-finally; so, the voiced /v/, /z/, and /Ω/ are represented only initially and medially in my word list. Additionally, the voiced post-alveolar fricative /Ω/ occurs only in borrowed words (often from French), so I did not attempt to find minimal pairs of it and other sounds.
My data exhibit each of these seven sounds with typical fricative characteristics: high-frequency intensity compared to other obstruents or vowels, decreasing frequency of high intensity as articulation moves farther back in the oral cavity, and visible formant transitions out of and into flanking vowels-particularly the "velar pinch" evident in velar fricatives where the F3 locus is relatively high and the F2 locus is relatively low. For the most part these were as expected, though in a few instances, the speaker produced voiceless fricatives where I expected voicing to be present. This was not entirely surprising, given that research on German is ambivalent (in my sources) as to whether there is a categorical qualitative phonation differentiation in obstruents. For the voiceless and voiced labiodental, alveolar, and post-alveolar sounds, some authors describe the primary difference between pairs at the same point of articulation as between fortis and lenis, rather than voiceless and voiced. That is, the distinguishing feature between the phonemes is taken to be the amount of respiratory energy used to produce airflow, instead of the activity of the vocal cords. In these cases, what are typically classified as "voiced" fricatives in German are actually considered "lenis," and voicing may or may not be present in a lenis segment (Benware 1986). Because of the discrepancies in the literature, for my project I elicited only what were considered "voiced" fricatives, without attempting to elicit both "voiced" and "voiceless" lenis fricatives; this is consistent with the approach that most authors seem to take, which classifies by phonation rather than respiratory energy (Kohler 1990: 48-49; Ladefoged & Maddieson 1996: 95-99; Moulton 1962: 21-23).
While my speaker's tokens of voiceless fricatives were as expected, several that I anticipated to be voiced were in fact produced without voicing. Namely, the speaker's production of Satz, where initial S is generally voiced [z] (according to the literature, which indicates that voiceless alveolar fricatives rarely occur word-initially), included a voiceless initial alveolar fricative. At first it was unclear whether this was a lenis voiceless or a fortis voiceless fricative. To try to determine which was the case, I took the initial alveolar in Satz and compared it to the medial voiceless and voiced alveolars in hasse ([s]) and reisen ([z]). (I did not include a word with the intended elicitation being voiceless initial [s], because the speaker objected to the single word in any of the literature (Satin) that included initial voiceless [s], saying that it would be no different from Satz. It is especially curious, then, that his articulation of initial s did not include voicing.)
As Fig. 1 shows, the spectrogram for Satz shows no voicing (there is no low-frequency intensity corresponding to a "voice bar"), yet it does have less intensity throughout its duration than does the medial [s] of hasse, which is also characteristic of voiced fricatives as compared to voiceless. And, its intensity follows a similar track as the voiced alveolar [z] in reisen. It also has slightly lower intensity than the final voiceless [s] in Reis. This may suggest that this is a lenis fricative, which most authors would lump into the category with voiced fricatives; however, I do not have a voiced alveolar fricative in initial position with which to compare.
A similar issue arose with the voiced post-alveolar fricative, which the speaker produced without voicing in Rage, as shown below in Fig. 2. Note that this medial fricative has a spectrogram almost precisely similar to the medial voiceless [ß] in Rauschen. He does have a voiced post-alveolar fricative initially in Jalousie; however, he commented that in casual everyday speech, he would likely have pronounced it as de-voiced. Although I can't make generalizations based on this data, it seems that my speaker does not produce voiced alveolar or post-alveolar fricatives consistently.
In addition to the seven fricatives mentioned above, German has a voiceless palatal and voiceless velar fricative that are in complementary distribution. In the literature regarding German phonetics and phonology, the phoneme is usually taken to be the voiceless velar fricative /x/, and the allophone is the palatal [ç].1 The palatal occurs following front vowels and consonants, whereas the velar occurs following back vowels. According to the literature, the velar also does not occur in initial position, though in my data it does occur in what seems to be a variant of /r/ (see discussion below). Hence, the speaker produces the palatal in Chemie where it is initial (despite the voiceless palatal being rare initially, according to Benware [41]), and reichen and reich where it follows a front vowel; he produces the velar in rauchen and Rauch, where it follows a back vowel.
Finally, the most complex aspect of German fricatives in this project was regarding variation in the sound represented in orthography by r, and according to the literature these are variants of the phoneme alveolar trill /r/. Kohler reports that the voiced uvular fricative /Ë/ is a phoneme and can be heard in words such as Rasse; however, the other sources I consulted claimed that the voiced uvular fricative is just one of several possible variants for the German /r/, not a phoneme itself (thus, I did not attempt to elicit it as a distinct phoneme). Other possible variants discussed are the alveolar trill and voiced velar fricative (Benware 1986: 44-45 and 68-9; Russ 1994: 147-149). Benware claims that variation is not phonologically conditioned for the most part and depends instead on dialect differences. Moulton, however, offers a lengthy discussion of /r/, where he says that when prevocalic it is generally a voiced uvular fricative [], but sometimes an alveolar trill [r]. He also discusses alternation in postvocalic non-prevocalic contexts, but I did not have any examples of this context in my data, so I will not discuss it here. I prepared my word list with several initial and medial /r/ tokens in order to explore what my speaker produced in these instances; however, by coincidence, my list also included many more initial /r/ words, so I was able to look at 15 tokens of initial /r/ in a five different following vowel contexts ([o][a][a¨][ˆ][aˆ]), along with five tokens of medial /r/.
Each of the medial /r/ tokens was produced as a velar, and while fahren and Waren are voiceless [], irre and führe are voiced []. The differences are apparent in voice bars in the spectrograms of the latter two, as well as a strong perceptual difference. The medial /r/ context spectrograms (for fahren1, fahren2, Waren, irre, führe) are shown below in Fig. 3.
In initial position, my speaker never produced the alveolar trill [r] (or a trill of any sort), nor did he produce a uvular []; both of these were variants said to be common in the literature. Rather, he always produced a velar fricative, though distinguishing between whether these were voiced or voiceless was often difficult. Some difficulties arose in determining voice quality via my own perception, and there was also heavy seeming coarticulation from preceding vowels often affecting phonation mid-segment. Voiced fricatives usually have either a low-frequency F1 "voice bar," less intensity than their voiceless counterparts, and/or visible vertical striations indicating vocal fold vibration. This was not clearly evident in many of the sounds, even ones that often sounded voiced. This might have something to do with their being prevocalic, or it might be that the lenis/fortis distinction is more relevant here than phonation.
The words whose spectrograms do show a voice bar are shown below in Fig. 4. These include reisen, reif, and Reis. In these sounds, you can see the low-frequency voice bar as well as vertical striations, which are fairly constant throughout the fricative.
In opposition to the above voiced fricatives, most realizations of initial /r/ were voiceless; however, there is wide variety in duration of frication, as well as the amount of voicing present within the fricative. That is, in most cases, the fricative began without voicing -- which is clear on the spectrographic information in that there is no low-frequency energy -- yet some amount of voicing occurred within the latter part of the fricative, evident in the voice bar and vertical striations. For example, in Rasse, total duration of the fricative is about 111 ms, and the last 53 ms of it are with vocal cord vibration -- more than half the total duration of the fricative. Nonetheless, the overall percept is voiceless. We might attribute this to coarticulation due to anticipatory voicing for the following vowel. By way of contrast, the initial [] in rauschen is about 128 ms, but it is only voiced for about 26 ms. This is just one example of the variety in phonation, shown below in Fig. 5.
The only initial /r/ in which at least some anticipatory voicing did not happen is rot, where there is absolutely no voicing preceding the vowel; the difference between it and other tokens is particularly striking and may be compounded by an articulation further front than many of the other velars, so that it has none of the trilling that can sometimes occur in velars farther
back (or in uvulars). The lack of voicing may in fact be an indication of articulation further front, when we consider that the fricatives produced further front seem more resistant to coarticulation from voicing of the following vowel. That is, initial [s] in Satz (which I had in fact expected to be voiced) and [ß] in Schatz, and medial [s] in Jalousie and hasse, for instance, show almost no voicing at all, as shown in Fig. 6 below. This is the case with other tokens of the voiceless alveolar and post-alveolar fricatives; although the palatal [ç] shows a fair amount of voicing in transitions into and out of it in, for example, reichen. So, the tendency for voicing to happen sooner in velar fricatives may be related to proximity to the glottis during articulation; where here, voicing starts sooner in the fricative the closer it is to the glottis. Or, in general, a fricative will be more resistant to coarticulation involving phonation if it is farther from the glottis in production. This makes sense considering that back closures cause a faster pressure drop above the glottis than more front closures (because air escapes the oral cavity sooner), which enables vocal cord vibration.
In sum, I have discussed three aspects of German fricatives as I elicited them from a native German speaker. First, the speaker's realizations of seven of the fricative phonemes were fairly regular, though I found that he did not produce voiced alveolar or post-alveolar fricatives in a few expected places. Second, the speaker's alternation between the voiceless palatal and the voiceless velar fricative was as expected, demonstrating the allophonic variation oft-discussed in the German phonemics literature. Third, in exploring the phoneme /r/, I found that the speaker only produced two variants out of at least twice that many possibilities, both of which were velar fricatives (the voiced and voiceless). His consistent use of the voiceless velar in the initial /r/ position is particularly interesting, given that none of the sources I have listed [] as a sound occurring initially, even as a variant of /r/. A follow-up project would take this as its main focal point, systematically eliciting a variety of phonetic contexts for /r/ to determine whether the speaker does produce [r] or [] in contexts other than what I elicited, or whether his dialect/idiolect consistently realizes [] or [].

The goal of this investigation is to provide evidence towards a universal theory of syllable structure as formulated by Duanmu (2002) following Borowksy (1989). The proposal that medial position syllabic rime is restricted to two slots maximally was demonstrated to be a tenable view in Borowsky (1989). In a departure from the more traditional viewpoint that onset clusters are constrained by principles of sonority, Dunamu (2002) proposes that complex onset clusters in English and Chinese are composed of one slot. Together, the two theories posit a CVX structure for syllable structure. The question remains as to whether this claim can be shown to hold for languages other than English and Chinese. This paper investigates the extent to which standard Dutch con be shown to conform to CVX theory. In section 2, I introduce VX theory as formulated by Borowsky (1989). In section 3, I introduce Dunamu (2002) and the extension of the theory to CVX.
In section 4, I present both sides of the tense/lax versus length vowel debate for Dutch, a pertinent issue in the consideration of our theories application. In section 5, I detail the methodology used in this study. Finally in section 6, I present concluding remarks.
Borowsky (1989) considers the composition of the syllable rime in English, noting that there exists an asymetrical distribution between rimes consisting of two slots and rimes consisting of more than two slots. This asymmetry can be explained, following Siegel (1974), by positing the existence of two levels of morphological affixation, level 1 and level 2. Under the Level Ordering Hypothesis, class 1 affixes, which trigger and undergo phonological processes, undergo affixation at level 1and can attach to free as well as bound morphemes. This type of affixation occurs first. Class 2 affixes, which are phonologically inert, undergo affixation at level 2 and thus can only attach to free morphemes (derived or un-derived words). Below are examples of English affixes classified accordingly (Spencer 1991):
1) Examples of English Affix Classification:
Medial syllables of the form VVC and VCC in un-derived mono-morphemic words and before the attachment of level 1 affixes are very rare. Borowsky proposes that those with more than two slots are limited to word edges, indicating a strict constraint on the syllable rime structure in English at level 1. The strongest evidence in support of this theory is that in English, vowels in medial positions shorten at level 1:
This observation assumes that long vowels take up two slots in English, which, as is discussed below, will not be the assumption made in this investigation of Dutch. The important principle we are testing is that, regardless of what parameter a language takes in defining what constitutes a filled slot, the number of available slots remains universal and constantly V(X), with V being a vowel and X being a variable consonant or vowel.
Borowsky (1989) states the Principle of Structure Preservation:
3) Structure Preservation: language-particular structural constraints holding for underlying representation hold also for derived representation, and vice versa.
This enforces conformity with the Coda Condition. After Level 1 Structure preservation is inactive and syllable structure less restrictive, allowing longer codas and making vowel shortening unnecessary. Thus, only the phonology of level 1 is to be taken as structure preserving.
(Dunamu 2002) extends the Borosky (1989) analysis of medial rime constraints though an investigation of the possible constituents of onset clusters. Traditional accounts of onset clusters are reliant upon the sonority scale. Duanmu notes the following problems with such an account (for English): Clusters which appear to violate the MSD are allowed in English i.e. [lj], English allows clusters composed of sounds in the same place of articulation i.e. [tr] [dr], and many sounds that satisfy the MSD don't appear in English.
Duanmu posits a single slot analysis of onset clusters in which occurring clusters are complex single sounds. Necessary to this are two assumptions: first that words around a syllable need not be a proper part of it (extra-metricality), and second that if a cluster is a single sound it is a good sound, and single sounds must adhere to the No Contour Principle (Duanmu 1994) stated in (4):
Example (4) demonstrates that for a complex sound to be a good sound no singlearticulator can simultaneously produce opposing values of the same feature. In (5), [schr], or [chr] with extrasyllabic [s], is treated as a complex onset occupying one slot only, [aa] is (crucially, for this study) treated as a single slot in the nucleus, and [l] is the coda.
Building on Borowsky's V(X) model, Duanmu thus proposes an articulator based feature theory of syllable structure in which non-final syllables are maximally of the form CVX.
The question remains as to whether such a theory can be proven to hold universally, which is the underlying goal of this study.
As noted above, in order for CVX theory to hold for Dutch, we must have some account of how vowels are to be represented moraically. Discussion of this matter has polarized in two directions. The first supports the view that they are to be represented in terms of tenseness and the second that they are to be represented in terms of length. Both views are discussed below, but for the purpose of this investigation, the tense/lax view is assumed to be correct.
In Standard Dutch, phonetic length coincides with phonetic tenseness to a large degree. It has been a issue of debate as to whether the distinction that both seemingly describe is to be attributed to the feature [+tense] or to length as represented by a geminate moraic structure. Van Oostendorp (2000) and Swets (2004) claim that laxness is the phonological feature underlying the contrast between phonetically long tense and short lax vowels. The most persuasive argument for this account relies upon the distribution of stress in the Dutch stress system. a length based analysis incorrectly allows CVV syllables to remain light while CVC are counted as heavy, which is in contrast to a widely held belief that when there exists a heavy light contrast, CVV counts as heavy.
Van Oostendorp (2000) concludes that a branching rime must be headed by a lax vowel.
Lax vowels do not occur syllable finally, as the vowel would then occur in a nonbranching rime. A branching rime can never be headed b a tense vowel, thus we never see a tense vowel in a CVC-closed rime:
Booij (1995, 2002) holds that systematic opposition between short and long vowels in
Dutch (except for high vowels) can be represented in structural terms rather than via binary feature (such as [+tense]). In a syllable, short vowels can be followed by at most 2 Cs i.e. V(short)CC, and after long vowels only one C can occur i.e. V(long)C (VVC).
Syllable structure is thus held to be more consonant with a theory of phonological length. Under this account, the binary nature of the rime, which consists of exactly two positions, accounts for the distribution of vowels in Dutch.
Example (7) demonstrates that in accordance with an exactly two-slot restriction in the rime a long/tense vowel cannot be followed by CC in its coda as this would necessarily result in three slots. Booij proposes that vowel length is not a purely phonetic property and that long vowels are represented as sequences of two identical [-consonant] segments/phonological units/moras. A long vowel, under this analysis, is a single feature bundle linked to two similar abstract representations.
With strong supporting evidence for both theories, we may conclude that at worst, the issue leans favorably towards both sides and is unresolved, and at best it is clearly in favor of there being one vowel representation, which occupies one slot and which varies in the [+tense] feature. On these grounds we can take the important step of justifying the removal of all VV occurrences in the lemma which are non-diphthongal and do not precede a CC+. Both of these qualifications can be taken up for consideration in the final stages of the study.
The corpus used in this investigation into the conformity of Dutch to CVX theory was a lemma obtained from the CELEX online database of standard Dutch. The lemma represents headwords in non-reducible forms. As stated in the release notes:
The following categories (Table 1) were utilized in informing the reduction of apparent CVX violations:
The original 124,136 items in the lemma were then broken down into workable files and lines missing information were identified and labeled as "no sound."
Compound words may be excluded from analysis. The composition of the lemma is such that it lists, in theory, every non-productive word in the Dutch language. Thus, we may expect that if a word is identified as a compound its compositional parts have been accounted for already. CELEX identifies compounds though the use of both hyphens (-) and word boundaries (#) in their transcriptions. Compound words that somehow escaped CELEX identification require manual identification and removal, with the assumption that their composite parts have also been accounted for in a the lemma.
In addition, there were 2,934 items for which no description existed in the lemma. These were excluded from analysis as no-sounds items, though they deserve further examination.
Working with the entire unreduced lemma, all no sounds and CELEX identified compounds were removed; compounds being identified as an occurrence of + or # in the phonological transcription. After removal of these items 63,152 items remain in the lemma.
The next important step is to identify all potential violators of CVX theory, manifested as all possible combinations of VXX super-heavy syllables in column: PhonStCVBr (stem CV). All instances of VXX are indicated in table 5.3.1, however many of these occurred within the same word. Total lines or items containing at least one CVX violation are listed in Table 5.3.2.
Within the 18, 471 potential violators of CVX theory we will be able to exclude many on certain grounds. For native speaker judgments, I enlisted the assistance of two native Dutch Speakers, currently residing in Amsterdam. The speakers were David Lingerak, age 35 and Martine Brinksma, age 33. Both were raised in West-Friesland (Noord-Holland) and are unaware of their particular dialect. the reduction proceeded as follows:
Hyphenated words are removed, ass shown in Table 5.4.1, as they represent non-CELEX identified compounds. 17, 936 words remain.
Level 2 affixes may be excluded, as they are phonologically inert and do not represent free morphemes. A potential confound may be the inappropriate removal of words with level one affixes. The classification of levels is certainly different in Dutch than in English and has not been examined in this essay. However, I believe the majority will be appropriate by the same reasoning as was applied to compounds. In my first reduction attempt I utilized a list of affixes derived from Booij (2002). This proved useful only in assessing a hypothetical number as I had to rely on statistics due to the error rate during my removal process. The method was abandoned in favor of one that yielded more concrete and tenable results. Searching re-occurring strings of five and six letters at the word edges we are able to reduce the lemma in a conservative yet productive way. Table 5.4.2 shows that after affix removal, only 1,151 words remain as potential violations of CVX theory.
By the same reasoning detailed above, compounds that were not CELEX identified, but were identified by the native speakers are removed.
Foreign words borrowed by Dutch may, in many cases be excluded, as their composition is purportedly not reducible in Dutch in the same way as in the original source language. for example, item 4008 /airconditioning/ is not reducible to /air/ and /conditioning/ in Dutch. I must acknowledge some degree of error in this removal as it is dangerous to assume that all borrowings are of such a nature and that there may well be overlap in simplest morphologically-free forms.
Borowksy attributes counterexamples found in proper names to the fact that they constitute a deviant subsystem with respect to many phonological phenomena. Assuming that most of them appear to be compounds or foreign, we can deem them derivations of the level 2 variety and exclude them from our list of CVX violators.
Of the remaining 413 words in the lemma, 278 were identified as containing long vowels or diphthongs.
These were marked for closer scrutiny, turning first to the case of long vowels. Assuming that differences in length can be explained by a tense/lax account, we extract all VV that are non-Diphthongal and that do not constitute any further violation of the rime (Non VVCC][ or VVCCC][ (marked with an X)). VV that meet these two criteria are removable under a theory which assumes long vowels to actually be identical to short with the exception of the feature [+tense].
Under Dunamu (2002) we can eliminate non-word initial diphthongs, yet word onset diphthongs may still present a problem as there is no way of attributing the initial vowel sound to a complex onset. An example from Chinese follows below:
Following this theory, all non-violating diphthongs are now removed (where a violation consists of non-initial diphthongs and VVCC+ violations) , as shown in Table 5.4.7.
For the remaining 76 words, a manual lookup was conducted to determine if any further reduction to the lemma could be made. From this 30 words were determined to be compounds, 13 had homorganic nasals which could be taken to constitute a long V and thus together with the vowel only one slot. Finally, 18 were excluded for various reasons examples of which are given below:
Exclusions similar to example (9) are excluded on the grounds that they contains homorganic nasals: a nasal followed by a consonant at the same place of articulation, in this case alveolar. In this case, the nasal can thus be assimilated into the vowel sound leaving the second slot in the rime for /t/. Example (10) illustrates a group of words that were extracted on the basis that /ex-/ constitutes a prefix. This may be a slightly contentious point, however it is assumed to hold here. Example (11) represents a class of words that we can exclude if we re-syllabify so that the offending C is relocated to the onset of the final syllable i.e. [bre:d][UIt] [bre:][dUIt], thus removing the problem of a diphthongal rime (VV). Fifteen violating words now remain, presented in table 5.4.8.
Of the remaining 15 some [VNC] rhymes that could be analyzed as [V~C]. einsel and punctum' may be homorganic, though the /n/ in punctum is not velar and einsel would continue to have a diphthong in its rime with no complex onset to attach to. 'Rucksichtslos' is probably 'reckless', but it has not been confirmed by my speakers.
Aside from a few unresolved exceptions, this study has shown that, if we assume a tense/lax and not length distinction, Dutch conforms to CVX theory. Many of these exceptions could most likely be shown to be legitimate exclusions though given current resources I have left them in as violations. The results would also be drastically different, and could be said to not hold true under a different account of vowel length. there would in fact be an additional 275 words to account for. However, given the unresolved state of the data, we have been just as justified in choosing the tense/lax account for the investigation. What remains to be proved is that the same holds true for other, less related languages than those for which similar studies have been conducted.

Word segmentation is to find word boundaries not marked by any delimiters in texts. It is the first step of text processing in languages without spaces as delimiters such as Chinese, Japanese and Thai. The fact that a sequence of characters can be grouped in several ways makes the segmentation task difficult. There is a sizable literature dealing with word segmentation. For example, Chinese word segmentation has been studied for decades with varieties of methods (Fan and Tsai, 1988; Sproat et al., 1996; Wu,2003).
Even for those languages with delimiters, word segmentation is also a necessary step in text processing. In Korean, though spaces are delimiters of Eojeol1 boundaries, automatic spacing is required in sentences with spacing errors in order to increase the readability and communication accuracy. Spacing errors cause misinterpretation to readers. For example, if (once in a while) is written as (to go once in a while), both its meaning and part-of-speech change. Spacing errors are common in Korean. It is hard to get correct word spacing even for human because people tend to use morphemes incorrectly. Consider (whole family). (whole) is an adjective and it should be detached from (family), but people often misuse as a noun, and they either attach or detach to. 2 In the literature, several methods have been used to deal with spacing (Kang et al., 2001; Lee et al., 2003). The task of word spacing can be taken as the task of word segmentation (Lee et al., 2003). In order to space a text, all the spaces are eliminated first and then the text is segmented into Eojeols delimited with correct spaces.
In this paper, we built a multilingual segmenter for Chinese word segmentation task and Korean spacing problem. We applied Viterbi algorithm (Viterbi, 1967), inword probability (Chen, 2003) and automatic linguistic rules as a plus.
Viterbi algorithm is a dynamic programming algorithm to find the best path through a probabilistic network given observed evidences. In word segmentation task, given a string of characters
ready, the algorithm then searches backwards through words to return the best probability path. For words not in the dictionary, we used add-one smoothing by adding 1 to frequencies of all entries and increasing total frequency N accordingly.
For words not in the dictionary, Viterbi algorithm prefers the longest fragment. If 1 u , 2 u , and 2 1u u are all unknown words, Viterbi algorithm segments the string into
In the second step of our system, we applied inword probability to combine a sequence of single characters into words. The in-word probability of a character is the probability that the character occurs in a word whose length is more than one, as in (3).
We built up an inword probability hash table for every character in the training data. Consecutive single characters are combined into a word if the inword probability of each character is over the threshold. According to our experiment over training data, we set 0.84 for Chinese inword probability threshold and 0.90 for Korean. Take a string of two single Chinese characters (family) (banquet) as an example. Our dictionary has no entry for, but the inword probability of is 0.87 and of is 0.93. So the two single characters are segmented as one word.
We extended inword probability to the recognition of numeric type compounds, including number compounds and time compounds. Since ASCII numbers are not included in our Chinese dictionary, the segmentation of numbers becomes the task of new words recognition. We combined the consecutive single numbers, including both digital numbers and Chinese character numbers together as one single word. We set inword probability for numbers as 1.0 if it is preceded or followed by another single numeric character or a certain suffix with inword probability assigned as 1.0, such as %. If the string is 1 2 %, then the inword probability will combine the three single characters together as one word 12% because all the inword probabilities of the three single characters are 1.0, above the threshold. If the compound of numbers is followed by a time unit, such as (year), (month), (day), the inword probability of the time unit is also assigned to 1.0. In this way, date and time compounds are combined into one word. For example, 2003 ? is segmented as 2003 ? (the year of 2003).
In Korean Eojeol spacing, inword probability helps to combine unknown transliterated names such as (carpet), (Persian). In , the two syllables3 and have inword probability above 0.98. A syllable has much higher inword probability when it is specifically used for pronunciation of a foreign character, because foreign characters often have unusual combinations of consonants and vowels.
In Chinese segmentation task, we applied linguistic knowledge as the final procedure to recognize unknown words after the implementation of in-word probability. First, we collected 50 suffixes from training data by implementing simple suffix extractor. The set of suffixes covers district units such as (country), (province), geographic suffixes such as (river), (mountain), road suffixes such as (road), (lane), and other suffixes such as (prize), (team). We attached a suffix to the previous word of two characters. Secondly, we extracted 100 family names from the training data. In the PKU training data, family names are separated from given names. If A is a family name in a sequence of single characters A B C, B and C are combined together as a given name only if C is not a family name and C does not belong to a small set of context words, such as (say), (and). The first restriction on C avoids the wrong segmentation of concatenated person names. In the sequence since the third character is also a family name, the given name segmentation does not combine and, because the latter could be the family name of another person name followed by.
As mentioned in the introduction section, Korean Eojeols can consist of more than one word. In the type of Eojeols consisting of a noun and a postposition, a certain noun can be attached by any postposition. For instance, the noun (lecturer) can be combined with different postpositions, and thus different Eojeols and meanings are generated. To list a few: is direct objective postposition), (of the lecturer), (to the lecturer), and (with the lecturer). This type of arbitrary combinations undoubtedly
yields some Eojeols not in the dictionary. We extracted a set of postpositions and built up the postposition attachment rule to solve this problem. If C in the segmented output A/BC after the second step is a postposition, the Eojeol A is combined with the Eojeol BC only if ) (AB P is higher than )
While resolving this problem, we followed the same procedure as the Chinese suffix extraction to maintain the consistency of our system. We collected set of postpositions from training data. Those postpositions are used to mark subject, direct object, indirect object, possession, location, direction, means, and groupings.
Both the training data and test data are from PKU corpora used in first international Chinese segmentation bakeoff4. The training data has 1.1M words and the test data has 17K words. The encoding of the corpora is simplified GBK. First, we made a unigram dictionary with distribution probability from the PKU training corpus. We did not include number digits and English letters in the GBK code, nor did we collect ASCII sequences. We also built up an inword probability list for each character in the training data. We did three steps in Chinese word segmentation. First, we applied Viterbi Algorithm. Secondly, we combined sequence of single unknown characters using inword probability. Finally we applied automatic rules of suffixes attachment and person names. Table 2 shows the recall, precision and Fmeasure after each step.
The performance of our system is promising compared to the first International Chinese segmentation bakeoff (Sproat et al., 2003). Table 3 shows the baseline, average and the highest scores of the bakeoff.
Among the errors our system produced, a small portion was caused by the inconsistent examples found in annotated segmentation between the training and test data, and the major errors came from our system.
The performance of the segmenter increased 4.9% after we implemented the inword probability algorithm. However, we could not avoid creating some segmentation errors. For example, the sequence of (cat) (winter) was combined incorrectly together as a single word because both of them have high inword probabilities. In the third step, segmentation errors appeared due to overgeneration and undergeneration of linguistic rules. The suffix attachment rule can be overgenerated when a suffix has ambiguous meanings. has two meanings, one means festival, used as a suffix, and the other means phase, used as a common noun. We did not make use of any context clues to distinguish the two meanings of. As a result, we attached in some wrong cases. The system segmented (the first phase), but the correct segmentation should be. As for segmentation of person names, we restricted that the third character could not be a family name. This rule can undergenerate some given names because characters used for family names can be used in given names also. In the name string, the last character is a family name, so this string was not recognized as a person name in our system. Some other errors were from segmentation of transliterated person names.
When evaluating the Korean spacing, both syllable based precision and Eojeol based precision
can be used. Syllable based precision is the ratio of the number of correctly spaced syllables over total number of syllables. Eojeol based precision evaluates the ratio of the correctly spaced Eojeols over the Eojeols from the system output. We adopted Eojeol based precision to maintain the consistency with Chinese segmentation domain.
In the evaluation of Korean spacing, a compound noun can be treated either as a whole or separate nouns. Thus we relaxed the definition of a compound noun to either case, and did the evaluation again. For a compound noun with three nouns (high-speed internet), and are also counted as the correct segmentations. Table 5 shows the improvement with the relaxation.
In this paper, we built a multi-lingual segmenter for both Chinese segmentation and Korean spacing task by implementing Viterbi algorithm and supplementing it with inword probability and automatic linguistic rules. It is the first try to segment languages with word boundaries and without boundaries in one segmenter. Experimental results show the efficiency of the multipurpose system.

Your company trains heating, ventilation and air conditioning (HVAC) technicians to install and optimize refrigeration and heating systems. After training, you test them on their ability to find a system's best operating condition but you have been receiving complaints that this test is unfair due to the uncertainties in the vapor compression cycle (VCC) training carts used. Thus, you have provided our company with the VCC cart used for training and requested that we quantify the coefficient of performance using compressor frequencies of 30 Hz, 45 Hz and 60 Hz and show the results on a temperature-entropy diagram. Additionally, you have asked for any other properties to consider when determining a unit's ability to heat or cool efficiently, and if they should also be used to test your technicians. We have completed these tasks. The purpose of this report is to present our procedure, findings, conclusions, recommendations and supporting documents.
We determined the values of the coefficient of performance shown in Table 1 below. We also constructed a temperature-entropy diagram (Fig. 2 on p. 3) from our results. Our results show that the coefficient of performance decreases as the compressor frequency increases, and that the values at each frequency are distinct even with uncertainties. The clear distinction in these values leads us to conclude that your testing is fair and that the uncertainties should not create any confusion as to which compressor speed produces the best coefficient of performance. We recommend that in addition to training your technicians on optimizing the coefficient of performance that you also train them to understand the minimum cooling and heating capacities needed for the building the system is used on. We observed the cooling and heating capacities (also shown in Table 1) decreased as the compressor speed increases, the opposite trend of the coefficient of performance. If the cooling and heating capacities are too low with respect to the volume of building they operate on, the system will not be able to produce enough heat transfer to cool or heat the building to a desired temperature.
This section details the equipment setup and the methodology we used for testing.
Your company provided us with a Hampden Model H-CRT-1 Refrigeration Trainer cart as well as a computer controlled data acquisition system to record data. The cart's working fluid was R-134a refrigerant. A simplified schematic of the cart can be seen in Figure 1 which shows the path of the fluid and the points where digital transducers measured temperature and pressure. The schematic omits additional elements (such as an oil separator for the compressor) and extra transducers because they were not used in our analysis. We have defined the following points on the schematic, which will be used later in reference to the vapor compression cycle: point 1 is at the entrance to the compressor, point 2 is at the exit of the compressor, point 3 is at the exit of the condenser, and point 4 is at the entrance to the evaporator.
We conducted our tests in a laboratory with ambient air temperature of 295.9±0.5 K and ambient pressure of 97.8±0.1 kPa. First, we turned on the fans inside the condenser and evaporator. Then, we set the speed controller to 30 Hz. Using LabView, we monitored the temperature and pressure through all of the transducers as well as the compressor input power and the flow rate into the throttling valve. We waited for readings to reach steady state, at which time their values were recorded five times at 10 second intervals. We repeated this process twice more with speed controller values of 45 Hz and 59 Hz.
This section outlines each process in the ideal vapor compression cycle and the expectations for each process in reality. It also presents the requested temperature-entropy diagram, coefficients of performance and all relevant derivations.
Each process in the vapor compression cycle occurs as the fluid passes through the compressor, condenser, throttling valve or evaporator. We assumed that the fluid does not undergo any thermodynamic changes in the tubes from one device to another.
The specific entropy at points 1, 2, and 3 were found in thermodynamic tables[1] using their measured temperature and absolute pressure. The specific entropy at point 2 during the 30 Hz test corresponded to a compressed liquid, which we know is not possible. This would mean the compressor was more than 100% efficient. We observed that the specific entropy value is sensitive to pressure changes, so we hypothesized that the pressure gauge at point 2 may not be measuring accurately. The specific entropy of a saturated vapor at the same temperature was used for this point.
The specific entropy at point 4 was determined using a different procedure due to a lack of a pressure transducer there. It was determined using the assumption that the process of fluid passing through the throttling valve was adiabatic and that the enthalpy remained constant. To find the specific entropy in the tables, we needed to know a pressure and specific enthalpy, so we used the pressure of a saturated liquid at the measured temperature and the enthalpy at point 3 as read from tables.
As requested, we made a temperature-entropy diagram using our results and plotted it on top of the R-134a saturation dome, as shown in Fig. 2. When looking at the process from point 1 to point 2, it can be seen that the cycle is not quite the ideal vapor compression cycle as the entropy does not remain constant. We observed that the entropy decreased through the compressor, contrary to what we expected. Also contrary to what we expected, we observed that the temperature and pressure increased through the evaporator at 45 and 59 Hz.
The values for the coefficient of performance (COP) for cooling and heating can be found in Table 2 as well the cooling capacity, heating capacity and power input to the compressor. Both values of the COP decrease as the compressor speed and power input to the system increase, and the values at each operating speed are distinct. These values do not come close to one another even at the far ends uncertainty. The values of the cooling and heating capacities increase as the compressor speed and power input to the system increase.
The coefficient of performance (β) in a VCC is defined as the heat energy sought divided by the power input (compressor power) into the system[2]. The heat energy sought in a refrigeration cycle is the cooling capacity from the evaporator and the heat energy sought in a heating cycle is the heating capacity from the condenser. These values are determined by the states of the fluid entering and exiting each device and represent the total amount of cooling or heating the system can do. The cooling capacity was calculated using Eq. 1 and the heating capacity was calculated using Eq. 2 where
We found the values of the specific entropy for each point in the VCC and plotted them on a temperature-entropy diagram in Fig. 2. Additionally, we determined the values of the COP for each compressor frequency as shown in Table 2. We observed that the values for both cooling and heating COP were distinct from one another with uncertainties. The clear distinction in these values leads us to conclude that your testing is fair and that the uncertainties should not create any confusion as to which compressor speed produces the best COP.
We recommend that in addition to training your technicians in optimizing the COP, you also train them to understand the minimum cooling and heating capacities needed for the building the system is used on. Our results showed that the COP was highest at the lowest operating speed, which also corresponded to the lower value in the cooling and heating capacities. If the cooling and heating capacities are too low with respect to the volume of building they operate on, the system won't be able to produce enough heat transfer to cool or heat the building to a desired temperature.

In the average internal combustion engine-driven automobile, only 25% of the energy generated from combustion can be used by the vehicle for mobility and accessories. The rest of the combustion energy is wasted, including 40% that is lost to exhaust gas [1]. Various methods have been developed to try to recover some of this lost energy, including turbogenerators, Rankine Cycle recovery systems, thermochemical recuperation, and thermoelectric generators (TEGs) [2]. Additionally, some hybrid cars including the Toyota Prius use regenerative braking to recover some of the kinetic energy lost when stopping the vehicle [3].
Thermoelectric generators are attractive potential waste heat recovery systems because they are relatively inexpensive, have no moving parts (and thus produce no noise and no vibrations), and are highly reliable [4]. Previous studies have attempted to develop TEG systems for waste energy recovery in light trucks [5][6][7], but these studies have not considered lighter cars with hybrid engines such as the Prius. In addition, previous researchers have not specifically taken into account the driving habits of university students.
In this study, we investigated the possibility of developing a waste heat recovery system mounted on the exhaust system of the Toyota Prius as driven by college students using TEGs coupled with heat sinks mounted on the exhaust system of the vehicle. Specifically our goals for this study were as follows:
All variables used throughout this report have been defined in Table 1, below.
We considered the effects of heat sink size, pin shape, pin height, and pin density on the voltage output of a thermoelectric generator. In order to determine the effects of these factors with a minimal number of experiments, we used a fractional factorial DOE methodology and analyzed the results with an analysis of variance (ANOVA). We then predicted the optimum heat sink configuration for our application based on the ANOVA and design considerations. This configuration was then tested under various wind speeds and temperatures to extrapolate the
TEG's performance at typical driving conditions as determined from a survey of college students.
The results of the fractional factorial analysis were used to determine the significant factors in TEG voltage output using an ANOVA statistical analysis. The ANOVA and power data from the initial tests were used to determine the best heat sink geometry.
A fan was placed to flow across the heat sink to simulate the air flow under a moving car. The fan was approximately 15cm from the TEG and was placed at approximately the same height as the base of the heat sink. The fan speed was kept constant at 2.7 ± 0.5 m/s for the initial set of tests.
For each test defined in the DOE table, we placed the corresponding heat sink on top of the TEG, turned on the hotplate, and recorded the temperatures and voltage as the hotplate temperature increased to steady state. For our tests the steady state temperature was approximately
190±10°C. However, this was difficult to control due to high uncertainty and uneven temperature distribution produced by the hotplate.
The thermoelectric figure of merit, ZT, under our test conditions was found using the following plots illustrated in Fig. 2, below. We evaluated the ZT value for a Bismuth Telluride (Bi2Te3) material, taking the average of the pand n-type values, at a temperature equal to the mean temperature of the test (~140°C). The resulting ZT value was approximately 0.7 for this case.
We then used the mechanical efficiency defined as the ratio of the electrical power generated to the thermal power into the device, given by Eqs. (2) and (3). This allowed us to determine the constant K for the TEG, which we assumed to be independent of temperature.
In these equations, the constant K (W/K) represents the lumped thermal conductivity of the TEG device, assuming it to be a semi-infinite slab according to heat transfer analysis. This constant incorporates the thermal conductivity, k, cross-sectional area, A, and height, L, of the TEG. The Voltage is given by V and the resistance is defined by R, assuming a value of 4.0 Ohms according to manufacturer specifications [1].
Since a gradient of temperature exists between the base of the heat sink, TL (where we measured), and the cold side of the TEG, we used the thermal conductivity of the TEG to predict the actual value by solving for TC using Eq. 4, below, which was determined by combining Eq. 2 and 3:
A heat transfer analysis using a conservation of energy approach for this setup resulted in Eq. 5 below. In this equation, the heat flux QH into the system is balanced by the convective heat transfer Q-convected out of the system and the power generated by the device. Equation 6, below, defines the convective heat transfer, where hA (W/K) is a measure of the convective heat transfer to the ambient.
Using the results of the optimal heat sink tests, we determined a relationship between the convective heat transfer coefficient, hA and the air velocity.
We used the survey results to estimate TH at various points in the exhaust system (Fig. 7 on page 10). Combining equations (2) – (6) yielded Eq. (7), below. This equation was solved explicitly in terms of TC to estimate the cold side temperature of the TEG for the estimated TH, taking into account the higher convective heat transfer with a moving automobile.
We estimated the ambient temperature as 10˚C, which was determined to be an estimate for the average yearly temperature in Ann Arbor, MI [9]. We used an iterative process to estimate the ZT value for various TE materials optimized for the calculated mean temperature and find Tc. This Tc value is then used in Eq. 4 to determine the output power. Since the TEG material we tested cannot withstand sustained temperatures greater than 250˚C [1], Fig. 4, on page 8, was used to estimate the figure of merit for various TE materials.
where ∆FE is the fuel efficiency increase in mpg, P is the electrical power generated in Watts and W is the additional weight of the system in lbs. Additionally, assuming a gas a price of $3.20/gallon and a car lifetime of 150,000 miles, the amount of money saved in fuel consumption over the lifetime of the car is given in terms of P, W, and the original fuel economy FE0 by Eq. 9. The EPA's estimate for the average fuel economy of the 2007 Toyota Prius is 46 mpg [11].
For an electrical generation system to be practical to implement, it should save the consumer more money in gas than it costs in additional vehicle price. Similarly, since one gallon of gas produces 9 kg of CO2 gas in the average car [12], and the Prius' CO2 emissions are about half of that [13], the reduction of CO2 emissions (in kg) over the lifetime of the car is given by:
We used the conducted survey to approximate the exhaust temperatures of the Toyota Prius at various component locations under the car. Using the dimensions of the exhaust, we then approximated the useable area for TEG implementation. Thorough analysis of the tested heat sinks yielded an optimal configuration taking into account power, cost, and added weight of the system. We predicted the thermal constants associated with the TEG and optimal heat sink at design temperatures. An Estimation of the performance and cost of the proposed system with current technology is then presented.
Table 3 on page 10 shows that the average time per trip is about 15 minutes and the average driving time per day is about 25 minutes. Additionally, the average time spent towing a trailer, letting the car idle, and driving on dirt or poorly paved roads is negligible for the purpose of this analysis. Finally, 27.7% of college students' driving time is spent on the highway, and college drivers drive aggressively 17.9% of the time. Both of these activities significantly increase the load experienced by the car.
Since the average student drives about 25.4 minutes per day, 72.3% city driving (approximately 30 mph) and 27.7% highway driving (approximately 70 mph), the average distance driven per day is 17 miles/day, so the average college student's car is driven about 6000 miles/year.
Figure 7 shows the exhaust system component temperatures at partial and full loads for a BMW 318i sedan.
Assuming that the Prius has a similar exhaust system, we determined the typical component temperatures for city and highway driving. City driving is approximated as driving at 30mph, and the component temperatures are approximated as the temperature partial load plus 25% of the difference between the full and partial loads. This estimate takes into account the transient behavior of the car as it warms up, driver aggressiveness, and the temperature fluctuation due to the unique engine behavior of the Prius (see Discussion Section). Highway driving is
approximated as the partial load plus 75% of the difference between the full and partial loads. This estimate takes into account vehicle transient behavior and driver aggressiveness. Table 4, below, summarizes these exhaust system temperatures with the corresponding useable area for mounting the TEGs.
Table 6, below, summarizes the results of our ANOVA analysis. The full ANOVA results outputted from MATLAB can be found in Appendix B. We examined the effect of various factors related to the heat sink geometry on both the power output of the TEG and the temperature difference across the device, ignoring interactions between the factors. Using a 90% confidence interval, we determined the only significant factor was pin height.
Using the ANOVA analysis and the power per cost ratio from Table 5, p.11, we determined that the best heat sink for this application is the Alpha Novatech PN S153020W.
Figure 8 shows that there is a linear relationship between the convective heat transfer coefficient, hA, and air velocity.
The hA values for city and highway driving were determined from the relationship shown in Fig.
8. For city driving (30mph), hA=0.28 W/K, and for highway driving (70mph), hA=0.62.
To maximize the power output of the system, the TEGs placed on the exhaust manifold and the exhaust pipe are filled skutterudite TEGs while the TEGs on the catalyst, the center muffler, and the rear muffler are Bi2Te3 TEGs (see Fig. 9). This configuration was chosen to maximize the ZT value at each component based on Fig. 4, p.8. It should be noted that HiZ 2 TEGs that we tested in the laboratory would not be acceptable for this application, as they cannot be exposed to sustained temperature of more than 250˚C, however more robust Bi2Te3 TEGs could be used. The performance of this system is summarized in Table 8, below. The filled skutterudite TEGs are assumed to have similar geometrical, mass, conductivity properties, and costs as the Bi2Te3 TEGs. The system weight is estimated as the weight of the TEGs and heat sinks, plus approximately 5lbs (2.3kg) for installation hardware.
Table 7. Summary of performance of TEG heat recovery system using current technology.
In order to draw conclusions about our data, we needed to assess the validity and implications of our results. We then also further investigated the characteristics of the Toyota Prius, and current and future TEG technologies.
While a small fan mounted on top of the heat sink may have increased the convective heat transfer from the heat sink, we chose not to propose the use of a fan for several reasons. The fans would take power from the system, add weight to the car, and add a degree of complexity to the system due to moving parts. Since the fans were only able to generate a maximum of about 3 m/s (6.7mph) while using about 1 watt of power, the increased convection due to the fan would not outweigh the energy cost of using the fan.
First, the hotplate had a very uneven temperature distribution. If the thermocouple used to measure the hotplate temperature was moved even slightly, the temperature reading would change by up to 20°C. In addition, the set point of the hotplate was not consistent. Setting the hotplate to 260°C resulted in hotplate temperatures of between 150°C and 210°C, varying from test to test. While we attempted to compensate for these problems by changing the set point to achieve a consistent temperature, and trying to place the thermocouple in a consistent spot, the temporal and spatial fluctuations made the hotplate temperature extremely difficult to measure accurately. A more precise heating element would have greatly improved our temperature reading accuracy, and thus improve our performance predictions.
Second, the contact resistance of the setup did not seem to be consistent. While we tried to use a consistent amount of thermal grease at each contact, other factors may have affected this resistance. For example, contact resistance decreases with increasing contact pressure [18]. However, we used heat sinks of different weights, so the contact pressure was never consistent. Additionally, to measure the heat sink surface temperature, we had to put some pressure on the heat sink with the thermocouple. While we attempted to keep this pressure consistent, we had to hold the thermocouple by hand, and the pressure may have changed from test to test. Our results would be more accurate if the contact pressure could be kept constant by using a clamp to hold down the heat sink and take temperature measurements. Additionally, the thermal grease seemed to clump over time and create air pockets that would increase the contact resistance. This could be remedied by using a different kind of thermal grease or by reapplying thermal grease between each test.
Finally, the resistor used to simulate a load across the TEG was attached very close to the TEG, so the resistor heated up during the tests. This temperature change may have changed the resistance, which would affect our voltage measurements. If the resistor had been placed in parallel with the TEG with longer wires, the resistor could have been kept closer to the ambient temperature, and thus maintained a more consistent resistance.
These laboratory testing problems probably affected our results for our ANOVA analysis and our extrapolation to real-world driving conditions. However, since the results of our system analysis show that the necessary TEG technology is an order of magnitude higher than that which is available today, our recommendations are not highly affected by our laboratory problems.
The Prius also uses fuel-saving technology other than the hybrid engine. The Prius' fuel efficiency is increased by the car's aerodynamic shape, which is due in part to its flat underside designed at for smoother air flow beneath the car (see Fig. 11, below). The Prius also uses a weight-saving design, using lightweight components as much as possible [3]. Any proposed changes to the Prius design should be made with these factors in mind. In our air-cooled system the TEGs and heat sinks would add both weight and drag beneath the car.
Figure 11. The underside of the Prius is flat to make the underbody air flow smoother and reduce drag. [3].
Fig. 4, p.8. These materials have lower ZT values than the materials listed in Table 8, so they were not considered for this application.Table 8. Comparison of characteristics for recommended TEG Materials.
The goal of several US Department of Energy programs investigating waste heat recovery is a fuel economy increase of 10% [14]. In the Prius, this corresponds to a fuel economy increase of
4.6 mpg. Neglecting the TEG system weight, Eq. 8, p.8 implies that this could be achieved with a TEG power generation of 1150 W. In order for this to be possible with the system proposed in the Results Section above, our models show that TEGs with a ZT near 3 would need to be developed. However, the cost of the system would still be a major hurdle before implementation could be realistic.
In order to estimate the requirements for a TEG system to be feasible on the Toyota Prius, an economic evaluation was performed to determine the point at which a TEG system could pay for itself over the lifetime of the car (the break-even point). One way to reduce cost would be to reduce the number of TEGs needed. By only mounting TEGs on the three highest-temperature elements (the exhaust manifold, exhaust pipe, and catalyst), the number of TEGs could be reduced to 155.
Figure 12 p. 18 shows the amount of fuel economy savings with respect to time and the figure of merit ZT. The plot shows that with a TEG cost reduction of 85% and a ZT of 4, the system will exactly pay for itself at the end of the car's life cycle (150,000 miles and 25 years). Figure 12 also shows the break-even points for other combinations of cost reduction and figure of merit ZT.
In order to increase the savings achieved with a TEG without increasing the number of TEGs, the figure of merit of the individual TEGs must be increased. Figure 13 illustrates that increasing ZT would lead to increased fuel efficiency. The figure also shows that there is a limit to how much the ZT can be increased before it does not significantly affect the fuel economy. This occurs around a ZT value of 20. For high ZT values, the factor limiting power output shifts from the material properties to the Carnot efficiency.
than that of the Bi2Te3 TEGs. Development of technology for automated fabrication should further decrease the cost of quantum well TEGs. In addition, quantum well TEGs are predicted to have very high efficiencies compared to the TEGs available today and can also be utilized in higher temperature ranges [8]. Figure 14 shows the relationship between efficiency and cost of future TEG modules with respect to temperature.
The temperature of the gas that flows through the exhaust system is higher than the temperature on the outer shell of the exhaust system. Since the TEG is mounted on this outer shell, maximizing this temperature would increase the TEG power output. To achieve this, the exhaust system could be constructed with a less thermally resistive material, or the exhaust gas could be forced to have turbulent flow with the help of spiraled fins inside the exhaust system [17].
In addition, a reduction in thermal resistance of the insulating wafer that is placed between the exhaust system and the TEG and also between the TEG and heat sink could also improve the performance [17]. Similarly, an increased thermal conductivity of the thermal paste would increase heat flux through the TEG system and thus improve performance.
Many studies of waste heat recovery systems in automobiles have used coolant from the car's engine to cool the cold side of the TEG instead of flowing ambient air [5][6][7][17]. Use of such a coolant system would increase the temperature difference across the TEG and thus increase our power output.
We have investigated the possibility of implementing an air-cooled TEG waste heat recovery system mounted on the exhaust system of a Toyota Prius driven by college students. Our conclusions are as follows:
While our laboratory tests had a high level of uncertainty, it is clear that the current TEG technology is far below the necessary level to be feasibly implemented. Assuming gas prices of $3.20/gallon, an average ZT of 4 and an 85% reduction in TEG system price would be necessary for the system to pay for itself in saved fuel costs over the lifetime of the car.
While the implementation of a TEG waste heat recovery system in the Toyota Prius is not practical using today's technology, future TEG materials may make such a system a viable option. Specifically, when ZT values approach 4 and TEG costs are reduced by 85% (a realistic target based on the prediction of quantum well TEG technologies), the system should be reconsidered.
The following recommendations should be considered when investigating a TEG waste heat recovery system in the future:

The Powertrain Division of EKA-KK Inc. has observed a low-frequency resonance on the drive shaft of the current prototype vehicle. Due to this resonance, passengers clearly sense the switching on and off of the DC motor, a major vibration problem. Thus, Systems Group has been contracted to identify the cause of the drive shaft vibration problem and to recommend four possible design modifications. Systems Group is to develop a motor/drive shaft model, perform computer simulations to validate the model, and come up with four design changes. All of the requested work has been completed, with the purpose of this report to provide results, conclusions, and supporting documentations for the drive shaft test and design alterations.
Our experiments have allowed us to create a mathematical model of the drive shaft and flywheel system. The cause of the vibration problem in the drive shaft was determined to be the drive shaft itself, in which initial assumptions of drive shaft rigidity were incorrect. Through sensitivity analysis we identified that the diameter of the flywheel, Dflywheel, the resistance of the motor, Rm, the motor constant, Km, and the damping of the motor, Bm, had the most significant effect on the magnitude of the resonant response. Table 1, below, shows how much each parameter needs to be changed individually to reach a 30% reduction in resonance magnitude. Additionally, there are four design options that combine changes in multiple parameters in order to reach the 30% reduction goal. We feel that proposed Design 4 will best suit your needs as it minimizes the amount of change to each variable.
This section highlights the procedures used during testing and data collection. The test setup consisted of a drive shaft with two flywheels, Figure 1 on page 2. On the right side of the drive shaft is a DC motor (Aerotech 1000 DC Permanent Magnet), which also can operate as a tachometer, and on the left is a tachometer (Servo-Tek SA-740B-1A). Both the motor and the tachometer are connected to an oscilloscope (Hewlett Packard 54602B) and the motor was driven by a function generator (Hewlett Packard 33120A).
With the system given in Figure 2, above, the damping coefficients of the bearing, Bb, drive shaft, Bs, and motor, Bm, could not be obtained individually. The system was separated into components and a system of equations was used to solve for all of the coefficients simultaneously. The combined damping of the bearing and shaft was obtained by locking Flywheel 1, applying a small force to Flywheel 2 to initiate rotation, and applying Equation 5 on page 3. An oscilloscope was used to measure the output angular velocity from the tachometer which is attached just beyond Flywheel 2. To determine the combined damping of the motor and shaft, the same procedure as above was used, with the exception that Flywheel 2 was locked, a small force was applied to Flywheel 1, the oscilloscope read the voltage from the motor, and Equation 6 on page 3 was used. We found the third equation by locking Flywheel 1, restraining the drive shaft so that only half of it rotated, applying a small force to Flywheel 2, and using Equation 5 on page 3.
To measure frequency response, an amplifier (Techron 7520) attached to a function generator applied an output signal at 1Hz to the motor of the drive shaft. The amplifier gain was then increased to 4V. The frequency response was measured for input frequencies between 1 and 12Hz in intervals of 0.2Hz between 5 and 6 Hz and intervals of 1Hz everywhere else along the given range.
The motor constant and resistance are related to each other through a function of the motor's voltage, current, and angular velocity; see Equation 9 on page 4. To find the motor resistance, Rm, we subjected the motor to a range of high frequencies, 20 to 30 Hz, and recorded the motor's voltage and current for each frequency. The motor constant, Km, was found by applying a steady DC voltage and recording the motor voltage, motor current, and tachometer voltage.
This section highlights the results of the variables calculated, the verification and improvement of the test model through the use of Bode plots, the cost function and sensitivity analysis, and the proposed design modifications to reduce the resonance peak of the drive shaft. To find an accurate mathematical model for this system, we needed to determine all of the parameters of the system's transfer function, Equation 1 below. This model takes into account that the model components are not rigid, as previously thought; a possible reason for the drive shaft vibrations. Once the parameter values were calculated, we performed a cost function and sensitivity analysis to identify which parameters should be changed to reduce the magnitude of the transfer function by 30%.
In order to determine the damping coefficients of our system, we isolated different combinations of the damping coefficients. We then used a system of equations to solve for the motor damping, Bm=9.56·10-3 ±7·10-4, shaft damping, Bs=2.06·10-3 ±6·10-4, and bearing damping, Bb=1.45·10-3 ±6·10-4, individually.
By drawing a free body diagram of the shaft and second flywheel we found the governing equation of motion, Equation 2 below. This can also be rewritten to include variables such as the natural frequency, ωn, and the damping ratio, ζ, as seen in Equation 3 below.
Btr, the damping coefficient of the shaft and bearing, was calculated using ζ, which was determined from Equation 4, below. After measuring the amplitudes of two peaks, x1 and x2, as well as the period, t, between those peaks of the damped response, we calculated the value of ζ and set it equal to Equation 3 above to solve for Btr, Equation 5 below, where Jf is the moment of inertia of the flywheel, and K is the spring constant of the shaft. An example of how x1, x2, and t were measured is seen in Figure 3 below.
Btl, the damping coefficient of the shaft and motor, was calculated with the same procedure as that to find Btr, with the exception that the inertia of the motor was a factor in the calculation. Thus, Btl was calculated with Equations 6-7 below, where Jm is the moment of inertia of the flywheel.
Finally, Btr2 was calculated by reducing the shaft length by 50% and using the same procedure as for Btr. The individual damping coefficients were calculated using a Gaussian distribution, Equation 8 below.
Error in the damping coefficients primarily came from the precision and resolution error of reading measurements from the oscilloscope, as well as the statistical error associated with having multiple trials.
Determining Motor Parameters Km and Rm
To verify or possibly improve the original system model, we experimentally determined the motor parameters Rm (2.458 ±0.1 ohms) and Km (0.056 ±0.03 V/(rad/sec)) using Equation 9, below.
When we gathered data for calculating Rm, we used the assumption that if high frequencies were applied to the system there would be a negligible amount of rotation and thus
After performing all the experiments, we gathered all the parameter values associated with our mathematical model. Table 2, below, gives a value for each parameter and any error associated with it.
Figure 5a, below, shows the frequency response of the system for the mathematical models and experimental data. In order to verify that we improved the original model by calculating the motor constant and internal resistance, we plotted the original model, our new model, and the experimental data together. As is shown, using the calculated values for Rm and Km shifts the improved mathematical model so that it has a better fit with our experimental data. To mathematically prove this graphical analysis, a root-mean-square analysis, as shown in Equation 10 below, was performed between the magnitudes of the experimental data and the two different mathematical models. This analysis indicates the variation, or error, between the test data and the mathematical models. From this analysis, we found the error of the original model and improved model, σ, to be ± 0.58 rad/s and ± 0.21 rad/s, respectively. The improved model effectively reduces the error between the mathematical model and test data by 36%. Additionally, the error is small enough in magnitude that we can conclude it is acceptable to use the improved mathematical model to simulate the design modifications to reduce the resonant frequency.
Figure 5b, above, shows a Bode phase plot to determine the phase shift present in the system at various frequencies. The phase shift in angular velocity changes from-90 º to-270º around the resonant frequency of 6.0 Hz. The original model already had strong correlation with the experimental results, to which the improved model further adds.
A cost function and sensitivity analysis was performed to determine which system parameters have the largest effect on the resonant peak magnitude. Calculating these parameters allows for minimal changes to the system while maximizing the reduction in the resonant peak magnitude. This was achieved by developing a cost function that measures the amount of perturbation that a parameter has on the resonant peak magnitude. Equation 11, below, shows the cost function used. This cost function looks at the area under the magnitude frequency response curve and compares it against the same curve when a parameter has been altered by ±1%. The area measured is the area under the resonant peak plus a 1Hz wide section on either side. The percentage difference in area corresponds with how sensitive the system is to that particular variable. The greater the cost function percentage, the more sensitive the system is to changes of that parameter.
[2] Equation 11
Using this cost function, all of the variables of the system were evaluated, shown in Table 3 below. The analysis found the diameter of the flywheel, Dflywheel, the motor resistance, Rm, motor constant, Km, and the damping coefficient of the motor, Bm, to be the most sensitive values to reduce the resonant peak. These variables will be the focus for reducing the resonant peak by 30% in later design modifications.
To determine possible design changes, the four variables Dflywheel, Rm, Km, and Bm were altered to find a resonant magnitude reduction of at least 30%. These variables were chosen from the sensitivity analysis results, as they produced the largest changes in the cost function. Table 4, below, shows the percent change needed for a single variable to reduce the resonant peak by at least 30%. The percent change was calculated by finding the difference between the resonant peak magnitudes of the original and reduced curves. The reduction in resonant peak magnitude can be achieved by increasing Dflywheel and Rm, and by decreasing Km and Bm, respectively. Figure 6, page 7, shows the simulated frequency response resulting from an increased Rm of 50.5%, while Figure 7, page 7, shows the simulation for a decreased Km by 34.5%. Both of these parameters alone can reasonably provide a frequency response that reduces the resonant magnitude by greater than 30%.
The cost function and sensitivity analysis was performed by analyzing the percentage change of the system output, regardless of whether there was a change in resonant peak (vertical shift), or change in resonant frequency (horizontal shift). Therefore, some variables not only reduce the magnitude of the resonant peak, but also shift the resonant frequency. This leads to a more significant change in resonant frequency than in reduction of resonant peak magnitude. This is specifically noticed with the parameters Dflywheel and Bm where a 30% reduction in the resonant peak could not be reasonably achieved by these variables alone. Furthermore, other less sensitive variables tested yielded the same results.
While three of the four parameters were able to reduce the resonant peak by the desired amount, such large changes on a single variable may be costly, and impractical given the physical limitations of the components and their interactions with other parts of the system. A solution for reducing the resonant peak would be more viable as a combination of changes on two or more of the most sensitive parameters.
Four feasible solutions are described below that will reduce the resonant frequency magnitude by at least 30%. With these solutions, relatively smaller changes can be made to multiple parameters in order to achieve the desired reduction of resonant peak magnitude, shown in Table 5, below.
The first design modification involves altering only Rm and Km; doing this reduces the resonant peak without changing the resonant frequency. This choice of modification allows for only one component of the system to be altered, the motor. This may be a costly change, however, if the motor specifications needed cannot be achieved. Alternatively, the second design modification allows for less change to the motor parameters but includes altering the damping coefficient of the motor, Bm.
The third and fourth design modifications involve balancing the system alterations between all four of the sensitive parameters. Design 3 focuses on using Dflywheel and Bm to reduce the resonant peak. Design 4, however, minimizes the alteration of Bm relative to Designs 2 and 3, a difficult parameter to reduce given material constraints of the motor. Additionally, it reduces Rm and Km relative to Design 1. This minimization leads us to suggest Design 4 as our recommended solution. Minimizing design parameter alterations makes accomplishing a 30% reduction in resonance peak more feasible and less costly. Using the parameters defined in our model, the Design 4 alterations would result in a Km of 0.045 ± 0.03 V/(rad/sec), a Rm of 2.93 ±0.01 Ohms , a Dflywheel of 0.143 m, and a Bm 9.08·10-3 ±7·10-4 kg· m/sec.
To demonstrate the feasibility of the Design 4 modifications, a replacement DC motor was found that meets the motor specifications. This DC motor, Model 14201S003 from Pittman as shown in Appendix A, would perform near the desired specifications, and would exceed the resonant peak magnitude reduction of 30%. Estimations show that this motor would achieve upwards of 60% reduction in resonant peak magnitude when used in conjunction with an increased Dflywheel of 5%.
In analyzing the system provided, experimental data was used to create a mathematical model of the drive shaft system. The linear mathematical model used proved to be an accurate approximation of the real system, in which the bode plots generated from the linear mathematical model closely match those of the output of the system measured experimentally. The vibration problem indeed was determined to be the erroneous assumption that the drive shaft was rigid. In order to reduce the generated vibration by 30%, it is recommended that the motor constant, Km, be reduced and the motor resistance, Rm, be increased by 19.5%. Furthermore, Dflywheel must be increased 5%, with a reduction in Bm by 5%. In increasing the motor resistance, however, one also must take into account the increased energy consumption required to run the system, to which metrics such as fuel economy must be recalculated. One must also note the shift in resonant frequency from 6 Hz to 5.4 Hz, to which external effects at this frequency on the system must be taken into account. Increases in the flywheel diameter must also be tested for geometric constraints of the car design as well.
While the mathematical model derived from the experimental data proved sufficient for the desired objective, it is most likely not as suitable for tests of high precision. In the mathematical model, the shaft moment of inertia, winding inductance, and non-linear factors were neglected. Parameters such as damping coefficients are also susceptible to change over time, a factor not taken into consideration in the model. The influence of these parameters not taken into account is unknown, and may require the development of a different model when testing other aspects of the system. Overall the model created has proven quite accurate in modeling the system given. This model confirms the recommended changes as reducing peak vibrations by at least 30%.

In 2005 a study completed by the Center for Disease Control found that 5,800 people 65 and older died from injuries related to unintentional falls and another 1.8 million received emergency room care. Of those treated, 433,000 were hospitalized for long term care [1]. Seniors who live alone may suffer from an unintentional fall and are often not found for hours or days. Long waits for medical care exacerbate their injuries and increase their risk of death or permanent hospitalization. However, fast emergency response to a fall reduces the risk of hospitalization by 26% and death by 80% [3]. The problem with seniors not receiving immediate attention needs to be addressed.
A National University of Singapore study researched and developed a garment-based 3-axis MEMS accelerometer designed as an alert system for senior citizen falls by recording an impact [2]. The system was to be worn by seniors in a pouch attached to the shoulder which contained the accelerometer. The power supply and blue-tooth wireless system to transmit an alert signal was located in a pouch attached at the waist. The chip itself was designed to record a heavy impact of magnitude consistent with a fall situation. When a fall was detected, the system would transmit a signal to the wearer's cell phone, which would signal an emergency response team and the person's family.
We have found several flaws in the system developed at the University of Singapore. Since the accelerometer measures impact, if it were to break during the fall it would not function properly and emergency response would not be alerted. In addition, a number of daily motion activities could cause g-force readings consistent with that of a fall, which would trigger a false fall alert and cause unnecessary emergency response action. If there were an emergency response unit which measured the fall itself and not the impact, the possible break would not be an issue and false alarms would not occur.
Our objective is to develop an emergency response system to detect unintentional falls of senior citizens. This system would be triggered by a characteristic acceleration response associated only with falling and would be accurate enough to not register false falls. In addition, we aim to use the device as a static orientation system that would be able to correctly measure the wearer's angle from vertical, as well as body orientation (i.e. face down, on left side, etc.) We have completed this work. In this report we will cover:
In order to reach our final goal of characterizing free fall several experiments were performed. The accelerometer ADXL203EB was first calibrated, and then used to record data for static orientation, fixed axis free fall, and daily human activities. Voltage input to the accelerometer was provided by the HP 33120A function generator. Voltage outputs were observed and recorded using the HP 5460213 oscilloscope. From there Microsoft Excel and Matlab were used to analyze data.
In order to interpret readings from the accelerometer we needed to determine a relationship between voltage and acceleration. The accelerometer was rotated to angles and orientations of known acceleration and the voltage output recorded. Using this data we were able to then know the acceleration of the accelerometer given a voltage at any point in time.
To gather experimental data for the orientation of the accelerometer in space we performed experiments holding the accelerometer in static orientations. The accelerometer was placed at the end of a pivot arm as shown in Fig. 1, below, and the arm was held at 0, 30, 45, 60 and 90 degrees from vertical. Output voltages were recorded at each angle. From there the accelerometer was rotated about its z-axis, which is depicted in Figure 2 on page 4. The accelerometer was rotated to 45, 90 and 135 degrees while the pivot arm was moved through the previously described static positions. Data was recorded at every orientation.
Once static orientation had been performed we experimented with free fall in a two dimensional plane. The accelerometer was reattached to the end of the pivot arm as shown in Figure 1, page 3, so that the accelerometer recorded data in the plane of motion. This meant that that analysis was only done in two dimensions to ensure that we were able to accurately measure the radial and tangential acceleration. The orientation of the accelerometer can be seen in Figure 3, below. The pivot arm was then dropped from vertical and data was recorded.
In order to compare data collected for fixed axis free fall motion to other motions we recorded data performing a variety of activities. The accelerometer was attached to the experimenters' waist and the subject walked, jumped, sat down and laid down. Data was recorded for every action.
During testing we were able to accurately calibrate the accelerometer and predict static orientation. In addition, we developed a theoretical model for falling, and gathered data for fixed axis falling and other daily activities.
During the calibration, numerous voltages were recorded for known accelerations and plotted against each other. This can be seen in Figure 4 on page 3. We found that there was a positive linear trend between the measured voltage and known acceleration. This relationship was used to calculate the acceleration of the accelerometer given the output voltages for the rest of the lab.
Logic was derived to use both the x and y axis data in conjunctions with the calibration data to determine the orientation of the chip which could be located on a person. It was able to accurately predict the angle from vertical within an error of ±2 degrees as described in Figure 5 below. It was also able to predict the rotation about the vertical axis of a person within an error of ±3 degrees as described in Figure 6 below. Combining these two angular outputs the orientation of a stable fallen person can be accurately determined.
In order to analyze a fall situation a theoretical model first needed to be derived as a comparison to real world data. A schematic of the setup can be seen in Figure 7 below. The chip was oriented so that the y-axis of the chip read the radial acceleration and the x-axis read the tangential acceleration. We simplified the problem assuming a frictionless pivot at the base of the arm and the force of gravity acting at the center of the arm. The shaft also had length l. By performing static and kinematic analysis we were able to predict both the theoretical x and y-axis acceleration and they can be seen in Figure 8 below. More information on the calculation for the theoretical model can be seen in Appendix A.
To verify our theoretical model it was compared to experimental data from the fixed axis free fall experiment. Our experimental data can be seen in Figure 9 below. There are results from three free fall tests plotted together in Figure 9, and the results are extremely similar from one test to another. The experimental data had results that were expected.
Other daily activities needed to be investigated to ensure that their results were not similar to those seen in the free fall situation data. Walking, jumping, sitting and lying down were investigated and their results can be seen in Figures 10-11 below and 12-13 on page 8 respectively.
To further investigate free falling we must compare the theoretical model to the experimentally collected data for free fall. Radial and tangential acceleration for three trials as compared to our theoretical model are shown in Figure 14 below. Note that experimental data does not exactly follow the theoretical model. The tangential acceleration has a very similar output but the radial acceleration seems to have a time delay. This could be due to a delay in the response of the accelerometer or a delay in the data collection of the electrical system. Further investigation of this discrepancy needs to be completed.
Assuming that the time delay can be identified and then predicted we can accurately model fixed axis free fall. Using the validated model we can define parameters to analyze data and sense if a fall situation occurs. One method of doing this would be to create bands around the validated theoretical model and detect if a majority of the data being collected was within the bands on both channels. If the data did fall within both channel margins it would signal a fall and actuate an alarm. The theoretical model with bands is shown in Figure 15 below.
In addition to investigating free fall situations we had to determine if the falling model was similar to any other daily activities. To have an accurate system, we had to ensure that there would be no false falling alarms. After comparing our theoretical model to typical daily motion activities in Figures 10-13 on pages 7 and 8 previously, it is clear that falling is not consistent with any of these plots. Therefore, the system will be able to distinguish between falls and any other daily motion activity and there will be no false falls.
We were able to use a MEMS accelerometer to characterize a free fall response and observe body orientation. Through this, we were able to develop a theoretical model consistent only with falling and not any other daily motion activity. This model could be used as trigger parameters for an emergency response system for senior citizens who experience unintentional falls. If widely used, this system could greatly decrease hospitalization and death as a result of slow emergency response and better many lives.
After completing the calibration we were able to accurately determine the static orientation of a person. Using our logic we could accurately determine someone's angle from vertical within 2 degrees as described by Figure 5 page 5 and rotation about the vertical axis within 3 degrees as described by Figure 6 on page 5.
Next we were able to create a theoretical model using kinematic analysis to predict what the free fall acceleration response would be shown in Figure 8 on page 6. This was then validated using experimental data and the only discrepancy was a time shift, which is shown in Figure 14 on page 8. Using the theoretical model we were able to design parameters to sense a fall as described in Figure 15 on page 9. We then were able to determine that there would be no false alarms due to other daily activities by comparing normal daily activities including walking, jumping, sitting, and lying down to the parameters used to sense falling.
Through this analysis we were able to use a MEMS accelerometer device to sense motion of the human body specifically in a free fall situation. This has many benefits to an elderly user. Even if the chip is damaged on impact, a fall is still recorded by the system and emergency response is alerted. Voltage characteristics consistent only with a fall situation eliminate false fall alerts that were present in the impact-based system. As discussed earlier in this report, faster emergency response leads to decrease in hospitalization by 26% and death by 80%. Therefore, we hope to drastically reduce the emergency response time through the use of this system and greatly improve countless lives.
We recommend further testing be conducted with a 3-axis MEMS accelerometer in order to have a better 3-dimensioanl analysis of human motion. Such analysis would be more advantageous than using the 2-dimensional analysis conducted in our experimentation. Also we believe that more full scale testing should be conducted. We also recommend generating models for other modes of falling. Since falling is not always likely to occur in a fixed axis position there are many different ways in which a person may fall. Further testing could analyze and characterize many falling different falling scenarios.
The next step includes development of a computer program that has the ability to recognize the data response consistent with falling. Such a program would then trigger an alert for emergency response to hospitals or family members. Also it is necessary to design the device with the accelerometer to be worn by the individual. Such a device would ideally be small so it would not disrupt normal activities. We recommend the device be attached to a garment on the individual's shoulder. The device must also be capable to wirelessly transmit data to a central computer in the home that would monitor for falling.

There are approximately 35 million people living in the U.S. who are over the age of 65 years (U.S. Census Bureau 2003). An injurious fall in an elderly individual can be devastating. Indeed, more elderly die from falls than die from motor vehicle accidents each year (CDC 1999). More than one-third of adults ages 65 years and older fall each year (Hornbrook 1994; Hausdorff 2001). Older adults are hospitalized for fall-related injuries five times more often than they are for injuries from other causes (Alexander 1992). The most costly fall-related injury is a hip fracture because it carries the risk for serious sequelae, including bone infection and pneumonia, both of which can be fatal. Other sequelae include loss of strength and self confidence, which can limit mobility and willingness to take part in social activities outside the home. A rise in serious head injuries resulting from elderly who fall has also been noted (Kannus et al. 1999). Not every fall results in injury. Roughly 5% of falls result in serious injury requiring medical attention (CDC 2003; U.S. Census Bureau 2003).
Common sense informs us that one can reduce the number of fall-related injuries in two ways: (a) reduce the number of falls by intervening on the risk factors for falls (for example, Weerdestyn et al. 2006), and (b) when a fall happens, reduce the risk of injury from that fall (for example, DeGoede & Ashton-Miller 2002 & 2003, Groen et al. 2005, Lo 2006). As far as the first approach is concerned, many extrinsic and intrinsic risk factors for falls have been identified (for example, Tinetti et al. 1994) and several types of interventions have been developed to address the most important of these factors. However, the number of falls each year remains high and the probability of ever reducing the number of falls to zero is non-existent. Furthermore, sooner or later everyone falls unintentionally, regardless of their age. So, the second approach leads us to want to find out what people need to know a priori and what they need to do during a fall so that they can avoid injury when they do suffer the inevitable fall.
It is known that when a fall results in the greater trochanter directly impacting the ground, then the risk of hip injury is 30 times higher that when the greater trochanter does not strike the ground (Nevitt et al. 1993, Hayes et al. 1993, Schwartz et al. 1998). The underlying message from these studies is that if an elderly person falls, "don't, ever, land on your greater trochanter".
The objective of this study was to estimate whether it is feasible for individuals to twist their pelvis during the ongoing fall in order to avoid landing on their greater trochanter. Do they have the hip external rotator strength that is needed to rotate the hip within the time it takes their pelvis to hit the ground? How do factors like age, gender, body mass, and obesity affect this calculation. Because, actual falls carry a risk for injury, we instead used literature values for muscle strength, our own experimental measurements of hip muscle strength in two individuals, and computer simulations to answer the question.
The technical question we wanted to answer is how long (in msec) it takes a healthy person to volitionally twist their pelvis through a set angle (in order to present the posterolateral part of their thigh/buttocks, rather than the greater trochanter, to the landing surface). In Feldman and Robinovitch (2007) the average axial rotation of the pelvis/hip upon impact from an unexpected lateral fall was 8 degrees in a posterolateral direction. However, from their Figure 2b, Appendix A, we see that an axial rotation angle of 20 degrees would suffice (a) to land on the maxim depth of muscular "padding" over the bony pelvis, and (b) to land on the aspect of the pelvis that does not include the greater trochanter. In other words, subjects need to axially rotate their pelvis at least 12 degree more than normal and ideally 22 degrees more than normal. We also chose to find out how long it takes to twist the pelvis through more than enough rotational angle, i.e., 30 degrees, given unilateral stance and full foot-floor frictional contact conditions. Finally, we need to find out how much this latency is affected by advancing age, increasing body mass, and decreasing hip muscle strength, and obesity.
A rough calculation for this time can be made by knowing the reaction time for the onset of muscle activity (~60-166 msec) (Ashton-Miller, Thelen 1996) and then adding the time required for muscle to develop enough contractile force and torque to rotate the hip externally 30 degrees, as well as counter rotate what was the stance leg in an internal hip rotation direction through 30 degrees in the opposite direction, thereby rotating the greater trochanter away from the impact site. This second rotation would require the hips to rotate 3° in the opposite direction relative to the first turn, and the leg to rotate 30°. If this time is less than the mean 626 ms (SD =40) (Feldman, et al. 2007) showed that it takes to fall sideways, then this calculation provides the theoretical basis for expecting that there is time for both young and elderly to present the "correct" posterolateral part of the pelvis (i.e., buttock) for impact with the ground in a sideways fall.
Two male subjects-A: 21 years old, 180 cm tall, and 72 kg, and B: 60 years old, 190 cm tall, and 84 kg, participated in the study. Each subject wore a belt around the top portion of their iliac crest (waist). A set of three infrared light-emitting diode markers was attached to the belt. Using an Optotrak Cetrus system each marker location could be measured in 3-D space and the change in hip angle during the course of a volitional axial turn of the pelvis could be calculated. The experiment began with the subject having one foot planted on top of an AMTI OR-6 force plate and the other foot lifted off the ground behind the individual. The subject would then twist their pelvis as quickly and as far as possible three times in either a clockwise or counterclockwise direction of rotation, while keeping their planted foot stationary. The peak torques, degree of turns, and time to turn was then calculated and descriptive statistics calculated.
The isometric torque values were recorded by having one foot planted on the force plate while the subject held a stationary ladder in front of them. The subject would then apply a torque to their foot while keeping their hips and foot stationary.
A model was created in Adams to simulate the motion of the hip, thigh, shank, and foot (Appendix K). One model, "fixed", was created which had the foot and leg had fixed to the ground which only allowed rotational motion of the hips via a torque source at the "hip". The angle of turn between the pelvis and ground, and the hip torque input was computed. For simplicity, a second model, "free", was created which represents the second part of the fall-when the foot has lost contact with the ground and a counter torque reaction is required at the hip. This counter torque moves the greater trochanter "away" from the impact area so that the posterior buttock becomes exposed to the impact. The values of inertia and dimensional values used for the hip, thigh, and shank were taken from Anthropometric Source Book (NASA, 1978).
The moment of inertia for the whole body was based on the results of David Carrier's article, 'Influence of Increased Rotational Inertia on the Turning Performance of Humans', for varying whole-body masses (Appendix B). These were extrapolated from the range of body masses of the subjects were used to the average masses of males and females for the young adults (age 20-29 years), heaviest age bracket, mid age, (age 40-49 years), and the old (age 60-74 years) (U.S. Department of Health and Human Services). The decrease in external hip muscle torque between 20 and 70 years was scaled from the maximum voluntary ankle dorsiflexor strength data reported by Darryl G. Thelen et al. in their article 'Effects of Age on Rapid Ankle Torque Development", Table 2 (Appendix D, E). We made the assumption that the hip internal and external hip rotator muscles have identical maximum torque-time characteristics to the ankle dorsiflexors. We made this assumption because we could find no data in the literature on age and gender effects on hip internal and external rotator muscle strengths, and because the volume of muscle is approximately similar.
Sample plots of the experimental results showing the angle of turn, torque, and angular velocity against time can be seen in Appendices F-J. The "best" hip rotator strength test results came from the trials which involved turning clockwise for both the right and left foot planted-which correlated to the internal and external rotation muscles respectively. The peak torque values were taken at the second peak, at which point the angle had the largest magnitude. The change in time for the angle was taken as the time for the angle to reach 90% of its maximum value from 10% of its baseline value at rest. Table 1 below, shows the averaged results of these trials with precision error.
Table 1: Maximum volitional internal and external hip rotator muscle strength and turn test results
These results were input into the Adams simulation along with the extrapolated values from the methods section. The results of these trials can be seen in Appendix N. All trials that did not complete the second rotation of turning the leg 30° could also not complete the internal hip rotation of 3° either. It was found that for the average inertia, 1.12 kgm2, if the time was 100 ms for the free leg, subject A at 50% of its strength and subject B did not complete the required angle of 30°, as seen in Figure 1 below. For the fixed leg simulation, only subject A at 50% of his strength was under the 30° for 400 ms. When running the simulation with subject A and B's experimentally determined torque and extrapolated inertias respectively, Figures 2 and 3 below, subject A met all requirements while subject B did not meet the internal goal at 100 ms and external requirement at 350 ms. When looking at the results for the extrapolated inertias and torques based on the averaged weights and loss of muscle strength for the young, middle aged, and old males and females, no females were able to make the required 30° external rotation at 100 ms while all males succeeded. For the internal rotation, all males could meet the requirements at 350 ms and above, while middle-aged females could not meet it at 350 ms, and old females could not meet it at 400 ms. In the extreme case for males and females 45 years old and above, who are of the upper 95 percentile of their weight bracket, none could meet the internal rotation requirement at 100 ms as seen in Figure 6. As for the external rotation, no male could meet the angle requirement by 400 ms, middle aged female by 450 ms, and old female by 500 ms.
The minimum required torques to complete a 3° turn for the internal rotation of the hip and 30° turn for the external rotation for different times can be seen in Table 2, below. The inertia values were extrapolated based on the average weight for males and females of ages 45 (mid) and 65 (old). In Table 3 the maximum allowable inertia and extrapolated mass was calculated for set average maximal torque values to complete the same turn angles. The average torque values came from the extrapolated average strength loss based on age and gender found by Thelen et al.
Table 2: Minimum torques to reach set angle of turn for average weighted people
Table 3: Maximum Inertia values to reach set angle of turn for average strength people
Based on the results, one can conclude that an individual's inertia and strength has a marked effect on their ability to rotate their pelvis and protect their hip from impact in a fall. Assuming that an angle of 30° is needed for the initial external rotation, 3° for the second internal rotation of the hip, 30° for the second rotation of the leg, a response time of 100 ms, and a time to fall of around 626 ms, those most at risk of not being able to exercise the proper fall technique would be elderly females, while the middle aged female would also be at a risk. This is due to the lower available strength in women in comparison to men. For the extreme case of having a weight in the upper 95% percentile, all are at risk of not being able to complete a fall with the correct technique. However due to increasing obesity in the population, one would have to look further into a scan of the pelvis region to see if an angle of 30° is required, or if there is enough excess tissue to supply enough cushion for the greater trochanter in a fall if one rotates the pelvis less.
The second turn completed by using the internal rotator muscles, was completed by all tests in less than 150 ms and therefore is not a bottleneck for the fall process. This is due to the leg having significantly smaller rotational inertia than the torso. The initial phase of external pelvic rotation holds the most significance. Based on the experimental results for maximal angular pelvic velocity (approximately 100 degrees/sec) and the knowledge that the normalized torque occurs at the same time no matter an individual's age or strength (Thelen), all turns seem feasible to complete in the required time. However, not all turns in the experiment were completed past 30°. This could be a flexibility issue which is a separate issue. Women would be expected to be more flexible than men, so lack of flexibility may not be a problem for them.
Limitations of the study are the assumption that maximum voluntary hip external and internal rotator muscle torques are similar to maximum voluntary ankle dorsiflexor muscle torques. In addition, our estimates of single leg rotational inertia were only approximate and need to be improved. The experimental hip torque data was measured with one foot planted on the ground while the subject anticipated the turn and started from a balanced stance. In a real life situation a loss/lack of friction under an individual's feet would cause an imbalance by surprise. In addition, when looking at the graphs for the experimental torque, there was often a high torque value in the opposite direction before the maximum angle of turn. This suggests a preparatory muscle tension in the experimental trials which would not be available in a surprise fall. This makes the experimental values an upper limit and with a short reaction time. This can also be seen when comparing the isometric torque values, Appendix O, with the experimental ones -- which are higher. Finally, we measured hip isometric strength and rotational performance data on only two males. More data are needed on men as well as women of different ages.
1) These calculations suggest that all healthy individuals should be capable of axially rotating the pelvis through an angle of 20° in order to avoid presenting the greater trochanter to the ground in a fall.
2) If a full 30° turn angle is required to be completed, then older individuals, and particularly females and obese individuals, over the ages of 45 years may not have sufficient strength to achieve the avoidance maneuver.
3) To avoid injury it is important to retain hip internal and external rotation flexibility, and for those at risk for hip fractures to increase their hip rotator muscle strength.
4) Further hip muscle strength and performance testing should be completed in order to obtain a better representation of the capacities of both genders of different weights and ages.
5) Further research should involve actual fall tests to see if the required neuromuscular coordination for the greater trochanter impact avoidance maneuver is in fact available during an actual fall to the side.

Current padding systems used for wood flooring are insufficient to dampen the impact vibrations and sound vibrations that carry through the floor to the room below. Sound vibrations in homes cause sleep loss, emotional and physical response, annoyance and activity disturbance. Sound vibrations have been studied using sound equipment; however, this is expensive. The equipment is large and bulky and has a limited dB level. Impact vibrations data collection and analysis is less costly and requires a simpler technique. Damping these vibrations leads to a better quality of life. It is the objective of this project to record impact vibrations using a Precision ±1.7 g Single-/Dual-Axis i MEMS® Accelerometer ADXL103/ADXL203, analyze the data, conduct evaluations of damping material based on the data collected and determine the best damping material, and relate impact vibration damping to sound vibration damping.
A: Total absorption
In order to simulate an actual residential environment, a particle board and joist system was constructed to replicate a flooring platform. A tough and groove wood flooring that was constructed using Brazilian cherry wood was clamped to the platform. The model MEMS accelerometer was attached using tape on the top surface of the wood flooring at the far end of the platform. A basketball was then dropped from a prescribed height of 0.762 m on to the center of the platform. This experiment setup is shown in Appendix A. Output voltages from the MEMS accelerometer were then collected using an oscilloscope, shown in Appendix B. Trials were repeated placing various damping materials between the platform and wood flooring. Five trials were conducted for each damping material. The experiment was then repeated using a softball as the impact object from a height of 0.864 m. Data was then collected with the MEMS accelerometer positioned on the underside of the platform as shown in Appendix C. Six different damping material configurations were analyzed in the experiment: no damping material, polystyrene, blue PVC vinyl foam, white vinyl foam (current industry standard), fleece and a combination of fleece and the blue PVC. The damping materials are illustrated in Appendix E.
The flooring system can be modeled as a mass/spring/damper theoretical model setup and is pictured in Appendix D. It is assumed that the platform is a rigid body, the Brazilian cherry wood acts as a mass and spring and the material acts as a damping system. By using this theoretical model's vibration dynamic equations in Equations 1 and 2 below, we can find the dynamic response.
The material properties for the Brazilian cherry wood are outlined below in Table 1.
These material properties can be used in Equation 3 to calculate the spring constant. This spring constant along with the mass of the wood is used to calculate the natural frequency in Equation 4.
A sample impact vibration recorded by the oscilloscope is shown in Appendix F. The magnitude of the first wave length X1, the magnitude of the consecutive wave length X2 and the period were collected from each test trial. This data was then used to calculate the peak to peak ratio in Equation 5 and finally the damping ratio in Equation 6 for each trial, shown below.
The average damping ratio was calculated for four cases: a basketball dropped with the MEMS accelerometer on the top of the floorboard system; a softball dropped, MEMS accelerometer on top; a basketball dropped, MEMS accelerometer on bottom; and a softball dropped, MEMS accelerometer on bottom. Each case study varied the configurations of the damping materials when measuring the oscilloscope voltage output from the MEMS accelerometer. From the four cases, an average damping ratio was then calculated for each damping material. The uncertainty was calculated by taking two times the standard deviation.
Table 2 on page 6 gives the averaged results of each damping material for the four case studies and their corresponding five damping material test trials we performed. Because the resolution error of the equipment was small, the error in the damping ratio was calculated from its precision error for each of the five tests trials we ran in a case. As a result of the high noise and inconsistent peaks in the data (sample graphs shown in Appendix F and E), X1 and X2 were difficult to measure and accounted for the majority of damping ratio error. In particular, the data using the fleece configuration damped in a very short time and left very few peaks to choose suitable measurement values for X1 and X2, which is why its error is so high. Table 3 shown on page 6, was created to show an overall summary of all of our trials and is an average of Table 2 values.
The MEMS accelerometer test data shows that the vibrations in the floor board are those of a damped harmonic oscillator. The initial research on the subject of vibrations and floor boards led to the conclusion that there should be some coefficient β, shown in Equation 21 on page 11, which relates radiated sound power to the peak vibration. Using this argument the highest damping value for the vibrations was tested for, since this will logically produce the least sound. From Table 2 an overall trend can be observed: damping ratio is higher for a bottom accelerometer position rather than for a top one. This occurs because the surface vibrations are more reliant on the material properties of the wood to damp most of the vibration, and the bottom vibrations have the additional damping of the test material and the joist. The difference in damping values could also be resulting from the difference in location of the MEMS accelerometer on the floorboard plane. The top position is located at the far end of the floor board as shown in Appendix A. The bottom position is located directly under the impact area secured between the cross wood supports, as shown in Appendix C, which allowed for easier attachment. The individual damping ratio values were averaged to determine the best material overall in all sound insulating conditions simulated. From Table 3, the PVC blue foam – fleece combination had the highest value at 15.15x10-5±6.66x10-5. The order for combination (either blue foam on fleece, or fleece on blue foam) did not matter for damping value with in error. Tests with combinations of materials showed an increase in damping values; however, laboratory time did not permit further testing of all configurations of current damping materials. To experiment with achieving a higher damping value, a vibration test for the combination of polystyrene and fleece should be conducted. The best individual insulating configuration is the fleece with a damping ratio of 14.28x10-5±12.8x10-5. The high error of the damping ratio is attributed to the inconsistency of the impact force. Shifting of the position of the accelerometer and its tape during impact contributed to additional error.
Noises in households are created from vibrations propagating through the floors and walls. It is possible to use MEMS accelerometers to determine the damping coefficient of wooden floor boards. The vibrations are identical to the case of damped harmonic oscillators, where higher damping ratio equals less vibration. We were able to determine the damping ratio for our test floor that was padded with sound insulating damping material. The damping ratio had unusually high error, which is attributed to the inconsistency with the impact method and accelerometer placement.
The test material with the highest damping ratio was the combination of blue foam on top of fleece. The best choice for commercial use would be the fleece only for several reasons. This material had the second highest damping ratio, is inexpensive, thin, and has limited deterioration over time.
More tests need to be done in the future to reduce experimental uncertainty and find better methods of reducing the propagation of sound waves from impacts. We could use a standard tapping machine for constant input vibrations to prevent human error in the drop force. Additional test trials can also be done with more materials such as composites, cork, foam rubber, and so on, to find the best insulator make-up. The effect of geometry on the insulating volume sound muffling abilities should be examined. For large-scale damping, we would like to investigate the effects of using vibration reduction devices, such as a shock absorber or tuned mass damper systems.
We would like to conduct further testing to determine if reducing the time and amplitudes of upper room impact vibrations using materials with increased damping constants, in fact, reduces the noise levels transferred to rooms below. We can also test the vibration response of the joist-setup to determine the system's natural frequency and model a differential equation (and its solution) describing the impact vibration response.
In order to reduce the error in the damping ratio that we found by dropping a basketball or softball, a more consistent force input method is required and was formulated. Appendix H shows the proposed force vibration experiment set-up. The test procedure is as follows: the DC motor rotates a mass connected to the floorboard at its midpoint; a linear variable differential transducer (LVDT) measures the vibration response; the resulting signal demonstrating floorboard deflection versus time is captured by an oscilloscope; the motor speed is then to be increased until the LVDT measures a local maximum in the deflection data; at this speed, the vibration dynamic period can be procured from the time for 10 cycles to pass at steady-state. The fundamental natural frequency of the floorboard in bending would occur at this maximum displacement, and is calculated as follows in Equation 7:
However, error can occur when assuming that this maximum displacement is completely a result of bending, as torsion could be identified incorrectly as bending by the LVDT. Our earlier force input method of dropping a basketball or softball could be used to simulate free vibration and verify the LVDT set-up's determination of the fundamental natural frequency, helping to reduce uncertainty about the bending mode of the floorboard.
Floorboard stiffness is a material property of the wood, and has been found to greatly influence the impact vibration response. There is much variation in the material structure of Brazilian cherry wood from piece to piece, so to find our floorboard set-up's stiffness, we would conduct static load testing by placing a point load at the middle position of the joist making sure the load was distributed uniformly across its width. The deflection would then be measured using a dial indicator. The stiffness is calculated from load and deflection information shown in Equation 8 below. It can be validated by Equation 9 using the fundamental natural frequency found from the above test set-up. The boundary condition parameter, k, can be found from previous experimental data for our joist (a pin-pin support structure) to be 2.46. Finally, we can find the natural frequency for the floorboard bending mode, shown in Equation 10.
The damping response behaves viscously – that is, where the damping force is proportional to the displacement and velocity of the floorboard vibrating. Therefore, a modification representing energy loss' contribution to the vibration response can be included in our predictive equations. Using strain gauges and force transducers, the strain and stress of the system can be recorded in the LVDT set-up. The loss factor, or the ratio of average sound energy loss per radian to the peak strain energy in harmonic oscillation, can be calculated. The loss factor is related to the tangent of the ratio of phase shifts between the stress and strain, shown in Equation 11 below.
Given that we displace our mass (the floorboard) and let it vibrate freely at is damped natural frequency (Equation 12), according to our characteristic equation (Equation 2 on Page 4), its homogeneous solution to is displacement as a function of time is shown in Equation 13.
With this equation, the initial conditions, and the experimentally determined natural frequency and damping ratio, we can know the under-damped vibration as a function of time, and, thus, the exact behavior of the floorboard. Using the loss factor of the floorboard, we can get the values for A and φ, as shown in Equations 14 and 15.
We can then put forth this relation using different damping materials to find out how quickly the vibration energy dampens out to a convergent value. Appendix I shows a sample under-damped response to be calculated from damping test trials.
To determine if increasing the damping ratio in the floorboard set-up actually reduces the noise level in the lower room, when there is impact vibration in the upper room (or vice versa), we can conduct an experiment comparing sound intensity. Sample experimental set-ups are shown in Appendix J.
Using a sound intensity probe and a portable analyzer, we can measure Ia and Ii of the source room, which is the origin of the sound. We can gather data about the source and receiving rooms' sound qualities (absorption coefficient, total absorption, and source room constant) using Equations 16 to 18, shown below.
Using a microphone and sound intensity probe, we can measure sound pressure and wave velocity, and, thus, the acoustic impedance of Brazilian cherry wood, as shown in Equation 19 below.
In order to validate our hypothesis that states that with less, more damped vibration there is less radiated sound, we must first relate the drop force to sound power radiated in Watts from the impact, shown in Equation 20 on page 11 and in Appendix K.
We can also find the effective radiated sound power, or the power experienced through a certain surface area, using Equation 21 on page 11. This can be related to our MEMS accelerometer voltage output levels, by multiplying the amplitudes by some coefficient, β. This value can be found by using the amplitude at the highest peak of the oscilloscope output to describe the initially loudest stage of an impact, where sound measurements are taken.
Given all of this information, we can find the transmittivity constant, which tells us the ratio of sound power experienced in the source room to that of the receiving room, shown in Equation 22.
The impact sound level (in dB) can then be found from Equation 23 below.
This value should be shown to decrease from Room 1 (upper source) to Room 2 (lower receiving) as more damping material is added. Using this equation, we can relate the oscilloscope output voltage from the MEMS accelerometer readings, and relate it to the impact sound level transferred from the upper source to the lower receiving room. This will to provide information on the effect of adding specific damping materials on reducing impact vibrations and noise levels in separate rooms of households.

For project 4, our team must evaluate the patented design suggested by TYR for their swimsuits. For part a, we had to calculate the natural flow without tripping around the sphere using Fluent. We than had to calculate the natural flow of the same flow tripped prematurely into turbulence. We evaluated the results of laminar and turbulent flow. For part b, we had to do the same as part a except model the whole body of the swimmer, rather than just the head. Next, we commented on the claims made by TYR's website and suggested some improvements on their design. The parameters used to find the speed of the Olympic swimmer and properties of the water are shown in table 1 below. For the laminar cases, we found that using steady or unsteady time resulted in similar data. Unsteady vortex shedding occurred for both laminar and turbulent cases. For the turbulent sphere case, unsteady 2nd order implicit time was used and for the turbulent body case, steady time was used because it resulted in the most accurate data.
Using Gambit, we created the mesh as was described in the problem statement. For part a, the mesh is shown in figure 1. We modeled a person's head as a sphere with a diameter shown in table 2. For part b, the mesh is shown in figure 2, and we modeled the whole person's body using body dimensions for the average human being also shown in table 2. For both parts, we used a tri pave face mesh and a tetrahedral/hybrid TGrid volume mesh. This mesh seemed to work best especially when we modeled the whole body of the swimmer due to the varying dimensions. For the mesh around the sphere/body we modeled it similar to project 2, only revolved it 180° and labeled the flat plane as a symmetry boundary condition. The total height and length from the sphere for part a was 10*diameter of the sphere, and 20*length of the sphere. For part b, a height of 7.5*chest diameter and length of 30*chest diameter was used. The meshes were then imported into Fluent where the calculations began.
Once the mesh was created in Gambit, we added the boundary layer thickness in TGrid for the second parts of a and b when turbulence was being tested. We found the location of the first grid point and boundary layer by using equations 1-5. L is the distance of the arc length of the top left quadrant of the sphere for the sphere, or half the total length of the body for the body, is the kinematic viscosity, Cf is the skin coefficient, is the friction velocity, and is the dynamic viscosity. Using prisms and the nominals pointing outward from the sphere/body (see figure 2) we set the first heights (BL/10 shown in table 3) with 10 layers, a constant growth method, uniform offset method, and weight of 1. New domains were made for the tri mesh, with the exception of the sphere/body, and then auto meshed. No boundary layer was created for the laminar cases. The boundary layer conditions can also be seen in figure 3.
Once the sphere was created and meshed in Gambit, we began by running tests in Fluent with fluid speeds equal to that of an Olympic swimmer with a Reynolds number approximately 403,000. By comparing our case with the coefficient of drag against Reynolds number for a sphere, as shown in figure 4, we concluded that due to the flat laminar region, we could run the trial at a lower Reynolds number and get the same coefficient of drag with more accurate results. We used a Re of 1584 and found the velocity to be 0.008 m/s; which was then inputted in Fluent to find the drag coefficient. In Fluent, the residuals were changed to 1E-4 to increase the accuracy of our results. The trial was initialized with zero initial conditions and the material and boundary conditions stated above were used.
The equation used to find the Drag coefficient is shown in equation 6. The frontal area used is half the cross-sectional area of the sphere since the mesh was created as a half sphere, πr2/2 with symmetry conditions. FDrag is the drag force obtained in Fluent (N), ρ is the density (kg/m3), u is the flow stream velocity (m/s), and A is the frontal area (m2). The total force, along with the breakdown of the pressure and viscous forces, and calculated Cd is shown in table 4 below. Pressure drag accounts for 69.6% and viscous drag accounts for 31.4% of the total Cd. According to [1], the Cd is approximately 0.4 for the Re that we ran it at, therefore, our results match fairly well with the reference. The velocity field, contour plot of pressure, and contour plot of streamlines are shown in figure 5, figure 6, and figure 7 respectively for Re equal to 1584 for laminar flow around the sphere. One can see that backflow occurs in the near wake region for the sphere in laminar flow.
For the sphere in turbulent flow, we used the same mesh as the one for laminar flow except we added a boundary layer thickness. We used a Re equal to 403,000 which was calculated using the average velocity of an Olympic swimmer using equation 1, where L is the diameter of the head. We made sure there was at least 10 grid points within the boundary layer when we doubled the mesh.
Using the parameters in table 1 above, the tests were run in Fluent. The k-epsilon model under viscous model was used with the default model constants. Realizable and non-equilibrium wall functions were used because we found from Project 3 that using those options gave us the most accurate results. The residuals were changed to 1E-4 for all to increase the accuracy of our results. The boundary condition of the velocity inlet was set to 2.0408 m/s and the model was initialized with zero initial conditions. We ran the tests in Fluent using a Hydraulic Diameter of 0.1783 m which is the diameter of the swimmer's head. We used 0.1% for the turbulence intensity.
Using equation 6, we found Cd for this model. Pressure drag accounts for 77.3% and viscous drag accounts for 22.7% of the total Cd. According to [1], the Cd is .08 for the Re that we ran it at, therefore, our results match fairly well with the reference. The velocity field, contour plot of pressure, and contour plot of streamlines are shown in figure 8, figure 9, and figure 10 respectively for Re equal to 403,000 for turbulent flow around the sphere. The amount of backflow and the Cd was greatly reduced from the laminar to the turbulent case for the sphere.
Shown in figure 11 below is the mesh used for the body in laminar flow. The same velocity used for the laminar sphere was used for the body with laminar flow. The total force, along with the breakdown of the pressure and viscous forces, and the calculated Cd is shown in table 6 below. Pressure drag accounts for 33.7% and viscous drag accounts for 65.6% of the total Cd. Our Cd for the body was similar to that of the sphere according to [1].The velocity field, contour plot of pressure, and contour plot of streamlines are shown in figure 12, figure 13, and figure 14 respectively for Re equal to 1584 for laminar flow around the body. One can see that backflow occurs in the near wake region for the body in laminar flow.
The total force, along with the breakdown of the pressure and viscous forces, and the calculated Cd is shown in table 7 below. Pressure drag accounts for 70.6% and viscous drag accounts for 29.4% of the total Cd. The velocity field, contour plot of pressure, and contour plot of streamlines are shown in figure 15, figure 16, and figure 17 respectively for Re equal to 403000 for turbulent flow around the body. When the flow is prematurely tripped into turbulence, the amount of backflow and Cd is greatly reduced as expected.
In order to help reduce the drag, we would recommend that TYR try a few additional techniques. Similar to the trip wire causing the flow to switch from laminar to turbulent, we would also recommend that the suit becomes porous. By becoming porous, it would increase the surface roughness and enhance turbulence. In addition to this, having a porous material may enhance the wall suction/blowing while the swimmer is moving. As the fluid particles get close to the wall (the suit), they lose their kinetic energy near the separation point. By replacing these particles with ones with higher energy, either by sucking them into the suit and allowing those with high kinetic energy to move down, or by pushing higher energy particles out of the suit, turbulence can be tripped. Another suggestion would be to decrease the friction between the water and the swimsuit by making the surface slippery. This could be done by applying a slippery liquid to the suit by either having it rubbed on before a race, or by adding a chemical when washing the suit.
Another suggestion would be to change the entire shape and thickness of the swimsuit. Looking at figures 13 and 16, one can see that the pressure around the swimmer's shoulders is relatively high. This is due to the fact that there is a sharp transition in shape from the swimmer's neck to their body. Separation arises from adverse pressure gradients. These must be removed or decreased if one wishes to decrease the drag coefficient. One way to decrease the drag coefficient and to decrease the pressure around the shoulders is to make the transition from the neck to the shoulders smoother. This can be done by using pads just above the shoulders or around the neck assuming it does not interfere with the motion of the swimmer.
In conclusion we found that our drag coefficient results for the sphere were similar to that of other studies [1]. It followed the trend that by tripping the flow from laminar to turbulent, the overall drag coefficient could be reduced. There was a significant loss of separation in the backflow region behind the sphere in our results as well as those claimed by TYR [2], figure 18 below.
TYR claims to be able to decrease their pressure drag by 18% [3]. According to our results, an accurate conclusion based on this claim cannot be made. We feel the geometry of the body greatly influences the separation formation. This in turn will affect the pressure forces that are exerted on the body. We do feel however that the decrease in Cd from laminar to turbulent is significant enough to justify the suit. Also, this proves that our results are accurate, just not in the sense of pressure and viscous forces. As shown in figure 18 from TYR, the flow separation region is greatly reduced in the turbulent case; the same is true with our laminar and turbulent models, shown in figures 5 and 8 respectively.

Our team recreated the experimental setup shown in the project 2 description and shown in figure 1 in Braza et al. We created two different meshes, shown in figures 2 and 3 below, to check for grid independence. The team used a uniform quad mesh. The meshes were then imported into Fluent where the calculations began. We doubled the resolution, until our results were very close to each other which indicated that the solution we obtained was grid independent.
Our first task was to show the vector plots of the velocity field in the wake region at each Reynolds number (Re) and this can be seen in figure 6 on page 5. Next, we had to show contour plots of pressure and streamlines in the wake region at each Re number. Our fourth task was to show a plot of the re-attachment length versus time for Re=40 and compare it to figure 4 in Braza et al; this is shown in figure 9 on page 8.
Next, we show a plot of the pressure distribution on the cylinder wall as a function of θ at Re=40 and compare that to figure 5 in Braza et al; this is shown in figure 10 on page 9. Our sixth task was to plot drag coefficients and separation angles as a function of Re and compare it to figures 6 and 9 in Braza et al; this is shown in figures 11 and 12 on pages 9 and 10. Next, we plotted the time history of pressure at a selected point in the near wake and used this to calculate the Strouhal number; this is shown in figure 13 on page 10. Our last and final task was to plot the Strouhal number as a function of Re and compare it to figure 9 in Braza et al; this is shown in figure 14 on page 12.
In Fluent, we used air at 273K with a kinematic viscosity (νk) of 1.32E-5 m2/s and an inlet velocity determined by equation 1. The same mesh was used for all Reynolds numbers (Re). The density of the fluid (air) used was 1.292 kg/m3 and the dynamic viscosity (νd) used was 1.71e-05 kg/m*s. We first scaled our mesh in Fluent to convert from cm to m. We solved with an unsteady time and 2nd order implicit unsteady formulation. The momentum was changed to second order upwind and the absolute criteria of the residuals were changed to 1E-5 to increase the accuracy of our results. The boundary conditions of the velocity inlets were set to have an x-velocity shown in the table below and the model was initialized with zero initial conditions. From figure 9a in Braza et al, we were able to find an approximate Strouhal number and estimate the period for each Reynolds number using equation 2 below. We divided the period by 20 to find an appropriate time step size while iterating. The max iterations per a time step was set to 100 to make sure that the solution would reach the residuals and converge each cycle. For Re values of 100, 200, and 400, the model first needed to be manually triggered to achieve a periodic steady-state solution represented by vortex shedding from alternate sides of the back of the cylinder. This was done by setting the bottom velocity inlet to twice the U velocity. The trigger was allowed to run until the iteration plot showed a fine steady line under the set residual. We used a time step size 100 times less than the predicted time step size while triggering. The pressure was plotted at a point in the wake region 0.03 m above and 0.15 m to the right of the cylinder while the model was iterated. From here we could see when the solution produced a steady frequency of at least five cycles at which point we took our data measurements over five different times.
Using Gambit, we created a mesh based on the experimental setup shown in figure 1 and tried to recreate the mesh shown in the project description. Using Gambit, many different meshes were created and tested and the best single mesh is shown in figure 2. The type of mesh used was a uniform quad mesh.
In order to verify that we had mesh independence, we compared the values of reattachment length, drag coefficient, Strouhal Number, and separation angles for the two meshes. After creating the single mesh, we doubled it and calculated the four variables we were looking for. After realizing they were not as close as we would have like them to be, we went ahead and halved the single mesh we had and calculated the four variables again. We believe the results obtained, shown in table 2 for the three different meshes, are sufficient enough for us to use the single mesh. We showed our mesh independence for Re=400 based on 2nd-order steady flow.
The boundary layers can be seen in figure 4 on page 5. We created two velocity inlets. The first one was the 2nd quadrant of the large circle attached to the top horizontal line of the rectangular box and the other was the 3rd quadrant of the large circle attached to the bottom horizontal line of the rectangular box. This was done because Fluent cannot naturally predict periodic unsteady flow. Also, an outflow was created at the end of the rectangle and both the top and bottom halves of the smaller circle were set as walls to model the cylinder.
As the Re becomes greater than 40, asymmetrical eddy patterns occur seen in c. and d. below. This is shown as the alternating separation of the vortices which have been diffused from the cylinder; these are known as the Karman vortex paths.
Using our single mesh, we obtained a plot of the reattachment length vs. time for a Re=40 as shown in figure 9 below. Our data analysis is shown in table 3. For Re=40, figure 6a on page 5, one can see that there are two attached vortices behind the cylinder. The flow reaches steady state at approximately 16 when looking at our experimental compared to 15 when looking at figure 4 in Braza et al. Our figure for reattachment length closely matches figure 4 in Braza et al. Equation 3 was used to make time (t) dimensionless. We obtained a dimensionless reattachment length by dividing it by the radius.
(Eqn. 3)
We were able to calculate Cp by using equation 4 below where P represents the total pressure and Po represents the initial pressure. Our values showed a similar curve to that of Braza et al, but were slightly lower than Braza's Re=40 values and more closely match Braza's Re=20. This can be seen in figure 10 below.
Cp=(P-P0+.5*ρ*U2)/( .5*ρ*U2) (Eq. 4)
To solve for the Drag Coefficient (Cd), equation 5 was used. In Fluent, we ran 5 trials for each Re and found the Drag Force. Then using density (), velocity, and the frontal area (A), we are able to produce figure 11 that shows Cd versus Re. This graph compares nicely to the one shown in figure 6 in Braza et al with our values being just slightly higher which shows that our data agrees. The area consisted of the height (diameter) of the cylinder and the width was assumed to be of unit length meter.
(Eqn. 5)
To calculate the separation angle (Ѳd), we ran 5 trials for each Re and took the average Ѳd. The results are shown in table 4 and in figure 12 below. One can see that our figure compares very nicely to figure 6 in Braza et al.
For each Re not including 40, we ran five trials and found the period. Then we calculated the Strouhal Number (SH) using equation 2 below. D is for diameter and u, the uniform-flow velocity, was calculated from each Re. Figure 14 a) below shows the Strouhal number for each Re which compares nicely to the reference b).
After running many simulations in Fluent, we noticed that at low Re values around 40, the flow was steady state with two separation bubbles as shown in figure 6a. As the Re values became higher there was vortex shedding behind the cylinder. In conclusion, we found that our calculations very closely match that in the reference Braza et al. The values for separation angle, reattachment length, Strouhal number, and drag coefficient are all within 7% for the single and double mesh. The coefficient of drag and wall pressure were the most different compared to the Braza et al., with our values leveling out around 1.5-1.6 and Braza's near 1.2 for the coefficient of drag and our Cp closer to Braza's Re=20 instead of 40.

Well-modeled heat transfer models are required to prevent microprocessor failure. To find how a microchip's heat transfer and resistance vary with changes in temperature, we conducted experiments that simulate microprocessor heating. The purpose of this report is to find both theoretical and experimental values of the thermal coefficient of our chip, the time constant to reach steady state temperature, the chip-to-ambient resistance (and its components), and then to develop a heat transfer model to predict the resistance of a chip to ambient air as a function of air velocity and fin length. Lastly, using our data and knowledge of heat transfer, we would like to recommend different approaches to allow better cooling of the system.
(see Appendix A on p. 4)
In the first experiment, we attempted to calculate the thermal coefficient of resistance of our microprocessor chip (shown in Appendix M, p. 9) by calibrating the electrical resistance of the sensor on the chip as a function of surface temperature. The chip was fixed to a hot plate using thermal grease and electrical resistance was measured using the '4-point probe method'. Surface temperature was measured using an Omega HHI2 thermocouple, placed on the resistor pattern by the researchers. Initial chip resistance was found after applying a voltage of VDC ~ 100 mV. The resistance was then measured at a temperature range of 30 °C to 120 °C, since we did not want to reach 150 °C, the estimated failure temperature of the chip. Because of the unsatisfactory results, we redesigned our test procedures.
Our improved test setup used an integrated circuit packaging which had mechanical support, electrical shielding, and permanent connection of leads for robustness. Appendix K on p. 9 shows a photo of the setup and equipment used. We repeated the procedure in Experiment 1with a smaller range of 22 °C and a smaller maximum temperature to avoid damaging the chip.
In Experiment 3, we applied voltages to the on-chip resistor pattern ranging from 5 to 25V in increments of 5V and determined the surface temperature of the chip by measuring its current draw. The heat-sink temperature was measured using our thermocouple. We collected data from three trials.
In the last experiment, we tested our chip with an Alpha Novatech S-1530 20W heat-sink (cut in quarters) in a model wind tunnel (Appendix J, p. 8). We measured different flow rates using an anemometer by varying the voltage, from 6V to 12V in increments of 2V, supplied to the fan and measured corresponding current. We then theoretically estimated the chip temperature, Tchip using results from Experiment 2 when the chip was held at voltages of 9.71, 14.55 and 19.41V. We calibrated the air velocity V∞ as a function of power input to the cooling fan.
From Experiment 1 we assumed a proportional fractional change in resistance to the temperature change by a coefficient, α, as shown in Eq. 1.
Using Eq. 1 our calculated α had large errors and inconsistencies between data sets, and was considered to be skewed. The results of Experiment 1 are shown in appendix C on p. 6.
From Experiment 2, with the ceramic wire-bonded chip packaging, our initial resistance value was 291.78 ± 0.02 Ω at 22 °C and α was 0.00283 ± 0.00035 1/°C. Using Eq. 2, where i represents each trial and α is the mean of all the αi, we found α = 0.003437 ± 0.00085 1/°C. The plot of R vs. T is shown in Appendix D on p. 6.
Heat transfer and the resistance from the chip to the heat-sink was calculated using Eqs. 3 & 4.
We found Rchip-to-heat-sink was 9.986 ± 1.154 W/K. Heat transfer vs. ΔT is plotted in Appendix E on p. 7.
To determine the time constant for the chip to reach steady state temperature we used Eq. 5.
where Cpackaging is the capacitance of the ceramic (Appendix B, p. 5). Experimentally, the time constant was 4.62 ± 0.53 s, which was calculated using the experimental resistance value and the theoretical capacitance. This is close to the theoretical value of τ = 3.95 s.
Our theoretical calculation for the chip-to-ambient resistance was modeled as a sum of the resistance from the conduction in the chip (silicon) the contact resistance from the silicon to the aluminum, the resistance from the conduction in the base of the aluminum heat-sink, and the equivalent resistance of the convection from both the base top surface and the fins of the heat-sink. For the contact resistance, we used the top surface area of the chip, which was less than the heat-sink base area, and a contact resistance given in Appendix F on p. 7. For modeling the resistance of the heat-sink, we approximated the fins as cylinders because we were unable to find square geometry constants at the tested Reynolds numbers for use in calculating Nusselt number Nu using the Hilpert correlation [3]. Also, because our chip had fewer than 20 fins, we multiplied Nu by a correction factor C, as shown in Eq. 6. The electrical circuit diagram of the system can be seen in Appendix H on p. X (all physical constants and coefficients used are given in Appendix A on p. X). Reynolds number ReD was calculated using Eq. 7 below.
where uf,∞ is the maximum velocity between the fin cylinders. From Eq. 5 we were able to find conduction coefficients, the heat-sink total efficiency, and ultimately the heat-sink resistance. The resistance values are summarized in Appendix F on p. 7.
Using our data from Experiment 4, we used the divided ΔT by QH to solve for Rchip-to-ambient. In Fig. 1 we plot the Rchip-to-ambient vs. air velocity for both our theoretical calculations and measurements from Experiment 4 at chip voltages of 9.71, 14.55 and 19.41V. The experimental data follows the same trend as the theoretical data as well as the manufacturer's data for the resistance of the heat-sink alone [2]. For the range of tested velocities, the percent error of our experimental versus theoretical resistances never exceeded 12.5%. In our plot, uncertainty is due to error in measurements of current and voltage and to resolution and precision error in the anemometer readings.
Assuming fatal failure occurs at a chip temperature Tchip = 150 °C, we calculated maximum allowable chip heat exchange for our tested velocities using Eq. 6. The results are shown in Table 1. As expected, maximum allowable chip power QHmax increases with increasing air velocity.
(Eq. 6)
The alpha obtained from experiment 2 was more accurate than from experiment 1 because we used a proper 4-point probe setup. This decreased the parasitic resistance, therefore lowering the initial and measured resistance. More importantly, we obtained the data without damaging the chip. In experiment 1, we scratched the surface of the resistor of the chip we were measuring, sometimes so severely that the resistance would increase dramatically between different attempts at the same temperature. The large temperature range also contributed the larger discrepancies from the linear model we were attempting to validate. We found the best alpha value for the second experiment using the normalized slope of R vs. T as it produced the smallest error.
The theoretical time constant, τ = 3.95 s, was smaller than the experimental value, 4.62 ± 0.53 s, possibly because the ceramic volume used to calculate ceramic resistance was estimated as that beneath the chip. The experimental resistance was also higher than the theoretical one which could contribute to the larger experimental time constant. They were within 17% of each other, which was considered satisfactory. Our theoretical results imply a required time of ~ 20 s to reach 99% of steady state. We may not have waited this long during the experiment, which may have added error to our results.
We were able to develop a heat transfer model which could predict Rchip-to-ambient for a given air velocity with increasing accuracy as the velocity is increased. Our experimental and theoretical calculations for Rchip-to-ambient vs. V∞ are closely related quantitatively and in trend, showing the validity of our model. They also agree with the trend given by the manufacturer: as the air velocity is increased, so does the maximum allowable chip power. This is valid as increased velocities would increase heat dissipation from the heat sink. Also, it was not surprising that the experimental resistances were slightly higher than the theoretical calculations which could be manufacturing inconsistencies. It appears that the resistance of the total system was not dependent on the chip voltage, which makes sense physically.
The results of our experimental and theoretical calculations for the S1530 20W heat-sink have close agreement and we have calculated maximum heat transfers to prevent failure versus velocity (Table 2 on p. 4). For increased chip cooling, resistance can be lowered by increasing the fin length and/or spacing, increasing the profile area, or use of a higher conductivity fin material, such as copper or graphite [4]. Our studies show that there is a threshold length of 120 mm beyond which increasing fin length does not help (see Appendix L on p. 10). Enhanced convection from a faster or nearer fan or liquid cooling can also help. Other options include refrigeration/solid-state refrigeration if packaging space is not a limiting factor.

There has long been and interest in studying the mechanics of aortic rupture [1]. The biomechanical behavior of the human aorta has been interest of study primarily due to its susceptibility to atherosclerotic occlusive disease [2] and aneurysms [3, 4]. These are due in part to the weakening of the walls of the aorta. The stresses and strains in these walls are factors in the creation and development of cardiovascular diseases. Thus, it is important to know how to predict the stress and strain states of aortic walls to discover the progression of certain vascular pathologies. Since stress cannot be experimentally measured, constitutive models are used to calculate tissue stresses. It is important, therefore, to identify the appropriate constitutive models that will accurately describe the biomechanical response of the biological tissue.
To understand this complex problem, one must examine the biomechanical behavior of the human aorta. In the past, studies were done to record the biomechanical response of human aortic tissue to uniaxial loading conditions. However, recent studies have shown that the behavior of human aortic tissue is nonlinear elastic, thus data from uniaxial tensile testing is unsuitable for this application. For a nonlinear elastic material, the slope of the stress-strain curve changes with deformation, hence the instantaneous stiffness of the material changes with deformation. Most biological soft tissues become stiffer with increased deformation. For these reasons, there is a need for a better description of the biomechanical response of human aortic tissue under biaxial stress. Biaxial mechanical testing allows for the investigation of the nature of mechanical anisotropy [5].
Constitutive models serve a vital role in studying the role biomechanical behavior because they are a quantitative measure of a tissue function. The linear elastic models do not do a good job of characterizing aortic tissue because a) soft tissues undergo large deformation and b) the relationship between stress and strain for soft tissues is nonlinear. This means that the stiffness of a soft tissue will change with deformation, unlike a linear elastic model where the stiffness is constant as long as the material is in the elastic range. For soft tissues, like aortic tissue, there may be one or more strain energy functions that work for a given tissue. After relating the tissue structure to this function, one may derive a structure-function relationship, from which one can quantitatively analyze how alterations in tissue structure due to aging or disease affect its function [6]. In this paper, aortic tissue data from Dinesh Mohan's Failure Properties of Passive Human Aortic Tissue II was optimized to fit to the Mooney-Rivlin constitutive model and the Neo-Hookian constitutive model to see if either model was a good fit.
In Mohan's paper, the aortic tissues were modeled as nonlinear elastic materials and were tested biaxially as flat specimens. Mohan's circular samples were tested using a biaxial test apparatus (see Fig. 1). In these tests, the specimen was inflated using the hand operated valve and the deformation was monitored using a movie camera. All tests were run till complete rupture of the tissue. The data used in this paper were the tissue samples tested quasi-statically.
A strain energy function is a model describing a hyperelastic material. According to Hollister, unlike linear elastic materials, "there is no 'one' material strain energy function for a nonlinear elastic material" [6]. For soft tissues, there may be one or more strain energy functions that work for a given tissue. Two classic forms of the strain energy function are known as the Mooney-Rivlin model and the neo-Hookian model. These models are commonly applied to analyze the deformation of materials such as rubber or plastics and assume both incompressibility and isotropy. Carew concludes that for most practical purposes arteries should be assumed to be incompressible, homogenous, and isotropic [1,7]. My reasons for choosing the Mooney-Rivlin and the neo-Hookian model are based on two papers that claim that the aortic wall displays rubber-like qualities that suggest that the aortic wall material is essentially elastomeric [8, 9]. The neo-Hookian model is the simplest model to describe such rubber-like behavior. The Mooney-Rivlin model is more accurate because the free energy function depends on two invariants. Based on various plots by Macosko, I hypothesized that the Mooney-Rivlin would be the better fit [10].
Now that we have defined constitutive models, the next step is to fit these models to experimental data to define the model constants. Since constitutive models relate stress and strain, we need to know the approximate stress and strain state. From this stress and strain state we then fit the constants from the constitutive model. When the material behavior is nonlinear fitting constants for the constitutive model requires multiple tests to mathematically fit the constitutive model to the data. To fit constitutive data, the square of the difference between the experimental measurement of stress and that calculated by the constitutive model is minimized using a numerical algorithm [6]. The algorithm modifies constants within the chosen constitutive model to minimize the error. The result gives the constants of the constituent model that best fit the data.
Full derivations of the Cauchy stress states for the Mooney-Rivlin model and neo-Hookian model can be found in Appendix A and B respectively. I began the derivation with the strain energy functions for the Mooney-Rivlin model (Eq. 1) and neo-Hookian model (Eq. 2).
Note that the neo-Hookian model is an abridged version of the Mooney-Rivlin. The second constant is truncated from the neo-Hookian. Both derivations followed the same steps. From here, I derived the 2nd Piola-Kirchoff (PK) stress tensor, Sij, from the strain energy function, W, in terms of the right Cauchy deformation tensor, Cij. The 2nd PK is then used in the equation for the Cauchy stress equivalents. ij. For the Cauchy stress, I had to find the deformation gradient tensor, Fij by solving using the three principal stretch ratios ( 1, ,2, ,3). Since the specimen is thin, only S11, ,22, and ,33 are needed. Due to incompressibility, d33 is set to zero, this makes it possible to solve for the hydrostatic pressure, p. I substituted the 2nd PK equations and hydrostatic pressure into the 11 and a22 equations. After simplifying, I applied the incompressibility assumptions (Eq. 3).
This gave me further simplified equations in terms of the stretch ratios and ∂W⁄∂r. This worked out well, since both the Mooney-Rivlin and neo-Hookian models may be defined in terms of principal stretch ratios. After substituting the derivative of the strain energy function in terms of the extension ratio, into the Cauchy stress equations we obtain Equations 4 and 5 for the Mooney-Rivlin model, and Equations 6 and 7 for the neo-Hookian. Notice the similarities between the two sets of equations.
The final step is optimization of the sets of equations using the formula below (Eq. 8):
For the neo-Hookian model, I did not use a constraint for the fitting of the experimental data. However, for the Mooney-Rivlin model, the Baker-Ericksen inequality was used (Eq. 9).
A detailed derivation of the inequality can be found in Appendix A. After derivation of the model equations, the next step was writing the MATLAB code. I chose 0 as seed value, for the both the Mooney–Rivlin, which has two material parameters, and the neo-Hookian model, which has only one. The code written for the Mooney-Rivlin fitting and the neo-Hookian fitting can be found in Appendices C and D, respectively.
Below, is the aortic tissue data from Mohan's Failure Properties of Passive Human Aortic Tissue II fit to the proposed constitutive models. Figure 2 describes the tissue data fit to the Mooney-Rivlin model and Figure 3 describes the tissue data fit to the neo-Hookian model. The experimental data are depicted by the stars and the predictions of the models are shown as lines.
As hypothesized earlier, the Mooney-Rivlin is clearly the better fit of the two. The neo–Hookian model is insufficient for describing the nonlinear mechanical behavior of aortic tissue. When the neo-Hookian model was used to predict stress relaxation at extension ratios between 1.1 and 1.25, the ability of the model to accurately predict the stress response of the tissue was limited. The Mooney-Rivlin model was closer to the experimental data, but still overestimated the stress for several data points.
The overall fit of each model's predictions is evaluated from the calculated coefficients between experimental and theoretical data. The constants are not unique meaning two very different sets of constants can generate an excellent fit for the plot. The constants c1 and c2 equal-14.0292 and 28.4883 respectively in the Mooney-Rivlin strain energy function equation. The constant c1 equals 29.6073 in the neo-Hookian strain energy equation.
In Appendix E, the polynomial residuals have been plotted and the norm of the residuals has been calculated.
This paper evaluated the fit of passive human aortic tissue, based on experimental response, to the Mooney-Rivlin and neo-Hookian constitutive models. The biomechanical response of the soft tissue was modeled as a homogenous, isotropic, and incompressible nonlinear elastic material using the two different strain energy functions associated with the constitutive models. It was discovered that neither model was an excellent fit, though the Mooney-Rivlin was a much better fit than the neo-Hookian model. The constitutive models were designed to accurately fit elastomers, so it is understandable that biological tissue, which is not as homogeneous as an elastomer, will not fit perfectly.
Accurately modeling the biomechanical response of soft tissue remains a challenge due to unique properties such as nonlinearities in the material, large deformations, anisotropy, and viscoelasticity [11]. Vande Geest suggests that difficult to model human aortic tissue past the age of 30 [5]. The age of the specimen in Mohan's study was 60 years of age. This was one of the limitations encountered. Other studies have suggested that aortic tissue cannot be modeled as isotropic. Patel found determined from the response of canine aorta that the tissue is anisotropic [12]. Neither model can be reliably used for the mechanics of passive human aortic tissue. One suggestion for further study are the exponential-based models to describe the behavior of aortic tissue [13]. In some cases, they have proven to be more effective in describing tissue behavior than constitutive models. Other studies have been conducted with soft tissue using the Ogden model, Martins model, or Humphrey model. I would suggest using the former two with aortic tissue because of the number of constants. For optimization, the number of constants to fit influences the fit directly. The Ogden model has six coefficients for fitting, and the Martins model has four.
It is important to identify the appropriate constitutive model that will accurately describe the biomechanical behavior of the aortic tissue. Such information is useful because identifying relevant changes in the biomechanical response of the aortic tissue can lead to an early prediction of an aneurysm.

HVAC Training Centers USA trains technicians to properly install refrigeration and heating systems, many of which utilize the vapor compression cycle (VCC). Your training program tests technicians on their abilities to tune the operating characteristics of the systems to deliver the best coefficient of performance (COP) for cooling and heating from a number of choices using the VCC training carts, and you have had some complaints that this test is unfair due to the uncertainties of your testbed systems. Thus, you have solicited our company's help in investigating this claim. Specifically, you have asked us to find the COP of cooling and heating for a VCC cart that you provided us with under three different compressor frequencies (30Hz, 45Hz, and 60Hz). In addition, you requested that we document the thermodynamic cycle on a temperature-entropy diagram with uncertainties at each state (using SI units). Lastly, you have asked that we report any other considerations that are important in determining the unit's ability to heat or cool efficiently and recommend whether they should be included in your testing of the technicians. We have completed these tasks. The purpose of this report is to provide you with our findings, conclusions, recommendations, and supporting documentation.
We have found the COP values of cooling and heating for the compressor frequencies of 30Hz, 45Hz, and 59Hz. The COP values decrease with increasing compressor frequency (Table 1); hence, less work is needed for lower frequencies to pump a specific amount of heat into or out of the space to be heated or cooled than would be needed for higher frequencies. We have also provided temperature-entropy diagrams to show the thermodynamic cycle at each frequency (Fig. 3-5, p.4). We have found that the uncertainties of the system are small enough that each frequency has a distinct cycle and a unique COP value. Thus, we conclude that your test is fair and the complaints about the test have been unfounded. We also have also determined that the cooling and heating capacities are key considerations in determining the unit's ability to heat or cool efficiently, and suggest that you test your technicians on their ability to tune the unit to give an appropriate value of the cooling or heating capacity. The values of the cooling and heating capacities were calculated for each frequency, and were found to increase with increasing frequency (Table 1).
Your company provided us with a VCC cart (Hamden Engineering Corporation Refrigeration Trainer, Model H-CRT-1), as well as a computer controlled data acquisition system (Labview 8.2) for recording the thermodynamic data. The VCC cart had two choices for the evaporator and several choices for the throttling valve; we utilized evaporator 1 and the capillary for these components, respectively. Schematics of the VCC cart are shown in Fig. 1 and Fig. 2. The working fluid of the apparatus was R-134a.
The data of interest for our analysis are shown in Fig. 2, where T refers to temperature, P refers to pressure, and Q refers to volume flow rate. The subscript refers to the point at which the value is measured. All the data was measured directly with Labview. However, Labview recorded gauge pressures, and therefore, we had to add the room pressure to each recorded value in order to get absolute pressure. The data of interest, for each point in the vapor-compression cycle, can be seen in Fig. 2; this data was used to find both enthalpy and entropy values at each of the points 1 through 4.
To find the COP of the VCC system, we set the compressor frequency to the desired value and waited for the system to reach steady-state. Steady-state was achieved when the computer acquisition system showed all of the measurements of interest to be constant values (straight, horizontal lines), and also, when the fluid coming out of the condenser at point 3 (Fig. 1) was completely liquid (no bubbles observed in the sight glass). We then used the Labview 8.2 software to record the data. The data was recorded ten different times for each frequency to allow us to compute the uncertainties in the data. This procedure was performed at 30 Hz, 45 Hz, and 59 Hz. 59 Hz was used instead of the requested frequency of 60 Hz, because this was as high as the compressor frequency would go for the system that you provided us with.
In our analysis of the thermodynamic properties of the VCC cart, we made several assumptions about the system. We assumed that there was no additional heat transfer apart from the heat flux in and out shown on Fig. 1 (perfectly insulation), that there were no additional pressure drops (no leaks), and that there were three reversible steps, taking place from points 1 to 2, 2 to 3, and 4 to 1 (Fig. 1). Also, oil had been added to the refrigerant to lubricate the compressor; however, we assumed that the added oil did not change the thermodynamic properties of the refrigerant.
All experimental data was taken at a room temperature of 294 ± 1°K and a room pressure of 99.13 ± 0.05 kPa. Room temperature and pressure were measured three times each with a thermometer (resolution of 1° K) and a barometer (resolution of 0.01 kPa), respectively.
In addition, as R-134a in liquid form can cause blindness, we were especially careful to wear appropriate protective eye gear.
In this section we will give the coefficient of performance (COP) of the VCC system and document the thermodynamic cycle on a temperature-entropy diagram for compressor frequencies of 30Hz, 45Hz, and 59Hz. We will also discuss the heating and cooling capacities of the system and their importance in determining the system's ability to heat or cool efficiently. We will also discuss the uncertainties associated with the data.
Both the COP of cooling and the COP of heating were found to decrease with increasing compressor frequency; thus, pumping a specific quantity of heat out of or into the reservoir requires less work at a higher frequency than a lower frequency. The values for each compressor frequency (30Hz, 45Hz, and 59Hz) can be found in Table 2.
Table 2: Coefficient of Performance of Both Cooling and Heating Decreases with Increasing Compressor Frequency
The COP of cooling was calculated using Eq. 1, where all of the variables refer to properties of the refrigerant, R-134a. is the volume flow rate, ρ is the density at point 3, h1 is the enthalpy at point 1, h4 is the enthalpy at point 4, and is the power input to the compressor (Fig. 1, p. 2). and are the cooling and heating capacities of the unit, respectively. h1 was found from the thermodynamic tables of R-134a [2], using the temperature and absolute pressure at point 1 (Fig. 1, p. 2). h4 is known to be equal to h3 because enthalpy remains constant though the throttling valve [1], and h3 was found from the thermodynamic tables of R-134a [2], using the temperature and absolute pressure at point 3 (Fig. 1, p. 2).
The COP of heating was calculated in the same way as the COP of cooling; however, we now make use of Eq. 2 instead of Eq. 1. h2 is the enthalpy at point 2, and h3 is the enthalpy at point 3 (Fig. 1,
p. 2).
For the frequency of 30Hz, however, the COP of heating had to be computed in a different way, due to a malfunction of the thermocouple that measured T2. The thermocouple recorded the temperature of point 2 (at 30Hz) to be too low (the temperature was clearly incorrect because the fluid must be a super-heated vapor at this point, but the recorded temperature tells us that it is a saturated liquid). Therefore, we instead assumed that the entropy, s, remained constant from point 1 to 2 (Fig. 1, p. 2), as it would in an ideal cycle. We then found h2 using the thermodynamic tables of R-134a given this constant entropy, s, and pressure, P [2]. This h2 value was then used in Eq. 2 to give the COP of heating value found in Table 3, p. 2.
The uncertainties in the COP values are due primarily to the precision errors in , ρ, h2, h3, and .
The temperature-entropy diagrams for the compressor frequencies of 30Hz, 45Hz, and 59Hz were plotted, and can be seen, respectively, in Fig. 3 through Fig. 5. Each diagram, labeled with the 4 points of Fig. 1 (p.2), shows the expected trend for a vapor-compression cycle, where point 1 is a saturated vapor, point 2 is a super-heated vapor, point 3 is a saturated liquid, and point 4 is a saturated mixture [1].
To plot the thermodynamic cycle, we had to find the entropy values for points 1-4 (Fig. 1, p. 2). This was done by using the R-143a thermodynamic tables to find si (entropy at point i), using Ti and Pi at points 1-3 and using T4 and h4 to find s4 [2]. This was done for the compressor frequencies of 30Hz, 45Hz, and 59Hz (Fig. 3-5). However, for the frequency of 30Hz (as mentioned in the previous section) we found that point 2 was not a super-heated vapor as it should be for a VCC, but rather a saturated liquid; we attribute this discrepancy to a malfunctioning thermocouple that recorded the temperature to be too low. Instead of using this point, which is clearly inaccurate, we assumed that the entropy, s, remained constant from point 1 to 2, as it would in an ideal cycle. We then found the temperature using the thermodynamic tables of R-134a given this constant entropy, s, and pressure, P [2]. This new point is shown in Fig. 3 as point 2'.
In the temperature-entropy diagrams (Fig. 3-5), the uncertainty in the temperature arises primarily from precision error, and the uncertainty in the entropy is due primarily to the precision errors in the temperature and pressure.
The cycles of the VCC cart (Fig. 3-5) differ slightly from the ideal vapor-compression cycle. In an ideal vapor-compression cycle, pressure should remain constant from point 2 to 3 and from point 4 to 1, and the entropy should remain constant from point 1 to 2. Because these values are not held constant between said points, we can conclude that the processes of compression (1 to 2), heat rejection (2 to 3), and heat addition (3 to 4) are not reversible. This is because of energy losses due to friction and unwanted heat transfer; for example, the pipes were not perfectly insulated and inevitably exchanged heat with the ambient.
When determining the unit's ability to heat and cool efficiently, it is also important to consider the cooling and heating capacities of the VCC system. We have calculated the cooling capacity, , and the heating capacity, , for the three compressor frequencies using Eq. 1 and Eq. 2 (p. 3) and we can see that both the cooling capacity and the heating capacity increase as the compressor frequency increases(Table 3). This means that a higher frequency will be able to pump more heat into a room in a certain amount of time than a lower frequency will be able to.
The cooling and heating capacities are important to consider for a VCC system, because even if the operating characteristics of the system yield a large COP, the unit still won't be efficient unless the cooling or heating capacity is appropriate for the type and amount of space to be heated or cooled. If the cooling/heating capacity is too small, the system will not cool/heat the room or space adequately; however, if it is too large, the system will turn on and off too often causing the efficiency of the unit to decrease and energy bill to increase. Therefore, we suggest that you test your technicians in their ability to match the cooling/heating capacity with the necessary factors, for example, the size of room to be cooled or heated, the number of windows, and the number of people typically inside room.
Through our analysis of the VCC cart that you provided us with, we have concluded that the test you give your technicians is fair, and the complaints that you have received were unfounded. With all uncertainties accounted for, each frequency gives a unique thermodynamic cycle with a distinct COP. Our analysis shows that both the COP of cooling and the COP of heating decrease with increasing compressor frequency (see Table 1, p. 1), which means that less work is needed for lower frequencies to pump a specific amount of heat into or out of the space to be heated or cooled than would be needed for higher frequencies.
In addition, we have concluded that the heating and cooling capacities are key components in determining the unit's ability to heat or cool efficiently, because if the cooling/heating capacity is too small, the system will not cool/heat the room or space adequately, but if it is too large, the system will cycle on and off too often causing the efficiency of the unit to decrease and energy bill to increase. We have provided you with the values of the heating and cooling capacities for the three compressor frequencies of 30Hz, 45Hz, and 59Hz in Table 1 (p.1), and have found that both the heating and cooling capacity increase with increasing frequency. This means that a higher frequency will be able to put more heat into or take more heat out of a space than a lower frequency will in the same amount of time.
We recommend that you test your technicians not only their ability to tune the cart to give a good value of COP, but also on their ability to choose a good value of the cooling or heating capacity, based on what type of space is to be heated or cooled. This will give your technicians the knowledge to help their clients save on energy costs.

Piezoelectricity is a special property possessed by some materials that allows either an electrical signal to produce a material strain, or a material strain to produce an electrical signal; the two are reciprocal properties. The former property has been used to facilitate actuation of many devices. High frequency applications, such as ultra-sound medical devices, have seen a particularly important benefit from these materials due to their extraordinary signal response time. The latter property can be found in applications such as accelerometers, thermal sensors and dynamical signal sensors. However, this ability to convert a mechanical strain into an electrical signal can also be used to derive energy from vibrating systems for a potentially unlimited renewable power source. Because of this energy harvesting capability, several researchers have investigated aspects of this process. This includes determining the available energy from a system, mechanical modeling of the piezoelectric element, developing energy conversion circuits, and energy storage methods.
Traditional mechanically vibrated systems have been analyzed to determine how much energy can be harvested from them (Vujic 2002). However, significant interest has also been shown in determining the amount of energy that humans exert during daily life that could be parasitically harvested for use in wearable electronics. Starner (Starner 1996) has calculated that nearly 67 W of power is wasted through the process of walking. He also conjectures that .33 W of energy could be harvested by integrating piezoelectrics within the joints of clothing; here, the process of bending would cause strain, and thus, energy generation. Several investigators have conducted studies regarding the use of piezoelectrics within shoes to scavenge power. Shenk (Shenk 1999) has worked on developing a working prototype of a military boot equipped with a power harvesting bimorph composed of piezoelectric elements known as THUNDER; this boot was capable of producing approximately 80 mW at its peak and averaged 2 mW. Paradiso's (Paradiso 2001) work is closely tied to Shenk's and, using a PVDF piezoelectric element within a shoe, has shown the ability to generate a peak power of 20 mW with an average of 1 mW. This work has proven the ability of power harvesting systems to generate low amounts of power for electrical systems.
The use of piezoelectrics as sensors and actuators has provided the groundwork for the development of energy harvesting systems that utilize them. IEEE standards (1987) exist that provide details on the constitutive equations that relate the mechanical strain and stress to electrical displacement and field. Two important relations that incorporate these four properties are:
where S is strain, s is modulus of elasticity, g is a piezoelectric constant, and D is electrical displacement, E is electric field, T is stress, and g and β are piezoelectric constants. From this, (Michael J. Ramsay 2001) derived power equations for a piezoelectric element excited in its 33-direction, which means that the application of load coincides with the direction of poling, and in the 31-direciton, which means that the application of load is in a direction (the 1-direction) that is perpendicular to the direction of poling. These 33 and 31 power equations are shown in equations 3 and 4, respectively:
where, P is power, d is a piezoelectric constant, t is material thickness, b is material width, l is material length and f is the frequency of excitation. Mechanical models that describe energy harvesting systems can readily be developed based on these equations.
Energy conversion and storage within a medium is another important aspect of energy harvesting that has received detailed investigation from several researchers. Most power harvesting circuits use a rectifier that ensures that all electricity entering the circuitry is positive, thereby protecting the downstream components. Beyond this, detailed work has been completed that involves conditioning this power and utilizing it to transmit signals via RF transmitters after a threshold voltage has been generated (Paradiso 2001). Ottman and Lesieutre (Geffrey K. Ottman 2003) have developed a power harvesting circuit that is continuously optimized, through the use of an adaptive controller and a DC/DC converter, to provide the greatest amount of power possible to the storage medium. They claim that their harvesting circuit improves direct charging of a storage medium by nearly 400%. Researchers use either capacitors or chemical batteries as an energy storage medium. While capacitors can be effective in powering systems that only require intermittent power--- for instance, to transmit an information signal--- they lack the ability to store large amounts of power, and they have fast discharge rates, thus limiting their utility (H.A. Sodano 2004). Therefore, to power a greater variety of electronic devices, power storage in batteries is required and is being investigated by research groups (Henry A. Sodano 2004).
The basic science behind power harvesting has been developed and there are several applications where a renewable power source is required, or would drastically improve current technologies. For example, health monitoring systems for building structures could utilize power harvesting to transmit signals from embedded sensors to a central processor. Or, RF tags used to track the migration habits of endangered animals could be powered through energy harvesting means, thus eliminating the need to capture the animals for battery replacement, as is currently done (Sodano 2004). Beyond this, everyday systems also show the possibility of benefiting from power harvesting. Tires, on automobiles and bicycles, deform significantly during their rotation and operate at frequencies that make them prime candidates for energy harvesting in order to provide energy to low power systems. For example, safety lights for bicycles are already available, but they require batteries, and can be expensive, and bulky. By comparison, an integrated power harvesting system used to power safety lights could be integrated into the tire, thus reducing the bulkiness and weight associated with traditional batteries.
This paper describes a case study in which the ability to install a power harvesting system on a bicycle tire is proven. Theoretical calculations used to determine the output power of the bicycle are described along with a comparison to actual results. The electrical circuitry of this power harvesting system is described along with improvements to enhance its ability to convert and utilize the deformation energy of the tire.
In this section, a static model of the piezoelectric-element, its bonding layer, and the material of the tire will be established, and a method of predicting the output voltage of the piezoelectric-element will be discussed and modeled. The results of this model will later be used in a comparison with experimental results.
In order to calculate the voltage produced by the PVDF, the amount of strain encountered during a single wheel revolution, or the tire loading, must be known. This paper investigates the former scenario. It is assumed that the bicycle tire is completely flattened when in contact with the ground, and that it reverts to the natural curvature of the bicycle wheel when not in contact. It is also assumed that the neutral axis is at the centroid of the bending section, because the thickness of the tire, piezoelectric-element, and bonding layer is sufficiently small in comparison to the wheel curvature. It has been noted that if the radius of curvature is more than eight times the depth of the bending element, then the error encountered in making this assumption is less than 5% (Young 1989). In this case, the radius of the bicycle wheel is 570 times the thickness of the bending element, and standard beam theory can therefore be used to calculate the strain while maintaining model validity.
From (Brei 2004) the strain encountered from a piezoelectric sensor can be found from the following:
where is the distance from the piezoelectric-element to the neutral axis.
The second term on the right hand side of Equation 5 is simply the curvature of the bent shape of the piezoelectric-element, which in this case is the curvature of the bicycle wheel. Assuming that the wheel is a circle, in Cartesian coordinates, the vertical distance along the perimeter is related to the horizontal difference and radius by Equation 6:
where is the wheel radius.
In order to properly calculate the distance from the piezoelectric-element to the neutral axis, it is necessary to appreciate that there are essentially four layers bonded to one another: the tire substrate, the Kevlar reinforcing in the tire, the bonding layer, and the piezoelectric-element. Figure 1 shows these four layers, the neutral axis (or centroid), and the respective distances from the top edge of each element to the neutral axis (C1, C2, C3, and C4).
The distance from the top edge of all four layers to the neutral axis, C4, is also equal to . To find this distance, it is first noted that for each layer the distance from the individual central axis to the laminates neutral axis is as follows:
where
where ,E1, E2, E 3, and E4 are the Young's moduli for each layer, C4 can be solved, since everything on the right hand side of Equation 11 is known.
With the ability to calculate strain, the output voltage of the piezoelectric-element can now be calculated. From (Brei 2004) it has been shown that, assuming that there is no applied electric field, voltage is related to strain through Equation 13:
where C is capacitance, V is voltage, A is area, and e is a piezoelectric constant. The following relationships between the given constants are known:
where k2 is the electrical coupling factor, c is elastic stiffness and g is a piezoelectric constant.
Combining Equations 14 and 15 with Equation 13 the output voltage can be written in integral form:
The capacitive value,, from Equation 16 is:
where is a known constant for the permittivity of the PVDF, and , , and are the length, width, and thickness of the PVDF, respectively.
Integrating Equation 16 over a constant width while maintaining as constant, the voltage formulation becomes the same as for a cantilever beam:
In this case, the flattening of the bicycle tire can be viewed as two cantilever beams anchored at their abutting edges (Figure 2). This is true because of the inherent symmetry of the bending element.
Therefore, Equation 18 is evaluated over two sections of half the length of the flattened section of the tire. Equation 6 can now be differentiated and inserted into Equation 18. The resulting equation is the total voltage produced by the flattening of a section of the bicycle tire:
Table 1 shows the constants assumed in calculating the voltage produced by the PVDF.
In order to determine the voltage from Equation 19, it is necessary to find the length of the footprint (or the flat area in contact with the ground). Assuming that the weight of the bicycle rider is evenly distributed between the front and back wheels, the length of the footprint is as follows, assuming that the width (tread width) is known:
where F is the half the weight of the rider, P is the tire pressure, l is the length of the footprint, and b is the width. Assuming that the width and length of the PVDF are from Table 1, that the tire is inflated to 100 psi, and that the rider weight is 100 lbs, then the amount of voltage recovered by the energy harvesting unit is equal to 10.1 V.
With a working model developed, and a prediction for the output voltage in hand, the process of validating the model can now be discussed. Since the PVDF was prepared, mounted, and installed in the tire prior to model development, all of the values and conditions assumed in the model discussed in Section 2 were obtained either by direct measurement, outside sources, or by reasonable approximation. For example, all of the dimensions of the tire, Kevlar reinforcement, and PVDF were directly measured with digital calipers. The thickness of the bonding layer could not be measured, but it was assumed that it was approximately the same thickness as the PVDF since a very small amount of epoxy was applied over a relatively large area and great care was taken in making the layer as thin as possible. The material properties of each layer were found from outside sources (Graham 1992; Brei 2004).
A small piece of PVDF was prepared for this experiment by etching the electrode with acetone and then cutting the desired shape. This was done to eliminate the possibility of electrical arching. An IRC High Pressure Bicycle Tire was obtained for the experiment, and the bicycle wheel and inner-tube were removed. A small area was prepared on the inside Kevlar reinforcement of the tire, and the PVDF was then bonded to the Kevlar with 3M Scotch-Weld DP110 epoxy adhesive. It was determined from the 3M Company that this particular adhesive would work well in bonding the PVDF to the tires Kevlar inner lining.
Along with the PVDF bonded to the inner Kevlar lining, some foam spacers were inserted to protect the outside surface of the PVDF from marring by the inner-tube. This was done from experience as an initial trial was rendered inoperable after it was damaged by the inner-tube in direct contact with the PVDF. The bicycle inner-tube was then inserted into the tire, and the tire was remounted on the rim of the bicycle wheel. Finally, the tire was inflated to 100 psi, and left to set overnight.
In order to test the model developed in Section 2, the two leads from the piezoelectric-element within the bicycle wheel were attached to an oscilloscope as shown in Figure 3.
With the oscilloscope set to trigger-mode, the bicycle tire was loaded by applying a force by hand and shocking the area where the PVDF was located along the perimeter of the wheel. It was noted, that while peaks as high as 20 volts were noted when the wheel was heavily shocked, most of the peaks occurred in the range of 10 to volts. Figure 4 shows a typical peak resulting from our test method.
The testing of the bicycle wheel assembly showed that our model results of 10.1 Volts were not only on the same order of magnitude as predicted, but also fairly close to the theoretical voltage prediction. It is estimated that the tire was exposed to a shock force of roughly 50 pounds, but the necessary equipment to accurately produce this force on a repetitive basis was simply to expensive and difficult to obtain. However, from an order of magnitude standpoint, this model correlated with experimental data, and if this project were to be further pursued, the model could be used to optimize the amount of voltage produced by the PVDF patch.
The energy of a single impact cycle, including impact and recovery, was calculated by observing the voltage signal across a known resistor of 100K on an oscilloscope. .It was found that a single impact yielded two approximately equivalent but opposite peaks, one from initial impact, and one from recovery. This voltage would become entirely positive if it were passed through a rectifier circuit. These impacts were approximated as triangular waves. Energy was then calculated from the waveform by integrating the instantaneous power over time:
where represents triangular relationship between voltage and time (which is an approximation of the actual signal):
and resistance R=100 K
Analytically, the total energy from impact and recovery resolves to:
in this case the maximum voltage . and t = 8.5 ms, is the time duration of a single peak.
The total impact energy can then be calculated to be 9.9nJ. Then, assuming that 6 patches could be placed on a 187 diameter tire, a rider could produce 740 aJ over a 3 hr ride. Average power can be calculated by dividing the energy of an impact/recovery cycle by the total period of the cycle, here 35ms based on measured data. This yields an average power of 0.283 .W. The average current produced by deformation of the PVDF can be calculated from the average power and the sense resistance:
whereis the average power.
The average current for an impact cycle through a 100K resistor is then calculated to be 1.7 A. It should be noted that this average current is specific to the 100KI resistor, and it would change depending on different resistance values used. However, the power output will not change with difference resistor values.
Knowing that the PVDF patch is capable of delivering voltages on the order of 10 volts, the possibilities of harvesting and storing this electrical energy could next be considered. In this section, the work done to design a functioning product will be presented. First, a short discussion of the importance of properly sizing the PVDF will be given. Next, a voltage rectifier circuit will be presented followed by a discussion of the circuit's ability to charge a capacitor. Efforts to develop a circuit that could automatically discharge this capacitor will then be discussed. Lastly, several potential applications for the energy harvesting PVDF and circuit will be given.
During initial experimentation, it was found that a large strip of PVDF, deformed in a small area, would suffer from a significant capacitive effect -- the inactive PVDF stores charge generated by the active region, and therefore reduces the voltage present between the two electrodes. In response to these findings, both the modeling and experimental studies (section 2 and 3 of this report) were modified in order to reduce the size of the PVDF strip to dimensions more comparable to that of the tire's contact surface. As a result, voltages of around 10V were capable of being generated. In order to properly design an energy harvesting element, it is consequently crucial to size the PVDF to suit both the harvesting circuit, and the bicycle tire in which the piezoelectric-element is situated.
Since the output of the PVDF is an alternating signal, it was necessary to produce a voltage rectifier circuit that would make use of both the positive and negative pulses. Four diodes, as seen in Figure 5, comprise the rectifier and a capacitor is placed across the output leads.
By bouncing the bicycle tire on a hard floor, it was possible to charge a capacitor to a sufficient voltage to power an LED.
With a rectifying circuit built and successfully tested, various circuits, capable of automatically draining the capacitor through an LED once a given voltage was reached during the charging of the capacitor, were next investigated. This proved to be a more difficult task than anticipated, since the circuits sought had to be self-sufficient. For example, a promising circuit made use of a thyristor, which behaves like a transistor but remains "on" until there is zero current running through it after it is initially switched "on". In order for the thyristor to activate, its base lead was connected to the capacitor, since a thyristor will turn on when the base reaches a threshold voltage. Unfortunately, the base also draws a very small amount of current -- enough in this application to keep the capacitor from charging to the desired level.
Efforts in designing an autonomous circuit have been purely investigative, but a more thorough study, possibly involving an electronics expert, should be able to produce a working device. However, in this case such a study is beyond the scope of the project. Additionally, if a design did not require self-sufficiency, there are many externally powered electrical components such as comparators, which would be able to produce the needed switching function.
While this project and report are intended to primarily serve as a proof of concept, there still are several potential applications, should this work be used to create an actual working product. From this investigation, a capacitor was able to be charged by the repeated shocking of a bicycle tire. Once the capacitor was charged above 3 volts, an LED was placed across the capacitor leads and a flash of light was observed. Clearly this shows that an LED could be periodically flashed if a controlling circuit were built, and products relating to safety lighting could easily be developed and marketed.
It was thought that the output of the PVDF could be used to charge a small battery after passing through the rectifier circuit, but as seen in the energy calculations in section 3.4, the output is quite small. It is still possible, however to charge a very small battery, or to increase the overall energy output by adding more PVDF elements in the bicycle tire. There also low-powered electrical accessories that are powered by small batteries, such as cyclometers used to measure speed and trip distance. Most of these electrical systems are on only when the bicycle is in use, and if an electrical circuit were designed to trickle charge the small batteries powering the systems, the batteries would only need to be replaced every few years (assuming a NIMH battery with the capacity for 1,000 charges (Energizer)).
This paper has shown that it is indeed possible to harvest electrical energy with a PVDF in the specific application of a bicycle tire. The underlying physics of the application were discussed, and a model was developed to predict the output signal of the PVDF embedded in the bicycle tire (accounting for four different layers bonded together). A discussion of experimental results was then provided, and it was concluded that the model verifiably predicts output voltage to a reasonable degree of accuracy. It was then shown that it is possible to transfer electrical energy to a storage medium for later use. While measurements of this energy show that the output of the PVDF is too low to charge conventional batteries, there are small capacity batteries that can still be charged with this power input. For higher power applications, it is recommended that a different piezoelectric is used, since the findings of this report show that a PVDF would be inadequate. If certain applications require that a PVDF be used (i.e. large strains that might fracture a brittle piezoceramic) it is suggested that a laminated PVDF be tested in a similar manner as this report. Once the laminate is fully characterized, decisions regarding its role in an application can be made.

The operation of internal combustion engines at their current speed range is made possible by turbulence. If complete combustion of the fuel-air mixture depended solely on the laminar flame velocity of the fuel, a typical four-stroke engine running on iso-octane (a hydrocarbon that approximates gasoline) would have a maximum speed of approximately 400 rpm [1]. However, turbulence wrinkles the flame front and causes entrainment of the fuel-air mixture by the flame front, leading to flame fronts with larger surface areas. This leads to higher burning rates and the high power densities of internal combustion engines. Turbulence occurs as the fluid jet coming through the intake valve separates from the valve seat, producing shear layers with large velocity gradients [1]. It increases as the engine speed increases, allowing internal combustion engines to operate over a large range of loads and speeds.
Proper combustion in internal combustion engines may be ensured by controlling turbulence in the flow within engines. If turbulence causes the surface area of flame fronts to become too large, excessive heat conduction out of the reaction zone causes premature flame extinction. Also, if the flame kernel from the early stages of combustion dwells near the spark plug for too long, it may be extinguished too quickly due to excessive heat loss, resulting in incomplete combustion. However, if the flame kernel moves away from the region near the spark plug too quickly towards regions of high ignition energy or low temperature, such as the cylinder walls, it may be extinguished too. Thus, a mean fluid velocity between 3 m/s and 5 m/s is optimal [1]. In direct injection engines, turbulence also controls the degree of mixing of the fuel spray with the air in the cylinder, thus controlling the equivalence ratio stratification and flame development.
As turbulence is very sensitive to initial conditions, there can be great cycle-to-cycle variability in turbulence in engines, leading to incomplete combustion or even misfires, and variations in the engine power. Accounting for cycle-to-cycle variability leads to design compromises that reduce engine power at full load and fuel efficiency at part load [1]. Understanding turbulence allows for the minimization of the effects of cycle-to-cycle variation in turbulence. This can lead to higher engine power, better fuel economy, and reduced emissions.
Turbulence may be studied by examining the fluid velocity fields in internal combustion engines. Particle image velocimetry (PIV) is a technique that uses a laser light sheet to illuminate tracer particles in the fluid in a transparent quartz cylinder and captures the movement of these particles using high-speed cameras. These images are then used to construct instantaneous fluid velocity fields. The experiment discussed in this report used a high-speed PIV technique that employed 355 nm lasers and silicone oil tracer particles to capture instantaneous velocity data in a motored four-stroke, single-cylinder, four-valve gasoline stratified spark ignition direct injection engine during consecutive engine cycles at every other crank angle from 69 °BTDC to 25 °BTDC. The engine was run at a speed of 2000 rpm with an intake pressure of 95 kPa and an intake temperature of 45 °C. Previous high-speed PIV techniques used lasers in the green range of the spectrum (510 nm, 527 nm, or 528 nm). However, lasers emitting green light cannot be used to capture reliable velocity fields in a fired engine as soot luminosity would interfere with the laser light scattered by the tracer particles. Using 355 nm lasers effectively circumvents this problem [1].
Instantaneous velocity vector fields were obtained from the raw images of tracer particles using LaVision Davis 7.1, a commercial software package [1]. These velocity vector fields from consecutive cycles were organized according to crank angle. Gaussian filters of wavelengths varying from wavelengths close to the resolution of the velocity vector fields (1 mm) up to wavelengths close to the size of the vector fields were applied to them (33.1 mm). The resultant velocity vector fields illustrate the flow structure at different length scales in internal combustion engines. They were subtracted from each other to obtain band passes that had the same size in wave number space. For example, the velocity vectors resulting from the application of Gaussian filters of 1 mm wavelength (1 mm-1 wave number) and 16 mm wavelength (0.0625 mm-1 wave number) were subtracted from the velocity vectors resulting from the application of Gaussian filters of 1.00100 mm wavelength (0.999001 mm-1 wave number) and 16.3 mm wavelength (0.0613 mm-1 wave number), respectively, to obtain band passes, both with a size of 0.001 mm-1, illustrating the flow structure at wavelengths 1.0005 mm and 16.15 mm, respectively. The following figure is a velocity vector field illustrating the flow structure at a wavelength of 8.03226 mm at a crank angle of 37 °BTDC:
Note that the larger area at the top of the image with an absence of velocity vectors denotes space occupied by the spark plug.
Next, mass-specific kinetic energy was calculated for each set of band pass velocity vector fields and for the unfiltered velocity vector fields at every other crank angle using the following equation:
where,
The vector fields were divided into several smaller areas and the ensemble mean velocity and the rms of the velocity fluctuations about the ensemble mean were calculated for each of the smaller areas. Then, an ensemble average kinetic energy false-color image was constructed using the ensemble mean velocity and an ensemble turbulent kinetic energy false-color image was constructed using the rms of the velocity fluctuations about the mean. Examples of such images are shown in the figures below:
Figures 2 and 3 show a small rectangular area selected at the top right-hand corner of the ensemble average and ensemble turbulent kinetic energy false-color images. The spatial averages of the ensemble average and ensemble turbulent kinetic energies in this rectangular area are referred to as the average kinetic energy and the turbulent kinetic energy, respectively. If the spatial averaging was done over the entire ensemble average and ensemble turbulent kinetic energy false-color images, the larger amount of instantaneous velocity data available at crank angles further away from TDC would have had an effect upon the average and turbulent kinetic energies calculated. The average and turbulent kinetic energy values were plotted against crank angle and wave number to examine the variations in average and turbulent kinetic energies with time and length scale.
The following figures show the average kinetic energy and turbulent kinetic energy values plotted against crank angle and wave number.
Figure 4 shows that the average kinetic energy is highest for flow structures at higher length scales. This illustrates that as energy cascades from larger length scales to smaller length scales, some of it is dissipated [1]. However, the dip in the plots between approximately 55 °BTDC and 40 °BTDC is hard to explain, as the average kinetic energy of the fluid in an internal combustion engine depends directly on the piston speed which decreases in a sinusoidal fashion from about 90 °BTDC (depending on the piston eccentricity) to TDC.
Figure 6 again illustrates the energy cascade from larger to smaller length scales. It also shows the dip in average kinetic energy more clearly, with 27 °BTDC having the highest average kinetic energy and 47 °BTDC having the lowest average kinetic energy among the crank angles plotted.
Figures 5 and 7 lead to a most interesting conclusion: turbulent kinetic energy does not vary depending on crank angle. It is expected that turbulent kinetic energy would increase as the piston approaches TDC, compressing the fluid within the cylinder, and reach a peak, possibly, before decreasing as the dissipation rate increases. However, the turbulent kinetic energy was calculated using only the data available in a small rectangular portion of the velocity vector fields examined. This rectangular region may have been less representative of the larger velocity vector fields at crank angles further away from TDC. Also, the rectangular region under consideration was next to the spark plug, where a strong tumble flow generated by the initial intake valve events dominated the flow structure. Thus, the large initial turbulent kinetic energy associated with the tumble motion might overshadow any smaller changes in the turbulent kinetic energy that occur due to piston movement.
The analysis of the data from the experiment under discussion indicates that the average kinetic energy reaches a minimum between approximately 55 °BTDC and 40 °BTDC, and the turbulent kinetic energy does not vary with crank angle. Reanalysis of the experimental data by finding the spatial mean velocity and the rms of the velocity fluctuations about this spatial mean and using these values to calculate the ensemble average kinetic energy and the ensemble average of the turbulent kinetic energy may confirm these conclusions. Examining the average and kinetic energies associated with a different, possibly larger, rectangular portion of the velocity vector fields and comparing them to the values presented in this report might also provide more insight. Further experiments may be required to confirm and understand these results. Calculating dissipation rates from the experimental data under discussion may also provide valuable information.
In order to develop a more complete understanding of turbulence in internal combustion engines, the high-speed PIV technique should be combined with high-speed planar laser induced florescence (PLIF) of biacetyl to simultaneously image the flow structure and the equivalence ratio distribution in a spark ignition direct injection engine in order to investigate events occurring near ignition, such as the interaction of the plasma channel produced by the spark with the surrounding fluid flow. Data obtained from such experiments may also be used to obtain kinetic energy and dissipation rate spectra that enhance the current understanding of turbulence in engines and help validate CFD models used to design and develop internal combustion engines [1].

Grills and Grilles (GG) has recently had a number of grilles which cover fluorescent lights mysteriously fail. GG suspects the aluminum alloy used in the grilles is causing this failure, and has requested that our engineering team determine key material properties including the density, modulus, yield strength, ultimate tensile strength, elongation at fracture, and fracture toughness of samples provided to us by GG. We have successfully determined these material properties and compared them to a reference to determine if the alloy truly is 7075-T651 aluminum. The purpose of this report is to present our findings, conclusions, and supporting documentation.
Our team successfully determined the density, modulus, yield strength, ultimate tensile strength, and elongation at fracture of the samples GG gave us; by comparing them to reference values from Dowling [1], we have concluded the samples we received are not 7075-T651 aluminum. We attempted to calculate the fracture toughness of the samples, but none of the four tests performed satisfied conditions set forth by the ASTM E 399 [2] standard for fracture toughness testing. Calculated and reference values with their associated uncertainty for each material property are located in Table 1, below. A graphical representation of the stress versus strain behavior is available in Figure 1 on page 3, with modulus, yield strength, and ultimate tensile strength depicted. Even though we were unable to obtain the fracture toughness of the material, the stress intensity factors (KQ) we did calculate are shown in Table 1, below. Since these stress intensities are not representative of the fracture toughness of the material, they should not be used in engineering calculations.
To provide the requested material properties and determine if this 7075-T651 aluminum, we used the six test specimens provided by GG: a solid bar, a tensile specimen, and 4 pre-cracked specimens. Relevant dimensions of these specimens were measured with calipers; before taking any measurements, we ensured that the calipers (± 0.0005 in) were properly calibrated to zero when in the closed position. We then measured dimensions of interest: the length, width, and height of the solid bar, the diameter and length of the tensile specimen, and the thickness of each fracture specimen. The solid bar was also weighed on a digital scale, and its mass was recorded. The measurements of each length and the mass were repeated four times. We then performed a tensile test and four fracture tests. Data was collected during both of these using LabView 7.1 and plotted using Microsoft Excel 2003.
Tensile Test: The supplied tensile specimen was placed in the grips of the 4206 Instron tensile testing setup. A 45-90 strain gauge rosette was fixed to the specimen; the middle strain gauge was directly along the longitudinal axis of the specimen, and the other two were positioned 45º on either side of it. We then began loading the specimen with the Instron's crosshead set to move at 2.54 mm/min and continued to increase the load until the tensile specimen eventually failed after brief necking.
Fracture Test: There were four different thicknesses of fracture sample (B) provided: 0.1" (0.254 cm), 0.25" (0.635 cm), 0.3" (0.762 cm), and 0.4" (1.016 cm). Each specimen was provided with a single notch in the center, also known as a single edge notched beam (SENB), and a fatigue crack already initiated. We set up the fracture test according to the 399 standard [2] for plane-strain fracture toughness (K1c) testing; Figure 1, below, shows the standard's specification for fracture specimen setup. A displacement gauge was attached to the specimen to measure the width of the crack. An 8516 Instron machine was used to load the specimen. We increased the load on each fracture sample until the crack propagated through most of the part. Once the crack was 2-3 mm from the edge of the SENB, it was removed from the Instron and fracture was completed by applying force by hand.
The stress-strain behavior of the tensile specimen is summarized in Figure 2, on page 3. Towards the end of the tensile test, one of the three strain gauges in the rosette broke off of the specimen. Strain values past this point are unreliable, but force data is still valid.
The density of the test specimen was determined to be 2.8 ± 0.4 g/cm3; Dowling [1] reported 2.7 g/cm3, which does not agree with our value. The density of this material was determined by dividing mass of the provided solid bar by its volume.
The yield strength for the provided specimen was determined to be 546 ± 6 MPa; Dowling [1] reported 469 MPa, which does not agree with our value. Yield strength represents the stress value at which a material begins to plastically deform, typically measured as 0.2% permanent strain.
The ultimate tensile strength (UTS) of the given specimen was determined to be 590 ± 5 MPa; Dowling [1] reported 578 MPa, which does not agree with our value. UTS represents the maximum amount of stress that a material can withstand before failure occurs.
The elongation at fracture was determined to be 17 ± 1%; Dowling [1] reported 11%, which does not agree with our value. This was determined by dividing the displacement of the crosshead at fracture by the original length of the specimen, with crosshead displacement being 0.01440 ± 0.00003 m and original length being 0.084 ± 0.001 m.
We determined the elastic modulus of the test specimen to be 68 ± 2 GPa; Dowling [1] reported 71 GPa, which does not agree with our value. In the early stages of the tensile test, the linear elastic region was examined. We used Eq.1, below, to calculate nominal stress (σn) in terms of load (P) and original cross-sectional area (A0). The specimen had an original cross sectional area of 0.000129 ± 0.000001 m2. We found our calculated nominal stresses to be within ± 3% of their actual value. True stress was calculated from these nominal stresses. The true strain in the direction of tension was calculated using strain values reported by the strain gauge rosette. True stress was plotted as a function of true strain for the tensile test and the linear portion of it was analyzed. The elastic modulus was determined with the average slope of best-fit lines incorporating the uncertainty in our measurements.
While we were able to find KQ (stress intensity factor associated with critical load PQ) values for each individual fracture specimen, we could not find K1c (plane-strain fracture toughness) using the conditions imposed by the 399 standard [2] for K1c calculations. The values of KQ we found are 24 ± 8 MPa√m, 30 ± 2 MPa√m, 36 ± 3 MPa√m, and 36 ± 3 MPa√m for specimen thicknesses 0.1" (0.254 cm), 0.25" (0.635 cm), 0.3" (0.762 cm), and 0.4" (1.016 cm), respectively.
The 399 standard [2] is very specific about how a test for K1c should be performed. The fracture specimen needs to be set up and loaded as shown in Figure 1 on page 2. Load (P) versus crack displacement (a) data need to be analyzed to determine if a specimen fails in Type 1, Type 2, or Type 3 fracture. The critical parameter is the load to be used in KQ calculations (PQ), and it is determined differently for each type of fracture; details on how to determine the type of failure are available in the 399 standard [2]. Figure 4 on page 4 shows a sample PQ calculation for a 0.25" (0.635 cm) thick specimen.
The final parameter needed to calculate KQ is a geometric correction factor f(a/w); this parameter adjusts the KQ formula for a variety of different geometries. The formula associated with the geometry of our test specimens is shown as Eq.3, below.
After PQ is determined, KQ can be calculated using Eq.4, above. KQ increases with specimen width, as shown in Figure 3, above; if KQ is K1C, the expected trend is an increase in KQ with specimen width followed by a decrease with specimen width, and then a plateau whose value represents K1C. Calculated values of KQ need to satisfy the conditions listed in Eq.5, above, in order to be the plane-strain fracture toughness. These conditions are designed to check if the part yielded excessively before fracture. If excessive yielding occurs, KQ is not the same as K1c, and cannot be used in future fracture calculations. Our team determined none of our calculated KQ values to be K1c, and our calculated KQ values should not be used in fracture calculations.
Our team found values for density, elastic modulus, yield strength, ultimate tensile strength, and elongation at fracture of the samples GG provided, and since none of them agreed with reference values from Dowling [1] I concluded the specimens provided were not 7075-T651 aluminum. Our calculated material properties are displayed on Figure 2 on page 3. The elastic modulus was determined to be 68 ± 2 GPa, characterized by the slope of the linear-elastic region of the stress-strain curve. The yield strength is 546 ± 6 MPa and is shown where the curve diverges from the linear path, which indicates permanent deformation. The ultimate tensile strength of the specimen is 590 ± 5 MPa, and is the maximum stress attained in the experiment before the onset of necking. The elongation at fracture is 17 ± 1%. We were able to calculate KQ values for the four fracture specimens provided by GG, but they were determined to not be K1C. The standard specifies that the linear elastic fracture mechanics requirements (Eq.5 on page 4) be satisfied for KQ to be K1C, but they were not. I recommend that GG further investigate the material actually used to make the grilles for the fluorescent lights, and either remanufacture the grilles using 7075-T651 aluminum or analyze whether the material being different gives rise to a redesign of the grilles.

Our design group was asked to develop a conceptual structural model for either a mid-size or small vehicle. We have completed this task using a mid-size monocoque vehicle. The following report presents our target customer description, the structural interpretation of the customer's expectations, an in-depth analysis of some important structural requirements, and our recommendations for modification of our structure in subsequent design steps.
A retired elderly couple was the target customer of our mid-size vehicle. They have a secure income and are looking for a comfortable vehicle to travel between their primary home and winter home in Florida. Their target price range is approximately $30,000. Using this customer description, we decided that a mid-size luxury vehicle was the target concept. We generated some general vehicle qualities that may be important to our customer. Table 1 below shows our breakdown of these qualities, their importance with respect to our customer, and the structural portions of the vehicles that may affect the qualities.
We completed our rough design concepts based on packaging, first order analysis, and customer requirements. First, we introduce the general strategy behind our structure and how we achieved the desired characteristics. Then, we will provide general overlays of our structure onto the vehicle plan views and the sizing of major elements.
Our design sketch is shown in Figure 1 below. Please consider vehicle symmetry while viewing the sketch. We also summarized the structural strategy behind our design concept as shown in the figure as well.
Our design is based off a monocoque structure and attempts to satisfy the expectations of our target customer. The material choice for the components was steel because it is highly durable and fatigue resistant. This coincides with our customer requirements for the expectations of high durability for this vehicle, considering its $30,000 price tag. Thus, we concentrated on enhancing the robustness of traditional steel designs. The mass of our concept structure as shown in this report is estimated to be 448 kg. However, there are many minor structural components we did not consider and thus, this value should only be considered a rough estimation. In any case, we found this mass to be larger than we required and we discuss how to address this later.
We addressed the needs of comfortable ride by attempting to make the body extremely stiff with respect to any suspension that may be added. We included 4 under-floor cabin cross members which can serve as attachment points for the seats and other passenger compartment structures. The front 2 cabin cross members are much larger in comparison to the rear. They have relatively large section sizes to 1) support the longitudinal rails during front impact and 2) react side impact loads. The 2 rear cabin cross members are designed to react side impact loads. All the cross members add bending and torsional rigidity to stiffen the structure which makes it robust to noise and vibrations. This NVH robustness was considered an important expectation.
We also facilitated ingress/egress and step over height by lowering the height of the rocker and increasing its thickness. This lead to a relatively large rocker section, but it was a compromise we were willing to make to cater to our customer's needs. However, we feel that lowering the rocker may compromise side impact safety, as the rocker may be too low to contact the barrier. We also tried to make the roof rail and B-pillar as thin as possible to create an even larger ingress/egress space. Our goal was to seat the retired couple with as little bending over as possible -- just in case their bodies are as limber as they used to be when they were younger. We intended to design the cabin to be spacious by disallowing any structural intrusions into the passenger compartment and by slightly crowning the roof panel to allow for ample head room. The goal was to make the cabin feel more like a living room than the interior of a car, hence adding to the luxury aspect.
We considered cabin safety very seriously and designed for robust energy dissipation in case of front or side impact. The cabin cross members serve as added load paths for side impact crashes and thereby increase the safety of the vehicle in the event of a side impact. We included dual motor compartment crushable rails on each side of the vehicle to react frontal impacts. All 4 rails have crush initiators to facilitate crush during a frontal crash. The 2 longitudinal rails extend from the cabin's 2 front cross members. The 2 outer rails extend from the rocker section and help react loads with the upper structure. Since our rocker was very large, we decided to make use of it as an additional load path for crash loads because it was roughly the same size as our longitudinals. This design provides 4 load paths for a full frontal collision and at least 2 load paths for an offset collision. Our design also indirectly increases safety by providing the driver with an increased range of visibility made possible through a thin A-pillar and upper B-pillar.
For ease of viewing, please refer to the appendices listed in Table 2 for packaging overlay views and for the major structural element dimensions in conjunction with the following discussion. The overlays primarily show the beams and panels that we sized for this initial concept. Also, the drawings reflect actual scaled dimensions for all beams and panels.
We laid our underbody structure on the top plan view as seen in Appendix A. The scaled structure appears to fit within the plan view from a packaging perspective. However, we want to note that any routing under the floor pan may require holes in the cross members and this may pose a manufacturing problem. The possible sub-systems affected are exhaust, HVAC, and fuel systems. We want to make note of the continuous section size we utilized along the motor compartment rails, the longitudinal rails, and the cabin cross members. In Appendix C, notice that all 3 rails have the same cross section but different thicknesses. This design facilitates a smooth load path along the entire perimeter of each section, which is advantageous in crash and other loading situations. This is another way we designed durability and robustness into the structure.
We also laid our beam structure on the side plan view as seen in the first figure in Appendix B. Again, we observe that the structure does fit with regards to packaging. However, it seems there may be a packaging problem with the mid-rail extending from the rocker. It may interfere with the shock tower and/or wheel hub. Additionally, we point out that the rocker is located on the low side of the suggested rocker position of the plan view. This was the result of a direct attempt to lower step-over height for easy ingress/egress for our customer. The relatively thinner A and B-Pillars show that visibility can be increased within the package constraints.
The second figure in Appendix C shows the major cabin panels (roof, dash, seat back, and floor) and their locations in the package. These appear to fit within the package and are relatively thin compared to the rest of the structure. Their final shape is highly dependent on routings, holes, curves, and manufacturing capability but we approximated them as flat panels for our initial assessment.
Thus, our final design sketch and concept package seem to fulfill many of our customer requirements and fit into the package space. A detailed analysis of how we arrived at our design requirements, section sizes, and final mass follows.
Prior to creating a structure, we performed a mass analysis to approximately determine the total vehicle mass. First, we determined the plan view area of the target vehicle, 9.42 m2. Then, we used a historical graph to estimate the nominal total weight of our vehicle. This graph of Vehicle Mass to Plan-View Area can be found in Appendix D on page 19. We determined the nominal mass to be 1620kg. Next, we split the vehicle into major subsystems and used typical subsystem masses for mid-size vehicles to estimate each subsystem's contribution to the nominal mass. However, we tailored the vehicle's mass to our target customer. This was done using a spreadsheet which incorporated subsystem mass iterations. When we changed the mass of one subsystem, these iterations helped adjust the masses of the other subsystems using historical data to calculate the interaction between subsystems' masses. In Table 2 on the following page, we outline our initial mass estimates for the subsystems, show the reasons for changing the mass of a particular subsystem, and provide the new subsystem's mass after interaction effects are calculated.
After finding our concept's final mass estimate, we determined typical structural requirements for the vehicle. These included the vehicle's bending moment diagrams, bending strength, bending stiffness, twist ditch torque, torsional stiffness, front and side impact parameters, and roof crush. For many of these requirements we mention a test mass, which we estimated as 2035kg. We discuss the development of the requirements below.
We determined the vehicle bending moment diagrams from a free body diagram of the vehicle as a simply supported beam with its typical loads at the gross vehicle mass condition. This free body diagram is shown in Appendix E, on pages 20 and 21. We included the dynamic bending moment diagram, assumed to be twice that of the static diagram. In addition, we considered front and rear towing conditions. All these bending moment diagrams are shown in Figure 2 below.
We determined that our structure should be strong enough to withstand an 11,000N force in an H-point bending test without permanent deformation. We arrived at this value by considering the maximum bending moment from Figure 2. Modeling the vehicle as a simply supported beam, we determined that an 11,000N force applied equidistant from the front and rear suspensions would yield an equivalent maximum moment of approximately 7,100,000 N-mm when it is the only force acting on the beam.
We determined the required bending stiffness of the vehicle to be 7700 N/mm. We derived this from a first order estimation of bending frequency. Historically, the desirable range for primary vehicle modes is 22-25 Hz. We decided to design our structure to a 23 Hz primary bending mode. We used Equation 1 below to determine the necessary bending stiffness, k.
The other variables are as follows: fn is the bending frequency, l is the wheelbase, L is the overall length of the vehicle, and M is the rigidly mounted mass, which we estimated to be approximately 40% of the curb mass. Hence, we set these bending requirements as our targets.
We determined the twist ditch torque requirement to be 8,200,000 N-mm. This was determined using Equation 2 on the following page, where t is the track of our vehicle (1560mm) and WAxle is the maximum value of the front or rear suspension reactions. In our case the front suspension was the largest reaction force at 10,500N. The free body diagram of Appendix F on page 22 helped us find these reactions.
We determined the torsional stiffness requirement of the vehicle to be 14,000 N-m/°. We arrived at this value by examining the torsional stiffnesses of benchmarked vehicles. The nominal value for a typical mid-size sedan was approximately 12,000 N-m/°. However, we want our vehicle's structure to be stiffer so that it is more robust to noise and vibrations. Thus, we decided to pursue a higher torsional stiffness.
We determined the amount of available crush space to be 550mm in the case of a frontal impact. We used the plan view of the vehicle to get a rough estimate of this value. Please refer to Appendix G on page 23 for the calculation and the plan view.
Additionally, we required the average front barrier crush force to be 335,000N. Using judgment and target retired couple, we determined that the deceleration during a crash must be better than most typical vehicles, which range from 20g to 30g. The deceleration must also be low enough to compensate for the anatomy of an older couple whose bodies are not as robust to acceleration as they once were. Thus, we required a maximum crash deceleration of 23g during the mandated FMVSS 208 30mph frontal crash. Using Equation 3 below, we determined that the structural efficiency needed to attain a 23g deceleration is 73%.
Here, Δ is the available crush space, η is the crush efficiency, v is the velocity prior to crash, and amax is the maximum deceleration desired. We believe this is practical because historically, structures range from 65-80% efficient in 30mph frontal crashes. Finally, the relationship defining the crush efficiency is shown in Equation 4 below, where Favg is the average crush barrier force, and Fmax is the maximum crush barrier force.
For side barrier collisions, we require that crush load for the vehicle side is approximately 200,000N. We know historically and from first order analysis that a large side crush load will decelerate the barrier quicker, which will result in lesser acceleration being imparted to the occupant. Also, the available side crush available in our package is about 300mm. There is 120mm from the outer portion of the door to the inner portion, and there is 180mm from the inner door to the occupant. We used a spreadsheet that utilized a first order analysis of side impact to determine the effects of the crush space on the occupant and to determine whether a package change may be necessary. Please refer to Appendix E for a detailed analysis which utilizes the FMVSS 214 standard for side crash.
In short, we found that at the current dimensions of the side of the vehicle and with a 200000N side crush force, the impact would result in about 57g being imparted to the occupant. If we simply flip the door dimension to 180mm and instead have the distance to the occupant as 120mm, then the impact would impart 38g to the occupant. This is a reduction of 33% by simply reducing shoulder room by 6cm. Although interior room may be diminished, the survivability chances of the occupant are much higher. Thus, a package change should be considered.
We determined that the roof must not deflect more than 5 inches under a 30,000N static load. This was determined by FMVSS 216 which states that the static load applied during the roof crush test is 1.5 times the vehicle weight. During the test, the roof must not deform more than 5 inches. Thus, our roof crush load was determined to be 30,000N using the test mass of our vehicle.
This section aims to describe how we achieved the final sizes of the major structures shown previously. These include the side frame, the cabin shear resistant panels, motor compartment crushable rails, under-floor longitudinal rails, and cabin cross members. We will summarily describe the steps we took and the first-order methods we implemented to determine the dimensions for these major structures. We begin with the side frame.
We determined the side frame beam dimensions mainly from the bending stiffness requirement and also from our customer description. Historically, the bending stiffness requirement is one of the harder requirements to meet. First, we analyzed the plan views of the vehicle and roughly determined the maximum allowable dimension for each of the 8 identifiable side frame beams: the A-pillar, hinge pillar, lower B-pillar, upper B-pillar, lower C-pillar, upper C-pillar, roof rail, and rocker. Next, we obtained a simple first order FEA modeler to perform an H-point bending test analysis. The modeler assumes rectangular sections and our dimensions are reported as such. Using the modeler, we applied the 11,000N load from our bending strength requirement to the lower middle of the side frame and began our analysis. We estimated the bending stiffness as the ratio of the 11,000N load to the vertical deflection of the application point.
We began the analysis by assuming typical joint stiffness values. In the first run, we included all the beams at their largest perimeter dimensions to determine the maximum bending stiffness attainable from our side frame at worst case for packaging. Having surpassed the requirement of 7700N/mm with this first run, we began to reduce the dimensions of the beams. Our first priority was to ensure that our target customers would have ample room for ingress and egress. Thus, we attempted to reduce the section heights of the rocker, the upper and lower B-pillars, the hinge pillar, the A-pillar and the roof rail. This would enlarge the driver and passenger openings in the side frame to facilitate entry and exit. As a bonus, the thinner A and B-pillars would also help increase visibility. However, at the same time, we monitored the beams with the highest strain energy -- the rocker and roof rail. We tried to change them as well since they would affect the overall stiffness the most, but we still wanted to facilitate easy ingress/egress. We reduced the section sizes and altering thicknesses until we met our bending requirement. A screenshot of our final side frame from the FEA modeler is shown in Appendix H on page 24. Please see Appendix C on page 18 for dimensions of each individual beam and its relative sizing.
Our final concept side frame has a total bending stiffness of 7750 N/mm according to our FEA modeler. Its total weight is 83.4kg and the deflection at the loading point was 2.84mm. Thus, we can make the observation that the structure meets the bending stiffness requirement. Additionally, the maximum observed stress in the structure was 95MPa in the roof rail. Recalling the yield strength of typical automotive steel as 207 MPa, we note that there was no yielding in the structure. Also, the width-to-thickness ratio, or "b/t," for the roof is less than 60, and therefore, we can conclude that it will fail by yielding, not buckling. Thus, we can conclude that the structure met the bending strength requirement as well.
We performed a torsion strength and stiffness analysis on the entire cabin. The cabin included both side frames, the seat back, the dash panel, the floor, the roof, the windshield frame, and the backlite frame. Our initial assumptions were that the shear resistant panels of the cabin (floor, dash, roof, and seat back) were all 1mm thick, flat, steel panels.
We determined that the 1mm shear resistant panels buckled under the twist ditch torque and resizing was necessary. Applying the twist ditch torque to the dash panel, we performed an in-depth load path analysis on the cabin structure to get the shear loads on each edge. Please refer to Appendix I on page 25 for specific details. This allowed us to analyze each of the 4 flat panels in shear. We noted that at 1mm, the width-to-thickness ratio "b/t" was greater than 60 for each panel. Thus, they would fail by buckling rather than yield. Therefore, we determined the critical buckling stress (σcr,Shear) and load (Fcr,Shear) for each of the 4 panels. Assuming the panels were simply supported, the following Equations 5, 6, and 7 applied.
Here, E is the elastic modulus, t is the thickness, μ is Poisson's ratio, b is the short dimension of the panel, K is the boundary condition factor, and a is the long dimension of the panel. Thus, if the shear loads on any side of the panels exceed Fcr,Shear, then the panel will buckle.
As previously stated, we determined that all 4 of the 1mm panels failed by buckling. Therefore, we iterated through the previous 3 equations varying the thickness until Fcr,Shear was less than the shear load found through the load path analysis. Please refer to Appendix I on page 26 for specific calculated values. The final panel thicknesses were: 1.4mm for the seat back and dash panel, 1.5mm for the roof, and 1.75mm for the floor.
In addition, we observed that the maximum stress in the side frame under the twist ditch torque was 115 MPa in the upper B-pillar. This stress is less than yield strength of steel and since the b/t value for this pillar is less than 60, we can conclude that it will fail by yielding, not buckling. Thus, the side frame met twist ditch/torsion strength requirement.
We estimated the torsional stiffness of the cabin to be 9,000 N-m/° without the front and rear windshields installed. We determined this by considering the twist ditch torque and the entire cabin in torsion once again. We utilized a spreadsheet to find the torsion stiffness whose basis was a first order equation for cabin stiffness shown below as Equation 8.
Here, q is the shear flow on an edge, T is the twist ditch torque, A is the area, G is the shear modulus, and t is the thickness of the panel, and i is the number of panels. Our load path analysis and the cabin itself told us everything except the effective shear stiffness (Gt)eff.
For the shear resistant flat panels, we determined the (Gt)eff by using the shear modulus and the panel thickness. However, the (Gt)eff of the windshield frames and the side frame had to be determined separately. We found (Gt)eff of the windshield and backlite frames by analyzing the deflection of a flexible frame under a shear force. We assumed small angles and equal angular deflections for all joints. We utilized the relevant joint stiffness used in our side frame FEA modeler. Thus, equating strain energy to work done by the shearing force, we developed Equation 9, which we used to calculate the effective shear stiffness of the windshield and backlite frames.
Here, Kjoint is the joint stiffness, A is the flexible panel area, and i is the number of the joint. Thus, we determined from our model that (Gt)eff of the windshield frame was 380 N/mm and that of the backlight was 252 N/mm. Finally, we found the (Gt)eff of the side frame by using the FEA modeler mentioned earlier to determine the fore-aft deflection caused by the shear force on the side frame. We determined the (Gt)eff of the side frame to be 244 N/mm.
Additionally, we determined the torsion stiffness of the cabin with the windshield and backlite glass installed. Assuming the glass to be rigid and the frame to be very flexible, we used the typical (Gt)eff of the adhesive to determine that the effective stiffness of the windshield and backlite frames was about 4500N/mm. Thus, the total cabin stiffness became 11,700N. We note that in either case, the structure does not meet our 14,000 N-m/° torsion stiffness requirement. We will address this further in the Recommendations section on page 14.
We sized the 2 longitudinal rails using a limit analysis with plastic joints, the maximum expected barrier crush force, and an assumption of square, steel sections. To clarify, we only consider the rails which extend from the cabin cross members, not the secondary rails extending from the rocker. We used a first order limit analysis on the rails to determine their size. Below, Figure 3 shows the equations and a brief sketch of the analysis. We assumed that joints 1 and 2 had equivalent fully plastic moments, Mp. Also, we assumed small translational and rotational deflections. In the following equations, the angle θ was taken from our side package overlay as 55º, L1 was 300mm, and L2 was 200mm.
Typically, 50% of the crash force is absorbed through the lower-middle structure. Our structure included 2 longitudinal rails and therefore, we needed each rail to react at least 25% of the maximum expected barrier force. The max crush force expected is approximately 460,000N during a 23g crash at our test mass of 2035kg. Thus, each rail needed to react 115,000N. Each rail was determined to be a steel section that is 100mm by 100mm and 4mm thick. The b/t ratio for these sections is 25 which indicates that they will fail by yielding.
We determined the size of our 4 crushable rails from the plan view, the average front barrier crush force, and empirical equations for axial crush of steel, square sections. Using judgment, we decided to use square sections of 100mm because this dimension would flow smoothly from the longitudinal rails thereby creating a smooth and robust load path. Typically, 50% of the average crush force is directed through the lower middle structure and 20% is directed through the upper structure. Thus, we sized the 2 lower rails to crush at 25% of the 335,000N load, which is 93750N. We sized the 2 upper rails to crush as 10% of the load, 35,000N. The empirical relations for steel square sections, Equations 9 and 10, were provided by SAE.
We set Pmax = 93750N and 35,000N, respectively, to indicate crippling and the formation of the first fold at this force. For the lower rails, we determined that a square section should be 100mm per side and 1.5mm thick. For the upper rails, we determined that a square section should be 60mm per side and 0.92mm thick. The b/t value for both sets of rails is greater than 60 which constitutes failure by buckling. This is the desirable failure mode for these crushable members. We could further initiate crush of these members by adding large radius ribs on the sides that act as crowned panels, which buckle more easily under loading.
We want to note that the sizing and packaging of the upper crushable rails must be checked so that they do not interfere with the shock tower or any other subsystem. The next design team may consider removing them, but we urge them to find an alternate method for enhanced energy absorption, as the safety of the occupant is of high importance.
Next, we sized the 4 cabin cross members under the floor. To do this, we considered only the worst case loading condition for one of the cross members. This worst case is shown in Figure 4. The front 2 cross members will be loaded in a front collision or a side collision. So we sized these beams for both impacts and chose the more conservative as the final set of dimensions for each cross member. We sized the rear 2 cross members for side crash only. To size the front 2, we iterated through first order buckling and yield. We recall the longitudinal rails react 115,000N during front impact. However, there are 2 cross members that can share this load from each longitudinal rail. So thus, only 57,500N would be reacted by one cross member. Also, the side impact should have a strength of 200,000N per our requirement. During this analysis, we only consider 100mm square sections because we want to fit within the package, and more importantly, the sections would flow smoothly from the 100mm square longitudinal sections to create a smooth load path.
The maximum moment exerted on the beam during frontal impact is about 22,000,000N-mm. Thus, we use Equation 11 to calculate the bending stress in the beam and also Equation 12 to consider buckling.
For Equation 11, σ is the bending stress, M is the moment applied to the beam, y is the maximum distance from the neutral axis, and I is the moment of inertia. The parameters of Equation 12 have been described earlier. Iterations through first order beam analysis, lead us to dimensions of 100mm by 100mm and 18mm thick to react the frontal crash condition. With these dimensions, the largest stress in the beam is 194MPa -- almost the yield of steel at 207MPa. Obviously, the b/t ratio for these beams is small and they will fail by yield. This strength will help ensure the passenger compartment remains relatively un-deformed during a frontal crash.
Considering side impact, we utilized AISI CARS 2005: Geometrical Analysis of Sections program provided by AISI. We performed an axial capacity trend analysis on a 100mm by 100mm section. The results are in the following plot in Figure 5. We note that a thickness of 2.2mm can react approximately 200000N. Thus, the rear 2 cabin cross members should be 100mm by 100mm and 2.2mm thick.
We observe that the frontal crash condition is far more detrimental to the beam that side impact because it is transversely loaded. However, the main function of the front 2 cross members is to keep the longitudinal rail rigid as it plastically deforms to protect the cabin. Thus, the stress in the beam is much higher in than in the axially loaded case.
After sizing the major structural members, we need to acknowledge manufacturing considerations with respect to our sections. The generic sections shown in Appendix C on page 18 are very generic and do not consider manufacturing. For example, none of the thin sections shown in Appendix C have any flanges for welding. So for instance, the motor compartment crushable rails could take on a somewhat different form, as in Figure 5 on the next page. The flanges may add enough length or width that the member may interfere with the packaging and or the aesthetics of the structure. Also, the positioning of the flanges affects the strength of the section as well.
There are ample other examples like this. However, we can also increase strength and stiffness of any of these sections if they are manufactured properly. For example, we can increase the plastic moment of the longitudinal rail by carefully welding another section within it. This would allow it to react a higher crash load we than expected. Thus, we could introduce higher safety factors into our section designs by reinforcing our sections. In Appendix J on page 27, we readjust some of our sections to account for packaging, manufacturing considerations, strength, or aesthetics.
We estimated the final mass of our structure to be 448kg. After finding all the section sizes, we estimated the total mass of the sized structures and then we simply estimated the masses of the structures that we did not explicitly size. Hence, we could compare it to our preliminary mass analysis. This allowed us to make design recommendations based on the any remaining mass available and based on any lack of robustness in the structure. Thus, having found all the section sizes for the beams, we determined their lengths by analyzing our plan views. Below, Table 3 lists the estimated masses of all the structures in our concepts.
The estimated total mass of our structure is about 448kg. This is well above the 418.7kg we approximated for the structural mass during our preliminary mass analysis. Thus, we need to make some changes and improvements to more efficiently make use of the mass. Primarily, we need to focus on reducing the mass of the front cabin cross members which account for 37% of our current structures mass. With this in mind, we pass the following recommendations along to the detail design team.
The structure we analyzed was a very simple one and there are many areas for improvement. For example, the current structure is not robust enough in torsion according to our first order model. We recommend testing be performed to obtain a more accurate torsion model. We were unable to model the effects of our cross body torsion members using first order analysis. High torsional rigidly is a high priority to stiffen the body and also to make it robust to vibrations.
Also, we recommend looking into using doublers or bulkheads for the high stress or high cycle joints. This would not only enhance the rigidity of the structure by increasing joint efficiency, but also it was increase the durability of the high stress and high cycle joints. Also, adding ribs to sides of beams and joints subject to buckling would help inhibit buckling by reducing the effective width of the section.
We recommend adding crush initiators to the front crush members. This will help initiate crush to dissipate crash energy and reduce the maximum deceleration of the vehicle resulting in less injury to the occupants. Along the same lines, the packaging around our upper crush members must be verified to ensure proper clearance from the shock tower and feasibility of design. The packaging of the secondary mid-rail extending from the rocker must also be verified to make sure it clears the wheel well.
Also, our sections are very generic as rectangles and squares. The finer details of styling the sections should be carried out, and then, the strength and stiffness of the sections should be reevaluated to maintain structural rigidity and integrity. We also recommend reinforcing the larger sections such as the rocker and mid-rails. This can be done by welding smaller sections within them to increase the stiffness and strength.
We feel that the rear of our structure is not as structurally robust as the front. Therefore, we recommend the addition of cross members or other reinforcing structures to stiffen the rear of the body. Also, the addition of rear crush members for rear impact should be considered as well.
Finally, the mass of our structure exceeds the requirements set forth in our preliminary mass analysis. This is mainly because our front cross members must react large crash forces in their transverse directions. We recommending investigating other possible load paths to lessen the transverse load applied to those beams. Thus, their size and mass can be significantly reduced.

Quantum dots are nanometer-sized particles or islands of a semiconductor material embedded in another semiconductor material. They have electronic properties different from that of the bulk due to quantum confinement, and thus hold a promise for nanotechnology applications such as LED's, detectors, data memory devices, lasers, and single electron transistors. However, fabricating a regular, perfectly aligned dot structure is still a challenging issue. Rather than using an accurate positioning device, such as a focused ion beam, it is preferable to use the technique of self-organizing/assembling growth via a strain relaxation mechanism. This is done by depositing the vapor of a material onto a substrate made of a material with a different lattice parameter. It is found experimentally in many material systems that heteroepitaxial growth results in spontaneous self-organization and assembly of islands. [1,2]
The driving force for the quantum dot formation is the reduction of the total energy with a contribution from the elastic strain energy. The elastic strain arises by the lattice misfit between the film and the substrate material. Initially, the combination of the surface/interfacial energies and the strain energy are such that the system favors wetting. Therefore, the film material forms a flat wetting layer over the substrate. As the film material is deposited, the thickness of the film increases and the strain relaxation mechanism will result in island formation, despite an increase in the surface energy. At the early stage of surface roughening, the material is more relaxed at the crest than at the valley. In other words, the area at the crest has a lower chemical potential. Consequently, the film material diffuses from the valley to the crest, which eventually leads to island formation.[3]
In this project, we examine the formation of quantum dots by employing the finite element method formulated by Ref. 4, 5.
Based on method developed by Ref. 4 and Ref. 5 and Ref. 6, we used the model for surface diffusion and evaporation/condensation. The weak statement can be written as
where J is the mass flux, I is the mass displacement, M is the mobility of atom on the surface, j is the volume of matter added per unit area of the solid surface per unit time, i is the volume of matter added per unit area and m is the evaporation-condensation rate, and δ G is the energy change due to matter relocation and exchange on the surface. The first term in the integral associates with surface diffusion and the second term associates with the condensation/evaporation process.
The quantity δ G can be obtained from the free energy consideration. Here we consider a) the surface energy, γ s , b) the elastic energy, w, due to a misfit between the film and the substrate, and c) free energy per unit volume of atoms in a bulk solid phase, g, to model deposition from the vapor phase. The expression
where l is the length of the surface, δ rn is the normal displacement of the surface.
Fig. 1 shows one element, with two nodes at the positions (x1 target="fn1"/>, y1) and (x2, y2). The element has length 1 and slope θ , which relate to the nodal positions by the expression
with the linear interpolation coefficients being
Also for velocity,
However, for flux,
where J1, J2 and Jm, are the fluxes at the two nodes and the mid-point of the element, respectively, we used the quadratic interpolation coefficients,
(7)
Writing the position and mass displacement in a column matrix δ q and the velocity and mass flux in a column matrix q'.
We can write the integral Eq. 1 as
where
where we have used the shorthand notation
Since we need velocities for more than one element, we have to generate global H matrix, which is diagonal and includes matrix for each element. And it also has overlapped part for the same node of two adjacent elements. Here is the part of MATLAB code for global matrix.
The equation (2) can be express in term of force on the element
where
Equating Eq. 10 and Eq. 12 yields
This equation is a set of linear algebraic equation for the generalized velocities. We use Matlab to solve for the velocity matrix. Note that taking an inverse of H is not possible since H is close to being a singular matrix. One way of suppressing the singularity is to add a small mass to the diagonal term of H matrix. In our program we use the command "psudoinverse", which also suppresses the singularity.
Once solved, the nodal velocities can be used to update the nodal positions. Here we use Euler method for time evolution
where xn is the current position and xn+1 is the new position. The accuracy of this method depends on the magnitude of time step. Since Euler method is an explicit method, the magnitude of time step has to be small compared with the magnitude of l. In our simulation we have dt/l =0.1.
Calculating w can be a cumbersome task. Here, we use a finite element package, Abaqus, to automatically calculate w. We use Matlab as a main program to evolve the morphology and only call Abaqus when strain energy calculation is required.
We model the effect of the misfit as a constant force applied at the edge of the film. This is equivalent to a film on a stiff and infinitely large substrate. This assumption is reasonable when the substrate is much thicker than the film.
The workflow is as followed.
a) Matlab generates initial positions of the nodes on the surface and write the data to a position-file.
b) Matlab automatically calls a DOS batch file. In this batch file, Abaqus was called to read the position-file and a pre-programmed input file. It then generates meshes over the entire domain and, with specified boundary force, calculates the strain energy. Note that we add additional two 10 ×10 grids on both sides of the film so that the area of interest is far from the boundary. This avoids error due to edge effect.
c) Abaqus then writes the strain energy associated with the nodes in the surface to the data file.
d) Matlab reads the data file and evolves the node on the surface and write the updated nodal position to the new position-file.
e) Repeat step b)
In our simulation we didn't need to recalculate the strain energy density at every time step. If the position doesn't change too much, the strain energy from the previous step is still valid. We found that calculate strain energy at every 10 time steps is a compromise between speed and accuracy.
The command for Matlab to call batch file is:
system('callabaq.bat')
The command in the batch file to call Abaqus is:
where elasden.inp is the name of the pre-programmed input file.
From Eq. 2, we have four contributions to the reduction of the free energy, which are surface energy, evaporation/condensation, elastic strain energy, and bulk free energy. Therefore, we would like to study each effect separately and we can combine them to simulate the quantum dot.
To achieve this, we set the coefficient in front of term of interest in Eq. 13 to be non zero while multiplying other term with zero. However, to suppress the effect of evaporation-condensation, we have to further set
In this simulation, we studied the effect of the surface energy. We set the initial perturbation,
Fig. 3 shows that with only surface energy contribution, the lowest energy state is the flat film. This is expected because the flat film has the lower surface area.
In this simulation, we study the effect of stress by setting the perturbation,
From Fig. 4, the surface moves down almost uniformly. There exist a small amplitude increase but it is small and we are not sure whether this is due to the numerical error.
The face that the surface moves down is reasonable. When including stress, the energy of atoms in the solid becomes higher relative to that in the environment. Therefore, evaporation becomes more significant.
In this simulation, we study the competition between stress and surface tension contributions. We used the same initial surface profile with both misfit and surface tension term non-zero. We set g = 0 .
Fig. 5. Shows that the surface moves downward as well as reduce the amplitude, which come from stress effect and surface tension effect, respectively.
The remaining parameter is the bulk free energy, g. We use the same surface profile while setting the surface tension and stress contributions to zero. We experimented the value of g from negative to positive.
Recalling Eq (2), the free energy variation can be written as:
Because the free energy variation is associated with unit volume of solid grown on the surface, define a driving force:
Then, if
In this simulation, we increased the magnitude of stress contribution and adjust g to prevent phase change.
Considering a dimensionless parameter which characterizing the relative significance of the elastic and surface energy:
If we set s large enough, the stress effect will dominate the surface movement, the amplitude of surface will increase. In this simulation,
If s is small enough, the surface tension dominate the surface movement, eventually, the surface will become flat. In this simulation,
In experiments, quantum dots form by vapor depositing film material on the substrate. Any seemingly flat substrate is rough when looking at the atomic scale. Therefore, we would like to use simulate the quantum dot from a rough substrate. We introduce random perturbation on the surface and use the same parameter as that of the previous simulation.
From Fig. 8, the amplitude of the surface increase, resulting in formation of 6 dots. The dots still look very rough since this is still very early in the evolution. Nevertheless, the simulation shows that the simulation of quantum dots is possible.
We summarized the important contributions in each simulation in the table below.
Table 1. Summary of significant contribution in each simulation
We have employed the finite element method to study the contribution from the surface tension, the strain energy, and the bulk free energy as well as simulated quantum dot formation. With the surface tension dominating, the system will try to reduce the energy by reducing the surface area, which results in a flat surface. The role of the strain energy is to promote an increase in the amplitude of modulation. At the same time, the stress affects an evaporation rate, causing the surface to move downward. In order to simulate the quantum dot formation, the downward movement is counterbalanced by the bulk energy term which promotes the formation the solid phase from the vapor phase. We show that it is possible to simulate the quantum dot behavior although longer simulation time is needed.

The aim of this project is to evaluate the feasibility of providing UM with one hundred percent (100%) renewable energy for its electricity demand. It intends to demonstrate an instantaneous matching of the energy produced from renewable resources with the corresponding electricity demand.
There is a growing realization that renewable energy resources must be afforded a deeper penetration to meet growing global energy demand. The most important driver for this is the scepter of climate change. Increase in atmospheric CO2 and other GHG emissions are providing impetus to the climate change phenomena. However, conventional power plants such as coal and natural gas which contribute significantly to CO2 continue to be built and developed. While the arguments for continuing with the status quo are compelling, it still remains that fossil fuel sources are rapidly diminishing and their supply cannot be taken for granted anymore.
Thus, it is essential that advances be made in the development and implementation of renewable energy as a base load source. Annually, Ann Arbor's electricity consumption approximates to about 1.6 billion kWh1. UM, with its annual consumption of 550 million kWh [1], puts a staggering 450 thousand Tonnes of CO2 emissions in the atmosphere.
Any effort to offset this situation towards renewable will result in considerable reduction in CO2 emissions (fig. 2).
Source: International Energy Agency; Benign Energy, The Environmental Implications of Renewables, 1998
Inspiration for this endeavor came from a similar study conducted by the University of Kassel, Germany [2]. Their study showed that a combination of different renewable sources (wind, solar, biomass) and some pumped storage could consistently provide 100% of the City of Kassel's electrical energy needs.
The major hindrance in exploiting renewable energy resources for base load electricity has been their intermittent nature & uncertainty in resource availability. Distributed power generation, coupled with a robust energy storage system is therefore the key to harnessing renewable energy resources to their full potential. Our effort comprised of an evaluation of renewable resources in Michigan and strategic siting of energy generation systems to be able to meet the electricity demand of UM.
This section discusses in detail, materials and methods used in this study.
This study was initially intended to model allocation of renewable energy resources for the city of Ann Arbor. However, suitable data for the city's electricity demand could not be obtained from DTE, the utility provider for south-east Michigan. The control volume was therefore scaled down to encompass UM's demand. Hourly demand data for UM central and medical campus buildings monitored at the Central Power Plant was used to generate load curve for the entire university.
Distributed generation (DG) is the term used to describe a mode of power generation characterized by a decentralization of the energy systems. Small-scale units, usually in sizes up to 50 MW, are located on the distribution system close to point of consumption. It is a particularly effective way to manage different renewable energy utilities as per their availability to meet a certain demand. The underlying premise of distributed generation in this context is that when conditions are unfavorable for power generation at a particular site, at another site they would be favorable, hence allowing the system to meet given demand.
Preliminary research of the available renewable energy resources in and around Ann Arbor did not reveal an optimistic picture. Washtenaw County falls in Class 2 wind region with marginal power generation potential [3]. Michigan's annual average solar radiation is 3-4 KWh/m2/day. However, it goes down to 0-2 KWh/m2/day for an extended period from December to March [4]. Washtenaw County has the potential to provide 150-200 Tonnes of fuel to a Biomass plant annually, sufficient for only 20MW power generation [5]. Hydro and landfill account for less than 8% of the city's electricity demand, with limited potential of further development2.
This makes it evident that no one resource has the capacity to sustain UM's demand round the year. The way forward to utilize renewable energies is to find the optimum, cost-effective combination of all of these resources to power UM. With such resources being available in abundance not too far from Ann Arbor, it can also prove rewarding to consider renewable options outside the city's limits.
A brief assessment of each of the renewable energies done is provided below. The modeling approach to optimize siting and sizing follows next
A distribution generation network is most effective when power generation units are not far removed from the site of power consumption. For this reason, the NREL wind maps were explored within 100-120 mile radius of Ann Arbor. Central and southern Michigan lie in Class 2 wind regions, with scattered pockets of Class 3 winds. Five sites were geographically dispersed in these high wind density pockets to suppress the inherent variability. The transient wind speed data at four of the five chosen sites -- Carsonville, Deerfield, Twinning and Hudsonville, had been recorded at a height of 30m [6], while the data at Chrysler Proving Grounds in Chelsea was at 80m3. The One-seventh power law4 [7] was used to condition the wind speeds to a height of 50m and 100m.
Three turbines with different power ratings were chosen for analysis -- Bonus 300, Nordex S77/1500kW and Nordex N100/2500kW5. The performance of these turbines was evaluated at each of the selected sites to maximize power output with least capital investment.
Offshore wind potential along the great lakes offers an extremely potent solution to Michigan's power demand. However the limited wind speed data and complexity of transmission and technology involved in such an analysis put it outside the scope of present study.
Building large-scale solar PV farms requires buying big blocks of land -- an additional cost over and above the cost of buying and installing solar PV arrays. The roof-tops of University buildings provide a massive flat area to tap solar insolation. Roof area in Ann Arbor is estimated6 at 85,800,000 ft2. We assumed 25% of this was for UM buildings and further, a maximum of 25% of that would be available for PV installations7. The availability of vacant/waste-lands for solar farms was also considered.
Due to weather patterns, solar insolation intensity at ground level can vary substantially in a matter of just a few miles. Reliable data for solar insolation in Ann Arbor was found only for one site [8]. To increase the fidelity of the system towards small scale weather changes within Ann Arbor, data was taken from 3 more sites close to Ann Arbor-Detroit Airport in East, Jackson in West, and Howell in North. To simplify the case, it was further assumed that the UM buildings in Ann Arbor are distributed evenly in North, South, East and West regions. This approach makes it possible to maintain proximity of power generation sites, while improving system fidelity.
Most solar PV panels available in market today operate in the range of 10-14% efficiency. However, practical-use multi-crystalline silicon solar cells have recently been made available, with efficiency of 18.6% [9] and efficiency as high as 40% have been achieved in laboratories [10]. With a projected time period of transition to 100% renewable energies of 30-50 years, it can be safely assumed that solar panels deployed for electricity generation will have an average efficiency of 18.6%.
Both Wind and Solar power are beyond human control. Therefore, it is imperative that biomass power generation plants with low response time be available.
Appendix A shows the map of Michigan with a circle of 50 mile radius drawn around Ann Arbor. A comparison with the NREL Biomass availability map [11] revealed that Washtenaw, Wayne, Lenawee and Hillsdale Counties have a combined biomass potential of 900,000 Tonnes -- enough to sustain a 100MW Biomass plant. These counties do not cater to any major Biomass plant as of now [12].
Ann Arbor is located in a strategically good location to accrue biomass from all 4 counties. Moreover, a Biomass plant here can be easily coupled with UM's transmission system, thus eliminating transmission cost.
Barton and Superior Dams located on Huron River, running at 60%-80% of their maximum potential, are able to generate only 8.5 million KWh. The city's landfill power generation facility is nearing the end of its service-life, with no immediate plans to replace it3.
Modeling of renewable resources against university demand showed that 37% of the time during the year, all renewable energies put together could not match university demand. In the period from late July to early October, the electricity demand peaks (Fig 4). This coincides with the time when average wind power is at its minimum and solar power generation is falling from peak. Ramping up biomass capacity is not able to compensate for the increase in demand. Therefore, it is extremely important to develop a long term energy storage facility that can store the excess electricity generated during winter months and discharge that during the summer to meet peak demand.
A short term energy storage capacity with extremely low response time is also required to compensate for peak-shaving.
Several energy storage technologies were considered [13]. Compressed Air Energy Storage (CAES) and Pumped Hydro Electricity Storage (PHES) technologies provide the option of energy storage over required period time. However, current CAES facilities utilize combustion of natural gas to boost efficiency of the system, which sets a drawback to the initial goal of going 100% renewable. The option of utilizing Biogas instead of natural gas was considered, but found to have impractical efficiency. The Ludington PHES located on Lake Michigan has the capability of storing 1.9GWh energy [14]. A similar facility can fulfill UM's long-term storage requirement. The technology has a conversion efficiency of 65-80%.
An optimization algorithm (Appendix II) was developed, and MATLAB script was written to systematically analyze various combinations of renewable energies (APPENDIX B). Several strategies were considered in the study to arrive at an optimized allocation of resources. One strategy dealt with large Wind farms with relatively small investment in Solar, while another focused on optimizing long-term storage requirement of the system.
For simplicity of the model only one storage system was considered, which could cater to both long and short term storage requirements.
Site selection for Wind farms and Biomass plant allocation was based on detailed evaluation of renewable resources in vicinity of Ann Arbor (siting shown in Appendix A). The Twinning location was found to be unsuitable for wind-power generation in cost effective manner and was dropped from the model. The MATLAB results showed that total wind power potential dropped 60% during the summer months. Increased solar insolation in summer was able to partially compensate for this drop. The size of total solar installations was iteratively optimized to 200,000m2 -- ~11% of total University roof-tops. The remaining electricity demand was fulfilled through a 55MW biomass plant and large pumped storage.
Several iterations were done to evaluate these resources and the following combination was found to be most cost effective
Our findings reveal that a combination of renewable energy resources strategically distributed in the state of Michigan together with energy storage can provide UM's electricity demand around the clock. However, this comes with a few caveats. A considerable quantity of energy needs to be stored. During the winter months when electricity consumption is relatively low, renewables are able to meet UM's demand and store excess electricity generated. In the summer, when consumption peaks, a significant proportion is met by the energy coming from storage facility and operating the biomass power plant at maximum capacity. Thus the importance of energy conservation and efficiency in reducing peak demand (and therefore, dependence on storage) cannot be overemphasized.
For solar PV, only a fraction of the available roof areas were considered for deployment in the current model. This is primarily due to the low efficiency of PV panels presently available. However, with 40% efficiency being achieved in laboratory experiments, higher emphasis on solar power generation can be expected. Also, preliminary assessment of UM north campus parking lot area totaled to 135,000m2 -- sufficient to provide 35% of North Campus' power. Utilizing these parking lots instead of building roof-tops for solar PV can provide a good cost-benefit.
Four sites were considered for wind-farm development in this study. It was found that Washtenaw and Deerfield have good wind speeds at 80m and 100m heights. Therefore, as per availability of land, it can be beneficial to only setup wind turbines in these 2 locations and reduce transmission costs.
Even though large investments have been made in wind power development, very limited transient data is available for wind speeds across Michigan. As was found in this study, even though Washtenaw falls in class 2 wind region, the wind speeds at Chrysler Proving Grounds qualify for class 3 regime, suitable for power generation. Many such potential micro-sites can exist scattered all over Michigan. It is therefore very important that wind speeds be monitored and recorded to ease integrated planning.
Due to sparse data, offshore winds along the Great Lakes were not considered in this study. However, these offshore winds are in class 5 and 6 regimes and hold a high potential.
Since Biomass power plants can be accessed on demand, they remain a key source for reliable renewable energy. Sustainable supply of biomass feedstock had been a problem for many years and a disadvantage to biomass implementation. However, new sources, such as Switchgrass and Reed Canary Grass are much easier to grow, are found in closer proximity to Ann Arbor and have higher energy content than previous biomass constituents. Biomass in the control volume considered needs a more thorough analysis. Additional Biomass production capability can be generated by dedicating Ann Arbor Green Belt to high-yield feedstock production.
To introduce robustness in the system, it can prove useful to build two small Biomass Power plants -- one within Ann Arbor, and one in the middle of a biomass-rich zone, as against one large biomass plant. This way, even if one of the plants needs to shut-down for maintenance and repair, the other plant can come on-line to meet required demand.
Pumped hydro was found to be most suitable for UM's energy storage requirements. However, a particular potential site for such storage could not be identified. The abandoned salt-mines along Lake Huron can be developed for Compressed-air storage, to be used in tandem with biogas instead of natural gas.
Throughout this study, it has been observed time and again, that the summer peak demand is a major hindrance in eliminating fossil fuel based power; it puts excessive load on the storage system. Efforts to achieve 100% renewable can only materialize through aggressive energy efficiency programs.
Finally, resources outside the bounds of Ann Arbor were explored and tapped to a limited extent in this model. Therefore, an integrated model for the entire state of Michigan, exploiting state-wide renewable resources should prove much more effective.

Walking robots have been a popular science fiction fantasy since the mid 20th century. The control community has been actively working on this problem for several decades. Various methods are used; one of the easiest to implement in practice is a robot with 4-6 legs to solve the balancing problem. Bipedal, human-like walking is much more difficult.
Human walking is fundamentally difficult to emulate because it is a highly dynamic process with many states and little static stability. Many existing bipedal robots get around this problem by altering the gait properties to eliminate the dynamic balancing problem; Honda's Asimo is perhaps the most famous example. This method is termed the "Zero Moment Point" method and is discussed in Section 2.
The goal of this project is to study the bipedal locomotion problem using nonlinear control techniquesin a systematic way. Stable walkinggaits correspond to stableperiodic orbitsinthe state space. Byusing a variety oftechniques, controlis appliedto createthese orbits, specific orbits are identified, and their stability can be proven. These controllers are then simulated on the full-dimensional robot model and shown to create the desired walking gait.
These methods have been extensively developed and applied to complicated robots [2], including hardware testing on a 5-link robot. The work presented here applies the methods of[2] to a simple 3-link walker; All theorems and methods presented here can be found in [2]. The main techniques used are Lagrangian Dynamics, Poincare maps, hybrid systems, zero dynamics, and feedback linearization.
The outline of this report is as follows: The robot continuous dynamics and impact map are derived. A simple walking gait is chosen. A periodic orbit is found and proved stable for two different types of controllers. The zero dynamics are studied. Finally, a more complicated, energy-efficient gait is studied.
The basic idea of the Zero Moment Point Heuristic is to use actuated feet with small, flat-footed steps. In this way, the robot is statically stable at all points throughout a step and the control problem is simplified. The interaction forces between the robot and the ground are lumped into a single force with no moment acting at the "zero moment point". If one wanted to support the robot with a point force, it must act at this point.
By designing the controller to keep this "zero moment point" within the support polygon of the robot, the robot will be statically stable. With one foot on the ground, the support polygon is the outline of the foot in contact with the ground. With both feet on the ground, it is the area between the outlying ground contact points, the front toe and back heel. This method is graphically illustrated in Figure 1.
This method is based on static stability, so it works for slow, near-static gaits. For dynamic gaits, the results break down. This method does not provide a rigorous stability proof for dynamic gaits.
During a typical walking gait, humans have large phases of underactuation as shown in Figure 2. During a typical step, the period during heel strike (start of a footfall) and toe roll (end of a footfall) consists of nearly point contact between the foot and the ground. During these phases the ankle and all other joints have no authority to impart moments between the robot and the ground. The zero moment point method cannot address this problem of point contact, and typical control methods will not work due to the unactuated degree of freedom. Therefore some sense of dynamic stability is required.
To explicitly address this problem, the robot studied here has point feet. This forces the controller the continuously deal with the underactuation. Once methods are developed to deal with underactuated phases, they can be applied to more complicated robots with actuated ankles.
The robot model studied for this project is the three-link walker showninFigure3. The robot has two legs of length r and a torso of length L. Each leg has a lumped mass m at the leg midpoint. There is a lumped hip mass MH and a lumped torso mass MT. The angles used to define the robot geometry are absolute angles measured with respect to the inertial frame.
Through the course of the system analysis and design, there is significant symbolic math, even for this simple robot. While the calculations could be done by hand, most terms are evaluated using the symbolic toolbox in Matlab.
The robot is described by a set of generalized coordinates
Lagrane's equation is
where
and (2) takes the form
where
Similarly, for a torque
To begin, the kinetic and potential energies are calculated by hand based on the system geometry. The only other quantity that must be calculated manually is the vector of generalized forces
First, the equations of motion are derived for the stance phase, when one leg is always in contact with the ground. These dynamics are denoted with the subscript "s". The foot of the stance leg is motionless, so there are three degrees of freedom. The kinetic energy matrix denoted
The potential energy
the matrix
The matrix
Finally, the vector of generalized forces is
where u represents the torque applied to the two legs.
The stance phase dynamics of the robot are now completely described by
For future use in deriving the impact model, the kinetic energy of the robot is also derived for the flight phase when there is no contact with the ground. There are now two additional degrees of freedom representing the horizontal and vertical positions of the robot, making five total degrees of freedom. The two additional states are appended to the end of the state vector q. The top left corner of the flight phase kinetic energy matrix
Now that the continuous dynamics are solved for the stance phase, an impact model must be developed to describe what happens at foot impact. The development is sketched here, the reader is invited to consult [2] for technical proofs. The fundamental assumption used here is that the forces applied to the swing foot when it hits the ground are impulsive. With a relatively hard walking surface and rigid robot, these forces are very fast in comparison to the rest of the dynamics, so this assumption is reasonable. The main result of this assumption is that these instantaneous forces can produce a step change in the velocities of the robot, but not the configuration. Therefore,
The first step is to augment the stance coordinates
By "integrating" (14) over the instantaneous impact [2], the impact event must satisfy
To relate forces in the inertial frame
where
These equations are simulatenously solved as
Solving these equations yields the velocities after the impact,
Normally, the switching surface in the state space corresponds to the swing leg foot impacting the ground. However, in a robot without knees, the swing leg will always scuff the ground when it crosses the other leg.
For this case, ground impact is assumed to be initiated by the controller. The swing leg is assumed to touch the ground only when allowed by the controller. One way to do this is to have the swing leg pivot slightly outward; this puts it outside of the walking plane and it can rotate forward without ground contact. The leg is brought back inline to initiate contact. Another method is to have a small portion of the leg that can "pick up" to avoid ground contact. Both methods have been shown to work in hardware versions of robots without knees.
The control designer must somehow specify the type of motion the robot will walk. Rather than specify time trajectories to follow, this is done by assigning a relationship between the various body configuration variables called a "virtual constraint". The controller then works to enforce these virtual contstraints between various joint angles. When these constraints are correctly enforced, the controlled joint states are defined, and the dynamics that remain are the "zero dynamics".
To execute this method, first choose a generalized coordinate that is monotonically increasing during a step. For this robot, the angle of the stance leg
Now, define the output functions
When the output y is zero, dynamics have only 2 states,
In order to enforce the virtual constraints, a feedback controller is used to drive the output y to zero. Feedback linearization and coordinate transformations are used to simplify the control problem. Several of the available stability theorems require the trajectories to be exactly on the zero dynamics manifold. Therefore a controller is required that zeros the output in finite time (within one step). In practice, a sufficiently fast exponential controller also works, so this type of controller is used as well.
Assign the functions
where
and
The parameters
where
An easier controller implementation is an exponential controller for the input-output linearized system. For this method, the control input is
where K1 and K2 are turning matrices.
To simulate the robot dynamics, two distinct steps are required. Starting at some initial condition, the continuous dynamics evolve according to (12) until the trajectory intersects the switching surface S. At this point, the reset/impact map
A very simple walking gait with a stable orbit generated by assigning the virtual constraints as
and, after applying the reset map
Notice that the virtual constraint
To study how the virtual constraints are implemented, the output function
The walking dynamics of the robot can be modeled as an autonomous system with impulse effects
The following hypotheses are assumed about system
In addition, the following hypotheses are applied to an invariant submanifold of the system
Define the restricted Poincaré map
where
With impulse effects(28) satisfies the hypotheses HSH1-HSH5. Suppose furthermore that
This theorem is used to prove the stability of the orbits described in Section 11 by checking the stability of the Poincaré return map. The main benefit of this theorem is that the stability check is carried out on the zero dynamics manifold rather than the full system dynamics. For this robot, this benefit means checking a one-dimensional system rather than the five-dimensional stability map of the full system. The zero dynamics of this model are the
Next, an exponential controller is used to zero the output function h as described in Section 8.2. Using the exponential controller rather than the finite-time version means that the dynamics are no longer guaranteed to exactly converge to the zero dynamics manifold with each step. As long as the exponential controller is "fast enough", the resulting gait is very similar to that obtained with the finite-time controller. The same figures are shown here as for the finite time controller. The output h in plotted in Figure 11 for both finite-time and exponential controllers. Note that the output h does not reach zero with each step with the exponential controller, as it does with the finite time controller. Videos for this "simple gait" are available at [1].
The hypotheses HS2 and HSH4 are strengthened to make the autonomous system with impulse effects (28) continuously differentiable.
Without explicit mathematical definitions, let TI be the time until impact from any state x. Let S˜ be the points in S that map to an x that eventually intersects the switching surface again. The following Corollary can then be used to prove stability of the system with an exponential controller.
By perturbing the initial conditions around the fixed point, the jacobian of the Poincaré map is numerically evaluated. Note that this is the jacobian on the switching surface and thus has five dimensions rather than the full six. The jacobian matrix is 5 x 5, and its eigenvalues are
All the eigenvalues have magnitude less than one, so the fixed point for the exponential controller is stable. If the gains of the exponential controller are decreased, the eigenvalues become unstable, as does the orbit. This happens when the controller is no longer fast enough to bring the dynamics back to the zero dynamics manifold between steps.
The dynamics of the system can also be studied by restricting the dynamics to the zero dynamics manifold. The virtual constraints are assumed to be enforced exactly, making
This can be reduced to a reset map that determines the velocity on the zero dynamics manifold after impact
For this robot and the gaits considered here, the function
The zero dynamics are simulated by making
By studying the stable orbits of the zero dynamics, one can find orbits of the full system by searching in a lower-dimensional space. Though not repeated here, theorems are available in [2] to prove that stable orbits of the zero dynamics are stable orbits of the full system under certain conditions. Results are available both for finite-time controllers and for "fast enough" exponential controllers.
Table 2: Parameters for an optimized gait
A more complicated gait was also simulated that is more energy efficient. This gait is defined by the virtual constraints
where
The output function h in (20) is now defined by these more complicated functions. The impact/reset map does not change. The full-dimensional system is simulated with a finite-time controller, and the results are shown in Figures 13-15. Note that
Stable Periodic Orbit with exponential controller
Full Dynamics
Biped walking is a challenging problem due to complicated dynamics and large state space for all but the simplest models. A fundamental challenge is the natural underactuation that occurs with walking. A useful solution to this problem is to define walking gaits by virtual constraints rather than trajectory tracking. This yields robust, provably stable orbits of the system that correspond to walking. When these virtual constraints are satisfied, the dynamics of the system can be analyzed on the zero dynamics manifold, allowing the designer to study a lower-dimensional system.
In this project, a 3-link biped walker was studied. The continuous dynamics and impact map were derived. Finite-time and exponential controllers were used to control joint angles, and the stability of the periodic orbit was proven for both cases. Two walking gaits were studied, a simple version and one that is more energy efficient.

Driver modeling is an essential part in studies on closed-loop human/vehicle/road systems. However, it is a challenging task to model human drivers not only because there are no equations or theories that fully describe the complex human cognitive process, but also because drivers adapt themselves to different driving and traffic situations, thereby constantly changing their strategies and characteristics. Although driver behavior is in general complicated, lane keeping is a relatively straightforward driving control task. Even drivers without much training are able to maintain lane position and maneuver when following twisting roadways.
Driver directional control models have been developed based on different philosophies and approaches, and detailed reviews can be found in MacAdam [1] and Ploechl et al. [2]. As suggested by McRuer [3], the driver directional control actions could be separated into three types: compensatory, pursuit and precognitive (Figure 1). Compensatory control is carried out in a closed loop. The driver is assumed to utilize feedback loops based on the position error and the heading angle with respect to road tangent to minimize undesired deviations. Pursuit control operates by using the driver's preview of the upcoming road path and initiating feedforward control actions. Precognitive control usually plays a role only for repetitive well-learned tasks such as pull-in maneuvers in a parking lot, and thus can be ignored in daily highway driving.
Among the existing driver lateral control models, the model developed by the University of Toronto [4] was composed of three components: curvature preview, heading angle feedback, and position feedback. It was assumed that the three components were sampled, possibly at different rates, and combined to generate the desired steering angle. At least five parameters needed to be specified. This model was applied to both lane keeping and lane change maneuvers. The STI (Systems Technology Incorporation) model detailed in [5] consisted of two parts: the motion feedback from vehicle yaw rate and the visual feedback from curvature error. The elaborate curvature error term included the contributions from lateral offset, heading angle error, projected future vehicle position, and aim point position on curved road. An extra "trim" term in integral controller was added to eliminate steady-state error. In combination, they were used to generate a desired steering wheel velocity. In the "structural model" developed by Hess et al. [6], the driver was represented by a low-frequency compensator and a high-frequency compensation block, which included driver time-delay, "proprioceptive" feedback, and a second-order model of the neuromuscular system. The outer loop was closed using visual feedback based on an aim point error. Qualitatively, a reasonable match between model data and simulator test data was achieved. In the optimal preview model developed by MacAdam [7], the driver was assumed to behave like a preview optimal controller with time delay. A path error functional was constructed by previewing the road. The control objective was to minimize the weighted integral of squares of the differences between the previewed path points and the corresponding estimated lateral positions over the preview horizon.
Vehicle lateral control is a broad notion, which includes lane keeping, lane change, obstacle avoidance, stability control in critical situations, and so on. Among these maneuvers, lane keeping on normal highways belongs to low-bandwidth, low-acceleration plain tasks. The two primary sources of stimuli for lane keeping control are: (1) the desired path provided by the roadway markers, and (2) the perceived vehicle states. A skilled driver is expected to use feedforward control to respond directly to the effective future road inputs, as well as feedback control to respond to deviations from the desired states.
In both [4] and [5], separate gains or transfer functions were proposed to associate with lateral deviation, heading angle error, and the road curvature term. Their model parameters were calibrated with a limited driver subject group and reported in mean values and standard deviations. These values were direct results of model fitting and no clear internal relationship among them was revealed. The "structural model" [6] is formulated as a feedback structure, and it does not explicitly take the upcoming road curvature into account. The MacAdam model in [7] is proven versatile enough to fulfill tasks such as double lane change and slalom if tuned properly. However, as will be shown later, for a less complex task like lane keeping, by partitioning the control actions into feedforward/feedback components, a compact system structure can be formulated, and only two control parameters remain to be specified.
The system analysis starts with investigation of the plant dynamics in the next section, followed by the formulation of a control-oriented system structure. The determination of control parameters is detailed in the subsequent section. Then simulation studies are presented to validate the proposed model, and finally conclusions are drawn in the last section.
The dynamics of a passenger vehicle can be described by a detailed nonlinear model [8]. Under simplifying assumptions, the vehicle lateral motion can be decoupled from the longitudinal dynamics and characterized by a classic 2-degree of freedom (DOF) linearized model [9]. This planar "bicycle model" assumes a small road-wheel steering angle, retains only lateral and yaw dynamics, and is parameterized by a constant longitudinal velocity. Grouping the two front wheels and the two rear wheels separately, one obtains the single-track model illustrated in Figure 2. The variables and parameters are explained in the Appendix. Under normal highway driving conditions, it is justifiable to assume that the lateral force varies linearly with the tire slip angle. By applying the force and moment equilibrium conditions to the free body diagram and substituting the kinematic relationship of the tire slip angles, the equations of motion can be expressed in the state space form:
The sign convention conforms to the SAE coordinates system, and the angles are positive in the clockwise direction. This 2-DOF model is known to predict vehicle lateral behavior reasonably well when the lateral acceleration is below 0.3 g [10]. The analysis in subsequent sections is based on the parameters of a full-size sedan summarized in Table I.
Since the relative motion of the vehicle with respect to the road is of interest, additional variables need to be defined. Two coordinate systems have been introduced in Figure 2. The inertial system (X0, Y0) is fixed on the ground, which serves as a reference frame for vehicle motions. The body-fixed coordinate system is denoted by (x, y) with its origin located at the vehicle's center of gravity (CG). The CG lateral displacement YCG and the relative heading angle need to be added into the states. As in [11], the state-space equation can be written as
The front wheel steering angle Tf is the control input and the road curvature (since rd = u00) enters the system as an exogenous disturbance input. For steady-state lane following on a road with a constant curvature , one supposes the vehicle CG tracks the curve successfully. By setting and , the steady-state values of the relative heading angle and the front wheel steering angle can be derived
where Kus is the understeer coefficient. Both
Apart from the lateral position of vehicle CG, the lateral deviation Yp of a preview point (P) down the road is of interest as well (Figure 3), since the driver typically looks forward, extrapolates from current states and projects to an "aim point" a finite distance ahead [12]. Accordingly, given a preview distance Lp, the trigonometric relationship can be derived from the law of cosines.
The preview distance Lp along the vehicle's longitudinal axis can be replaced with the preview time Tp, namely
where
Evidently Yp consists of three terms, which arise from the local lateral deviation at the vehicle CG, relative heading angle, and curve bending, respectively. If the vehicle CG tracks the desired path successfully, Eq. (8) represents the previewed lateral deviation at steady state.
Hence Yp* is also proportional to road curvature, and the proportional constant is a function of vehicle parameters, longitudinal velocity, and preview distance. Consequently the complete state-space description can be expressed as a two-input-two-output system:
The control objective of lane keeping can thus be formulated as follows: to regulate lateral displacement YCG by assessing the previewed lateral deviation Yp and manipulating steering wheel angle input f to counteract road curvature disturbance . How a skilled human driver might process the perceptual cues and translate them into appropriate steering wheel angles, so that an adequate performance and stability margin can be achieved, is the topic of the following study.
A human driver's lane keeping control behavior can be divided into the open-loop pursuit and the closed-loop compensatory parts. The pursuit part previews the upcoming path and generates the primary portion of the steering action, whereas the compensatory part attenuates the remaining errors.
In the subsequent analysis, we assume that for lane keeping control, a driver only makes use of the estimated road curvature within the driver's preview distance, vehicle heading angle, and current lateral displacement from the reference position, which may be corrupted by noises or biased due to human perception limitations. The driver processes this set of information, follows the road curves, and stabilizes the vehicle. We also assume that the driver is a competent, but not necessarily perfect, lane keeping controller. Because in real life, most drivers manage to maintain lateral positions in spite of various disturbances, but few of them make every effort to stay at a single desired lateral position. Actually as long as sufficient stability margin and performance requirements are fulfilled, the resulting controller can be deemed a valid representation of drivers' control action.
Motivated by the pursuit/compensatory dichotomy, the proposed control structure is illustrated in Figure 4. The vehicle/road dynamics module is driven by the control input (u) and the disturbance input (d). The desired output (z) is the lateral displacement of the CG, and the measured output (y) is the lateral position at the preview point, which is corrupted by noise (n) then sensed by the driver as measured output (ym). The driver controller has three major elements: disturbance feedforward Gff, reference generation GR and feedback compensation Gfb. The driver controller also contains a multiplicative curvature estimation uncertainty (.) and an inherent human remnant term (nn) [13]. One additional degree of control freedom, Tp, is hidden in the vehicle/road dynamics module.
In actual driving, the driver looks ahead and perceives upcoming road geometry. The curvature disturbance can be anticipated and a feedforward compensator can be used to alleviate its effect preemptively, so that the driver turns the steering wheel based on his/her internal empirical model of the vehicle yaw dynamics [14]. The mathematical relationship for this curve negotiation behavior at steady state is expressed by Eq. (4). Essentially the driver tends to match the yaw rate of the vehicle with that of the road tangent. Therefore
For the reference generation, Eq. (8) is used to derive the desired lateral deviation of the preview point, therefore
If the road is straight, the reference lateral position stays at zero. However, if there exists a substantial road curvature, the reference position should be biased. The magnitude of the bias is dependent on vehicle parameters, the preview distance, and the curvature itself. Figure 5 shows how Gff and GR vary as a function of longitudinal velocity when other parameters and variables are fixed. For easier interpretation, the unit of the curvature is taken as 1/km, and the steering angle is computed at the steering wheel in degrees.
The feedforward loop partially inverts the vehicle dynamics so as to achieve faster response. Any remaining deviation needs to be taken care of by the compensatory loop. As an initial step, a basic feedback controller is assumed, which is composed of a pure proportional action along with human cognitive limitations:
In brief summary, a skilled driver is supposed to take advantage of knowledge about future disturbances and relies on feedforward and feedback actions to reject the disturbance, essentially reducing the norm of the closed-loop transfer function Tzd from the curvature input to the lateral displacement at CG. Given the proposed lane keeping control structure in Figure 4 and the above analysis, two control design parameters (Kp and Tp) remain to be determined to achieve satisfactory performances compatible with what drivers normally do.
To visualize the potential effects of driver's forward-looking, the Bode plots for the plant dynamics (from steering wheel angle lSW to the previewed lateral position Yp) are shown in Figure 6, with varying preview times. In general, a longer preview results in more substantial phase leads. However, a larger preview time does not necessarily lead to more substantial phase margin. The maximal phase margin occurs when Tp takes an intermediate value. Figure 7 presents the Bode plots for the open loop transfer function (GfbfGyu) with fixed preview time and varying feedback gains. Due to the human time delay, the phase plot has a much different pattern from that in Figure 6. It is evident that a low gain results in small phase margin and low cross-over frequency, whereas an improperly large gain reduces both gain margin and phase margin, thus undermining closed-loop stability. Therefore an appropriate range of feedback gain exists for achieving adequate performance and stability margin.
The selection of the control parameters Tp and Kp is conducted with an optimal search procedure. During the optimal search, it is not aimed to achieve the best feasible performance. Instead, as long as sufficient stability margin and performance requirements are met, the solution will be accepted.
Sufficient phase margin and gain margin are essential in order to avoid excessive steering oscillation and maintain stability. In an automatic steering system design [16], a 50-degree PM was obtained by optimization. In an earlier study [3] by McRuer et al., a PM of approximate 40 degrees was determined for compensatory driver steering control tasks. In the present optimal search procedure, constraints of a 40-degree PM and 3.2 dB GM will be imposed. In order to avoid lane straddling, maximal CG lateral displacement has to be lower than 0.9 m in response to a 0.25g lateral acceleration disturbance. According to the AASHTO Green Book [17], the minimum radius for freeway horizontal alignment generally results in lateral acceleration lower than 0.25g. The 0.9 m constraint is based on the 1.8 m nominal vehicle width and the prevailing 12 ft (3.66 m) highway lane width [18].
The objective of the optimal search is to obtain the minimal preview time and the associated gain that satisfy the above constraints. The assumption regarding the preview time is: the farther the driver has to look ahead, the noisier the measurement becomes and the more stringent the visibility condition is. So the driver settles for short previews as long as system performance is ensured.
The search procedure is implemented as follows. For every vehicle speed and for any given preview time, the optimal feedback gain is sought to minimize the infinity norm of the closed-loop transfer function Tzd, while satisfying all the constraints on PM, GM, and maximal CG lateral displacement. Then among the feasible solution pairs Tp and Kp, the smallest Tp and its associated Kp will be chosen. This process is repeated for a range of reasonable highway driving velocities and the resulting optimal parameter pairs are illustrated in Figure 8. Also shown is the equivalent optimal preview distance (further divided by 10).
An inspection of Figure 8 reveals the following observations. The optimal preview distance increases with the vehicle velocity. The optimal preview time has a slight downward trend, but overall falls into a narrow range (1.4 ~ 1.7 s) over normal highway travel speeds, which is consistent with the values reported in [19]. A large feedback gain is required at low vehicle speeds. Beyond 20 m/s (45 mph) the optimal control gain varies quite gently; its smoothness is expected to lead to consistent driver behavior as velocity changes.
Figure 9 shows the Bode plots for the open loop and closed-loop transfer functions after the optimal control parameters are specified (at u0 = 25 m/s). The open loop frequency response is in agreement with the Cross-over model [20]: around the gain cross-over region, the magnitude response can be approximated with a-20 dB/decade line. The right panels show that under ideal circumstances (accurate knowledge of vehicle parameters, no measurement noise, no curvature estimation uncertainty), the closed-loop transfer function Tzd has an infinitely small DC gain; effectively the road disturbance can be completely rejected. In practice, despite parameter uncertainty and estimation error, adequate performance can still be maintained, which will be explored in the next section.
A simulation of lane keeping despite curvature disturbance was implemented first in Simulink. The linear model of combined vehicle and road dynamics in Eq. (9) was employed and the driver controller was parameterized as discussed in previous sections, with 0.2s time delay and 0.15s neuro-muscular lag. To test the robustness, it was assumed that the curvature perceived by the driver was only 80% of the true value. No driver remnant or measurement noise was included. The simulated road track consisted of a circular arc with two straight segments appended on both ends. The curvature was chosen to induce a 0.25 g lateral acceleration at steady-state curve negotiation.
Figure 10 presents the closed-loop responses and driver inputs at u0 = 25 m/s. The abrupt transitions between straight and circular segments result in step changes in road curvature at simulation moments 2s and 15s. In the subpanels, the dashed lines denote the theoretical values of steering wheel angle, yaw rate, lateral acceleration, heading angle, and preview point displacement respectively, if the vehicle is to track the circular curve perfectly.
Figure 10 demonstrates that despite curvature estimation inaccuracy, the lane-keeping task can be accomplished successfully despite estimation errors. The steady-state values of the solid lines converge to the theoretical levels marked by dashed lines. The transient phase is brief and without much oscillation. In the topmost subpanel, the total steering wheel angle is consistent with the theoretical computation. Although the feedforward part does not perform well due to the estimation inaccuracy imposed by us, the remaining deviation is nulled by the feedback part. Eventually YCG at steady state is nonzero, but almost negligible. During the transience YCG is within the limits of 0.9 m, and no lane exceedance occurs.
Then simulations based on a nonlinear vehicle model were conducted by using the CarSim software from the Mechanical Simulation Corporation [21]. CarSim simulates and analyzes the dynamic behavior of light vehicles on 3-D road surfaces. It is capable of predicting 3D forces and vehicle motions in response to driver inputs such as steering, throttling, and braking. CarSim also generates a great number of output variables for visualization and analysis, and allows an interactive animation of simulated tests.
Figure 11. A serpentine roadway geometry, also used in [6] and [22].
Vehicle parameters were specified in CarSim, especially tire characteristics. Comparison tests were run to verify responses from the 2-DOF model and CarSimr model against field experiments [23]. The driver steering module, implemented in Simulink blocks, was interfaced with CarSim to provide lateral directional control. A serpentine roadway (Figure 11) with an approximate total length of 2000 m was constructed in CarSim (also used in [6] and [22]). Within the CarSim environment, the upcoming road curvature will not be provided directly to the driver module. Instead, it is assumed that the driver retrieves the coordinates of five points on the road centerline within preview distance, performs a circle fitting by least squares [24], and derives the corresponding road curvature for feedforward control. Figure 12 shows time histories of estimated curvature, CG lateral position, steering wheel angle, and lateral acceleration at a cruising speed of 50 km/h (13.9 m/s). The control parameters are determined for this velocity as discussed before and adequate performance is attained. Except at locations with large and fast-changing curvatures, the lateral displacement can be well maintained close to the lane center. No lane excursion occurs. Qualitatively the pattern of the steering wheel angle bears a close resemblance with the simulator results in [6].
Although human characteristics are clearly nonlinear, linear analysis can still provide significant insights into human behavior. Motivated by human perception of upcoming road geometry and vehicle states, an effective albeit simplistic approach is proposed to analyze driver lane keeping control. The driver is assumed to look ahead and make use of future road curvature and lateral deviation of an "aim point" to adjust the steering wheel angle. The control system structure is established on linearized curve negotiation dynamics, and only two control parameters remain to be tuned. The resulting driver controller reflects the characteristics of human operators.
Satisfactory system performance is validated in realistic nonlinear simulation environment. Moreover, the proposed feedforward/feedback control structure can potentially be implemented for automatic lane-tracking if road preview information is made available by machine vision [23] or magnetic markers [25]. However, practical constraints, such as sensor noises, measurement or estimation accuracy, and bandwidth limitations of the steering actuator dynamics, need to be carefully addressed to maintain consistent and robust performance. Naturalistic driver lane keeping models can also be developed from this template if the necessary measurements are available.

Zooplankton are an integral part of freshwater lake ecosystems. As primary consumers, zooplankton can control the abundance and composition of phytoplankton and can affect water quality, trophic state, nutrient cycling, and food web resilience to perturbations (Dini et. al., 1987, Carpenter et. al., 1992, Cottingham et. al., 1997, Taylor & Carter, 1997, Stephen et. al., 1998). Zooplankton are also an important food source for planktivorous fishes and larval fish of many species (Wetzel, 1975), and can affect growth rates and survival and recruitment of planktivores (Cryer et. al., 1986, Bremigan & Stein, 1997, Dettmers et. al., 2003). The relative influence of zooplankton on lower and higher trophic levels varies as a function of community composition and size structure (Brooks & Dodson, 1965, Galbraith, 1975, Dini et. al., 1987, Bremigan & Stein, 1997, Dettmers et. al., 2003). Therefore, understanding the factors that regulate community composition and size structure is of great interest.
Biotic interactions such as predation and competition are important in shaping zooplankton community composition and size structure (Brooks & Dodson, 1965, Gliwicz & Lampert, 1993). These processes do not act independently and Brooks and Dodson's (1965) "size-efficiency hypothesis" describes how zooplankton community composition, and thus size structure, change via different competitive scenarios under varying predation pressure. Fish tend to be size-selective in their feeding and prefer larger-bodied individuals (Brooks & Dodson, 1965), especially large cladocerans such as Daphnia (Brooks, 1968, Vinyard, 1980, Turner & Mittelbach, 1990). This size-selectiveness means that fish predation serves to shape zooplankton community composition and size structure (Mills and Schiavone, 1982, Hobæk et. al., 2002).
The stability of predator-prey dynamics often depends upon the presence of a prey refuge from predation (Sih, 1987). In a broad sense, a refuge is a strategy that decreases the risk of predation. One common strategy is the use of a spatial refuge (Sih, 1987). Seven possible refuges from predation have been identified for lacustrine zooplankton: gradients of light, temperature, and dissolved oxygen (DO), macrophytes or other physical refuges, open water interference refuges, behavioral refuges, and predator inefficiency refuges (Shapiro, 1990). Perhaps the most important physical-chemical refuge for Daphnia is a region with DO concentrations too low for fish survival (Shapiro, 1990). Avoidance of predation has been accepted as the ultimate reason for diel vertical migration (DVM) by many zooplankton species between these oxygen-poor deeper waters and feeding areas (Zaret & Suffern, 1976, Stich & Lampert, 1981, Gliwicz, 1986, Dodson, 1990, DeStasio, 1993). Tessier and Welser (1991) found that the presence of a refuge from predation was important in determining zooplankton species abundance and diversity. Refuge availability also plays a significant role in Daphnia population dynamics (Wright & Shapiro, 1990).
There are many possible ways to define the extent of hypolimnetic refuges available to zooplankton. Tessier & Welser (1991) defined a refuge as the thickness of the water column between the bottom of the epilimnion and the depth at which DO is less than 0.5 ppm (hereafter referred to as critical depth) . The controls on these two parameters will thus affect the presence and size of refuges for zooplankton in lakes. Thermocline depth is a function of lake morphometry (particularly fetch), wind strength, and turbidity (Patalas, 1984, Mazumder & Taylor, 1994, Kalff, 2002). Hypoxia is a natural occurrence in the hypolimnion of stratified lakes (Charlton, 1980) because respiration rates are higher than photosynthesis rates in deep, unmixed waters. Oxygen depletion can be greater in eutrophic lakes due to large amounts of organic matter sinking to the lake bottom and being decomposed, and although eutrophication can be a natural process, it is often accelerated by human activities (Wetzel, 1975). Deforestation increases runoff and results in more minerals being leached from soil, providing ample nutrients for algal blooms (Hargrave, 1991). The dumping of wastewater into aquatic systems and runoff from agricultural fields can also contribute to nutrient enrichment (Kalff, 2002).
It is clear that anthropogenic forces can cause or exacerbate eutrophication that results in hypolimnetic anoxia – one of the parameters determining refuge size. Human perturbations that increase the turbidity of water also can affect thermocline depth (Mazumder & Taylor, 1994), the second control on refuge thickness. Therefore, humans could have a significant impact on the size of the refuge available to crustacean zooplankton. Since refuge size has been shown to influence zooplankton community structure (Tessier & Welser, 1991, Bertolo et. al., 1999), human activities could potentially affect community structure as well. My goal in this study was to determine what factors influence zooplankton community and size structure in lakes in southern Michigan. In particular, I was interested in whether or not residential lakeshore development has an affect on zooplankton community metrics such as taxa richness and mean size.
All lakes used in this study were warmwater and located in southern Michigan. Study lakes were selected by first identifying all southern Michigan lakes that had a surface area between 4.0 and 81.0 ha and a maximum depth of at least 6.1 m. These selection criteria were used to reduce the influence of lake size (Dodson et. al., 2000, Kalff, 2002) and stratification pattern on zooplankton community composition and size structure. To ensure that the study lakes spanned a gradient of residential lakeshore development, lakes on the initial list were plotted on a map of public land ownership using a Geographic Information System (GIS). Lakes were grouped into three categories based on the amount of state- or federally-owned land surrounding their shores: Group 1 lakes were surrounded by two-thirds or more state land and were defined as low impact, Group 2 lakes were surrounded by between one-third and two-thirds state land and were defined as medium impact, and Group 3 lakes were surrounded by less than one-third state land and were defined as high impact. Three lakes from each impact group were selected for sampling. Data from an additional 11 lakes fitting the selection criteria and representing a range of human development were obtained from the Michigan Department of Natural Resources (MDNR). All data were collected using similar methods and were pooled for analysis.
All lakes were sampled between early August and early September from 2003 to 2006. Temperature and dissolved oxygen profiles were measured in the deepest basin of each lake using a YSI 600 QS-650 MDS water quality monitor. Readings were taken every 0.91 m until the thermocline was reached or the lake bottom was approached, in which case readings were taken every 0.30 m. Water clarity was assessed by measuring Secchi depth. In lakes with more than one distinct basin, profiles and Secchi depths were measured for each basin. A plankton net with a mesh size of 153 μm and a diameter of 0.13 m was used to make vertical hauls from each of four quadrants (approximately aligned with the cardinal directions) in each basin (Galbraith & Schneider, 2000). The mouth of the net was lowered to the critical depth. In lakes having no critical depth, the mouth of the net was lowered to approximately 0.91 m above the lake bottom in order to avoid stirring up sediments and sampling benthic organisms. Zooplankton were anesthetized in carbonated water and immediately preserved in a 70% ethanol solution.
Temperature and oxygen profiles were used to estimate a hypolimnetic refuge following the methods of Tessier and Welser (1991). They defined a refuge as the thickness of the water column between the thermocline and the critical depth. This definition was appropriate for this study because southern Michigan lakes are dominated by Centrarchids (particularly bluegill sunfish) that are unable to feed below the thermocline (Werner & Hall, 1977, Tessier & Welser, 1991) and because zooplankton typically are not found in water with a DO concentration of less than 0.5 ppm (Tessier & Welser, 1991). In lakes with more than one basin, thermocline depths and critical depths were averaged across all basins to obtain an average refuge thickness.
Human development was quantified by counting the number of houses located directly on the shoreline of each lake. Houses on artificial channels or across roads were not included in these counts. House counts were divided by shore perimeter to obtain a dwelling density for each lake.
Each zooplankton sample from each lake was subsampled and placed in a counting wheel to estimate taxa richness, relative abundance, and size structure. The first 50 zooplankton encountered in each subsample were identified and their lengths were measured using Image-Pro Plus imaging software. The remaining individuals in the counting wheel were counted. Most zooplankton were identified to genus using keys in Edmondson et. al. (1959), Balcer et. al. (1984), and Aliberti et. al. (2007). A few genera that were difficult to identify were grouped together. Skistodiaptomus and Leptodiaptomus were considered Diaptomus and Acanthocyclops, Diacyclops, and Tropocyclops were all considered Cyclops. Zooplankton lengths were measured from the top of the head to the end of the caudal rami for copepods and to the base of the tail spine for cladocerans.
Twelve different crustacean zooplankton taxonomic groups were identified across the study lakes (Table 2). Cladocerans, and calanoid and cyclopoid copepods were found in all lakes. Among cladocerans, Daphnia were most ubiquitous followed by Diaphanosoma, Bosmina, and Ceriodaphnia. Eubosmina, Chydorus, and the predacious Leptodora were relatively less common, occurring in only 3 of the study lakes. Among calanoid copepods, Diaptomus were most ubiquitous, being found in every lake, while Epischura were less common, occurring in only six of the study lakes. Among cyclopoid copepods, taxa in the groups Cyclops and Mesocyclops were very common, occurring in all lakes. Ergasilus were found in only five of the study lakes. A higher number of cladoceran genera appear in Table 2 because members of this group were more easily identified and were not lumped together as was done for copepods.
Measures of size structure appeared to be influenced by zooplankton community composition. Proportion of the community comprised of Daphnia had the strongest influence of any variable on mean length of all zooplankton (Figure 4) and mean length of cladocerans (Figure 5). Zooplankton mean length and cladoceran mean length tended to increase with refuge thickness, although these trends were not significant. Mean length of Daphnia was the only measure of size structure that was correlated with dwelling density (Figure 6).
Secchi depth appeared to be the most important abiotic factor controlling community composition and indirectly controlling size structure in the study lakes. This finding contrasts with Tessier and Welser's (1991) finding that seasonal zooplankton community change within a similar set of southern Michigan lakes was best predicted by the decrease in refuge size over the summer. The difference between these studies may be due to the time scale over which response variables were measured. Since the present study focused on refuge and zooplankton community characteristics for a single point in time, a hypolimnetic refuge may still be important in the study lakes if viewed across the growing season.
Across the study lakes, as Secchi depth increased the zooplankton community shifted to a cladoceran-dominated, and more specifically, a Daphnia-dominated assemblage. Since Daphnia-dominated zooplankton communities tend to have larger mean lengths (Taylor & Carter, 1997), this result is consistent with Stemberger and Miller's (2003) finding that mean cladoceran body length was positively correlated with Secchi depth. A possible explanation for this result is Daphnia's superior ability to reduce phytoplankton biomass through grazing (Dini et. al., 1987, Carpenter et. al. 1992, Cottingham et. al., 1997). Another possible explanation for the increase in percentage cladoceran with increasing water clarity involves the differences in feeding strategies between cladocerans such as Daphnia and copepods. The foraging efficiency of Daphnia decreases at very high food densities due to their complicated filtering mechanism (Wetzel, 1975, Starkweather, 1978, Rigler, 1961). In the study lakes with shallower Secchi depths, phytoplankton concentrations may have been too high for Daphnia to forage efficiently, allowing copepods to exploit the food base in lakes where Daphnia handling costs are high.
The results of this study also suggested that biotic interactions may be important in structuring zooplankton community composition. Percent of the community that was comprised of calanoid copepods was inversely related to percent cladocerans, Daphnia and cyclopoid copepods. These relationships could be a result of competitive effects between superiorly competitive cladocerans such as Daphnia and copepods (Brooks and Dodson, 1965, Gliwicz and Lampert, 1993). It could also be due to the complicated interactions described in Brooks and Dodson's (1965) size-efficiency hypothesis. Indirect effects of predation and trophic cascades could also be shaping the community composition in these lakes. For example, in the presence of bass, cladocerans tend to be more abundant, while copepods are more abundant when bass are absent (Turner and Mittelbach, 1990). Additional information on fish assemblages in the study lakes would be valuable in understanding the factors influencing zooplankton community structure.
Variation in zooplankton size structure across the study lakes was strongly influenced by community composition, in particular, the relative abundance of Daphnia. This finding supports previous conclusions that zooplankton assemblages dominated by Daphnia tend to have larger mean lengths due to the large body size of members of this genus (Taylor & Carter, 1997).
Dwelling density, the variable being tested for its effect on zooplankton community structure, was not significantly related to many parameters in this study. It was, however, negatively correlated with mean Daphnia length (i.e. lakes with a larger number of houses per km shoreline had smaller Daphnia). The reasons underlying this relationship are not clear. It is possible that people prefer to live on lakes with good recreational fishing quality and thus lakes with smaller Daphnia were lakes with high numbers of fish whose preferred food is large cladocerans like Daphnia (Brooks & Dodson, 1965, Brooks, 1968, Vinyard, 1980, Turner & Mittelbach, 1990). The observed decline in Daphnia mean length with increasing residential shoreline development also could be due to a refuge effect. If it is hypothesized that residential shoreline development changes limnological conditions in a way that would decrease the size or availability of a refuge from predation, since large-bodied Daphnia are preferred by fish (Brooks & Dodson, 1965, Brooks, 1968, Vinyard, 1980, Turner & Mittelbach, 1990) they should disappear first in zooplankton communities in lakes with more development. A positive trend was seen between refuge thickness and cladoceran mean length, but this relationship was not significant.
For the purposes of this study, it was assumed that the relative effectiveness of a refuge was positively related to its thickness (Wright & Shapiro, 1990). However, it should not be assumed that the presence of these refuges is an indication that DVM is taking place or that the refuges represent an area of decreased predation pressure (Wright & Shapiro, 1990); if this were the case, refuge availability and size would have no impact on zooplankton community structure. There are at least four plausible explanations for why refuges may not actually be regions of decreased predation: firstly, zooplankton may not have been present in the refuges at all. Horppila et. al. (2000) found that most cladocerans in a stratified lake inhabited the epiliminion due to several factors, including a metalimnetic oxygen minimum and predation by the midge larva Chaoborus in the hypolimnion. While no metalimnetic oxygen minima were observed on the sampling days, Chaoborus was found in several lakes. Deep water refuges are less important in lakes with effective deep water predators (DeMott & Edington, 2004), so DVM may not have occurred in lakes where Chaoborus exerted predation pressure in the hypolimnion. Secondly, even if zooplankton were distributed throughout the water column, including in the refuge, this does not necessarily imply predator avoidance behavior – they could simply be distributed such that the most vulnerable individuals occupy the deepest layers. This phenomenon is related to food distribution; with homogenously distributed food it is not necessary for zooplankton to pay the energetic costs associated with DVM (Pijanowska & Dawidowicz, 1987). Thirdly, in conditions of extremely scarce food, zooplankton will expose themselves to greater predation risk by remaining near the lake surface to feed during the day and night (Johnsen & Jakobsen, 1987). Lastly, fish may feed below the bottom of the epilimnion and the refuge definition used in this study, therefore, may be inappropriate. In order to be confident in the definition used, planktivore distribution in the water column would need to be determined. Information on the abundance and vertical distribution of zooplankton, their food, and their predators within the entire water column would need to be collected in order to be certain that refuges are affecting zooplankton community structure.
The temporal component of refuges also must be considered. Thermocline depth and hypolimnetic anoxia change both seasonally and from year to year (Shapiro, 1990, Tessier & Welser, 1991). In lakes in southwestern Michigan, refuge size was reduced by 50% between June and August (Tessier & Welser, 1991). Zooplankton community composition and size structure may change as a result of changes in refuge thickness (Wright & Shapiro, 1990). Since my study lakes were sampled once during the entire season, only a snapshot of refuge availability and thickness was obtained.
The results of this study indicate that zooplankton community structure is affected by numerous, interrelated factors. Information on the distribution and community structure of both fish and phytoplankton populations would lend much more insight into the factors shaping the zooplankton populations in these lakes. It is possible that human shoreline development could have an important influence on any number of things that would in turn affect zooplankton, and future research should take a holistic ecosystem approach in investigating these connections.

Tsihrintzis (1997) found that in 1972 the Federal Water Pollution Control Act set standards to drastically reduce point source water pollution. Amendments to the Clean Water Act in 1977 and 1983 caused even further reductions in point source water pollution. As a result of these reductions in point source pollution, non-point source water pollution quickly became the major source of water pollution in the United States and continues to be the major contributor even today.
Due to this, in 1990 the EPA mandated that municipalities must develop monitoring programs for their storm water discharges (Tsihrintzis 1997). According to Bobrin (2000) the city of Ann Arbor, Michigan has developed such monitoring programs for its effluents into the Huron River Watershed. From 1995 to 2020, Washtenaw County is expected to see an increase in population by 28% and 26,000 more acres will become developed. This is likely to have a large impact on storm water pollutants if no action is taken. In previous investigations of the Huron River's quality by Wiley and Martin (1999) they have suggested that it may be in the early stages of ecological degradation due to an observed decreasing trend in invertebrate species diversity.
However, according to Bobrin (2000), the Ann Arbor-Ypsilanti Watershed Management Programs focuses on monitoring the river itself and its major tributaries without an emphasis on studying the physical storm effluents into the watershed. Studies assessing the storm drain effluents could help to better understand specific pollutants entering the watershed, as well as help to locate probable sources of the pollution.
The purpose of this study is to better understand the effects of pollutants directly entering the watershed via storm drains into the Huron River. Broccoli was chosen as a plant of study due to its high rate of germination and quick growth rate. Its function in this study is to represent a plant in the watershed. Damselflies were chosen as the animal of study due to their high sensitivity to pollutants and the fact they are naturally present in many aquatic environments, including the Huron River.
I hypothesized that, first of all, broccoli plants grown in differing concentrations of urban storm water will grow at different rates due to pollutants and toxins present in the runoff. I predicted that in this experiment, a greater overall biomass will be generated by the broccoli seedlings grown in the pure water, with a decreasing amount of biomass created by those plants grown in greater concentrations of storm water due to the growth inhibiting action of such pollutants as motor oil and metals. In addition, I also hypothesized that damselflies placed in varying concentrations of storm water will exhibit different lengths of survivorship also due differing levels of pollutants present in the storm water. I predict that damselflies grown in increasingly greater concentrations of storm water will die quicker due to the toxic effects of motor oil and metals.
I collected storm water in a 5 gallon jug from a storm drain effluent emptying into the Huron River near Depot St. and Broadway Rd. I collected the water during a rain event on October 4, 2007. I next diluted the storm water in one gallon jugs with tap water to 25% and 50%. In addition, 100% storm water was put into a jug and pure tap water was poured into another jug (later referred to as 0% storm water). The 0% storm water acts as the control in this experiment to contrast the effects of storm water against clean water on the growth of plants and animals.
I chose broccoli seeds as a plant of study to determine storm water's effects on plant growth. I chose them due to their high germination rate and fast growth. Ten broccoli seeds were placed into four separate 25 cm x 25 cm plastic planters with about 2.5 cm of potting soil and the seeds sowed 1 cm below the surface. They were watered exclusively with one of the four separate concentrations of storm water every other day for about one month. Eaching watering was about 200 mL. Due to logistical problems, I grew the plants in a 22 C apartment near a glass doorwall. I rotated the planters daily in order to help ensure equal amounts of sunlight to each planter.
After one month, to determine the effect on growth of varying storm water concentrations, I cut the seedlings at the surface and weighed them in order to find the total biomass generated in each of the four planters. Since not all 10 of the seeds germinated in each plot, I calculated the average single plant biomass within each of the four plots in order to standardize the data. Using SPSS stastical software, the average plant mass in each of the four concentrations was ploted against the concentration and a regresion analysis was conducting to determine correlation between plant growth and storm water concentration.
In addition, I harvested 20 damselfly larvae using a net and were taken from the pond at the University of Michigan Matthaei Botanical Gardens in Ann Arbor, Michigan. I filled four 6 oz glasses with each of the concentrations of storm water and five damselflies were placed in each of the glasses. I recorded their daily survivorship. I kept the damselflies in a 22 C environment, away from direct sunlight. I put a 1 g piece of bread in each glass at day 1 as a food source for the damselflies.
After all of the damselflies had perished, I analyzed the length of lifespan data. In order to avoid pseudo-replication, the average damselfly lifespan for each concentration was calculated rather than treating each damselfly as an individual data point. To determine the effects of storm water concentration on damselfly survivorship, the average damselfly lifespan for each concentration was plotted against the concentration level and a regression analysis was conducted using SPSS statistical software.
Finally, I used test strip indicators to identify the presence of oil, lead and phosphates. I dipped the strips in the 100% storm water concentration mixture and the strips were formulated to change colors upon the presence of each of the various pollutants.
A strong positive correlation is observed between increasing storm water concentrations and average broccoli plant biomass. The r-sqaured value is 0.942. In addition, this correlation is determined to be statistically significant as a 0.05 significance level since the p-value was found to be 0.029.
A moderatley strong negative correlation is observed between incrasing storm water concentrations and length of survival of damselflies. The r-square value is 0.813. This correclation is not statisticaly significant at a 0.05 significance level since the p-value was found to be 0.098.
The test strip indicators determined oil and lead to not be present in the storm water, though a moderately high level of phosphates were found to be presenet.
The first hypothesis that broccoli plants grown in differing concentrations of urban storm water will grow at different rates due to pollutants and toxins present in the runoff is supported by this study since each planter produced a different amount of biomass. However, the prediction that a greater overall biomass will be generated by the broccoli seedlings grown in the pure water with a decreasing amount of biomass created by those plants grown in greater concentrations is not supported by this study. The broccoli plants grown in higher concentrations of storm water produced a greater biomass. Since an R2-value of near 1 and a p-value of less than 0.05 was determined for this set of data, this provides strong evidence that this sample of storm water contained pollutants capable of increasing plant growth rates.
Since phosphates, possibly from lawn fertilizers, were observed in the storm water, it may be that lawn fertilizers present in the storm water caused the broccoli plants to grow at a higher rate at increasingly higher storm water concentrations. Previous studies of the Huron River by Bobrin (2000) have indicated that the biggest pollutants include suspended solids, phosphorus, bacteria, and metals. In addition, Wiley and Martin (1999) found that large algal blooms in Ford Lake, downstream from Ann Arbor, are frequently observed during heavy fertilizing periods throughout the year.
The second hypothesis that damselflies placed in varying concentrations of storm water will exhibit different lengths of survivorship due to differing levels of pollutants present in the storm water is supported by the general trend illustrated in this study. The data reflects a negative trend in which damselflies generally died quicker when placed in higher concentrations of storm water. However, the damselfly data are not statistically significant and thus further studies should be conducted to verify. Originally I had predicted the negative trend in lifespan would be due to increasing levels of metals and oil, though this study found no oil or lead present. Even though lead and oil were not detected, it is quite possible that other harmful metals and pollutants are present in the storm water since heavy metals can be correlated in watershed systems with the decline in invertebrate species diversity. Gray (2004) found that heavy metals, once inside the body of aquatic invertebrates, can accumulate and interfere with crucial biological processes such as metabolism.
One major error of this study is the lack of replicates. Ideally, multiple broccoli planters and damselfly sets should have been used for each storm water concentration rather than only one. This would have created a larger sample size and in the case of the damselflies may have provided statistically significant data. In addition, broccoli is not naturally occurring in the Huron River Watershed. Ideally, further follow-up studies should utilize native plants of the watershed. It may be that native plants react differently to some pollutants compared to broccoli plants.
Overall, this study illustrates that storm water does contain pollutants that are capable of effecting both plant growth and aquatic invertebrate survival. Even though this study was conducted at relatively high storm water concentrations that likely are not representative of average conditions in the watershed, it is possible that such concentrations are reached during large rain events and in areas of the watershed in close proximity to sewer effluents. As the watershed region becomes more urbanized, non-point source storm water pollution is likely to become more of a problem. Construction sites, fertilizers, automobiles, and pet wastes are some of the major contributors to decreased watershed health and all of these factors will likely increase as the region grows (Carpenter et al 1998). If polluted storm water can affect plants and animals, human exposure to polluted storm water in the watershed may be a danger as well, posing threats to the safety of those participating in recreation activities in the region.

The demands of large-scale human settlement, such as agricultural production and the building of urban environments, often come into conflict with the welfare of non-human natural systems. Human society requires a constant stream of resource inputs from the natural environment and also creates a constant stream of waste outputs that must be dumped into one environmental "sink" or another. One of the most problematic of these waste streams is sewage effluent, since it can cause significant damage to the environment into which it is dumped if left untreated, and it is inevitable. Thus, modern industrial societies since the nineteenth century have developed increasingly sophisticated treatment systems in order to at least partially mitigate the environmental damage resulting from sewage.
Of course, the impetus for sewage treatment has largely been government regulation, since the benefits of treatment (cleaner water, healthier natural systems and more robust fish populations) are widely distributed within a watershed and the costs are highly concentrated in the organization responsible for the treatment. These costs are significant, and no private entity on its own has an incentive to guarantee clean water for everyone else. In other words, the benefits of sewage treatment are public goods, and economic theory predicts that public goods will be inadequately supplied unless the government intervenes and requires a particular entity to provide them. Also, there is a question about exactly what minimum level of cleanliness is necessary to maintain a healthy environment, and government must set a common standard throughout a watershed.
The assumption in the case of sewage is that it must be treated in order to ensure the health of the local and downstream environment, and that some form of regulation is required in order to guarantee that a certain minimum amount of sewage treatment takes place. This lab, then, is designed to model the costs and benefits of selecting various sewage treatment options and the variables which affect that selection. By exploring the effect of these variables on the total costs to society from sewage treatment using a mathematical model, one can make better-informed decisions about which sewage treatment options are most cost-effective.
The system modeled in the lab is the case of a sewage treatment plant releasing effluent into a river. There are several variables considered in the lab: the design of the treatment plant, the strictness of regulation, the rate of river discharge, and the channel flow. The effects of manipulating each variable, independently of the others, are modeled in MathCAD.
There are four possible treatment plant designs to be modeled: secondary treatment (about 90% of [BOD] removed, with about 50 mg/L left); two different levels of tertiary treatment (which reduce [BOD] to either 30 mg/L or 25 mg/L); and a newer technology that reduces [BOD] to about 15 mg/L. The more that [BOD] is reduced, the more expensive the treatment plant is. All other things being equal, rational choice theory predicts that society would prefer the plant with the lowest total cost.
Regulation in the case of sewage treatment plants is assumed to take the form of a fine for non-compliance with water quality standards. These water quality standards require that the waste load in sewage effluent not cause dissolved oxygen [DO] in the river to drop below a certain threshold at which the ecosystem would be unacceptably damaged. There are two different fine levels, $5,000 per violation and $5,000,000 per violation; and also two different [DO] standards, 6 mg/L and 4 mg/L. The higher standard would be necessary to preserve all fish species in the river, whereas the lower standard would be required to maintain only smallmouth bass and carp.
The last two variables are river discharge rate and channel flow. These both affect the river's flow and therefore the [DO] available in the water. River discharge rate can either be set at twice the normal flow rate in order to model a wet year, or at half the normal flow rate to model a dry year. Channel flow, a measure of the slope of a river and therefore of water speed, can be set at 0.00001 to model the presence of a dam, or 0.0001 to model the removal of the dam.
The lab includes four different experiments, each modeling the effect of a different variable. In each experiment, one variable is tested at two different values, and the performance of each plant design is predicted at each of the two values. Plant performance is measured by the failure rate, i.e. the frequency with which the plant fails to meet the water quality standard. An annualized plant cost is then calculated, as a sum of capital and operating and maintenance costs plus the cost of any fines incurred as a result of failure to meet water quality standards. This total cost is used to rank the desirability of each design in each experimental situation. In Experiment 1, the effect of different fine levels is tested; in Experiment 2, the effect of different [DO] standards is tested; in Experiment 3, the effect of different river discharge rates is tested; and in Experiment 4, the effect of different channel flows are tested.
Since it would be extremely costly and time-consuming to go out into the field and perform all of these experiments in the real world, mathematical modeling with MathCAD of these different sewage treatment options is perhaps the most direct way to explore the question of sewage treatment. So long as the assumptions, variables, and model equations themselves are sufficiently sophisticated to produce realistic results, this approach is an excellent way to enable students to make conclusions about sewage treatment options using hard data. The only problem encountered during the lab was the minor hindrance of having difficulty with inputting values in MathCAD.
The lab itself yielded a number of important results. From the first experiment, it is clear that the level of the fine imposed on the sewage treatment plant has a large impact on the total cost of treatment. In this case, comparing fines of $5,000 and $5,000,000, the higher fine actually reverses the order of preference for the plant designs. Whereas with the low fine Design 1 yielded lowest total costs, with the much higher fine the lowest total cost is generated by Design 4 (Figure 1). This is because the cost of the fines in this case greatly outweighs the capital and O&M costs. Such a result suggests that, if it is concluded that it is critically important to maintain water quality at a certain level and keep violations below a certain level, a high fine will provide a powerful incentive to install the best technology for [BOD] reduction.
From the second experiment, one sees that even at a lower [DO] standard, the fine level of $5,000,000 still generates total costs such that Design 4 is preferable (Figure 2). One may have predicted that a lower [DO] standard would give results that make a less expensive plant design seem preferable, but that is not the case here. This may not be true at all [DO] standards -- for example, at a [DO] standard of 1 mg/L Design 1 will most likely be preferred -- but at least for the two values tested it appears that the effect of the fine level outweighs the effect of the desired [DO] value. Because of limited time in the lab, there is not very comprehensive data, so it would be hard to predict in general which parameter, fine level or [DO] standard, would have the greater effect on total costs for each increment of change.
The third and fourth experiments provide a different sort of information than the first two experiments. Rather than modeling the effects of parameters imposed solely by humans, they model the effects of natural parameters (i.e. local conditions) over which humans have little or no control. The third experiment shows that, though the flow rate does not affect the order of preference of the plant designs, it does affect the total costs (Figure 3). Thus, the total cost to society will change based not on difference in human choices but on uncontrollable natural conditions -- a drought year will make it more difficult for any plant design to maintain water quality. If there is still a preference for the best technology (Design 4) under this scenario, it may make sense to lower the fine in drier years, because the purpose of incentivizing the optimal technology has been achieved and keeping the fine high will raise costs unnecessarily.
The fourth experiment demonstrates that the channel flow and water speed have a significant impact on the water quality. With a shallow slope and slower water speed, the most expensive plant design is still preferable (Figure 4). A steeper slope and faster speed in this experiment meant that no violations occurred with any of the plant designs -- in other words, the choice of plant design would have no impact on water quality and therefore one could choose the lowest-cost design (Design 1) without any negative environmental consequences.
This lab vividly demonstrates the impacts of various variables on the desirability of different sewage treatment plant designs. Before gathering data, one may have drawn widely divergent conclusions based on certain assumptions. For instance, one might have assumed that the more expensive plant design (Design 4) was always best, because cleaner water is always worth the cost. The results of the lab show that this is not always the case. Assuming that the goal of sewage treatment is to optimize cost/benefit ratios for both human and natural systems (i.e. optimize the cost/benefit ratio for the whole human/non-human system), it is not always necessary to choose the most expensive treatment option, because this may impose unnecessary costs on the human system which provide little benefit to non-human systems, creating a sort of "deadweight loss." In certain cases, one can maintain a high level of water quality even with the cheapest treatment plant. Experiment 4 suggests that one must first examine local conditions in order to determine what water quality standards are necessary to maintain environmental health, because a stream with a steeper slope and faster water speed will require less stringent regulations on water quality.
The thesis that a regulatory system with substantive, punitive fines is essential to the protection of ecological values in our rivers is not always true. Fines are one way to create a disincentive for disposing of untreated human sewage, but another method may be for downstream users to pay the upstream sewage treatment operator to maintain clean river water. Besides financial incentives like fines or ecosystem services payments, the government may choose to set water quality standards and institute a criminal penalty (e.g. prison sentences) for failing to achieve them. Though it may not be very practical, one could also institute a cap-and-trade program for sewage dumping within a particular watershed. Thus, there are other methods besides punitive fines for maintaining the ecological value of rivers.
If one does choose to use fines, Experiment 1 shows that they must be set high enough to overcome other costs. Low fines are not any more effective than no fines at all. Experiment 3 demonstrates that it would be wise to allow for an adjustment of fines based on environmental conditions like flow rate. As mentioned above, in a drought year even the most advanced treatment plant design will have a higher violation rate and this will create unnecessarily high costs for treatment through more frequent fines. The goal is to create an incentive for the most cost-effective way to maintain environmental quality, and so once the fine level has led to the adoption of the best technology, it is no longer cost-effective to allow the cumulative amount of fines to increase further. Flexible fine levels, which would decrease in dry years and increase in wet years, would maintain water quality more cost-effectively.
Through all of these experiments, though, it is clear that water quality will not be maintained unless local sewage treatment operations are subject to some form of external regulation, either through government-imposed penalties or private ecosystem services payments. Regulation, then, in one shape or another is essential to the protection of ecological values in our rivers.

To use arcGIS to create a future land use map that appropriately allocates development, agriculture, and nature preserves, protecting biodiversity in northeastern Washtenaw County.
Habitat loss has been described as the primary threat to the loss of biodiversity at the landscape, species, and genetic levels (Wilson, 2002). In northeastern Washtenaw County, biodiversity is becoming more threatened as the area continues to face rapid economic growth. Land use for development and agriculture has led to the fragmentation of nature reserves throughout the area. Such fragmentation is seen by many landscape architects as a major issue for the protection of habitats; small habitats are found to be more homogenous than larger habitat in terms of soil content, species populations, and landscape character (Collinge, 1996).
Upon the observance of a 1995 land use map, I decided that an allotment of land conserving natural areas with fair distance from the urban growth of Ann Arbor should be a priority in land allocation. Since the three land use types influence each other through numerous and often unquantifiable interactions, each land use type was carefully considered. A target for allocation of developed, agricultural, and natural area was provided, with each land use type to equal 10%, 36%, and 54% of available land, respectively. This means that future land allocation would equate to 31.5% developed, 48.1 % natural, and 20.4% agriculture. With these goals in mind, I was able to produce allocations of these three land uses, while maintaining a commitment to the minimization of habitat fragmentation.
This study followed the directions provided in the lab handout, which can be summarized as a six step process. My first task was to use the vast compilation of maps and raster grids provided for this experiment to create a series of sub models for each land use type. In arcGIS, ModelBuilder and the Spatial Analyst tool box were used to quantify the criteria for each of the three land use suitability maps; all recommended criteria were used. Each criterion involved a model that produced a score (1-100) to be used for suitability mapping. The second step utilized the criterion to create the suitability maps in ModelBuilder. Each criterion was assigned a weight through the website provided in the lab handout; weights can be viewed in Tables 1, 2, and 3, below.
Third, weights were distributed with environmental protection as the primary objective. Water sources were secluded from agriculture and development. Since wetlands are a unique habitat that has been diminished in the state of Michigan, it was important to allocate natural areas around wetland habitats. In Table 1, it is clear that clay content was a low priority, since the data source was deemed unreliable (improper scale). In both agriculture and development, slope was a priority because of the prospect of erosion, which directly ties into the aim of environmental protection. Upon the creation of the weighted suitability maps, I discovered that each land use type had different ranges of scores (all within 1-100). To eliminate the prospect of preferences in the proceeding steps, each range was sliced into values from 1-100, through the equal area setting, giving each land use type equal value. I considered giving a preference to the natural land use type since it had the largest target area, but I was most concerned with grouping natural areas together to minimize fragmentation. If natural area was allowed to have a higher score range, the resulting allocation map might prescribe natural preservation to areas that are better suited today for agriculture or development
Once the three final suitability maps were created, I used the Band Collection Statistics tool to generate Table 4, which displays the correlation between each land use type. I proceeded to perform the land allocation through arcGIS's Highest Position tool, as directed in the lab handout. The final step involved the displaying of results through a comparison to the 1995 land use map, provided. This comparison was also carried out though the use of the various recommended spatial analyst tools.
Table 4 illustrates the negative correlation between each land use type. Figure 1 shows two land use maps: "1995 Land Use" is a reclassified version of the provided raster dataset, while "Allocation for Future Land Use" illustrates the results of this lab's land use redistribution. Between the past and future maps, land allocated as nature preserve decreased by 4.6% of total land, agriculture decreased by 7.8%, and development increased by 15.4%.
The land allocation model produced results that are surprising on several accounts. First though, the negative correlations illustrated in Table 4 communicate the fact that none of the land uses were conflicting. An analysis of Figure 1 proves that the modeling technique was relatively successful at minimizing habitat fragmentation, while completely missing the targets of future land use allocation. Many nature areas were clumped into larger habitats. As expected, most of these areas were distant from development, since agriculture was weighted to be closer to urban areas. Coincidentally, habitat corridors were common, strengthening an attempt to maintain biodiversity. Corridors are beneficial, providing access for mobile terrestrial species to large habitats relatively far away. However, they bring completely unintended benefits to the allocation strategy, likely attributed to the value of streams and tributaries in the suitability weighting. Unfortunately, this model did not erase habitat fragmentation, as there were many small areas placed as natural preserves that would not be able to support healthy population sizes of organisms in need of a large habitat area.
Land for natural preservation was short by 13.5% of overall land, while development was 4.6% steep and agriculture was 8.8% steep. Land allocation would have been closer to the target were more of the agricultural areas allotted for natural preservation. I stumbled across a more appropriate allocation when I neglected to rescale the agricultural score, which ranged from "0-59". Since the highest areas were still much lower than the "1-100" scales of the other two land uses, much of the land that I present as agriculture was natural or development. To maintain consistency I sliced agriculture to a scale of "1-100".
With the information that this model provides, it is easy to see that many patterns are at work, with and against each other, in ways that cannot be observed until the model is ran. Development virtually took over the southwest side of the study area since this is the closest area to Ann Arbor, while insulating the major roads crossing through the study area. The small natural areas may be indicating the presence of wetlands and tributaries. These patterns, and many more that go unnoticed after a quick glance, could not be easily predicted without running the model itself. If this weren't a fictional situation it would be necessary, and relatively painless, to edit the criteria and tweak the weights. To ensure distance between natural preserved areas and development it would only seem wise to use this as a major criterion (however this would inhibit the planning of city parks). Since the recommended agricultural area is so small, it would be necessary to reduce the "1-100" scoring scale to "1-80" or "1-75".
Given the stated objectives, it would be important to make these changes, as well as limit the amount of development by 6-9% overall land allocation. Perhaps another model could be run based on this model that works to allocate land in the developed area for parks and recreation. The 1995 land use map indicates a number of areas for outdoor recreation, so the model could use the Euclidean Distance tool to determine where these areas could be appropriately expanded, so that not to infringe upon other existing development. Adding or subtracting criteria, establishing more model parameters, and shifting weights are all ways that arcGIS makes the editing process of modeling very quick and simple.
The model building function of arcGIS is invaluable to the formulation of land use allocation maps, because of its data processing capacity and flexibility. With the ability to run a model, display the results, and enable the user to edit and run the model again, this is a vital tool for land use planning, especially with specific goals, such as the protection of biodiversity.

The Mount Graham red squirrel (Tamiasciurus hudsonicus grahamensis) is one of twenty-five subspecies of North American red squirrels. The Mount Graham red squirrel separated from other subspecies about 10,000 years ago. Studies have shown that it is genetically different from other red squirrel subspecies in North America. The Mount Graham red squirrel is only found on the Piñaleno Mountains in southeastern Arizona. As of present, the squirrels are known to occupy mixed conifer and spruce-fir habitat zones on the mountains. More specifically, they only inhabit mountain zones from 8,700 feet and up (Mount 2006). Because of this limited habitat range, the Mount Graham red squirrel has a relatively long history of susceptibility to environmental variability.
The Mount Graham red squirrel has been a conservation topic for several decades. Believed to be extinct in the 1950s, it was again sighted in the 1970s and added to the Federal Endangered Species list in 1987. There is no known data regarding the population of the squirrels prior to the mid 1950s. In the late 1980s, biologists believed that the squirrel could only occupy spruce-fir habitats in high elevations on the Piñaleno Mountains (see Figure 1). The U.S. Fish and Wildlife service assigned these higher areas as Mount Graham red squirrel refugium in 1988. The refugium requires a permit for people to enter the area. More recent research has shown that the Mount Graham red squirrels also occupy slightly low elevations. This raises questions as to the effectiveness of the refugium and the need for possibly expanding its boundaries (Mount 2006).
Currently, the Mount Graham red squirrel habitat covers approximately 6,460 hectares of Piñaleno Mountain upper elevations. Some biologists believe that competition with the introduced Abert Squirrel has resulted in some of the Mount Graham red squirrel's population decline. In addition, the loss of habitat due to the development of a university observatory, roads and other facilities are believed to have impacted the species population as well (Arizona Game 2003). Previous logging throughout the 1800s and 1900s also contributed to substantial habitat loss (USDA Forest 2001). The 2007 Arizona Game and Fish Department survey data estimates their population at approximately 299 individuals as seen in Figure 2. Data is collected by studying the activity rate at known areas where the squirrels store their cones (also called middens). The population data collected from 1991 show a sharp rise and decline in population between 1998 and 2000. This is believed to be caused by a range of factors threatening their habitat such as drought, poor cone crops, fire, and insects. After this point, their population appears to oscillate around 250 individuals (Arizona Game 2007).
Since the population size and habitat range of the Mount Graham red squirrel is so restricted, research is somewhat limited on its life history traits. Despite this, several physical and reproductive characteristics are known (Table 1). The squirrel's average length is 13.3 in and its average weight is 8.3 oz. Studies estimate that their breeding season lasts from January to April. Mount Graham red squirrels use leaves and twigs to build nests in snags, hollows of living trees, logs, underground, or in the branches of trees. Sometimes they may even use pre-existing tree holes built by other animals like woodpeckers. Thus, squirrel reproduction is very dependent on the presence of trees or appropriate nest habitat (USDA Forest 2001).
A female's first reproduction occurs after her first winter. The proportion of yearling and adult squirrels that breed is not consistent on a year by year basis, but the adult females always reproduce at higher rates than the yearlings. The squirrel's gestation lasts from 35-40 days. It is believed that the Mount Graham red squirrels have two litters per year. Each litter usually consists of three young. Once born, the squirrels are nursed in the nest for 6-8 weeks. Only the mother gives parental care and she begins to wean the young at 7-11 weeks (USDA Forest 2001).
The Mount Graham red squirrels also exemplify territorial behavior. They establish territory by using piles of their meal "leftovers" (cone debris) to build middens around their nest. These middens are used for storing food. Generally, middens are located on or around trees or logs that also function as the nest and protection from predators. Middens are almost always occupied by only one squirrel. Even after breeding, the females force the males out of their midden/nest area (USDA Forest 2001).
The juvenile mortality for Mount Graham red squirrels is about 67%. This generally occurs due to the harsh effects of winter between weaning and first reproduction. On average, Mount Graham red squirrels only live to age two to three. Survival rates are believed to vary according to the availability of closed cones which are their winter food source (US Fish 1993).
Examining life history is an important part of establishing good management plans for threatened or endangered species. In order to examine the life history of the Mount Graham red squirrel, a Leslie Matrix was assembled to study age specific survival rate and fertility. Data was used from previous research determining the red squirrel's demographic parameters (Buenau and Gerber 2003). The average age-specific survival rates and fertility for juveniles, squirrels in year two, and squirrels in year three or higher were used to construct the Leslie Matrix (Figure 3). This information was then used to develop a life-cycle diagram (Figure 4).
The Leslie Matrix was analyzed in Microsoft Excel through the PopTools Matrix Tools Basic Analysis. This software allowed for the calculation of valuable life history information. The largest Eigenvalue, 0.61, provided an estimation of , or the per capita geometric rate of increase. In addition, the per capita exponential growth rate, r, was calculated to equal-0.48. Thus the growth rate was negative for the red squirrels. The net reproductive rate, , was 0.18. The Eigenvector demonstrated that a stable age structure for red squirrels would consist of 38% year zero, 21% juveniles, 19% year two, and 22.4% year three and up.
The analysis of the Mount Graham red squirrel life history reveals information that could help shape effective conservation management. As Figures 3 and 4 indicate, the Mount Graham red squirrels exhibit higher fertility later in life. Thus it is important for them to be able to survive past juvenile stages in order to maximize their reproduction. Unfortunately, as the figures indicate, juvenile squirrels have the lowest age-specific survival rates at 0.33 while older squirrels have a much higher survival rate, 0.73. The primary reason for this low juvenile survival rate is the impact of wintering (US Fish 1993). If juvenile squirrels have not acquired sufficient winter food storage or physical strength to withstand harsh winter weather, they may not survive the severity of the season. Thus they will not survive to reproduce later in life. Management can play a role in helping juvenile survivorship by preventing the deforestation of trees or the destruction of habitat where Mount Graham red squirrels build middens to store food for the winter. Since biologists now know that the squirrels occupy both mixed conifer and spruce fir zones on the Piñaleno Mountains, managers could enforce specific limitations in these areas. Although there are already permit restrictions in the spruce fir zones, it could be very important to expand those restrictions to mixed conifer zones within the elevation where the Mount Graham red squirrel is known to reside.
The basic matrix analysis also provides information relevant to management. Since the per capita geometric rate of increase is less than one, it the model indicates that population is decreasing. The negative per capita exponential growth rate also suggests this. While the fact that the population is decreasing may not provide much guidance in itself to managers, it does offer evidence that protecting this subspecies should be a priority since the population is clearly decreasing. The matrix analysis also provides the net reproductive rate. This rate indicates that an estimated 0.18 daughters are born to each female. This is a very low number considering that females have an average three young per litter and also have a relatively short parental care period. This can again be attributed to habitat loss. In order for Mount Graham red squirrels to mate, they need a nesting site. If their habitat is disrupted or destroyed for development, the squirrels cannot easily create nesting sites.
Finally, the analysis offers a look the age distribution within a stable age structure for the Mount Graham red squirrel. The structure suggests that the highest percentage of squirrels should be from zero years to juvenile. Since a large number of these young squirrels do not survive the winter, more young squirrels are needed so enough survive to be able to reproduce in the spring. Although research sources with information regarding the Mount Graham red squirrel's current age distribution could not be located, the stable age structure information could still be valuable to conservation managers. Knowing that a stable age structure would have more young squirrels than other ages informs managers that they need to find ways to protect young squirrels. Since the most important threat to the young is wintering, managers could ensure that plenty of closed cones (the squirrel's winter food source) are available for young squirrels to collect and store. They could also ensure that middens, where the squirrels store their winter food source, are not disturbed by humans. In addition, managers could purchase property that has already been developed and replant the native trees that red squirrels rely on for their habitat.
While it may still be possible to successfully carry out a recovery plan for the Mount Graham red squirrels, I would think the impact of such a population decline would not leave them unaffected. More specifically, it could be helpful to conduct research regarding the remaining genetic diversity within the Mount Graham red squirrel gene pool. I would imagine that much of it has been lost since their population is now down to only a few hundred squirrels. This research could also be helpful in determining if there are any genetic traits that make the Mount Graham red squirrel more vulnerable to habitat destruction than other squirrels. It might then be possible to genetically engineer these traits so as to enhance the survival of the Mount Graham red squirrel. Without accounting for genetic diversity, a recovered population of squirrels could be even more susceptible to extinction due to their genetic homogeneity.
Habitat loss results in detrimental biodiversity threats around the world. This is no different in the case of the Mount Graham red squirrel. Because of its limited habitat range, it is very dependent on a safe and undisturbed habitat. Logging, road construction, recreation, and other human activities have stressed the squirrel's already limited habitat. By cutting down trees and building roads, humans are removing the nesting sites, food sources, and habitat corridors that the Mount Graham red squirrel relies upon for survival. The life history of the red squirrel indicates that a suitable habitat is especially important for juvenile squirrels. They are the age group most vulnerable to wintering, and need to be able to find food and storage areas for surviving the winter. If low proportions of juveniles survive, then there will be less left to reproduce, and the population decreases. This seems to be the current situation as the population is reduced to no more than several hundred individuals. In order to prevent their ultimate extinction, strict habitat protection policies need to be enacted and enforced. In addition, research regarding the genetic diversity left in the Mount Graham red squirrel population would help develop an even better understanding of the severity of their situation. Although past habitat destruction has caused tremendous problems for the Mount Graham red squirrel, there is still hope in the species recovery. Previously thought to be extinct, the Mount Graham red squirrel has proved it has some resilience in surviving such extensive habitat damage. If managers work to protect the habitat and encourage the growth of the Mount Graham red squirrel, it may be possible for the species to recover.

The level of dissolved oxygen (D.O.) in rivers is a main variable in assigning the streams health. D.O. is used by all aquatic organisms, most as the only means of available oxygen. The amount of dissolved oxygen, usually measured in parts per million (ppm) is one of the main determining factors of what type of biota will live in the rivers and streams. Many big game fish, such as trout, are very sensitive to D.O levels, and require high levels to survive. The pollution put into river systems consists of excess nutrients, and raw sewage full of bacteria.
The current pollution problem in many streams and rivers are caused by sewage plants dumping too many nutrients into the water. Because water treatment plants need someplace to put the treated water, most industries are found near natural stream and rivers. However, the efficiency of these sewage treatment plants varies greatly. How efficient a treatment plant is, is determined by the amount of bacteria and nutrients that is put into the river as "treated" water. The nutrient rich byproduct from sewage plants entering the river increase the bacterial levels in the river, and thus increase the biological oxygen demand (B.O.D.) The B.O.D. is the amount of oxygen needed for bacteria to decompose waste. Increasing the concentration and overall amount of bacteria will raise the B.O.D. If the demand for oxygen is increased, all the oxygen in a reach of a stream, near the pollution site, will be used up by the bacteria, leaving nothing for the other organisms, mainly fish, and microorganisms to use. Healthy rivers will be able to absorb a fair amount of nutrients and bacteria before the oxygen levels drop to dangerous levels. However, sewage treatment plants tend to dump massive concentrations into the river, which it cannot process in a short period of time.
The problem addressed in this essay is to see how different factors will lead to building treatment plants of different efficiency. Also, looking at the system between which design of treatment plants will be build, the cost to the city members, and the cost to the environment. In this experiment four general designs of plants are available to build. The designs are numbered one through four, and vary in efficiency, one being the least efficient and four being the most efficient at removing bacteria and nutrients from the treatment water. Another key factor when looking at the efficiency of the different plant designs in that the more efficient a plant design is the more expensive to build it is for the people of the city. This brings up the common problem of saving money or doing what is right for the environment. In writing this essay, the topic of saving money going against caring for the environment will be a central discussion point. In discussing this topic, a few variables such as the amount of a fine, as well as setting the minimum ppm standard will be included. I predict that is using a ranking system that places priority on the cost per person and not the cost to the environment, high fines will be needed to preserve the river ecosystem.
Using the computer program Mathcad, our group set up, using mathematical equation, a chart where we could find the cost of building different designs of sewage plants, relative to different variables.
Failure rate indicates the number of days, per year that the D.O. level of a stream, that the sewage treatment plant is on, drops below the standard 6ppm. Dropping below this standard rate will result in a fine for that day. The higher the percentage of the failure rate, the more fines that specific design will cost the people of the city. Cost per person is used as a price set, due to the setting of the people of the city having the decision of which plant to build. For this study, we will assume that the people of the city will always choose the cheapest design, even if it is not the best environmental choice. The final column is for ranking the sewage treatment plants in order of cost, 1 being the least expensive, and the choice picked by the city, and 4 being the most expensive.
Four main types of experiments were run in lab. One experiment used the effects of fines to rank which sewage plant design would be cheapest per person. Two limits of fines were used, one setting the fine for going under the standard D.O. at five thousand dollars everrry day of failure, and another test run setting the fine at five million dollars. Another variable looked at in our system was one of setting the standard of B.O.D. at 4 ppm, rather than 6 ppm. This lowered the amount of pollution the sewage plants would have to correct for, and thus had an effect on the price and ranking of each sewage design. Other variables included the effects of a wet or dry year on design cost. Also how a dam, and dam removal would affect the cost ranking of the different sewage designs.
For experiment number one, two different fine levels were set and then run through our mathcad system in order to rank the sewage plant designs in order from cheapest to most expensive per person to build and maintain.
In the above table the failure rate is highest for design one of the sewage treatment plants. This is a way of indicating that design one is least efficient in removing bacteria from sewage waste water. The higher the bacteria levels in the incoming waste water the more it will drive the D.O. level down, increasing the number of days of failure. When comparing the cost/ person and rank the table must be broken down further into A and B sections. In section A, where the fine for failure to meet the standard 6ppm D.O. level is five thousand dollars, we find design one is least expensive, and design four most expensive. This is due to design four being more expensive to build then the less efficient design one. The cost of the fines for the increased number of days for design one, is not enough to outweigh the cost of the more expensive plant design. In section B, of Table 2, the cost / person is more for design one, then design four, opposite from section A. In this case with the fine being extremely high at five million dollars, the cost of the more expensive plant, design four, is much less than the cost of paying the fine more often, in the case of the high failure rate of designs one.
Another variable viewed in the lab experiment was the effect of lowering the minimum D.O. level from 6ppm to 4ppm. Table 3, shows the results.
In table 3, lowering the minimum standard ppm from 6 to 4 ppm has a reversal effect on the ranking of the sewage treatment designs. Lowering the minimum standard ppm lowers the impact that the sewage treatment plants have to counteract. With lower standards less must be put into the efficiency of the plants to meet the limit. This lower limit is most noticeable when comparing the failure rates of similar designs in section A, and section B. The lower standard limit lowers the number of failure days of each design. Special note is added to design four in the 4ppm standard section where the efficiency of the sewage treatment plant, allows the plant to have a failure rate of zero.
The other two experiments; wet and dry season, as well as a dam, and dam removal, are not vital to my discussion so the results of those experiments will not be explained at this time.
The overall focus of this experiment is to discuss whether or not substantial fines are necessary to protect the ecological values of our rivers and our systems. In my opinion substantial fines are necessary to protect the ecological value of our river systems. Substantial fines are needed to force sewage treatment plants to take accountability for their waste deposits, and also for the members of the city to provide funding for the best water treatment possible.
When looking at the results from the fines experiment, when fines are low (five thousand dollars) the cost per person is lowest if the least effective treatment plant is built. When determining the cost per person, two values must be assessed. The first value is the basic cost of the treatment power plant, which is dependent on how efficient it is. The second cost is the cost of the fines given to the treatment plant for failure to meet the set standards of nutrient levels, which effect B.O.D. In the case where the fine amount is low, the less efficient plant is still cheaper to build then the more efficient plant, even though it is fined nearly four times as much. The price of the more efficient treatment plant (design four), even though it has a failure rate of .258, compared to the .992 of design one, is more expensive to build. In ranking the designs by cost per person, we are saying the determining factor for how well we care for the river system, is the cost per person of the treatment plant. This is not necessarily the best way to rank treatment plants, if caring for the river ecology is one of our main goals. Luckily, we can change the severity of the fine, and force the city to choose a more ecological friendly choice. As seen in the results, when the fine amount for minimum dissolved oxygen failure is increased to five million dollars the ranking of sewage treatment plant designs is reversed. In this case the high percentage of fines of design one, outweighs the high initial cost of the more effective design four. Although the cost per person is higher in every category then when the fine was five thousand dollars, when ranked by cost per person, the most efficient treatment plant is chosen, and our rivers will be cared for much better. This portrays when ranking treatment designs strictly by cost per person when considering installation, the environment takes a back seat to money in the pockets of the city members. But in making the most environmentally sound choice the most affordable, through heavy fines, doing what is right for the environment can be chosen through this ranking system.
It should be noted that smaller fines are not always bad. In this system of the city, treatment plants, and the river ecosystem, lower fines have benefits for the city. A lower fine means less cost per person in the city, which is always a good thing. In this particular example, however, we are pinning what is good for the pocket books of the people directly against what is good for the environment.
The other experiment that shows how environmental factors take a back seat to money in the city members pockets is the experiment done adjusting the minimum dissolved oxygen ppm level. Although the ranking of the different treatment design systems stay the same for both experimental findings, interest should be taken in the costs. When the minimum ppm level is dropped to 4ppm, the costs to meet this standard are dramatically lowered, when compared to the 6ppm minimum standard costs. In the real world, industries will always do just enough to stay above the failure rate. Even if dropping the minimum standard to 4ppm, instead of 6ppm, would have drastic effects on the river ecosystem, the treatment industry would not care, because it is cheaper to meet the minimum standards for 4ppm, then 6ppm levels.
The ranking system in this group of experiments always placed more emphasis on the price per person, then the consequences to the environment. Sadly, this is the way that the real world works as well. If tables were made ranking different situations by their total negative effect on the environment, many findings from this lab would be turned completely around. However, in what may seem like a system setting up an environmental crisis, there are ways to set standards for environmental care. If we set fines or consequences, on environmental harm, higher than the amount saved by doing the cheapest design, then the cost per person ranking system will pick the most environmentally friendly choice. In this aspect high fines are a necessity for preserving our river ecosystems, and our environment in general. Finding the right amount to fine is another story. In the above experiment, it is obvious that a five million dollar fine would never work in the real world. As long as we keep the combined fine amount above the overall cost difference between environmentally friendly choices and those that may harm the environment, we will be able to preserve our fragile ecosystems, in a world where everyone is looking for the cheapest alternative.

The Florida Panther, Puma puma concolor , is a large predatory cat which has an average length of around 7 ft. including the tail. Females weigh in general 35-45 kgs, while males are larger by about 15-20 kgs. The coat is generally dark brown, with light spots of darker brown, almost black markings. Small patches of white hair can be seen on front section of the body (head, and shoulders). Florida panthers used to have a range that included all of the Southeastern United States. Due to human interaction and development, range has been reduced to small isolated areas in southern Florida. The type of habitat that Florida panthers usually seek is a combination of forests and swamps, with heavy vegetation. A low reproduction rate is one of the reasons that Florida Panther is in need of conservation. With adults not becoming reproductively active until the second or third year of life, and parental care lasting one and a half years, reproduction rates are not fast enough to keep the present populations stable. With the long parental care time period, this forces female to breed only every other year, further decreasing the reproductive rate. The Florida Panthers have been on the endanger species list since 1973. Along with habitat reduction and fragmentation, other human impacts include hunting, and killing the panthers as to protect livestock. (1)
Florida panthers genarlly live to be 3-5 years old, in the wild. For our simulation we only accounted for age classes of newborns, first year, second year, third year, and four year individuals. There was a record for five year old individuals, however in our simulation, no individuals ever survived to an age of five.
With these numerous problems facing the declining populations of the Florida panther, action must be taken if we don't want to lose this vital species. Many different conservation techniques could be used in order to try and strengthen the populations of the Florida panther. One method would be to try and connected the isolated populations of the Florida panther populations, in order to gain more cross breeding between sub populations, as well as give the panther more room to hunt, and gain less interaction with humans. This method would work, however given the tight hold that development has in Southern Florida, converting land back to panther habitat is not very likely.
Another method would be to bring in panthers from other regions to increase the number of panthers in Southern Florida. This would also increase the genetic diversity of the populations of the Florida panther, and strengthen the populations. As with the first method, importing panthers also has a downfall. Bringing in other non-Florida panthers would remove the pure Florida panther sub species, once outside genes were brought into the population through breeding. Losing the pure Florida panther species is a concern to many.
In order to have these methods work more effectively in helping the Florida panther, we must understand it's life history and how factors affect the population structure and stability. The life history of the Florida panther, for this particular lab, can be summarized in figure 1 below.
In figure 1, the values along the bottom are the survival rates of one year of age from the previous year (Px). For example the 0.8 between 1 and 2, show that the probability that a one year old panther will survive to be two years old is 80%. The curved lives near the top of the figure are the fecundity of each age class within the panther population (Fx). These numbers show how many offspring will be produced from that specific age class, per individual of that age class. For example the curved line from circle 3 to circle 0, shows that for every panther that is 3 years of age, 0.38 newborns will be produced.
We can combine this information with the Florida panther's life history traits, to better understand how to help increase the population. The Florida panther is a k-selected species, in that it is slow to evolve to changes in its environment, due to slow reproduction rates. As mentioned above, the slow breeding rates, and long term parental care, are two key examples of a k-selected species.
In the lab we used a Leslie Matrix to see how exactly changes in life history traits of the Florida panther will affect the populations stability. Using the numbers presented in figure one, we constructed the Leslie Matrix seen in table 1.
The Leslie matrix was generated from the age, lx, mx, Px, and Fx values given to us at the beginning of the lab. The vector for this lab was set at each age group beginning with 25 indivduals.
Using the present matrix as basis analysis was run and found values for r (rate of population increase) Ro (expected number of replacements) and T (generation time.)
Along with a basic analysis, a projection was also run. The projection was used to see how the population as a whole, and individual age classes would fluctuate during the next ten years. Each age class began with 25 individuals, and any numeral value under one, was considered to be extinct / removed from the population.
Elasticity was also calculated using the values generated by the Leslie matrix. Elasticity, which is a values relative impact on the overall system, was calculated for each age class survivability (Px) and fecundity (Fx). The higher the elasticity of a value the more impact it will have on the entire system.
The original Leslie matrix was used to generate basic analysis, projection, and elasticity values. Next the matrix was changed in one area, in this case P1, and the analysis was run again. For a third and final time, more numerical values were changed in the matrix, and the outcomes were viewed and compared to the first two trials.
In using the original Leslie matrix (Table 1) we calculated the r value to be -0.37012, a Ro value of 0.353, and a T value of 2.8133. The projection for the population, starting with 25 individuals of each age group, found the population unstable. After ten years, no single age group had a concentration over 1, which indicated that all age groups, and therefore the population was extinct. The elasticity of the first analysis found the P1 and P2 values most elastic at values of .34 and .23.
For our second analysis, the P1 value was changed from 0.5, to 0.8. For a real life example, this would mean that the survivability of newborns to first years had a slight increase. This small alteration of the Leslie matrix had an effect on the analysis, projection and elasticity, but was not significant. The r value was still negative at -0.206, the Ro value was 0.56, and the T value was decrease to 2.77. The elasticity was still greatest for the P1 and P2 values, at .35 and .22. The projection analysis was found to have higher numbers after the ten year period; however, the population was still unstable. As seen in table 2.
After ten years the values are all above one, which indicates that individuals in all of the age classes are still alive. However, the values are not the same as the starting values, so the population is not stable.
In the final analysis, a number of changes were made to the Leslie matrix. The F1 and F2 values were increased to 1.2, and the F3 value was increased to 0.8. The P2 value was increased to 1, the P3 value was increased to 1.2, and finally, the P4 value was increased to 0.8. These changes had a dramatic effect on the outcomes of the basic analysis, projection and the elasticity of the model.
In the basic analysis the r value had increased to a positive value of 0.18, the Ro value had increased to a value of 1.68, and the T value had increased to 2.87. In the projection the final population dynamics after ten years were found to stable, and growing at an accelerated rate. Beginning with 25 individuals in each age class, this number was increased to 308 individuals for newborns, 130 individuals for one year olds, 107 individuals for two year olds, 109 individuals for three year olds, and over 70 four year old panthers were present in the population. The elasticity had minimal change. The elasticity of P1 was 0.35, P2 was found to be 0.21, and the elasticity for the fecundity values were all between 0.14 and 0.82.
In the first two trials the population was found to be unstable, simply because the starting age structure was not represented after the ten year projection. After ten years, the entire population in the first trial was extinct. In the second trial, with the increased P1 value, after ten years, individuals of each age class were still present, but in lower numbers. The drop in the numbers in individuals in every age class suggests that the population is unstable, and will become extinct eventually; it will just take longer than the ten year projection. It was interesting to see how such a small adjustment in the life history make-up of the Florida panther could have such an effect on the population dynamics.
In the third trial, the small increases in the Leslie matrix, had a huge affect on the population dynamics. The small changes made to the Px and Fx values, allowed the population to increase to nearly exponential levels. With the adjustments made to the matrix we overshot the goal of attaining a stable population.
With one of our trials falling short of a stable population, and the final trial overshooting a stable population, we have a general idea of where the life history trait values need to be to achive a stable population. It would be only a matter of time, and trials, before a stable population could be simulated, using the Leslie matrix. Once survival and fecundity values of a stable population were found, then this would give us values to achieve in the real population. We can influence the fecundity and survivorship values of a population by using conservation techniques, to help raise the values, to support a stable population. This is how we can use the Leslie matrix, and other tools, to help us conserve populations of endangered species to help preserve the fragile populations.
If I had sole control over how to save the Florida panther, I would try and connect the isolated populations, in order to achieve one, larger, more diverse, population. I realize that this would take a huge cooperation with land owners in Southern Florida, and would definitely decrease profits for industries, and reduce the amount of land available for development. As well as turn massive amounts of developed land, back into useable panther habitat. In increasing the habitat available, and increasing the gene exchange between populations, a stronger single population will be built, and will have a much better chance of being stable. This tactic would also keep the Florida panther sub species a pure species, by not bringing in individuals from other sub species of panther to aid the failing populations of the Florida panther.

Carbon dioxide is a green house gas produced by the burning of fossil fuels. Since the industrial revolution the amount of CO2 produced by human action has increased the amount of atmospheric carbon to levels never seen before. With the increase of atmospheric carbon global warming has occurred due to thicker atmosphere not allowing radiation to escape as easily as when less carbon was present. The carbon cycle allows carbon to be transported between the atmosphere, Land and shallow oceans, and the deep oceans. In a simplified carbon model, used for this experiment, several inputs and outputs were used. Industrial emissions are the main current source for placing carbon in the atmosphere, mainly in the form of CO2. The atmosphere on average holds about 735 Gt (giga tons) of carbon. Through the processes of fixation and respiration carbon can be exchanged between the atmosphere and the earth's surface (land and shallow oceans). Fixation is the removal of carbon from the atmosphere to the earth's surface usually stored in organic forms, trees, plants, animals etc... Respiration is the release of carbon from the earth's surface to the atmosphere through the biologic processes of burning of organic matter and breathing. Another exchange occurs between the shallow oceans (which include the carbon found on land in this simplified model) and the deep ocean. Around 781 Gt of carbon can be stored in the shallow oceans, compared to the massive 19,230 Gt's that can be stored in the deep ocean. Downwelling and upwelling are the two major exchange processes that occur between the shallow and deep ocean. A minor amount of carbon is deposited in the deep ocean floor, but the small amount is not a primary focus of this experiment. This simple model is the bases for the MathCAD program used.
Carbon sequestration is the process by which carbon is moved by the cycle and stored within the deep ocean. The deep ocean has a large capacity to hold more carbon than any other part of the carbon cycle. With the increase of carbon in the atmosphere leading to global warming, it is in our best interests to store carbon in a place where it will not have such a strong effect on our earth's climate. Carbon sequestration appears to be a reasonable way to remove the carbon from the atmosphere and aid in lessening the effects of global climate change the earth will be facing in the future. In looking at the variables we can use to move the bulk of the atmospheric carbon to the deep oceans we have fixation, downwelling and deposition. Going against the movement of carbon from the atmosphere to the deep oceans are respiration, upwelling and emissions. Controlling emissions has been a main concern ever since we have realized the problems that are caused by the increased CO2 in the atmosphere. Special organizations and regulations such as the Kyoto protocol (Kyoto) have been initiated in trying to manage the amount of CO2 emissions produced by industry. To return the global carbon balance back to how it was prior to the increase in CO2 emissions, humans would have to cut carbon emissions by 80% (Agrawal). A stabilization point where we are producing the maximum amount of carbon the natural cycle can handle was also found. This value would be around a concentration of 500 ppm of carbon in the atmosphere. Due to the difficulty in the market and political world of cutting emissions, carbon sequestration is viewed as a way to fight global warming without reducing emissions to their lowest levels. MathCad was used to project what the levels of carbon would be in the atmosphere when different degrees of emission controls were used. Also the sensitivity of the atmospheric CO2 levels to the different carbon transporting variables was viewed as to how they function in the global carbon cycle.
In the first section of the lab experiment, emission levels were changed and CO2 levels were graphed and recorded for each emission change. The time scale was also varied throughout the experiment. Time ranged from a span of 340 years to up to 1000 years time, to view the reaction of atmospheric CO2 levels to the change in the level of emissions. The second section on the experiment focused on the sensitivity of atmospheric CO2 to different processes. Five variables were altered in this experiment, fixation, upwelling, respiration, downwelling and deposition. The five variables were altered either by decreasing by 50% or by doubling the rate at which they are able to move carbon through the global carbon cycle. The response of the concentration of atmospheric CO2 was recorded after each individual variable was changed. Each variable was changed independently of all other variables, in order to record the direct effect each variable had on the atmospheric concentration of CO2. The mathCAD program was based on a simple equation, that resembled the simple global carbon cycle mentioned in the introduction. MathCAD took the five variables, as well as emissions and combined them in groups as they effect the three main areas of carbon storage (atmosphere (At), land / shallow oceans (St), and deep oceans (Dt).) The equation can be seen as Figure 1.
When changes were made to the amount of emissions produced overtime the level of atmospheric CO2 in Giga tons of carbon were graphed overtime. In the most complex example, emissions prior to the year 2100 were set at 20 GtC (giga ton of carbon) and emissions after 2100 were set at 10 GtC. The results showed a steady increase in giga tons of carbon until the year 2100, then after the emissions were reduced to 10 ppm??? the amount of carbon in the atmosphere was maintained at around 1560 Gts. The concentration of atmospheric CO2 followed a similar trend. This trend occurred over a 340 year span. The CO2 concentration increased from 280ppm to about 840ppm prior to the year 2100. After the year 2100, the CO2 concentration held constant at about 820ppm (Figure 2).
Figure 2 can be compared to figure 3 which shows the same data when emissions were set to zero. In figure 3, the tons of carbon in the atmosphere fall steadily to zero, and the CO2 concentration holds constant at about 280ppm during the entire time span.
For the second section of the lab experiment the sensitivity of atmospheric CO2 concentration were measured as a result of independently changing each of the five emission variables. The CO2 concentrations were measured for two specific time periods. The first set of sensitivity was measured at the year 2200, the second set was measured to find the maximum concentration the CO2 would reach over the 1995 – 2335 time period. The results followed general increasing or decreasing trends, when considering which variable was manipulated. When variables that work to remove carbon from the atmosphere such as; fixation, downwelling, and deposition, were decreased by 50% the concentrations of CO2 were found to increase in the atmosphere. Counter to that point when the variables that increase the amount of carbon in the atmosphere such as; respiration, upwelling and emissions, were decreased by 50%, the CO2 concentration in the atmosphere declined. All results can be seen in Table 1.
It was also found that if emissions were cut to 25% of original levels then the atmospheric CO2 concentrations were found to be 498ppm for year 2200, and 631ppm for the maximum over the 340 year period. When fixation was increased 3 fold, the 2200 CO2 concentration was found to be 460ppm, and the maximum for the 340 year period was 660. Also if respiration was reduced to 25% of original level the CO2 concentration was found to be 615ppm for year 2200, and a CO2 concentration of 895 was found as the maximum over the 340 year span.
In terms of setting a optimal CO2 emission limit, I feel that finding the balance point at which the natural carbon cycle would be able to cycle all our emissions would be the ideal amount. This would be more profitable then working to reduce CO2 emissions to zero, and would also allow the natural process of carbon cycling to not be overwhelmed with the amount of carbon released by our emissions. To reduce emission rates our way of life must change. Our current emission rates if not controlled and reduced will have us facing devastating effects of global climate change. Ways to reduce our overall emissions include standards and regulations for industrial companies and engineering more fuel efficient vehicles. Simple life changes such as using a programmable thermostat or turning off unused lights would also greatly reduce human carbon emissions.
When looking at using carbon sequestration to reduce the amount of carbon in the atmosphere, the data suggests that it could be done by adjusting the rates of the variables that effect how the carbon moves through the cycle. The costs and energy needed to for example increase the fixation rate, should then be compared to the costs and energy needed to reduce emissions to reach the same CO2 concentration in the atmosphere. The overall amounts of carbon that is exchanged between the atmosphere, shallow oceans, and deep ocean is very small when compared to the amounts of carbon that is held in each area, especially when considering the deep ocean. This suggests that the large amount of CO2 we would want to remove from the atmosphere by sequestration would take an extremely long time. It would be much more effective to focus our attention and energy on reducing carbon emissions. This would have a more direct effect on the amount of CO2 in the atmosphere. Controlling CO2 emissions would also allow us to engineer greater technologies to help us continue to reduce emissions for the future, rather than just shifting the bulk of the carbon to the oceans from the atmosphere.
The effects of carbon sequestration have not been thoroughly studied. How the oceans will react to the increased amount of carbon are greatly unknown. From a human standpoint it seems that we want to use carbon sequestration as a way to put the CO2 and excessive carbon out of sight, and out of mind. With CO2 emissions we didn't worry about the affects of putting excess carbon in to the atmosphere until we started to see complications arise as in global warming. Moving excessive carbon from the atmosphere ultimately to the deep oceans will be difficult and the results are not certain. Focus should be placed on holding more carbon in fixation in forests and plant life on land and coastal areas, as well as reducing overall emissions. Carbon sequestration though it would reduce the amount of CO2 in the atmosphere seems like just sweeping dirt under the rug. We know the carbon is still there, we are not worried about it currently, but at some point we will have to deal with it.

Atmospheric concentrations of greenhouse gasses (GHGs) have been steadily rising since the start of the Industrial Revolution due to anthropogenic emissions. This has caused global mean temperature to rise by an estimated .5 degrees Celsius, with some regions experiencing much higher increases than the average (Agarwal, 2008). Higher temperatures have ushered in rising sea levels, more erratic weather patterns, and adverse human health impacts. This trend is difficult to reverse because human emissions are a net addition to the carbon cycle; they represent an additional input to the global carbon balance which has led to ever-increasing net storage. Furthermore, GHGs persist in the atmosphere for long periods of time. In order to reduce atmospheric concentrations of GHGs, either the inputs (primarily emissions) must be lowered or the outputs (primarily carbon sequestration in forests) must be raised. This paper looks at the relative efficacy of different approaches to reducing the concentration of carbon dioxide, and concludes that a combination of measures, including both emissions reduction and sequestration, is needed to achieve stability.
I first performed background research using lecture notes and existing literature to determine how climate change works, what the impacts of climate change are, how carbon dioxide is cycled through the oceans and atmosphere, and how carbon dioxide concentrations have changed over time. Next, I modeled these flows using the upwelling-diffusion climate model from the Intergovernmental Panel on Climate Change (Harvey et. al., 1997). This model is shown in Figure 1. There are three "storage" compartments for carbon dioxide: the atmosphere, land and shallow oceans, and deep oceans. The IPCC model identifies the input rates and output rates from each of these storage boxes. For example, the outputs from land and shallow oceans are downwelling into deep oceans, at a rate of .11S gigatons per year, and respiration up into the atmosphere, at .27S per year.
Using this model, I simulated future scenarios using IS92 data as my baseline. In these scenarios, I demonstrated the impact of halving and doubling each of the six flows (emissions, fixation, respiration, downwelling, upwelling, and deposition), while holding the other flows constant. The model produced future projections for atmospheric carbon dioxide concentrations based on these parameters for the year 2200 and the maximum level. The output from this scenario modeling allowed me to determine the impact of each "lever" on achieving carbon reductions. In my analysis, I assumed that sequestration is equivalent to fixation (as it represents the earth's ability to sequester carbon in vegetation). However, one could also consider man-made carbon sequestration and storage (CCS) techniques in devising a practical, comprehensive strategy to reduce GHG concentrations (for instance, the possibility of trapping emissions from coal-fired power plants and burying them underground). I also applied learnings from my literature review to assess which levers could in fact be altered and by how much.
The results of the scenario modeling show that the most effective lever in altering atmospheric carbon dioxide levels is emissions: the range in carbon dioxide concentrations that result from halving or doubling emissions is the highest, at 1,229.9 ppm in year 2200 or 2,072.5 ppm maximum. Fixation is a close second, with a range of 1,028.2 ppm in year 2200 or 1,684.6 ppm maximum. Respiration is still somewhat effective, while downwelling, upwelling, and deposition yield smaller and smaller changes in storage. However, note than even a halving of current emissions still leads to atmospheric concentrations of 702.9 ppm CO2 by the year 2200 – this is twice the level deemed "safe" by Hansen and his team of scientists! Doubling fixation also does not achieve our goal, reaching only 653.2 ppm CO2.
Furthermore, this simple model omits several complicating variables that would have to be considered in crafting a realistic response to climate change. Scientists still don't fully understand the whole suite of "interactions between the terrestrial biosphere and climate"( Harvey et. al., 1997). For instance, what will the impact of melting permafrost be on methane emissions? Additionally, cooling mechanisms such as aerosols and cloud cover are poorly understood and not reflected in the model above. Indeed, earth scientists are still struggling to create and run models which incorporate the true complexities of climate change.
Carbon sequestration and reduced emissions are the most viable strategies for reducing atmospheric concentrations of GHGs. But while changes in emissions have a slightly greater effect on carbon dioxide storage than fixation, neither one alone can reduce atmospheric concentrations to a level low enough to avoid runaway temperature increases resulting from the initiation of positive feedback loops. Earth scientists have yet to derive completely accurate models, but it is clear that a multi-pronged strategy is needed to curb – and hopefully reverse – the accumulation of these gasses in the atmosphere. Intensive reforestation measures must be coupled with rigorous efforts to reduce anthropogenic emissions of GHGs.

Estuaries and other coastal ecosystems represent interfaces between terrestrial and marine worlds, and an important role they play is in the assimilation of nutrients such as nitrogen (N) and phosphorus (P) that enter the water from the land. Growth of plankton for example, preyed upon by benthic filter feeders like oysters, or the growth of sea grasses grazed upon by larger aquatic animals are portions of some of the chains by which nutrients are taken into estuarine ecosystems (Jackson et al., 2001). These ecosystems typically are comprised of multiple trophic levels, any or all of which may be affected by environmental changes. The goal of the current study is to look at the relative importance these disturbances, grouped very broadly into bottom-up (meaning changes to nutrient inputs) and top-down (meaning changes to the relative presence of upper trophic levels due, for example, to changes in harvesting rates) perturbations may have across different aquatic ecosystems.
Urban development in areas surrounding coastal ecosystems has been accompanied by changes in ecosystem structure. A commonly reported estuarine malady is an increase in the frequency and severity of seasonal hypoxic and anoxic conditions, associated with increased levels of algae and phytoplankton in the system (Jackson et al., 2001; Brawley et al., 2000). These events are often explained as being caused proximately by increased anthropogenic contributions to estuarine nutrient loads and have triggered significant effort into the modeling of nutrient inputs to estuaries from groundwater, point and non-point source runoff into estuarine feeders. The goal of such work is to determine estimates of critical nutrient load, or loading targets to guide agricultural and land-use policy (Brawley et al., 2000; Cerco et al., 1995). However, it is important to recognize that increased runoff and nutrient input is not the only anthropogenic influence on coastal ecosystems. Habitat destruction and fishing practices have affected the ability of macrofauna, like the oysters, to exert control over the structure of their ecosystems. The oyster reefs of the Chesapeake Bay were once capable of filtering the entire water column in only a few days, and it is only since exhaustive dredging of the bay led to the collapse of the oyster fishery that hypoxia and anoxia begin to be observed (Jackson et al., 2001). Jackson et al. discuss this and other correlations of ecosystem decline with destructive fishing practices, making it clear that comprehensive attempts to curb hypoxic and anoxic events via policy should consider the potential for both bottom-up and top-down effects to be significant in a target system.
Whether looking at top-down or bottom-up control, or both, a difficulty in modeling all estuarine systems is that the term "estuary" in itself is a fairly loose categorization of bodies where freshwater and marine systems meet. Estuaries may differ greatly in, among other factors, the structure of their biological communities, the distribution of nutrient and chemical inputs, and their basic physical structure (NRC), which in turn affects another important factor in estuarine heterogeneity-hydrodynamics. Both the volumes of river and tidal flow into the estuary, and the way that they flow into and out of the estuary, will affect the degree of mixing and the salinity gradient, an important characteristic of a freshwater-marine interface (NRC). Mixing characteristics of flows entering an estuary determine the dilution of nutrients entering with the flow, and flow levels in and out of the estuary, in concert with mixing, control the residence time.
In short, estuaries can be nearly as different as they are similar, presenting a difficult problem in ecosystem management. Management tools must be somewhat general in order to be useful over a wide enough range, but must also describe systems well enough to give meaningful results. In the case of estuaries, this has proven to be a difficult balance to achieve. Sophisticated three-dimensional hydrodynamic models have captured many of the flow and salinity characteristics of specific estuaries (Chau et al., 2001; Cugier et al., 2002), but the calibration and use of these models is resource intensive. In contrast, simple mechanistic box models that explain well some important processes in estuarine ecosystems have been around for decades (Kremer et al., 1982), but lack the spatially explicit resolution needed to make judgments. An open question in estuarine ecosystem modeling then, is "how general can the model be before it is no longer meaningful?" This question frames the second objective of the current investigation.
Estuarine modeling is at this time a rich and well-developed field but there does not yet exist significant work analyzing the relative importance of both bottom-up and top-down anthropogenic influences on ecosystem conditions. The long-term goal of the current study is to contribute to this work by comparing these effects across several ecosystems.
The immediate deliverable within the time constraints of the course project is a modeling framework linking nutrient input, anthropogenic harvesting and habitat-destructive effects to planktonic and macrobiotic growth in the ecosystem. This simple module will form the ecosystem component of larger, spatially explicit simulations that examine entire ecosystems. Within the reduced scope of the course project, the questions framing the overall study collapse then, to "can top-down and bottom-up effects be observed in physical subcomponents of an estuarine ecosystem?" The first target system for this study, and the system that is the scope of the course project, will be the Chesapeake Bay.
Already an extensively studied ecosystem, model results and historical data looking at estuary nutrient loading (Brawley et al., 2000; Cerco et al., 1995) and macrofauna (Miller, 2003) in the Chesapeake Bay exist as a backdrop against which to compare the results of this integrated study. In particular, Ulanowicz et al. have assembled box models of the mesohaline reach of the Chesapeake Bay that provide an excellent initial comparison for model results.
The Chesapeake Bay is a deep estuary, dominated by plankton growth and possessing a long residence time (NRC). A large share of phytoplankton production fuels zooplankton growth, which in turn are preyed upon by ctenophores and sea nettles, a seasonally-variant process peaking in summer, and by fishes such as the striped bass (Baird et al., 1989). Remaining phytoplankton may sink through the water column where it feeds several commercially important benthic macrofauna, namely oysters and blue crabs, which are thought to have significant roles in benthic community structure.
Seasonal variations in light and nutrient availability result in annual cycles of the ecosystem (Baird et al., 1989). While these should be incorporated into the simulation within the larger context of this study, for the purposes of this project only a single season (summer) is used for data input.
GeiLoVe utilizes simple Lotka-Volterra dynamics, which allow for four different types of fluxes into, out of, or between model pools (Ulanowicz et al. 1992):
While simple in their treatment of food web interactions, Lotka-Volterra dynamics are simple to solve, as there exists only a single unknown coefficient for each new flux to the system. A single snapshot of the food web fluxes and average pool values over some interval is all that is required for input data.
GeiLoVe reads all pool and flux values from an input file (Appendix A) and calculates the model coefficients by simple algebra. A mass balance is then calculated for each pool to capture any mass imbalance in the input data. GeiLoVe then performs a simple forward time step of the form:
where M is the vector of pool biomasses, dM/dt is the accumulation of biomass over the interval dt, and E is the vector of mass imbalances arising from the model input. Summing the mass contributions to each pool in the current interval based on the calculated Lotka-Volterra coefficients and the values Mt generates the vector dM/dt. The calculations required for this time-step are in no way computationally limiting, and there is no need to sacrifice ease-of-coding for more computational efficiency.
GeiLoVe was applied to a set of season-average summer food web data for the mesohaline reach of the Chesapeake Bay (Appendix A). Dr. Robert Ulanowicz of the Chesapeake Biological Laboratory provided the raw data, as well as the software AGG.EXE used to aggregate the large number of pools into the 14 boxes used in this study. The boxes in the aggregated food web model are labeled as follows:
These 14 boxes form a moderately complex food web from which a variety of aquatic species are harvested (Figure 1).
As a first check of the model, a non-perturbed system should remain at steady state for any length of time t. The constant mass profiles over the period of about 5000 days, generated in MATLAB from the model outputs, show this to be true (Figure 2).
Stepping through the model in this way serves to check that mass is being conserved in the model, beyond the error allowed in the input data.
The input data for the summer season has a single fixed input to box 13 (Suspended Nitrogen), and as a first test of GeiLoVe's performance this input was manipulated to 110% of the original input value (Figure 3).
The most striking result of this manipulation is that a number of food web members, such as the zooplankton and all fish species, have gone extinct within 1000 days in the run. Without needing to compare explicitly to experimental data, it is fairly clear that this is a specious result, an artifact of the simplicity with which the food web is modeled and the lack of the myriad negative feedback processes that in a real food web might serve to dampen the effect of perturbances on the system.
A rudimentary attempt at recreating some of these stabilizing bounds might be to assume that a given perturbation in any parameter should not move any state variable beyond a certain fraction F of its initial value. Such an approach compromises conservation of mass, but at least allows stocks whose prey pools would otherwise have speciously dropped to 0, a chance to respond to these dynamic changes. As a test, the same run was repeated with F = 0.2; that is, no state variable may drop below 80% of its initial value given the perturbation of 10% in the input nutrient flow (Figure 3).
The effect of these bounds is immediately obvious. While some species, such as the attached bacteria and suspension-feeding fish, still tend as much toward extinction as possible, others have rebounded (Figure 4). At least two cyclic patterns can be seen. One includes the zooplankton, the sediment feeders, deposit feeders, and others, and exhibits three peaks between day 1000 and the end of the model run. The second pattern is the predator-prey lag relationship seen between the carnivorous fish and the benthic fish. This cycle exhibits a distinctly different period than that observed in the first pattern.
Looking now to effects from the top down, the parameter for harvest (export) of carnivorous fish was dropped to 99% of its original value with all other parameters retaining their original values (Figure 5).
Since intuitively, a decrease in fish harvests (i.e., a decrease in human impact) should restore stability to the food web, the mass extinctions that follow may seem surprising. Again, the limitations of the simple dynamics used in the model are exposed. A check was performed using the same bounding assumption as before with F = 0.2 to see the effect on the food web (Figure 6). However, in this case the application of bounds did little to preserve food web dynamics, and the system appears to be headed toward an invariant steady state beyond 2600 days of the model run. Noting that the carnivorous fish pool is small (on the order of 4mg/m2) compared with lower trophic levels such as phytoplankton (on the order of 400mg/m2), it is perhaps not appropriate to use it as a perturbation variable in the noisy context of the Lotka-Volterra dynamics.
There are other commercially important species in the Chesapeake Bay, such as crabs and oysters. These are included in the input to GeiLoVe as the deposit feeder and suspension feeder boxes, respectively. A manipulation of the deposit feeder harvest coefficient to 90% of its original value (with a bounding factor F = 0.2) yields an interesting dynamic solution (Figure 7), and so the deposit feeder pool is the first target for a response surface analysis.
The harvest coefficient for deposit feeders was varied from 85% to 115% of its original value, and that for the nutrient load was varied from 85% to 115% of its original value to generate a multivariate response surface (Figure 8 A and B). Since in many cases the food web settled into stable periodic dynamics rather than a steady state, both the minimum and maximum observed in the last 25% of the run were plotted in order to more fully represent the state of the system. In general, the periodic dynamics occurred when nutrient loading was high, whereas the system reached a stable steady state when nutrient loading was kept low (Figure 8 B). Since the results in 5.1 to 5.4 indicated that a stable steady state was linked to an extinction of many of the food webs, it should be noted that this may be true for many (or all) of the steady state cases shown in the response surface.
The response surface reveals that when nutrient loading is high and harvests are low, the deposit feeder stock is quite high (Figure 8 A). In contrast, when harvests are high and nutrient loading is low, the stock is low. These results have intuitive appeal. The shape of the response surface is also important, in that it has a significant slope along both axes of perturbation. Taken together, these results suggest that in the case of deposit feeders, perturbations to nutrient load as well as harvests can act as independent control knobs on the simulation, with their effects being discernable within the simple model framework.
The response of the suspended nitrogen pool to the same set of perturbations was also generated (Figure 9 A and B). It is clear that when harvests of deposit feeders are high, the level of nutrient in the system is also high (Figure 9 B), supporting the intuitive notion that upper trophic levels have a strong influence on the ability of the food web to take up nutrients.
In the same way as for the deposit feeders, a set of response curves showing the responses of suspension feeders (Figure 10 A and B) and suspended nitrogen (Figure 11 A and B) to perturbations in suspension feeder harvest and nitrogen loading were developed.
The suspension feeder stock responded to perturbations in nitrogen load in a very similar way to that of the deposit feeders, and the suspended nitrogen response was similar to that in the previous case. However, both pools appeared relatively insensitive to changes in suspension feeder harvest over the range of analysis.
Since it is expected that when the relative perturbations for both variables are 1, the stock response is also 1, it may seem surprising that this point does not appear on any of the response surfaces. The reason for this is that the due to grid spacing (a total of 50 grid points over the range 0.85 to 1.15), the point shown as 1.00 is actually 0.997. Noting that the system is further constrained by the bounding factor F = 0.2, which in many cases pulls the pool values away from their unconstrained state, it is not surprising that pool values for systems even close to the unperturbed state will be very different.
Ulanowicz et al. (1992) used a set of analogously derived data using a carbon basis to look at the effect of oyster harvest on the same mesohaline reach of the Chesapeake Bay. Their study also made use of Lotka-Volterra dynamics; in fact, much of the method applied in the current study used this Ulanowicz study as a basis.
The Ulanowicz study used a 13-box model of the system, one of which modeled oyster biomass. They derived the Lotka-Volterra coefficients defining the food web model in the same way as defined in this paper, and then proceeded to decrease the coefficient defining oyster export from the system in increments of 1%, allowing the system to solve to a steady state at each step.
One key difference in methodology is that they used an iterative Newton zero-finding approach to find the time-invariant solution (i.e., dM/dt = 0), and used that solution as the steady state value. It is clear that such an approach would not have solved satisfactorily in most of the cases in the current study – periodic dynamics are not time-invariant and thus there is no zero to be found. The forward time-step method applied in the current study is more appropriate to this type of behavior.
The Ulanowicz study also observed what was regarded as specious extinction of a pool, though not to the extent observed in the current work. The major finding of the modeling study was that a XX% reduction in oyster harvest (as a proportion of the standing stock) led to an overall higher catch volume. This is consistent with the findings of this study that low harvests of deposit feeders, and to a lesser extent harvests of suspension feeders (oysters) led to overall higher stocks (Figures 7 and 9).
Lotka-Volterra dynamics alone are not sufficient to reproduce the food web dynamics of the Chesapeake Bay, although it seems that an augmentation of these simple dynamics may help. One possibility would be to allow the Lotka-Volterra flux coefficients to vary as functions of the biomass pools they connect to. As prey grow scarce, they become harder for predators to find, which in the Lotka-Volterra framework can be interpreted as a decrease in the value of the coefficient for a feeding flux. This semi-mechanistic approach may help to dampen the wild oscillations in the current system that contribute to extinctions of upper trophic levels, and should be a target of future work.
While the current model was unable to adequately simulate effects of perturbations to exports of the top trophic levels in the food web, a two-factor response surface showed clear and distinct responses to perturbations in nitrogen input and deposit feeder export. Specifically, deposit feeder stocks were high under conditions of high nutrient load and low harvest, and low under conditions of low loading and high harvest. Additionally, high harvest of the deposit feeders led to high levels of suspended nutrients.
Without reproducing similar response curves for other pairs of perturbed variables in the system, these results are at best anecdotal. However, they satisfy intuitive expectations for the system and suggest the potential for application of this method to other systems.
In its current form, the food web model generated with Lotka-Volterra dynamics in GeiLoVe is not stable enough to reproduce the dynamics resulting from perturbations to flux parameters. Even small perturbations can cause some pools, particularly in the upper trophic levels, to go to extinction. However, artificial bounding of pool values in some cases allowed the system to rebound and produce interesting dynamics. If this effect could be reproduced in a more mechanistic way by augmenting the basic Lotka-Volterra dynamics, the resultant model might be sufficient to study top-down and bottom-up effects on the Chesapeake Bay and other relevant coastal ecosystems.

Healthy People 2010 defines several health risks for adolescents. Diabetes, depression, inactivity, sexually transmitted infections (Health & Human Services, 2000), and HIV are just a few of these health risks affecting adolescents. Providing the education for adolescents to protect themselves from STIs and HIV can be easy and effective and has the chance to make a major difference in the health of this population. One of the goals for Healthy People 2010 is to prevent HIV infection and its related illness and death. Increasing the proportion of sexually active persons who use condoms from 23 % to 50% is one objective for decreasing HIV infection (Health & Human Services, 2000). Education on consistent condom use as a method to prevent HIV infection as well as other methods of risk reduction and harm reduction will contribute to the 2010 goal of preventing HIV infection.
In 2004 the Centers for Disease Control and Prevention (CDC) reported an estimated 4,883 young people as receiving a diagnosis of HIV infection or AIDS, representing 13% of people diagnosed that year. Young people are particularly at risk for HIV infection because of a lack of knowledge of HIV transmission or prevention. In conservative communities such as Monroe County, HIV education may focus on abstinence and adolescents may not be taught about harm-reduction methods for HIV prevention. Reports from the National Youth Risk Behavior Survey by the CDC (2006) show 46.8% of students grades 9-12 had sexual intercourse during their lifetime. 14.3% of students had sexual intercourse with four or more persons during their life. Of students engaging in sex, only half report using condoms consistently. Among rural high school students, a study of STD-/HIV-related sexual risk behaviors and substance use revealed 37.9% of the survey population was sexually active. Of these sexually active rural teens, 57.4% reported using a condom the last time they had sex (Yan, Chiu, Stoesen, & Wang, 2007). White rural adolescents in this study were most likely to report not using a condom the last time they had sex. Female students were also more likely to report not using a condom (Yan, Chiu, Stoesen, & Wang, 2007).
The Health and Consumer Protection Directorate of the European Union (2007) published a press release saying 24% of EU citizens are wrongly convinced you can be infected with HIV/AIDS by kissing on the mouth and 30% are unsure on this, meaning half of all EU citizens do not understand how HIV is transmitted. Although no such statistics exist for the United States, we can assume a percentage of this population is also unsure how HIV is transmitted.
A study by UNAIDS reported less than 50% of young people surveyed in 18 countries had comprehensive knowledge about HIV. In an overwhelming majority of these countries, young women knew significantly less about HIV than young men (UNAIDS, n.d.). Because of lack of knowledge and inconsistent use of protective measures for HIV, these studies show young people, particularly women and those in rural areas, are at a high risk for HIV infection.
When considering the epidemiologic triangle (agent, host, and environment), the best way to affect HIV transmission, with our current technology, is by breaking the chain between agent and host. There are two ways to stop transmission of the virus: eliminating activities that have been proven to transmit HIV (risk reduction) and using measures that have been proven to reduce the chance of transmission during such activities (harm reduction). Education is the cornerstone for HIV prevention because it gives people the options to protect themselves from infection.
Education related to prevention is necessary for decreasing HIV incidence because the natural history of HIV does not allow for recovery, only death. Early diagnosis is also important for HIV because it is now possible to maintain a healthy life for a longer period of time than ever before. The CDC estimates, of the 1-1.2 million persons in the U.S. that are infected with HIV, one-quarter are unaware of their infection. 54%-70% of new sexually transmitted HIV infections can be attributed to these 25% unaware of their infection (Branson, 2007). By preventing new infections of HIV with prevention education and providing early diagnosis to prevent spread by unknown infected persons, HIV incidence could decrease dramatically.
Ida Township is a rural area of Monroe County, Michigan. Monroe County's HIV infection rate is 32 per 100,000 (Michigan Department of Community Health, 2008), much higher than the national rate of 18.5 per 100,000 (Centers for Disease Control and Prevention, June 9, 2006). Ida High School has no limits on what can be mentioned during HIV education, unlike some other schools in Monroe County. Ida Township, however, has very few resources for HIV testing. The four closest testing sites to Ida are all about a 40-minute drive away. STI testing is available at the Monroe County Health Department, about a ten-minute drive from Ida. Some public transportation exists between Ida and Monroe (where the health department is located) but none exists between Ida and any of the HIV testing areas. Without personal transportation, Ida community members cannot receive HIV testing services.
A class of 12 Ida High School students was selected to receive an HIV education program intervention. The class included 12 white seniors, 4 of which were male. The intervention was a comprehensive HIV education program designed to not only provide risk reduction options for HIV prevention (i.e. abstinence), but also provide harm reduction options for prevention (i.e. condoms). The rationale for adding harm reduction options to the standard risk reduction options comes from the National Youth Risk Behavior Survey by the CDC, which shows 46.8% of students grades 9-12 had sexual intercourse during their lifetime (Centers for Disease Control and Prevention, 2006). As almost half of American adolescents have had sex, an intervention using only risk reduction is no longer an option.
The intervention aims to increase knowledge of HIV on three domains: cognitive, affective, and psychomotor. The cognitive domain is affected by memory, recognition, understanding, reasoning, application and problem solving. Recognition was started with a pre-intervention questionnaire where concepts were introduced, memory and understanding of the information was tested in a post-intervention questionnaire, and reasoning, application, and problem solving were assessed during student participation throughout the intervention. By informing students about HIV, the intervention hopes to change the affective domain and increase sensitivity to the subject. The psychomotor domain was affected while students actively participated in the intervention by standing in the front of the room and holding signs of different activities or body fluids and their likeliness to transmit HIV (Stanhope & Lancaster, 2004).
Objectives for this intervention include the ability of every student, post-intervention, to list at least one way they could protect themselves from HIV. All objectives are listed in table 1.
After designing an educational program on HIV it was presented to seven senior nursing students and one public health nursing instructor. The students and instructor critiqued the program and appropriate changes were made. Arrangements were made to implement the program to a group of 12 students at Ida High School in Monroe County.
The intervention acts on the level of primary prevention. Primary prevention efforts in public health aim to inhibit development of disease before it starts. The design of the intervention is to take HIV-negative youth and provide them with the knowledge to protect themselves from infection and remain negative for their lives. The intervention also works on a secondary prevention level. Secondary prevention in public health aims for early detection and treatment. Students participating in the intervention are given information on HIV testing for early detection. The intervention also provides a small amount of information on strategies to stay healthy for those who are HIV positive.
The implementation of this program could, at a minimum, take only one person. A contact at the school is important to set up an aggregate for the presentation. The program was designed with a PowerPoint presentation so a computer and projector are required. The time limit, enforced by class length, is a major limitation to a truly complete education program.
Prior to the intervention the 12 students were given a questionnaire on HIV. Following the intervention the same questionnaire was given to determine the change in knowledge from the intervention. Ten students returned the pre-intervention questionnaire and 11 students turned in the post-intervention questionnaire. Results of the questionnaires are below in table 2. The first two sections of the pre-intervention questionnaire are labeled as having inaccurate data. The students were still in possession of the questionnaire during the beginning of the intervention and they used this learned information on the questionnaire, making it an inaccurate estimate of their original knowledge pertaining to HIV.
The most important goal of the intervention was to provide the students with the knowledge needed to protect themselves from HIV infection with risk-reduction methods or harm-reduction methods. Ten of the eleven students to return a post-intervention questionnaire reported abstinence as a method they could use to protect themselves from HIV, seven students listed safe sex or condoms, five listed not sharing needles, two students listed one partner or limiting partners. Most students, 90.9%, listed risk-reduction methods, abstinence, as a method to protect themselves from HIV. This shows that despite the inclusion of harm-reduction information, students still understand and may use risk-reduction methods. More students identified abstinence as a way to protect themselves from HIV over other harm-reduction methods.
Objective one, correct identification of the acronym HIV, was met, 90.9% of students correctly identified this. Only 45.45% of students were able to identify correctly the acronym for AIDS, most students were able to identify most of the acronym, just not all four words. 90.9% of students could identify the difference between HIV and AIDS, meeting objective three. 90.9% of students could list four ways to transmit HIV, not quite the goal of 100% but an improvement from zero students who were able to list four ways to transmit HIV before the intervention.
The intervention was effective as no student could list four ways to transmit HIV before the intervention and after the intervention 90.9% of students could list four ways to transmit HIV. The students all, after the intervention, could list at least one way they could protect themselves from HIV, the main goal of the intervention. Providing the students with the knowledge to protect themselves from HIV infection was a major strength of the intervention. The active audience participation is also a strength of the intervention. The weaknesses of the intervention include the inability of the program to clearly define the acronym AIDS and its meaning. The time limitations of the program is also a weakness of the program because an hour is not enough to implement a complete HIV education program. A limitation of the study was the small study size, not being able to provide accurate information on the intervention's effectiveness.
If the intervention was to be re-implemented several improvements could be made. More audience participation could improve knowledge retention after the intervention. For long-term knowledge retention, students could be given an HIV fact sheet to refer to at any time after the intervention. It would be ideal if the intervention provider could, before the intervention, evaluate the HIV knowledge of the students with time to tailor the intervention to the specific knowledge needs of the population. Condom use was mentioned several times as a harm-reduction method of HIV prevention. With the current intervention, it is impossible to know if the students understand proper condom use. The effectiveness of the HIV education program could be increased if it included a condom demonstration. The intervention would also be stronger if it detailed protective measures for each method of transmission. For example, the current intervention mentions not sharing needles as a way to prevent HIV transmission. If the intervention went into more detail, about bleach methods for cleaning needles and needle exchange programs, it would be more effective.
Overall the program was very effective in providing the small study group with knowledge relating to the definition of HIV, the difference between HIV and AIDS, and methods of protection from HIV infection. The intervention was mostly effective in providing students with the knowledge of ways to transmit HIV and it was not effective in explaining the definition of AIDS. The most important part of the program was imparting the knowledge of prevention methods so the students possessed the knowledge to protect themselves from this preventable infection.

School age children (ages 6-11) used to have worries of homework, tests, and picking out a best friend. Currently, they are at high risk for obesity, type II diabetes, severe food allergies, poor self-esteem, bone and joint problems, and high cholesterol/blood pressure (Oakland county health profile, 2002). All of these health problems have a direct correlation with physical activity and obesity (Oakland et al., 2002).
The surgeon general has recognized childhood obesity to be an epidemic. An epidemic is a sudden increase of a condition or illness within a population (Stanhope & Lancaster, 2000). Contributing factors for obesity include genetics, lifestyle, diet, and medical conditions (Oakland et al., 2002). Risks determined by race are more difficult to determine. African-Americans, Hispanic, and American Indian's have a slightly higher prevalence of obesity (Institute of Medicine, 2004). It has been determined that having a low socio-economical status and living in the south is directly correlated with childhood obesity (Institute et al., 2004). Obesity can affect any child, no matter what age, gender, or ethnicity. Many studies have concluded that health problems caused form obesity could be prevented and/or controlled through physical activity.
A study from the University of Minnesota found that time spent performing physical activity significantly decreased from early adolescents to late adolescents (University of Minnesota, 2007). Decreasing physical activity becomes a habit, and a person will develop a sedentary lifestyle. This lifestyle leads to increase risk for obesity and other related illnesses (University et al., 2007). In today's world it is often harder to be active then it is to be not active. The priority of convenience and time saving habits leads to less leisure time, and less activity. People depend on cars more for transportation to decrease effort and save time, work leads to a decrease time to participate in healthy activities.
A study published by The Journal of Adolescent Health verifies the high prevalence of minimal activity in adolescent males and females. Sixty percent of females, and forty-three percent of males did not meet the national guidelines for activity. Compliance was defined as being physically active for 60 minutes five times a week. This study also concluded that race or region of residency did not have an impact of amount of activity. Girls were more likely than boys to decrease activity as age increased (Butcher, Sallis, Mayer, Woodruff, 2008). This article supports efforts to increase the amount of physical activity beginning at or before adolescents. This will prevent the onset of a sedentary lifestyle as well as the physical and emotional complications form inactivity. (Butcher et al., 2008).
Drextel University studied the contributing factors of childhood obesity by observing the physical and social environment changes. The study recognized the physical environment decreasing opportunity for activity, and the social environment promoting food high in fat content and calories. The study concluded the need for a shift of assuming individual responsibility for obesity, and recognition of the environment as the primary determinant of obesity (Budd, Haymann, 2008). While there may need to be a larger focus on the contributing factors to limited activity among the population, an individual must do what s/he is in control of. Drextel recommended the use of nurses to promote and support individual change while setting an example for the community (Budd et al., 2008). Until America can accept responsibility and admit its faults, individuals must do what is in their power to decrease their risk of the complications of inadequate activity.
Using descriptive epidemiology the problem can be summarized using person place and time. While all persons are at risk for health complications related to inadequate activity, the focus is on adolescents and pre-adolescents. Studies show that there is a significant increase in illnesses and diseases which could be prevented or managed using physical activity. Targeting the age-group where the activity ceases will prevent the onset of a sedentary lifestyle while prolonging good health habits. Inadequate activity affects females more so than males, perhaps this is related to the stigma of participating in sports and being athletic for males. Inadequate activity affects all races and all income levels. Studies show all places are affected by inadequate activity. While there are indications that the south may be effected more than the north, all geographical regions have an increase in childhood obesity, therefore, no region should be overlooked while educating about the need for activity. There is less physical activity during winter months than during warmer climate (Yasunaga, Togo, Watanabe, Park, Park, Shephard, & Aoyagi, 2008). Suggestions of activities that can be performed during winter months should be made, as well as summer months.
Becoming less active is similar to many disease processes: there is an onset of symptoms, and if not treated promptly, it worsens and leads to many other health problems. Acquiring a sedentary lifestyle does not have one path of transmission like other health issues. Typically, a young child will be very active. Once s/he begins school the time for "play" is limited to gym, recesses, and after homework is completed. In high school, gym and recess are eliminated, but sports are available. The focus becomes more academic and work related, and eating becomes rushed and usually less healthy. A significant amount of teens report inadequate activity levels. With college, it is harder to become involved in sports, and there is little encouragement to become active. There seems to be a stigma that sports are for the younger population, which may discourage adults from participating in recreational activities. This, along with self-consciousness and decrease accesses to activities, eliminates many adults from participating in an active program. The problem resolves with the increase in opportunity to perform activities, and a increase in self-responsibility to take action. The aggregate population studied is the school age population. At this age, activity usually begins to decrease will continue to decrease throughout their lifespan. Using this population, the inadequate activity will be prevented, rather than treated. In order to prevent the decrease in activity, enabling factors must be assessed.
Decreased activity is determined mainly by lifestyle and social surroundings. While it is an individual choice to participate in activities, access to the opportunity is a strong determinant of commitment to physical activity. As age increases and the opportunity for activity decreases, many people shift their focus of activity and playtime to more sedentary behaviors (Butcher et al., 2008). Additionally, surrounding environment has a direct correlation on activity status of the population. One study concluded that winter months had a significant decrease in activity level than other seasons (Yasunaga et al., 2008). Similarly, a parent's goals and priorities are often transferred to their children. If parents do not recognize physical activity as an important part of everyday life, their children may be less likely to engage in regular activity. Making physical activity a priority for families will have a positive impact on the family as a whole (Villaire, 2008). A decrease in physical activity will decrease self-esteem and motivation, which in turn will decrease the likelihood of physical activity. The cycle continues and poor habits are developed, and a sedentary lifestyle begins.
The web of causation can be used to describe the multiple factors that influence the sedentary lifestyle, and the consequences that occur with failure to attain adequate activity (See appendix). Attaining adequate activity is determined by many factors, additionally, many consequences of inadequate activity present additional barriers. This begins a cycle which is difficult to overcome.
Healthy People 2010 recognize obesity as a problem that needs to be addressed. Objective number 19-3 is to "reduce the proportion of children and adolescents who are overweight or obese". Objective 7-11 supports prevention by having a goal to increase culturally appropriate health promotion and disease prevention programs in health departments (United States department of health and human services, 2001). However, the statements of the need for change are only the beginning to a revelation. The trends of America are showing that even with the knowledge of the problems that occur with a sedentary lifestyle, change is not guaranteed.
In 2004, nineteen percent of Americans ages 6-11 were overweight (Morbidity and mortality weekly report, 2007). This is an increase from eleven percent in 1994, and is continuing to rise (Morbidity et al., 2007). While there are no conclusive statistics for the city of Walled Lake, or Oakland County, obesity has been recognized as an epidemic the county is working to prevent and treat cases of obesity. Of those surveyed, over 21% had a body mass index greater than 30, and 45% reported that they were trying to lose weight (Oakland et al., 2002). The amount of physical activity can contribute to a person's body mass index. In 1996, 26.9% in the United States report getting no leisure time or physical activity in the past month, compared with the 22.9% of Michigan. There were no statistics for Oakland County. In 2002, 33.8% of Oakland County reported getting no leisure time in the past month. (Oakland et al., 2002). There are no comparisons to Michigan or the United States for the year 2002, the significance of the data suggests the need to address this issue.
Oakland County Health Department has made resources available for their population. The website includes information on the topic, preventions, and treatments. There is a list of activities to suggest to children, activities to do with children, and activities that can be done in the cold weather. Additionally the "Count Your Steps" program will restart in the community. People who participate will wear pedometers and be rewarded for taking the most steps in one month. In 2005, 4.4 billion steps were taken by third and fourth graders (Patterson, 2006). With the success of community involvement, the decision was made to continue this program to support the goals of increased health in childhood.
An intervention was developed to be used as a prevention tool for diseases that are caused from inadequate activity. Because of the age group of the intervention group, the focus was to increase activity, not to prevent obesity, being overweight, or diabetes. Exercise was always referred to as "activity" to decrease association with sounding mandatory and helping to imply opportunity. The children were encouraged to be active for fun and to feel good. To demonstrate how fun activity can be, a competition was organized. An assessment determined the knowledge of the importance of activity, the importance of a warm up, and cool down, and how to tell if a person is playing to hard/not hard enough. Finally, an explanation was delivered about the competition. Since there are four 5th grade classes, each competed against each other. For every 15 minutes of activity (plus the warm up and cool down) the children recorded a tally on their own daily log. At the beginning of each school day, they tally's were added up. At the end of 6 days (from Friday to Thursday), the class that was the most active received a prize. The prize rewarded were free activities around the community. The prizes were donated by family owned bowling alley's and mini-golf facilities that were willing to support the good cause. Additionally, this prize helped the business, promoted their facility, and helped the community from an economical standpoint. This intervention was appropriate for this age group; it motivated individuals to become active, and allowed for self-achievement, which also increased self-esteem. It did not mandate anything to be done, but rather encouraged proper behavior. Additionally, it allowed children to work at their pace and engage in activities they enjoy.
Our short term goals would be to see adequate activity levels among these 5th grade classes. With thirty students in the class, exercising for 60 minutes a day for six days, we would expect to see at least 720 tally marks by the end of the competition. With this achievement, our goals for adequate activity will be attained. Additionally, during the knowledge assessment, children should be able to answer all questions regarding activity at the end of the discussion.
Long terms goals include seeing an overall increase of physical activity, better attention in school, and an increase in mood. While these will not be able to be assessed in this study, compliance with the activity recommendations would result in all long term goals being met. If we were to measure these long-term objectives, we would assess the grades overtime and expect to see an increase. Additionally, there would be less need to report people to the office, and less need to remind children to be quite during class time.
The amount of activity was measured based on tally marks. Each tally mark represents fifteen minutes of activity. On Thursday, the amount of activity of all four classes were evaluated and analyzed. There were expectations of many tally marks, which indicated a lot of activity. With the recommendations of sixty minutes per day of activity, tally marks will determine if these objectives were met. With thirty students in the class, exercising for 60 minutes a day for six days, we would expect to see at least 720 tallies. With this achievement, our goals for adequate activity will be attained. Although the activity will likely decrease after the competition is done, there will be ongoing benefits from the competitions. After a week of activity, the children will notice some of the benefits. These include sleeping better at night, able to pay attention in school, and feeling better (Patterson, 2006). There was an expectation for an increase in general mood during the post-assessment with the children. The children were encouraged to perform activities with friends, which will start trends for the children, allow them to make it a regular activity, and provide support and encouragement for their activities.
This implementation is a primary intervention because it is working on prevention and education of health problems such as obesity, diabetes, and poor emotional health (Stanhope et al., 2000). For those children who are already faced with the struggles of these health problems, it is a tertiary intervention because it focused on delaying the progression of the disease (Stanhope et al., 2008). To implement this program, there is little to no monetary cost. The program will take 6 days to complete. Children are able to put as much or as little time into the program, depending on their motivation and time capabilities.
No money is required for this program. Approvals were needed by the organizations tat donated prizes. Without their approval, the main motivation (the prize) would not exist. At this age, having a materialistic prize to work for is beneficial, therefore the approval was a vital part of the program. The personnel involved are the teachers. The teacher's cooperation to keep tallies on the board allowed for record keeping for the class during the week. The original plan was not accepted due to policies from the health department. To support the goal of making things as convenient as possible for students a checklist was designed. On the days activities were performed, a checkmark would be placed next to the listed activity. Since this checklist list did not have the heath department logo on it, it could not be handed out. This presented a problem because not only would it make our project less convenient for students, but it also meant there were not any handouts for the class. As a substitute, hand outs were printed and used as an example for children to see. Examples of activities and guidelines for the activities were organized and placed on the chalkboard. Information presented included warming up, beginning the activity, and cooling down. However, by allowing the children do document their own activities, they will gain more independence. Additionally, giving a list of the possibilities would have made them feel limited on the activities they could do if many of their everyday activities were not listed.
During the presentation a verbal evaluation was conducted. Students were asked the importance of activity, and the benefits of warming up/cooling down. The students were engaged in all aspects of the presentation and were very knowledgeable about the subject. An open discussion occurred about the importance of activity, and the key points were reiterated by the leaders of the study. After the explanations, children were assessed by asking important points of activity. All questions were answered correctly.
Many of the objectives were met: students were able to list activities, the time that should be spent on the activity, safety precautions during activity, and warning signs of when to slow down. By the end they were motivated to engage in the competition and daily activity.
During the follow up visit, the winning team was awarded their prize. The short term goal of achieving 720 tallies was not attained. Although the willing team did not reach the expected goal, there are many factors which could have contributed to that. The students were instructed not to tally points for gym class or recess. This was to motivate activity outside of the time provided for specific activity. Adults are less likely to engage in regular physical activity when it is removed from their mandatory schedule. By not allowing children to count time where physical activity is required, it will encourage self-discipline and responsibility. Allowing the school activities to be tallied would make this goal more achievable.
This intervention had many positive contributions to its effectiveness; it provided motivation for activity, while explaining the benefits of activity. Additionally, it did not force students to do anything, but gave them the choice and inspiration. The information being delivered by nursing students also gave the information more accreditation because of the direct connection with health care.
The weakness of this activity is the possibility that the activity level will return to its baseline after the competition is over, making this a temporary solution. However, it is likely after the week the students will feel the benefits of activity, and enjoy engaging in activities with their friends, which will motivate them to continue with activity. Additionally, the education presented about activity is not temporary; therefore they are able to make an informed decision about activity. If they chose not to have regular activity, they will know the risks that are involved with their decision.
If this intervention were to be redone, there would be significant benefits to having a handout that could be distributed to each individual student. Having a personal reference as a reminder would increase the likelihood of engaging in activities. However, since this was not possible, a fact sheet was given to each individual class as a reference. Having more attainable short-term goals or allowing for tallies to be added from school would be beneficial as well. Additionally, providing more time for the intervention would allow for a thorough pre-assessment, post-assessment, and research to be done on factors impeding activity in our aggregate population.
While studying this aggregate, the importance of early intervention became very apparent. It is important to assess factors contributing to the specific population, rather than the population as a whole. Providing this individualized care for the aggregate population will allow for attainable results. The study performed, along with the implementation of an intervention, demonstrated the benefits of an individualized plan for the population.

Asthma is a chronic respiratory disease that is characterized by three equally causative components (Berger, 2004). The first component is chronic inflammation of the lining of the airways (Bernstein, 2000). This inflammation can develop over a period of time due to the presence of chronic stimulation by asthma triggers and untreated or under-treated bronchospasm (Bernstein). Constriction or spasm of the muscles that surround the breathing tubes, commonly called bronchospasm or bronchoconstriction, is the second component of asthma (Berger). This constriction usually occurs due to acute exposure to triggers such as allergens or cold viruses and may also occur in response to exercise (Bernstein). The third component of asthma is the tendency of the lungs of an asthmatic to produce thick, tenacious mucus when over-stimulated (Berger).
Asthma, as briefly described above, is a significant cause of morbidity and mortality among school aged children in the United States (Doull, Williams, Freezer & Holgate, 1996). Childhood asthma is a disease entity that has a significant impact on pediatric patients as young as kindergarteners in terms of limitation on activity, missed days of school and emergency medical treatment (Grant et al, 1999). Pediatric asthma also has a significant impact on society due to the demand for healthcare resources, and the impaired quality of life of children with poorly managed asthma and their families (Clayton, 2005).
Unlike some other childhood health conditions, asthma is highly treatable and its long-term sequelae highly preventable (Creer, 2001). Children with previously undiagnosed or currently under-treated asthma are being identified in vast numbers due to a plethora of in-school asthma screening programs springing up as of late. In spite of these programs, asthma remains the leading cause of pediatric hospitalization and school absenteeism in this country (Clayton, 2005). In addition to the discussion of programs for school-based screening of children for asthma that may have been previously undetected or under-treated, recent literature has also discussed many asthma education and self management programs, frequently incorporated into the regular school curriculum of asthmatic children (Velsor-Friedrich & Srof, 2000). Most pediatric asthma assessment tools published to date focus on quality of life issues or rely on partial parental input for completion.
This pilot study will test a pre and post educational intervention assessment tool designed to aid the asthma educator in gauging the effectiveness of a given intervention program. It may also be useful in an office setting to gauge the effectiveness of educational information provided during an evaluation. This will involve revision of an existing assessment tool for adults with asthma, the KASE-AQ, to be appropriate to a pediatric population. The proposed tool is intended to be completed solely by the pediatric patient with asthma. Continued efforts in the detection and management of asthma with school based programs nationwide, may significantly decrease school absentee rates, activity limitations, hospitalizations and emergency room treatment required by children as a result of asthma (Christiansen & Zuraw, 2002). These improvements in pediatric management would contribute to meeting the objectives of Healthy People 2010 that relate to asthma and decrease the financial burden on the healthcare system that is caused by asthma related expenditures; however, a tool to assess the effectiveness of such programs is indicated. Such a tool must be developmentally appropriate in content, pertinent to the everyday life experiences of children with asthma and broad enough in scope of material covered and questions asked in order to be applicable and useful with various asthma education programs.
A review of recent literature reveals varied questionnaires and surveys designed to assess assorted aspects of asthma in children. Many published pediatric asthma assessment tools to date examine only one aspect of this chronic condition, typically quality of life (QOL). While quality of life is a very important aspect of any chronic condition, assessing the knowledge of a child regarding his or her asthma is not addressed. Many asthma assessment tools rely on input from a parent or completion of the survey entirely by the parent. One might argue that a survey tool completed by the child would better determine his or her own attitude regarding his or her asthma and the level of self-efficacy of symptom management.
The Paediatric Asthma Quality of Life Questionnaire, the Feeling Thermometer (Juniper, Guyatt, Feeny, Ferrie, Griffith & Townsend, 1996) was one of the first questionnaires published in recent literature that deals primarily with evaluating asthma in children. This 23 item tool uses a Likert-type scale to assess different aspects of everyday life that a child with asthma might find troublesome. To use this questionnaire, a trained interviewer would administer it one on one with a child at a clinic visit. The interviewer asks a question and the child responds by choosing an answer from color-coded flash cards. By incorporating changes in the questionnaire over time, coupled with objective data such as pulmonary function test results and physical assessment, the authors suggest that a more complete picture of a child's health might be evaluated. Clinical testing of this tool noted that it had valid measurement properties in children from age 7 to age 17. The limitation of the questionnaire identified by the authors at the time of original publication is the small sample size, 52 children, used to test it. Strengths, as discussed by the authors are that it "is simple and easy to use and is applicable to children as young as seven" (Juniper et al).
Bursch, Schwankovsky, Gilbert, and Zeiger (1999) reviewed four health belief measures constructed for children with asthma and their parents. The Parent Barriers to Managing Asthma scale measures perceived barriers such as lack of access to medical care, transportation difficulties, side effects of prescribed medications, cost, time constraints, child care burdens, difficulty understanding medical devices and problems getting the child to take the asthma medication. The Parent Asthma Self-Efficacy scale measures self efficacy of the parent with regard to prevention and management of asthma attacks in the child. The Parent Treatment Scale measures the parents' beliefs about whether various asthma treatments will be effective in preventing or managing asthma symptoms. The Child Asthma Self-Efficacy scale measures the self-efficacy of the child with regard to prevention and management of asthma attacks. One limitation of these assessment tools explained by the authors is that they were tested only among families enrolled in the Kaiser Permanente Health System. Because this is a pre-paid health plan, member's perceived barriers may differ from those of families with an asthmatic child who is enrolled in other health care plans. They suggested future research with a broader population to examine the relationships between these scales.
Bukstein, McGrath, Buchner, Landgraf and Goss (2000) described a parent-completed asthma quality of life questionnaire. For evaluation of this tool, a parent of an asthmatic child completed a general questionnaire and an asthma specific questionnaire during an office visit. Parent responses to the survey questions were compared to pulmonary function test results and physical assessment finding in order to gauge changes over time. The final result was an eight item, asthma specific questionnaire, the Integrated Therapeutics Group Child Asthma Short Form. A limitation of this tool is that the parent assesses the quality of life of the child without input from the child. Though this may be applicable and likely necessary with younger children, one might surmise that health-related quality of life of adolescents and teens might be more accurately assessed by patient completed questionnaires. One strength of the tool is that its brevity makes it practical for use in clinical and office settings.
Santanello (2001) contends that asthma medications may not be adequately tested in children because of the unique challenges inherent in pediatric clinical trials. In attempting to address these challenges, she discusses the validation of two symptom diaries, conducted by researcher in the pharmaceutical industry and used for pediatric asthma assessment specifically in clinical trials. The Pediatric Asthma Diary is a patient completed tool designed to evaluate daytime and nighttime asthma symptoms in children ages 6 to 14. The Pediatric Asthma Caregiver Diary is a parent completed survey to assess asthma symptoms in two to five year old children. Responses to the diary questions by either the parent or child were compared with objective assessments of health and asthma symptoms made by health care professionals. Results of both diaries correlated accurately with physical findings of a health care provider over the course of the pilot project. Santanello concluded that diaries such as these can provide a reasonable assessment of pediatric participants in asthma trials but that because varied aspects of the asthma disease state are measured, accuracy increases when diaries are used in conjunction with physical examination and pulmonary function testing.
Hall, Wakefield, Rowe, Carlisle and Cloutier (2001) validated a questionnaire designed to aid primary care providers in diagnosing pediatric asthma. The Easy Breathing Survey, as it is called, was administered to all new pediatric patients seen at six primary care clinics in Hartford, Connecticut. The finished product is a four question survey that can be used in children ages 6 months to 18 years. It can be completed by parents of children who are too young to read or by the children themselves. A combination of the responses on the survey, additional verbal questioning and history from a child's medical record was used to determine if the child had asthma. Major strengths of this survey are that it can be completed rapidly and that it demonstrates high sensitivity for picking up severe levels of asthma. One weakness is that it is not as accurate in diagnosing milder asthma in patients being seen for the first time.
Redline, Larkin, Kercsmar, Berger, and Siminoff (2003) conducted a school-based asthma and allergy screening project utilizing survey instruments completed by parents and children. The Parent Symptom Questionnaire included five questions regarding asthma symptoms, four questions that deal with allergic rhinitis symptoms and one question that pertains to symptoms of atopic dermatitis. The Student Symptom Questionnaire contained 25 questions assessing asthma, allergic rhinitis and atopic dermatitis symptoms. The authors commented that their validation sample was rather small but that the surveys were accurate in recognizing previously undiagnosed asthma and allergy type conditions in the participating children. The major drawbacks of any school based program, such as this one, are the expense involved and the need for repeated attempts to collect data due to initial low response rates.
Gorelick, Scribano, Stevens and Schultz (2003) discussed a tool designed to predict response to intervention in children treated for acute asthma. The Child Health Questionnaire was administered at the time of treatment for an acute exacerbation of asthma and then at a 14 day interval following emergency room treatment. The majority of the patients enrolled did not have regular access to health care and tended to use emergency services as primary care. While this is not an ideal situation for long term asthma management, it worked well for this project as the majority of the patients returned to the emergency department for the recommended 14 day follow up. Overall, changes in the perceptions of the children with regard to asthma symptom status correlated with objective findings. A weakness of this study is that physiologic measures, such as peak flow values or pulmonary function testing were not used as comparative measures at the follow up visit.
Varni, Burwinkle, Paroff, Kamps and Olson (2004) designed and tested a modular instrument to measure quality of life in asthmatic children ages 2 to 18. They explained the benefits of incorporating disease specific language and assessment parameters into a generic quality of life survey. They noted that the cross information obtained from surveying both the pediatric participants and their children supported the need to measure both perspectives in order to fully understand the impact of asthma symptoms on quality of life. They also explained that the correlation between the responses of the younger children and the responses of their parents was important in that the parental perspective can be utilized at times when children are unable or unwilling to answer questions.
Chan, Mangione-Smith, Burwinkle, Rosen, and Varni (2005) used a shortened version of the same pediatric quality of life tool in a later project. For the purpose of this project, children without asthma, termed "healthy" by the authors, and asthmatic children were surveyed with the same instrument in order to determine if the instrument could distinguish between the different clinical statuses of the groups. They determined that the shorter version of the survey should be useful in assessing treatment effectiveness and quality of life changes in clinical research studies of children with asthma. One module of this survey deals with activity limitations in the form of missed school days. A weakness of this tool is that the missed school days are not qualified as pertaining to asthma symptoms or to other, un-related illnesses or reasons.
While all of the reviewed pediatric asthma assessment tools report accurate measurement of quality of life and related changes, whether completed by the pediatric asthma patient or by the parent of the patient, none assess concrete knowledge about asthma symptoms or how the child feels about his or her asthma. The KASE-AQ measures three aspects of chronic disease management that contribute to quality of life. Attitude, the first variable of the KASE-AQ, is an important variable in the success or failure of treatment. Self-efficacy, or the confidence that a patient has in successfully implementing his or her treatment plan is a second variable measured by the KASE-AQ and not specifically addressed in a quality of life tool. Knowledge of asthma physiology and symptom recognition is the third variable specifically addressed by the KASE-AQ. If a patient exhibits a positive attitude, he or she will be more likely to be compliant with medication and to devote time and effort toward symptom management. A high level of self-efficacy, coupled with a positive attitude and a working knowledge of asthma symptoms, triggers and self-management techniques may not only increase treatment compliance and allow for recognition of early warning signs of asthma destabilization, but also increase the persistence of a patient in attaining control of symptoms during periods of symptom exacerbation.
The Knowledge, Attitude and Self Efficacy Asthma Questionnaire (see Appendix A) was originally designed as a pre and post adult asthma intervention tool to aid in determining the effectiveness of a program about asthma management (Wigal, Stout, Brandon, Winder, McConnaughy, Creer & Kotses, 1993). The authors created a questionnaire designed to assess three domains: knowledge of asthma, attitude toward one's asthma and self-efficacy of asthma management. Each domain has 20 questions for a total of 60 questions that comprise the entire questionnaire. Each question is presented in a multiple choice format with a total of five possible answers supplied. Only one answer for each of the knowledge questions is correct. Rather than a correct answer, the attitude and self-efficacy scales are used to assess the effectiveness of an intervention by demonstrating a positive shift in answers between the pre test and the post test.
The knowledge domain considers factual information that an adult may have regarding asthma. A patient, in conjunction with his or her health care provider, may work better to manage the symptoms of a chronic condition if he or she posses a working knowledge of the disorder. In the case of asthma, a patient who recognizes early warning signs of an asthma exacerbation may better manage the increased symptoms by knowing the appropriate steps to take. An example of a question from the knowledge domain would be one asking what part of the body is not a component of the respiratory system.
Which one of the following is not a component of the respiratory system?
The attitude domain is the second component of the KASE-AQ. This domain is used to evaluate a patient's outlook on his or her illness or condition. If a patient exhibits a positive attitude with regard to his or her asthma, he or she will probably be more compliant with prescribed medication and other interventions suggested by the given health care provider. In the pre and post intervention test scenario, a change in attitude, ideally a shift toward the positive, would be exhibited as a result of a successful intervention program. An example of a question dealing with a patient's attitude toward asthma would be one that uses a five point Likert-type scale, from "True" to "False" to assess the patient's attitude at that time. Since these questions deal with the patient's attitude toward his or her own asthma, there are no correct or incorrect responses. A higher score suggests a patient has a more positive attitude toward his or her asthma and would likely be more willing and eager to work in cooperation with the physician to manage asthma symptoms. A lower score is suggestive of a more pessimistic and uncooperative attitude.
My physician can handle my asthma without my having to become involved.
The self-efficacy component of the KASE-AQ aids in assessing an individual's confidence in his or her ability to contribute to the management of his or her asthma. It has been hypothesized by asthma researchers that when a patient is non-compliant with a prescribed medication or treatment plan, health care providers frequently do not know if the subject is simply not cooperative or if he or she may lack the confidence in his or her skills with regard to asthma management (Creer & Levstek, 1996). An example of a question from the self efficacy domain would be one that is answered on a five-point Likert-type scale, from "True" to "False" and asks what a patient thinks about self-management techniques. Since these questions deal with the patient's self-efficacy regarding his or her asthma, there are no correct or incorrect responses. The higher the score, the more confident the individual would be in his or her ability to manage and control the asthma. The lower the score, the less confident the individual would be in his or her ability to manage and control any asthma symptoms.
I can take the necessary steps to avoid or to manage an asthma attack effectively.
In order to determine initial usefulness of the proposed tool in the pediatric population, approximately ten subjects, ages 9 to 12 inclusive, were recruited by means of flyers posted in the office of an asthma specialist. Criteria for inclusion into the pilot test group were boys and girls, ages 9 to 12 inclusive at the time of the project, who carried a diagnosis of asthma. The subjects had to be taking at least one daily controller medication to manage asthma symptoms. Subjects also needed to read and understand English. No specific exclusion criteria, other than not satisfying any of the inclusion criteria were employed. Subjects were recruited from a single asthma practice so as to produce a more homogenous knowledge base regarding asthma terminology and asthma symptom management, prior to the pilot testing project.
The instrument tested was based upon the Knowledge, Attitude and Self-Efficacy Asthma Questionnaire (KASE-AQ). This tool was designed in the early 1990's for use as a pre-post asthma intervention assessment in adults (Wigal et al). The KASE-AQ looks at three variables, as described previously, which the authors of the adult questionnaire explain successful medical regimens, for any disease process, are dependent upon (Wigal et al).
For the purpose of this project, the KASE-AQ was re-written in a language that might be more conducive to use in an early adolescent population (see Appendix B). All of the proposed revisions were reviewed by one of the primary authors (see Appendix C). The nature and order of the original questions was not altered in order to preserve the integrity of the scoring system. Because asthma treatment has undergone major changes since the publication of the KASE-AQ (NAEPP Expert Panel Report Guidelines for The Diagnosis and Management of Asthma-Update on Selected Topics 2002), some questions were no longer appropriate. For example, question 58 in the adult version refers for possible side effects of theophylline, a medication commonly used to treat asthma at the time the original tool was developed. Since theophylline is rarely used by asthma specialists at the present time, the population being tested has no knowledge of the side effects. This question was changed to refer to side effects of bronchodilators, the rescue medication prescribed for each of the ten participants selected and commonly used for asthma treatment today. Other changes included terminology substitutions as children in this age group think in concrete terms rather than abstractly. Question number 23 in the adult version assesses the person's confidence in managing exercise induced symptoms. For the purpose of the pediatric questionnaire, the question was altered to ask about gym class rather than exercise, thereby making an abstract idea, namely exercise into a concrete condition, that of gym class, so as to be more easily understood by children. Permission of the authors was sought prior to revision and use of the tool. The proposed questionnaire, an assent form and a consent form, and a recruitment flyer were approved by the University of Michigan Institutional Review Board (see Appendix D).
Flyers were posted in the office of an asthma specialist in Northwestern Ohio who is also one of the authors of the adult version of this assessment tool. Children and parents who were interested in participating in this project filled out a form and were contacted, via telephone, by the primary investigator to determine eligibility. The first ten children who satisfied the aforementioned inclusion criteria were scheduled to come to the physician's office for one appointment.
At the time of the appointment, the research project was explained to the child and his or her accompanying parent and any questions they had were answered. Each was given time to read the consent and/or assent forms and again asked if they had questions or did not understand any of the material provided. Each was asked questions, by the primary investigator, in order to determine that all of the material was understood. Informed consent was sought from each parent (see Appendix E). Assent was acquired from each child (see Appendix F).
Verbal directions consisting of "please read each question thoroughly and circle the best answer" were provided to each child by the primary investigator. The children were further instructed that in addition to assessing their ability to choose the correct answer, the project would also attempt to assess their ability to understand the language of the questions. Hence, they would not be allowed to ask questions of or request assistance from the parent who accompanied them to the appointment. The children were told that if they had a question, they would be able to ask the primary investigator only. All of the children verbalized understanding of the directions as presented. The only questions asked were regarding clarifying a word that likely would have directed the child to the correct response. In each instance, three in total, the child was reminded that part of the project was to assess their understanding of the tool. Each was encouraged to choose the best answer as well as they could and save the question until they were finished with the project. All verbalized agreement.
The only potential risk to any of the subjects that was identified by the investigator or the asthma specialist was that reading the questions of the tool might trigger some negative or undesirable feelings about asthma exacerbations or related instances that may have occurred in the past. The children and their parents were told that if any concerns or ill feeling arose as a result of reading the questions, they could feel free to speak with the primary investigator, the physician or any of the office staff regarding these matters. None of the children verbalized concern or ill feelings during the completion of the questionnaire or upon the conclusion. When each child finished the questionnaire, they were verbally asked to write those thoughts about it on the last page of the questionnaire. No mediations were administered and no changes were made to any of the participating subjects' current asthma therapy as a part of this project. Upon completion of the questionnaire, each child was given twenty dollars as a thank you for their time and participation.
The first ten potential subjects who responded to the posted flyer each met the aforementioned inclusion criteria and completed the project. Of these ten, six respondents were boys and four were girls. Four of the subjects were age nine, three boys and one girl. Three of the subjects were ten years old, two boys and one girl. Three of the subjects were 11 years of age, one boy and two girls.
As this project was conducted during the summer, the children were asked to provide the grade of school that they would be entering for the coming school year. Five subjects, four boys and one girl reported they would be entering the fourth grade. Two subjects, one boy and one girl, indicated they would be entering the fifth grade. Two subjects, one boy and one girl documented they would be entering the sixth grade. One subject, a girl, denoted she would be entering the seventh grade. The sample breakdown is provided in the following table. For the purposes of this pilot project, no special attention was paid to creating an even ratio among the variables of gender, age and grade in school. The following table denotes the sample breakdown.
Table 1 Sample Descriptors
There were no missing items on the ten questionnaires completed. It took the participants between 20 and 40 minutes each to read and respond to the entire survey. All were told prior to beginning the exercise that they could take breaks if they got tired. None of the respondent requested a break. Questions asked of the primary investigator revealed a rather large span of vocabulary knowledge, or lack thereof, among the children who participated. For example, several of the younger children were unfamiliar with the organs of the respiratory system, the words "resent" and "manage" and the meaning of the phrase "get the upper hand." The older children did not ask questions about these items.
Internal consistency coefficients were determined for the attitude and self-efficacy domains individually and in combination. A Cronbach's alpha value of .800 was demonstrated for the attitude and self-efficacy scales in combination, 40 items in the entirety. Seven of the items of these domains utilize reverse scoring, such that a response of "false" might indicate the most desired answer and "true" the least desired. These items are numbers 22, 28, 37, 40 and 50 of the attitude scale and 52 and 55 of the self-efficacy scale. Removal of the seven items of the attitude and self-efficacy scales from the analysis demonstrates a Cronbach's alpha of .892. Cronbach's alpha values for the individual scales were: attitude .843 and self-efficacy .767.
The results of the knowledge domain responses from the ten participants demonstrate a ceiling effect on one question, meaning that all of the children answered the question correctly. Ninety-percent of the children gave correct responses for one additional question. Of the remaining items pertaining to the knowledge domain, three items had eight correct responses and four had seven correct responses. The responses to the remaining ten questions were varied with no predictable pattern. There were no statistically significant correlations between correct responses and age of the child or grade in school. No statistically significant correlations were found between self-efficacy or attitude scores and age of the child and/or grade in school.
Each item of the knowledge domain was scored based on one point for the correct answer and zero points for any incorrect answer. There were 20 questions in the knowledge section for a total of 20 point s if all items were answered correctly. The mean response of the ten children who participated in this pilot project was 10.4 with a range of 7 to 15 points. A ceiling effect, meaning every child gave the correct response, was noted on question number 30, a question asking about things that could make an asthma attack worse. None of the children correctly answered question number 33, which dealt with side effects of rescue medication.
The attitude domain was made up of 20 questions, each with five responses. The responses were scored on a scale of one to five with a "perfect" score being 100. The mean score of the ten participants was 80.2 with a range of 55 to 94 points noted.
The self-efficacy domain of the tool was also comprised of 20 questions using the same five point scale for scoring. The range of scores for the pilot testing was 59 to 97. The mean of the ten children's scores was 80.5.
Additionally, a final page was included with each child's questionnaire asking for feedback about the instrument. The participants were given a list of descriptors that might be pertinent and asked to circle all that they thought applied. From the list provided, three children thought the exercise was fun while four felt it was not fun. One thought it was short, versus six responding that it was long. Six children chose interesting from the list while two chose boring. Seven indicated that it was easy to read. None of the children chose hard to read as a descriptive term. Lastly, six circled easy to understand and two circled hard to understand from the list of terms provided. Comments are displayed in the following table.
Table 2 Children's Feedback on the questionnaire
In addition to the list, the children were also afforded space in which to write any thoughts or comments they had about the tool. Of the ten participants, one chose not to write any thoughts about the survey in the space that was provided. The responses collected ranged from "some of the questions were strange" to different variations of it was fun and would help people. Two of the children commented that it didn't take as long as they had thought it would to complete the questionnaire. One said it seemed long because he "mostly knew most of the stuff." Actual comments given are listed in the following table.
Table 3 Subject Comments
Absence of missing items suggests that children of the age group employed were both willing and able to provide quality data of this nature. Most of the children indicated that the instrument was easy to read, though most said it was long. On average, it took approximately 30 minutes to complete, perhaps making it a bit lengthy to be used in its entirety during a clinic visit. In general, the instrument was understood by these children. The questions asked by the children were few and were mainly related to a wide variation in vocabulary knowledge among this age range.
The children who participated in this project came to the situation with much the same knowledge base regarding asthma and symptom recognition or management, having all been treated by the same physician and educated by nurses with similar training. The question with the ceiling effect dealt with situations that might make an asthma attack get worse. It stands to reason that the population examined would know the correct response since they had specific and repeated education about this during clinic visits. As expected, nine of the children responded correctly to a question about peak flow meters. This can be attributed to the fact that most of them have used a peak flow meter at some time. This information is also regularly reviewed during clinic appointments. None of the children knew the answer to a question regarding exercise-induced asthma. It is likely that these children were educated about how to prevent such symptoms rather than the physiologic processes that caused it making this finding understandable. It may also indicate an area where clinicians need to put a greater emphasis.
The attitude and self-efficacy scores showed a positive skew for most of the children. Even the lowest score among the ten was above the 50th percentile. One would expect scores of this nature, given the strong educational input and clinical support these children have received as part of their care at this specialty practice. It would be useful to compare these scores to those of children who are followed for asthma in a primary care practice.
The reliabilities for the attitude and self-efficacy scales combined and as individual scales were good. According to Nunnally (1978), in early stages of development, reliabilities of .70 or higher are sufficient. If significant correlations are found, further refinement of the instrument is warranted. A Cronbach's alpha of .800 for the attitude and self-efficacy scales combined indicate high reliability. Removal of the reversely scored items strengthens the reliability further to .892. Separate reliabilities of .843 for attitude and .767 for self-efficacy were also good. This suggests that the scales could be used separately or combined depending on the purpose of the research or clinical encounter.
In examining the items utilizing reverse scoring, the responses provided by many of the children are not consistent with their responses to other items of the same nature and topic. The fact that the combined reliability improved with removal of these items calls into question the effectiveness of such questions in this population. It is possible that asking a question in negative terms that expects a positive answer is more abstract than these children were able to comprehend. Further examination is needed to determine if some items could be deleted or modified. The tool is long and the possibility of reduction of some items warrants further consideration. This may increase its practicality and usefulness.
There are several weaknesses of the KASE-QA with pediatric revisions as demonstrated by this pilot testing. One weakness is the lack of clarity regarding the use of negatively worded questions or questions that seem to require a negative response in the early adolescent population. Further exploration of this area may reveal that more concrete questioning would better serve the need this tool aims to meet.
Another weakness is the length of the tool if used in its entirety. Though none of the children declined to complete the questionnaire, a majority did comment that it was long. This could be a limiting factor in usefulness in a clinic setting unless the time needed to complete the survey was calculated into the appointment. The reliabilities for the individual scales suggest that it would be possible to use the individual scales separately.
Another weakness to be discussed is the small sample size used for this project. The instrument needs to be administered to a much larger sample in order to do more advance statistical testing of its psychometric properties.
A final weakness of note is the relative homogeneity of the pilot sample. The ten children who participated in this project are all treated for asthma by the same specialist hence their knowledge of asthma and attitude toward asthma is consistent with the knowledge and attitude demonstrated by the staff of this office. Testing this instrument with children who are newly diagnosed with asthma or who are followed for asthma in a primary care setting is also needed.
More diversity in the sample demographics would also be useful. A majority of the ten participants come from families that enjoy higher socioeconomic status than that of average Americans. Many of these ten children come from homes that enjoy a two-parent household with a stay-at-home mother. The majority of the children have other family members, either a parent or sibling who are also patients of he same asthma specialist. A tool such as this might not reflect the same findings if used in a general population of asthmatic children.
Future work with this questionnaire should encompass a much larger sample size with special attention to an even ratio of boys and girls, ages nine, ten, eleven and twelve, and the grades in school representative of this span of ages. Testing in a population of asthmatic children from a more diverse background, both socioeconomic in nature and in the access they have had to health care professionals is also indicated to determine if the aforementioned findings hold true. Some items in the knowledge domain could be simplified to better represent the vocabulary range of the intended population. A comparison between the attitude and self-efficacy domains with and without items that utilize reverse scoring is also indicated. Consideration of this tool for use as two or three separate measures may also be of value.
Overall, the findings suggest that further testing and evaluation of the KASE-AQ with pediatric revisions is warranted and would be valuable. An instrument of this nature could be helpful to both researchers seeking to understand the mechanisms children use to cope with asthma and to clinicians working with these children in office or school based settings.

In Naming & Necessity [1980] Kripke claims that there are truths that are both contingent and known a priori, and others that are both necessary and a posteriori. In this paper I aim at two goals: (1) to study the cases of necessary/a posteriori and contingent/a priori truths using Stalnaker's two dimensional apparatus; and, given the results of such evaluation, (2) defend that there is a puzzle here, a genuine philosophical befuddlement that should be properly solved.
Stalnaker's two dimensional apparatus is a technical device that is meant to be useful for understanding a particular kind of speech act, that of assertion. It works mainly by accounting for the different things (e.g. propositions) that might be said by (i.e. expressed as the content of) different assertive utterances of any particular sentence.
Assertions are the speech acts by which we communicate truths. It is my goal to find out whether certain particular statements express truths that are both necessary and a posteriori. Thus if there are such truths we could, at least in principle, assert them. I will focus on the content determination feature of Stalnaker's apparatus in order to explore whether there are any assertions that may communicate something that is both necessary and a posteriori.1 That is why the two dimensional apparatus is relevant for the discussion. So, let us see how the apparatus works.
Some definitions are important to bear in mind:
(W) A possible world (w from now on) is a complete way in which the world might be. (P): A proposition (P from now on) is a representation of the world as being in a certain way (more formally, it is a function from w's to truth values). (PC) A propositional concept (PC from now on) is a function from ordered pairs of w's into truth values. (SP) A speaker presupposition SP is a proposition the truth of which is taken as a background assumption for the conversation. (CS) A context set CS is the set of all w's which are compatible with all SP's.
Stalnaker's apparatus has the following tenets:
 (i) An utterance U of a sentence S expresses a proposition P. (ii) A proposition P is determined by the set of w's which it represents. (iii) A proposition P is determined by the set of w's in which S is uttered. (iv) The main goal of assertion is to reduce CS as to eliminate all w's incompatible with what is said.
(i) is an assumption that a good number of philosophers share nowadays. I will not comment on that in this paper, although I think it might be a problematic tenet. What about (ii) and (iii)? According to (P), for every P there is a set of w's which are represented by P and which share that particular way of being. But not only, it is also true that for every set of w's that share a particular way of being there is a P which represents that. Thus, there is, as Stalnaker puts it, a one on one correspondence between P's and sets of w's. The proposition will be true in all the w's of that set and false in all others. That is why it makes sense to understand propositions as functions from w's to truth values; and also why (ii) is part of the tenets of the apparatus.
But the content of S is not only determined by the set of w's which it represents but also by the set of w's in which S is asserted. The content of an utterance (e.g. the assertion of P) is determined by the facts of the world in which it is asserted. The world plays a determining role as the context of utterance. This is another sense in which a set of w's determines a proposition, and which is implicit in (iii).
As for (iv), suffice it to say that when a speaker asserts something it is her intention to get the hearer to believe in what is asserted (e.g. a proposition) and thus, to take the actual world as being in that way. In order to do this it is necessary to reduce the set of possible situations compatible with what is said so that, at the end of the day, there will be one P taken to be said and one w taken to be the actual. If this were not true, then asserting would stop being useful, and perhaps end up being among the endeavors that humans take just for fun. No wonder why Stalnaker takes CS and SP to be central notions for the characterization of speech contexts.
To be fair to the ways in which the content of an assertion is determined one must take an ordered pair of w's and P's, such that for every w we get a P. Suppose that I assert (1),
and let us take it as an example of a stipulation. Now, evaluate it according to the following different worlds: where the standard meter is one meter long (w1), where it is two meters long (w2), and where it is three meters long (w3). This is supposed to be a case of a statement that expresses a contingent a priori truth; let's assume, if only for a brief moment, that by "truths" we mean "what is said" by the utterance of the sentence (i.e. in this case a proposition). A will be the propositional concept of (1), where the horizontal lines represent what is said by utterances of (1) in different w's taken as contexts.
We can see that there are only contingent propositions represented in A. This seems a little bit awkward. At a first glance, if I stipulate the length of a meter by asserting that The standard meter is one meter long, what I say is something that is true in every world where I make the assertion (i.e. in every world where I make the stipulation), and, thus, should somehow have true evaluations in every world. Put in other words, there is certainly a sense in which I cannot be mistaken about the truth of my assertion, a sense in which it is necessarily true. This being such because in every world where I assert (1) I know a priori (i.e. thanks to my stipulation) what the length of the standard meter is.2
This has to be among the things that I could say by an assertion of (1); i.e. it is at least among the information that could be imparted by it. However, there is no horizontal representation of such a proposition in A. Nonetheless, such necessary proposition is in fact represented diagonally from left to right and from top to bottom in A. Since there is only one such proposition, we can call this the diagonal proposition of A. We can make use of a two dimensional operator (i.e. one that a PC and makes it into a PC) called dagger operator, such that it takes the so called diagonal proposition of a PC and projects it into a PC. dagA is the PC which results from applying the dagger operator into A.
dagA accounts for the possibility of my expressing a truth in every w as a context by asserting (1). Let us then take A and dagA as offering the set of propositions that a speaker may convey by asserting (1); which, as (iv) says, will be ideally reduced to up to one line, such that both speaker and hearers will know what is asserted.
This is enough about the two dimensional apparatus for our purposes. With it we are able to see which propositions can be expressed by the assertion of a particular statement. These propositions are the contents (truths and/or falsehoods, according to our assumption) that a speaker gets to assert. Now we are up to see whether this standpoint helps us finding out if there are contingent a priori truths and/or necessary a posteriori.
At this point you might wonder whether I still need to say something in order to show that there is a truth that is contingent a priori. You might even think that I am too stubborn in aiming at this same spot once again. And you might be right, but not for the proper reasons. Yes, A clearly has propositions that are contingent. And, yes, thanks to the device of the diagonal proposition, it is also clear that there is a proposition that is a priori. But, no, up to now we have no clue as to which one is the truth that is both contingent and a priori. I am sad to say that (given our assumptions) no such clue will be offered, or so I will argue.
We can see here that a particular relation holds between the diagonal proposition and the (a priori/a posteriori) type of knowledge that the speaker has of the proposition asserted according to the context. Which relation this might be is not clear at all. Stalnaker puts it this way
If we take Stalnaker's claim to be merely about sentences -- i.e. about things like "The standard meter is one meter long" -- then it is trivially true that there is a truth which is both contingent and a priori. But this just means that the sentence can express contingent and a priori contents. Whatever is contingent and whatever is a priori would not be the truth (i.e. the sentence) but the content of the truth (whatever that means). You might then argue that it is not the sentence but the whole of A which is contingent and a priori. This cannot be the truth in question, for several reasons. First, according to tenet (i) when uttering (1) I expressed some content or other among the ones that A has, but not all of them. What I expressed is a proposition, not five, or ten, or a thousand of them. Second, according to tenet (iv), if I were to do this (i.e. assert A and not just one proposition among the set) then my assertion would be useless. I would not be able to reduce CS and, hence, to communicate something at all. My assertion would not help to carve logical space in a way that it helps us locating the actual world. Propositions seem to be a good candidate for whatever is expressed in our utterances.3 Why not take propositions to be the truth in question?
This option, however, is also problematic. We should be careful not to mistake Stalnaker's claim above. It might be taken to say that the diagonal proposition is that which constitutes the a priori truth. But this is a mistake, since the diagonal proposition is necessary and the a priori truth -- at least in our case -- is contingent. It seems that we should take claim that the proposition that constitutes an a priori truth is not a necessary proposition, but that in order for it to be a priori, the diagonal proposition of the PC in which such proposition is represented must be a necessary one although not the one which constitutes the a priori truth. This seems like a nice interpretation, but it is not.
We have to ask now which proposition is the a priori truth. As we have seen, it cannot be the diagonal one, but it cannot be either any of the horizontal propositions for one main reason: none of these propositions expresses a truth in every context, as Stalnaker thinks the a priori truth does. Of course, if we take all the horizontal propositions together we could say that there is at least one truth expressed in every context. But then we would, again, be conflicting with (iv). You can see why I think there is a problem here.
You might wonder whether my argument is flawed. I did myself, but I don't now. I have a way to solve this particular disagreement about my argument. Why not take a look at the other interesting case, that of necessary and a posteriori truths? According to Kripke, statement (2) can express such a truth.
If, say, a famous chemist in the 1950's, asserted (2) while talking to one of her colleagues -- she just found out about the chemical structure of gold -- she would have uttered (if a truth) a necessary truth for two reasons. One, she would have used a rigid designator, a natural kind term such as "gold" that would refer to all and only the instances of gold in every possible world where such natural kind has instances. Two, she would have ascribed an essential property: having atomic number 79. Hence, a property that (if true) is true of an object in every possible world where such object exists. So, our chemist would have uttered something true of gold here and true of gold in every possible world. She would have uttered a necessary truth. But, of course, as I said above, she found that out (presumably after many years of tough empirical research). So, what she claims to know she knows it a posteriori.
Now, let's run the two dimensional apparatus for these w's: w1 (the actual world where the natural kind Gold is the kind of elements that have atomic number 79), w2 (a counterfactual world where the tokens of Gold have atomic number 15 and where the element with the atomic number 79 does exist), and w3 (a counterfactual world where Gold is not a basic element and where there is no element having atomic number 79). Accordingly, the PC of (2) is given by B
In B we find either necessary truths or necessary falsehoods (or impossibilities if you want), but nothing that might look as a contingency. However, there is a sense in which what our famous chemist uttered in 1950 could have been false. After all, that's what she found out after some research, perhaps she messed up some stuff and came out with a mistaken result. Who knows? The point is that it seems to be true that it is not necessary that she came out with those results and knowing what she knows. So what she knows -- which allegedly is what she asserted -- is something contingent. This, as we have seen in the former case, is not a problem. There is, at least Stalnaker thinks so, some contingent proposition represented in B. That is the so called diagonal proposition.
We said that the truth expressed was a priori if and only if the diagonal proposition was a necessary truth. Mutatis mutandis we can say that the truth expressed is a posteriori if and only if the diagonal proposition is contingent. We may ask now, which one is the necessary and a posteriori truth-taking truths as propositions, not as sentences. This should be easy for there is only one necessary proposition in B. Is that our proposition? Well, it depends. If our proposition could be false, as it seems it could, given the fact that it is a posteriori, then this is not our proposition. Well, then, the proposition must be the diagonal one. That one certainly could be false for different w's, it is actually false for different w's. Is it? Well, its not. Our proposition is not only a posteriori, it is also necessary and the diagonal proposition is not.
There seems to be some incompatibility between a proposition being necessary and its being known a posteriori, mutatis mutandis for the other case. We come to a point where one must stop looking for such a proposition, at least within our two dimensional apparatus. This however is no satisfactory conclusion, for one main reason: the knowledge of the famous chemist who asserted (2) was in fact false for different w's as context, as much as my knowledge of what I assert with (1) is in fact true in every w as context. Presumably what she and I know is just what she and I respectively asserted. But what she asserted is just necessary (i.e. true in every w) and what I asserted is just contingent (i.e. true in some w's and false in others). If you feel like there is some sort of problem here you have the proper feelings (if not, you should read back again).
You might still think that this does not prove Kripke to be mistaken. I agree. We must try to make sense of this somehow, but we should be careful not to do it in the wrong way. I think there is a bit of a problem here for philosophers to solve. This is because, as I shall argue, Kripke is not only correct in claiming that there are necessary a posteriori truths. Even more, Kripke must be correct if our commonsensical view that empirical scientific knowledge is possible is to be correct.
Our problem goes like this: there is this peculiar set of truths which we can know a priori regardless of their being contingent (mutatis mutandis for the necessary a posteriori); however, there is no such proposition which is at the same time contingent and a priori (...). Nevertheless, it seems clear that what is expressed by a sentence, understood and known by the speaker who understands the content of a utterance, is a proposition.
One way to eliminate this problem would be to claim that nothing can be, at the same time, contingently true and known a priori (the same for the necessary a posteriori). Knowledge and truth go together in only one way, if it is necessary it must be a priori, if it is a posteriori it must be contingent. So, actually, there is no problem here. There was just confusion to begin with.
There are some reasons why we should avoid this strategy. First of all, it will force us to accept either one of these two really bad claims. If we accept that a posteriori truths are only contingent -- assuming the necessary ones are a priori -- and claim that we can know what the world is
Perhaps we don't want this conclusion, but still claim that only a priori truths are necessary and the rest are contingent, so we choose the second bad option. If we accept the former, and defend that, nevertheless, we can know what the world is about by a priori means, then: (b) we can know about the nature of the empirical world without even looking at it, we just need to engage in some a priori reflections and we are set, we will know everything there is to be known about the nature of things.
These two claims are equally bad and for the same sin: they are both too polarized. The first one seems to be some extreme realism of the form you can't -- where "can" has a metaphysical strength -- really know what things are made of. The second one seems to be some extreme idealism of the form it's all in your mind, you can know everything by just deploying the proper concepts. The first one demands too much about our epistemic apparatus; the second one demands too little about it. We should avoid both as much as possible.
Another strategy is perhaps to avoid the problem . This would be possible if we just decide not to talk about a priori and a posteriori truths, for instance. We might be content if we just talk about the necessity or contingency of what is asserted or known. But this does not seem to solve the problem We can still raise the problem by claiming that there are necessary truths that are known by doing empirical research; and we still have to explain how this is possible. I think this is just a cheap way out and, as many have said, there are no free lunches in philosophy.
So, if we don't want to end up getting nothing (i.e. no solution) for too little (e.g. avoid talking about a posteriori/a priori), or asking too much (i.e. extreme realism) or too little (i.e. extreme idealism), we should look for a solution. Something, that is, which might explain how a truth can be contingent and known a priori, and even more importantly, how scientific truths are a posteriori and nevertheless necessary. I guess there are many different strategies to follow from this point. I think, however, that the most promising one is to revise our assumptions, particularly tenet (i) according to which what is said, expressed and known is identified with one and the same single proposition. We might need to make some complex distinctions within the content in order to solve this, perhaps distinguishing -- as Evans [1979] suggests -- between that part of the content which has the modal properties and that which is known, believed, desired, etc.

Light incident on a metal releases electrons, called a photocurrent. This well-known phenomena is called the photoelectric effect. In 1887, Heinrich Hertz accidentally discovered the photoelectric effect during his famous experiment in which he produced and detected electromagnetic waves (Tipler & Llewellyn 1999). Hertz was using a spark gap in a tuned circuit, and he noticed that the spark length was shorter when he enclosed the apparatus in a dark case. Though Hertz was annoyed by the effect at first, he later studied it and published his results. Other physicists confirmed and extended Hertz's research. In 1900, P. Lenard determined that the particles were electrons (Tipler & Llewellyn 1999). Classical electrodynamics suggests that the maximum photocurrent is proportional to the intensity of the incident light. Lenard observed this as he expected. However, Lenard also observed that there is no minimum intensity necessary to induce a photocurrent, which contradicts classical expectations. More generally, the energy of the emitted electrons does not depend on the intensity of the incident light.
In 1905, Albert Einstein explained these confusing observations by assuming the energy quantization hypothesis used by Max Planck in his solution to the blackbody problem (Tipler & Llewellyn 1999). If it is supposed that light of frequency ν is composed of discrete packets (photons) with energy hν, where h is a constant, the above results are expected: an electron is ejected only when a photon of sufficient energy hits the metal cathode. If the frequency is too low, no photons will be emitted. The intensity does not affect the energy of the emitted electrons, but instead determines the emission rate.
According to Einstein's predictions, the energy of an ejected electron is
where W is the amount of energy necessary to eject an electron from the metal.
Robert Millikan conducted careful experiments that showed the electron energy is linearly proportional to frequency. His results, published in 1914 and 1916, confirmed Einstein's prediction and gave a value of h that agreed with Planck's value. Einstein won the Nobel Prize in 1921, and Millikan won it in 1923. One of the main reasons that both Einstein and Millikan won the Prize was their work on the photoelectric effect.
We sought to confirm Einstein's predictions and measure the value of h through the photoelectric effect. We used an apparatus made by the Daedalon Corporation that consists of a photocell (large photocathode and thin wire anode) housed inside a metal box. There is a retarding potential applied across the photocell, i.e., a power source with positive terminal attached to the photocathode and negative terminal attached to the anode. The retarding potential can be adjusted with a knob on the box. Also, there is a current amplifier that converts the pA anode current to a measurable voltage. The zero point of the current amplifier can be adjusted with a knob on the box labeled "Zero Adjust". See Amidei (2004) for schematic diagrams of the apparatus.
The apparatus has output jacks that can be connected to a digital voltmeter (DVM) to measure the retarding voltage. There is an analog ammeter on the front of the box that measures the anode current, but we ran leads out of the box so we could read the anode current off of a DVM.
We used a mercury arc lamp and a helium-neon laser as our light sources. In order to isolate single lines of the mercury spectrum, we clipped filters onto the front of the box. We used high pass filters with wavelengths of 546 nm and 405 nm and interference filters with wavelengths of 577.7 nm, 435.8 nm, and 546.1 nm. (High pass filters pass all wavelengths above the specified value. This does not matter, however, because the stopping potential is eqal to the energy of the highest frequency, i.e., shortest wavelength, electrons.) We also tried to use the mercury UV line by not putting a filter on the box, but this did not work because the maximum retarding voltage is less than the stopping potential for the photoelectrons ejected by the UV light. The wavelength of the laser is 632.8 nm.
In order to minimize the effect of ambient light, we placed the lamp and laser as close to the window of the box as possible, covered the apparatus with a black cloth, and turned the room lights out.
For each wavelength of light, we measured the photocurrent as a function of retarding voltage. Before we took measurements, we determined a rough estimate of the stopping potential and set the zero point for the anode current. Starting with retarding voltage VR = 0.00 V, we measure the photocurrent I (output as a voltage) every 0.10 V until we neared the neighborhood of the stopping potential VS. When close to VS, we decreased our step size. By observing the range of values of the anode current for each retarding voltage, we determined uncertainties on the current values. The uncertainty decreased as we neared the stopping voltage; in this regime, the uncertainty was on the order of ±0.1 mV.
It was expected that the uncertainty in our final values would be dominated by the uncertainty in I and by systematic error introduced through our method for determination of VS. The uncertainty in wavelength is minimal, because Professor Amidei showed that the FWHM of the mercury lines is on the order of angstroms. The uncertainty in VR is also negligible because the DVMs have sensitivity of ±0.1 mV.
Plots of photocurrent I versus retarding voltage VR for each filter are given in Figure 1. The colors of the data points correspond to the filter used, as is explained in the key. (The filters denoted "cheap" are the high pass filters.) It is easily seen that lower wavelength (higher frequency) light required a greater stopping voltage. This is in qualitative agreement with Einstein's predictions. Error bars are not plotted to prevent the figure from being cluttered. As mentioned above, uncertainties near the stopping voltages are on the order of ±0.1 mV.
We extracted VS from the curves of Figure 1 through a simple procedure: we found the two smallest values of VR that had the same value for I and defined VS to be the retarding voltage halfway between the two points. We defined the symmetric uncertainty to be half of the difference between the points.
The above procedure is the biggest source of possible systematic error in our measurement. The stopping voltage is not well-defined for various reasons. Most importantly, the photocathode has a nontrivial depth, so the emitted electrons will have a range of energies. It is useful to think of the cathode as a diode. The natural direction of current flow for the photocell is from cathode to anode, and the retarding voltage resists this. The plots of I vs. VR resemble the same plots for a diode, as neither are sharp delta functions as they would be ideally (Melissinos & Napolitano 2003). Also, in both situations, if a large enough voltage is applied, current can be made to flow in the opposite direction.
The stopping potential vs. frequency is shown in Figure 2. We did linear regression on the data and found that the best fit is given by
The standard deviation of the fit is 1.61, which is of order 1, so the fit is good and uncertainties are reasonable, i.e., the fit residuals essentially obey a Gaussian distribution.
Figure 2: Stopping voltage (V) vs. frequency (Hz), with uncertainties. The six data points are fitted with a linear regression line. The slope is the measured value of Planck's constant and the y-intercept is the work function of the photocathode.
The measured value
The work function of the photocathode is
We have verified Einstein's model of the photoelectric effect and measured Planck's constant. It was observed that photoelectron energy is linearly proportional to frequency with proportionality constant equal to Planck's constant. Our value

Chemistry is intrinsically concerned with the structure of molecules and the reactivity of molecular systems. One of the principal concerns of chemistry is to find the energetically accessible confirmations and/or equilibrium structure of a given chemical system.
In general, the potential energy surface (PES) of a molecule is a 3N-6 dimensional surface, where the coordinates are the 3N-6 internal coordinates obtained when translational and rotational degrees of freedom are annihilated. In theory, one can calculate the PES for any system by solving the time-independent electronic Schrodinger equation for every possible position of every atom, however, such ab initio calculations are too expensive to be considered.
Many force fields used in practice today are empirical force fields (AMBER, CHARMM, etc.) that can model the PES of a large variety of chemical systems. Such empirical force fields are based on theory, experiment, and also intuition. Therefore, in practice, the PES is almost always defined through the choice of an empirical force field that describes the inter-atomic interactions in a molecule. This is basically choosing the "correct" functional form and the parameters which combined, will describe all inter-atomic interactions.
In considering chemical systems, the PES is a 3N-6 multidimensional function. From basic multivariable calculus, one knows that at a minimum, the first derivative of the potential with respect to each individual variable vanishes (that is) and that the second derivative of the potential with respect to each variable must be greater than zero. For a chemical system, finding the minima on the PES corresponds to finding the stable points of the system and thus, finding the geometry of the molecular system. Finding the global minimum corresponds to finding the conformation of a system with the least energy, the equilibrium conformation. Transition states that connect minima on the PES are defined as saddle points, that is a point on the PES where the PES is at a maximum with respect to one coordinate and a minimum with respect to all other coordinates. The transition mode, which will describe the motion in going from one minimum to the next, is described by the single coordinate that is at a maximum at the saddle point.
In this laboratory exercise, we will focus on the Newton-Raphson method and steepest descent methods to find minima of a potential energy surface. Both of these methods are based on the Taylor expansion of the PES:
Note that the Newton-Raphson method, which uses up to second derivative information, assumes that the PES is a harmonic potential. For a simple quadratic function, the Newton-Raphson method can find the minimum in one step, but for more complex systems, it will take several iterations. For the Taylor expansion above, the first derivative of V(x) is:
If the function is purely quadratic, which is assumed in a harmonic potential, the second derivative is the same everywhere, that is:
At the minimum point x=x*:
Therfore the minimum point is:
This can be extended to a chemical system if one makes each variable x and V a matrix of 3N-dimensions, where N is the number of atoms in the system and V is defined by some empirical or ab initio force field.
Note that this is computationally expensive since it requires inversion of the Hessian matrix. Therefore, in most quantum chemistry programs (Gaussian, Q-Chem, Jaguar), quasi-Newton-Raphson steps are used to find a minimum.
In exploration 1, we will be using the steepest descents method which uses only first derivative information to find a minimum of a PES. In this method, the "ball" on the potential energy surface will move in the direction exactly parallel to the net force. For 3N coordinates, this direction is conveniently represented by the negative of the unit vector of the gradient of the PES, that is:
The step size that one takes in moving along this PES is defined in the programs that we use in this exploration.
The function given to our group was:
A contour plot and a surface plot of this function are shown below:
Note that there are two local minima on this surface.
For this exploration, we tried using several step sizes, initial positions and tolerances. Below are the results of these explorations. First consider changing only the initial guess:
Note: For these trials, the tolerance was held constant at 0.0001 and step size was held constant at 0.001.
When the initial guess was changed and all other parameters such as step size and tolerance were held constant, we see that the minimum that is found in a given optimization is dependent on the initial guess. This is because the steepest descent will always tend to push the "ball" on the PES in the direction of the steepest negative slope. Therefore, sometimes it will push it all the way to the global minimum and sometimes it will push it to the local minimum. Once the "ball" reaches any minimum point on the surface, it is no longer subject to a force since the gradient of the potential is zero at any minimum.
Below are plots of minimization tracks for various initial guesses:
Consider changing the tolerance:
When the tolerance was changed and all other parameters such as initial guess and step size were held constant, we tended to arrive near the same minimum point, but not exactly. The tolerance indicates the maximum value for which a gradient is considered zero by the algorithm. Thus, for a larger tolerance, there is a larger space around the exact minimum point which will be considered to have a gradient of zero. We see that at low tolerances such as 0.0001, we reach a fairly accurate point but as we increase the tolerance by orders of magnitude, the minimum tends to shift away from this point. Once the "ball" reaches the cusp of the space where the gradient is considered zero, it will stop because there is no force acting on the ball. Thus, at higher tolerances, we will actually reach a less accurate minimum point. In order to have acceptable values, the tolerance should be less than or equal to 0.0001, however, one should always try to use the lowest tolerance possible to attain accurate results.
Consider changing the step size:
When the step size was changed, we tended to find drastically different minima. When the step size was increased by one order of magnitude from 0.001 to 0.01, the minimum point found differs slightly. When the step size is too large, such that moving along the steepest gradient with a given step size will move to a point that is not defined by the function (f(x,y)=0), the steepest descent method essentially stops. This is because moving to a point that is not defined by the surface then has a value of f(x,y)=0, which has a derivative with respect to all coordinates which is also zero. Thus, no further movements will be made toward a minimum. When the step size is increased by two orders of magnitude from 0.001 to 0.1, we find that the large step size causes the steepest descent vector to move to an "undefined" point very quickly in the minimization routine. This is why the minimum point found is so drastically different compared to the results for the other two step sizes. In order to be accurate, one should use a step size which is less than or equal to 0.01 for qualitative results which are accurate to the tenths place. However, to get quantitative results, the step size should be less than or equal to 0.001.
Note that in the above exploration, we have located all minima of the surface that is described by the function
For exploration 2, we will be using the Lennard-Jones 12-6 potential to model the van der Waals interactions which hold the two and three atom clusters together which we will be studying. The Lennard-Jones 12-6 potential is of the form:
This potential is empirical and has been derived from experiment, but it has firm theortical grounding. It is important to note that the Lennard-Jones 12-6 potential only models pairwise interactions, however, for many systems, this proves to be a fairly good approximation. The first term,
For explorations two through four, we will be using the following parameters given to our group:
For exploration 2a, we produced a contour plot and a surface plot of the constant angle potential for the three atom van der Waals molecule, Ar-Kr-Ar. This constant angle potential corresponds to the angle indicated being held constant and the other two internal coordinates, r1 and r2, are free to vary.
Below is the equation which is used to calculate the LJ-potential in the polar coordinates which will make it easy to develop a constant angle potential.
Below is the surface plot and contour plot of the constant angle Lennard-Jones 12-6 potential when the angle θ is held constant at 70.777° as calculated by Mathcad after geometry optimization in Cartesian coordinates. Below is the calculated cluster geometry after energy has been minimized.
Note that we reach a minimum at a certain distance, but that at inter-atomic distances closer than the equilibrium distance, the potential goes to infinity, representing the repulsion of the atoms at close distances. This is modeled by the (1/r)12 part of the potential. Also note that the potential is symmetrical since the pairwise interactions between krypton and each of the two argons are exactly the same. Also, at distances which are greater than the equilibrium distance, the potential rises up out of the minimum (modeled by (1/r)6 part of LJ potential), but at a slower rate than when the inter-atomic distances are less than the equilibrium distance.
For exploration 2b, we set out to construct a Lennard-Jones 12-6 potential for the van der Waals cluster Ar-Kr-Ar where the bond length between the two argon atoms remains constant while the other two parameters vary. For this case, it will be easier to use Cartesian coordinates to construct the potential.
Note that when the interactions are between Kr and Ar, we take the averages of the ε and σ parameters of the two atoms to describe the interactions. When it is the Ar-Ar interaction, we only use the ε and σ values for argon to describe the pairwise interaction. Using this information, we calculate the Lennard-Jones potential for the Ar-Kr-Ar van der Waals cluster when the Ar-Ar bond length is held constant:
Where x1 is the distance from the argon to argon along y=0, y2 is the y-axis projection of the vector connecting the Ar atom to the Kr atom, and x2 is the x-axis projection of the vector connecting the Ar atom to the Kr atom. Also note that as stated above, ε12 and σ12 are defined as:
Mathcad minimizes this Lennard-Jones potential to give an equilibrium geometry. The equilibrium geometry is as pictured below with the coordinates indicated:
Below is a surface plot of the Lennard-Jones potential where the Ar-Ar bond length is held constant at 406.672pm as determined by the Mathcad minimize function. Note that the minimum energy state has energy of-3.148*103 J/mol.
Note that in this plot, there are two global minima, but several local minima which kind of surround the global minima. There are two global minima due to the inherent symmetry of the system. That is to say that the values of r2 and r3 in figure 9 may exchange such that r2=______and r3=_________. Therefore, we reach a symmetrically equivalent molecule with the same energy. Also note that at distance closer than the minima (local or global), the potential tends to infinity as expected. This of course is dictated by the repulsive nature of atoms at close distances which is governed by the (1/r)12 term in the LJ potential. Also note that the global minima has energy which correspond to the calculated minimum energy above. As the atoms move farther away, that is, when interatomic distances become large in either direction, the energy increases with respect to the minima, but not nearly as fast as when the interatomic distances become very small. This is because as the atoms move far apart, they are largely governed by the (1/r)6 term rather than the (1/r)12 term.
In exploration 3, we constructed a program to perform a steepest descent minimization of the van der Waals cluster consisting of Ar-Kr-Ar. The potential is still described by the Lennard-Jones 12-6 potential, but this time, no parameters are held fixed as in exploration 2. We have calculated the equilibrium geometry and energy of the van der Waals cluster for five different initial starting geometries to test the robustness of this steepest descent minimization. Below is a table summarizing our results. The program which was used to run the steepst descent algorithm is attached in the appendix.
From these results, it appears that the steepest descent minimization routine is not robust as it tends to lead us to different equilibrium geometries based on the initial guess. For the fourth trial where the initial guess has all the atoms aligned in the x1-x2 plane, it converges to another geometry which is also planar. Since y2=0, the first derivative of V(x1,x2,y2) with respect to y2 is always zero. This implies that the force in the y2-direction is always zero, therefore, the atoms will never be pushed out of the x1-x2 plane. For the last guess, which involves an initial guess where the atoms are absurdly far apart, it appears that the geometry only shifts in the x1 direction. This indicates that at very far distances for x2 and y2, the potential has the following characeteristics:
However, it appears that the steepest descents method takes us to the correct place for x1, which indicates that there is a significant gradient with respect to x1 at x1=1000 to push us down into the minimum.
In exploration 4, we will be looking at a van der Waals cluster of Ar5Kr whose minimization is governed by the minimization routine in Mathcad. According to valence shell electron pair repulsion theory (VSEPR), the equilibrium geometry of such a 6 atom cluster should be a trigonal bipyramid. For the first few starting geometries, we chose the trigonal bipyramid geometry to be sure that nothing was incredible wrong with the minimization routine. Below is a summary of our results, which includes starting geometries, bond lengths between all Ar-Kr atoms and energy at equilibrium geometry.
Starting geometry given in Cartesian coordinates (x,y,z) for each atom. Note the the last row is the krypton atom, while the other rows represent the argon atoms. The diagram to the right indicates graphically what the q matrix looks like in space.
Starting initial geometry is given in the same matrix as in trial 1 and a diagram is included once again to indicate graphically what the q matrix looks like in space.
Starting initial geometry is given in the same matrix as in trial 1 and a diagram is included once again to indicate graphically what the q matrix looks like in space.
This time we start with all the atoms in a plane. Starting initial geometry is given in the same matrix as in trial 1 and a diagram is included once again to indicate graphically what the q matrix looks like in space.
According to our results, we have found four different energy minima which indicates that this energy minimization routine is not robust. Furthermore, each initial guess appears to converge to a different geometry, most notably the case where the atoms all start in a plane (trial 4). It appears that this minimization routine must somehow use a steepest descent method because the optimized geometry also has a planar geometry. As discussed before, if the z-component of every atom is always zero, then the partial derivative of the potential with respect to z will always be zero, which indicates that there is no force to push the atoms out of the z=0 plane.
Below is a figure comparing the energies of the various confirmations of the local and global minima that were located. It appears that the first equilibrium confirmation is the global minimum and according to chemical intuition, this should be the global minimum since six atom clusters should tend to a trigonal bipyramidal structure.
In this exploration, we will be using the Lennard-Jones potential to calculate vibrational frequencies of a Ar-Kr-Ar cluster (part a) and a Kr-Ar-Kr cluster (part b). To calculate the vibrational frequencies, we expand the potential in a Taylor series and eliminate all anharmonic terms so that we work in the harmonic approximation:
We choose V(0)=0 out of convenience and note that at local minima, the second term will vanish. Thus, we are left with a potential of the form:
If we use Hooke's law to describe the restoring force, then this potential becomes:
Now we can describe this motion by Newton's second law:
This is a second order linear differential equation which may be easily solved, however, it is not done here. For more complex systems involving more degrees of freedom, the challenge of calculating vibrational modes is rather complicated because the relation between the PES and the vibrations is inherently more complex. Note that for a system of N particles, there are 3N-6 vibrational degrees of freedom since there will be 3 rotational degrees of freedom and 3 translational degrees of freedom.
We may derive the equation of motion for this system which involves taking the 3N coupled differential equations in Cartesian coordinates and doing a unity transform to the 3N-uncoupled differential equations of the normal modes. By following this procedure, each vibrational spatial coordinate, qi is described by the following differential equation:
The solution to this differential equation yields:
Therefore, one may reconstruct the differential equation above as:
And this may be written in terms of matrices and vectors which will give us a "prescription" to find its result:
The non-trivial solution of this equation occurs when the coordinates qi are not zero. Thus, the normal mode motions are described by the eigenvectors of B, also known as the mass-weighted Hessian or mass-weighted "force constant" matrix. Its eigenvalues will give us the corresponding vibrational frequencies for each vibrational motion.
The vibrational coordinates of the Ar-Kr-Ar cluster and the Kr-Ar-Kr cluster differ slightly. This is due to the increased weight of the Kr atoms with respect to the Ar atoms. By having a heavier system, this will slow down the vibrations, as seen by the reduced frequencies in part b of this exploration. Thus, one witnesses that the eigenvalues of the Hessian matrix are larger in part b where the mass weighting slows down the vibrations since having two Kr's versus two Ar's will slow down the vibration.
In exploration 6, we use Spartan '04 (Windows edition) to explore the robustness of various molecular mechanics force fields, especially the Merck Molecular Force Field (MMFF). In this exploration, we will be running experiments on hydroxyamphetamine (a derivative of methamphetamine, a popular street drug):
In this exploration, we will try different starting geometries to see if using the molecular mechanics method with the MMFF parameters finds the same minimum. The starting geometries were varied by rotating around the single bonds in the floppy alkyl group of the molecule. Below is a summary of results and diagrams of the optimized geometries for trials 1 and 3:
It should be noted that the optimized molecular geometry of trial 3 differed greatly from the optimized molecular geometries of trials 1 and 2, whose optimized geometries looked relatively similar.
The calculated minimum energy confirmation is dependent upon the input of the geometry. This is because once the minimization routine finds any minimum, local or global, the gradient of the potential with respect to all coordinates is zero which implies that there is no force to push the structure toward a lower energy. Thus, for trials 1-2, the minimization routine found a different minimum point on the PES than for trial 3.
For exploration 6b, we used a Monte Carlo search method to find the conformational distribution of our molecule in a particular starting geometry. We used the input geometry for trial 3 as our initial guess.
This Monte Carlo algorithm employed here does a Monte Carlo search with a path that biases low energy conformers. Although we might not find the global minimum, Spartan finds the conformers that one keeps constitutes a Boltzmann distribution.1
For the given starting geometry, the Monte Carlo conformational search found 18 different minima. The lowest energy confirmation has an energy of 17.3791 Hartrees and its geometry is plotted below.
In exploration 6c, we rotated around one of the flexible bonds in our molecule to develop an energy profile for rotation about this bond. Below is an image of the molecule which indicates the bond about which the molecule rotated and the confirmation that corresponds to the zero degree mark.
For this exploration, we have used two different force fields, MMFF and SYBYL, to calculate an energy profile. Below are plots of the results:
The results of this exploration indicate that using different force fields will give different results. This is because different force fields correspond to different functionals which may have been subject to different target sets. MMFF has been "specifically parameterized to reproduce geometries and conformations of organic molecules and biomolecules" while SYBYL is parameterized for the entire periodic table.
Below are hand-drawn sketches of three vibrational modes of hydroxyamphetamine in its minimum energy state found in part c. All of these vibrations will be seen in the IR spectrum (which is plotted below). This is because the overall dipole moment of the molecule changes upon excitations at these specific frequencies.

Properties of electrons can be studied in order to determine the e/m ratio of electrons. The e/m ratio is important because it absolutely predicts a particles direction of travel in a vacuum when subject to magnetic or electric fields. In this lab a deflection tube is used to measure the effects of an electric field on a beam of accelerated electrons from a cathode ray tube. Helmholtz coils are used in conjunction with the cathode ray tube to measure the effects of a magnetic field on a beam of accelerated electrons. Measurements of the deflection distance of the beam at various electric and magnetic field strengths can be used to determine the e/m value for an electron.
To discover the deflecting force a magnetic field causes on an electron accelerated perpendicularly through its field and to use these data to find the e/m ration of an electron.
The magnetic field produced from the helmholtz coils creates a force that causes the originally straight electron beam to be deflected, either positively or negatively, depending on the direction of the current. The measurements recorded are then used to calculate the e/m of an electron. The above calculations were completed using the following equations:
The results of the individual calculations for the e/m are fairly far from the accepted value of e/m = 1.7589 x 1011 C/kg. However, the average value of 1.369 C/kg is closer, less than 2.5 standard deviations from the accepted value.
Choosing a point with a large x-value value will increase the accuracy of the measurements because it will yield larger y-values, which will decrease the relative error. However, a source of error in the data may be because the beam was aimed to intersect (0.10, 0.01) instead of trying to keep IB as close to 0.150 A. This technique may have caused increased error in the measurements, such as that found in the positive deflections where the experimental e/m is calculated to be less than the accepted value for e/m. Due to the force of the earth's magnetic field higher values than the accepted e/m would be expected.
Charge-to-mass ratio calculation accuracy increases when balancing both the positive and negative values which serves to negate the effects of the earth's magnetic field. Additionally, the electron beam may not have been perfectly at zero y and finding the average value would eliminate this error in measurement.
The objective of this experiment was accomplished -- the data revealed that the electron beam will move in a circular path in accordance with the magnetic field force equations (F = iL x B which causes an inward acceleration, leading to a circular path). Also, the e/m ratio for the electron was successfully calculated using the data collected. The experiment was important because a method for calculating e/m was learned.
To observe the deflection of an electron beam when accelerated through an electric field and to report the experimental slope of the electric field using these measurements.
The electrostatic field causes the beam of electrons accelerated perpendicularly to be deflected along a parabolic path. This is different from the magnetic field which causes electrons to travel in a circular pattern because the magnetic force causes an inward acceleration. In the above graph x2 is used instead of x because of the results found when the following equations are solved for y. The explanation of this parabolic path and the expected slope of this graph can be derived from:
The calculated experimental slope (Δ y/Δx2) of our graph is 2.44.
The electric field strength can be calculated two ways. Easily by using the equation:
A more precise calculation can be derived from combining the following two equations and replacing ve2 in the first by solving for it in the second.
Now the e/m ratio can be eliminated from the equation and E can be solved for using the slope.
This second value for the electric field is more accurate because the experimental electric field is not constant across the capacitor plates. The size of the plates is of the same order of magnitude as the distance between them; therefore, the electric field near the ends of the plates varies from that of the center. The equation Vd/d assumes a constant electric field, while the other equations allow the experimental slope to be used in order to more accurately represent the non-constant electric field strength.
The experiment's objective was accomplished. The experiment was important because the slope of the electric field was calculated which can now be used in the last experiment to more accurately find the e/m value. The experimental slope yields more accurate results than using the equation Vd/d when capacitor plates are not infinite.
To balance the forces from both an electric and magnetic field on a beam of accelerated electrons and use the experimental deflection values to find the e/m ration of an electron.
The above calculations for IB, e/m, and e/m* were completed using the following equations:
Charge-to-mass ratio values calculated using E calculated from the slope instead of Vd/d serve to increase accuracy by accounting for the non-infinite plates of the capacitor. The mean values for the e/m in both cases are very far from the accepted value of 1.759 x 1011 C/kg. Subtracting the average values shows that the value corrected by using slope is closer to the accepted value. The corrected value does have a smaller standard deviation which may have resulted from increased precision with this method. However, both e/m's are very far away from the accepted value.
There are many compounding sources of error in this experiment. Again, there is the problem that the electric field is not constant across the plates because they are not infinite. This causes systematic error in the experiment such that the trajectory of the electron beam is bowed in the middle with the B field unable to completely cancel out the E field. Also, the effects of the earth's magnetic field have been disregarded and may have caused inaccurate measurements.
The goal of the experiment was poorly accomplished with the measured e/m being very far from the accepted value. This experiment was valuable because it facilitated the investigation of various sources of systematic error.
Theoretically, there would be no change in the trajectory of an electron doubled in mass and charge in a magnetic field because it is dependent on the ratio of charge to mass which would be unchanged.
The charge-to-mass ratio of an electron was calculated using two separate methods. First, the deflection data of an electron beam accelerated between two capacitor plates were used to find the e/m. The second method involved balancing the forces on an electron beam from an electric field and a magnetic field. More accurate results were found using the first method which may be because the second method had more sources of error due to error from both the E field (capacitor plates) and B field (earth's own magnetic field). Additionally, it was learned that the slope method for calculating E yielded more accurate results than using the equation Vd/d when finding the e/m of an electron.

Gamma rays are highly penetrating photons produced by positron-electron annihilation or by the decay of a radioactive nucleus. Gamma decay tends to occur following an alpha or beta decay to bring the nucleus down to ground state. These photons were first detected by Antoine Henri Becquerel in the late 1800's. Becquerel deduced that gamma rays interact with matter like light using photographic film to detect the radiation. The detection of gamma rays using scintillation spectroscopy rests on this discovery that gamma rays are photons. Gamma rays interact with matter by three processes -- the photoelectric effect, Compton scattering, and pair production.
Each of these processes transfers energy to electrons, which then lose energy by ionizing atoms as they travel through matter. In the case of the photoelectric effect, all of the energy of the gamma photon is transferred to an electron. Since the ionization energy of the atoms in the scintillator is small compared to that of a gamma ray, the electron is considered to have an energy equal to that of the incident photon. This process is dominant at gamma energies lower than 511 KeV. For a nuclear charge Z, the photoelectric cross section for a K shell electron is:
(1)
Compton scattering is a demonstration of the particle-like behavior of light. During this process, a photon and an electron collide elastically, producing a scattered photon of lower energy and an electron with the energy lost by the photon. The energy of the new photon is dependent on the angle of deflection. The energies of the scattered photon (Eγ') and electron (Ee) for an incident photon of energy Eγ are:
The Klein-Nishina formula gives the differential cross section for Compton scattering:
where P(Eγ,θ) is the ratio of the final photon energy to the initial photon energy, re is the classical electron radius and dΩ is the solid angle. Compton scattering is dominant for middle-energy gamma rays.
At energies higher than the rest mass of two electrons (
All three of these processes result in the transfer of energy from a photon to an electron. As the electron travels through matter, it ionizes atoms through a series of inelastic collisions. This ionization is measurable by a number of techniques making it useful for detecting particles.
As described above, gamma rays deposit their energy in matter such that a number of ionized atoms are created. In a scintillator, this ionization energy is converted into visible light. This property was first employed in 1909 by Ernest Rutherford who used a ZnS screen to detect scattered alpha particles. The visible light produced in the scintillating crystal is linearly proportional to the energy input of the crystal. In this experiment, we used sodium-iodide, an inorganic crystal, doped with thallium, which has an efficiency of 12% and produces 4×104 photons per incident MeV and has a time constant of ~200 ns. The NaI(Tl) crystal is enclosed in a sealed can and optically connected to a photomultiplier.
We used a standard photomultiplier tube (PMT) from RCA. The PMT proportionally converts the visible light from the scintillation crystal into a small electrical pulse. The first part of a PMT is a photocathode, which converts light into electrons by the photoelectric effect. Next comes a series of dynodes made of materials with good secondary electron emission, between which a high voltage is applied. This creates a potential difference of 50 to 100V between dynode pairs. The HV source used in this experiment is an Ortec 478 HV supply at positive 1.1 kV. The electrons emitted from the photocathode are accelerated to the first dynode, which then emits a proportional number of electrons to the next dynode. This signal continues to amplify along the chain of dynodes, adding up to a total amplification of ~106. This pulse is collected at the anode, which is connected to a preamplifier. The photomultiplier sends out a negative pulse of charge with a width on the order of 6μs and amplitude of about 200 mV.
The preamplifier converts the charge on the anode to a positive voltage signal with width of about 140μs and sends it to a pulse shaping amplifier. An ORTEC 485 amplifier was used in this experiment. The pulse shaping amplifier removes the effect of the scintillator time constant on the width of the pulse. The pulse width is now controlled by the shaping time of the amplifier. Typical pulse widths were on the order of 7μs with a height of around 25 V. The now amplified voltage signal is digitized using a Pocket Multi Channel Analyzer analog-to-digital converter. The MCA is connected to a computer in which a histogram of voltage pulse heights is plotted. This plot gives the energy spectrum of a gamma ray source, once channels are calibrated to energies. Cesium-137 was used to calibrate pulse height to photon energy (see appendix for details).
The cobalt-60 gamma spectrum shows two strong gamma lines. This indicates a cascade of two gamma emissions following a beta decay. The first gamma has an energy of 1173.7 keV and the second has an energy of 1332.5 keV. As seen in the decay scheme to the right, cobalt-60 occasionally decays by a high energy beta emission followed by a single gamma decay to ground state.
The presence of two different gamma energies means that there should be two different backscatter peaks and Compton edges. However, these features were predicted to be very close to each other and are not distinguishable on the spectrum. The backscatter peak is at approximately 251 + 20 keV, which is ~40 keV higher than expected. The Compton edge is located at about 980 + 20 keV and is in between the two expected values.
In 89.8% of cases, 22Na decays by the emission of a positron followed by the emission of a 1277 keV gamma photon to the ground state of 22Ne. The positron annihilates with an electron, producing two gamma rays of 511 keV, the rest mass of an electron. This reflects the conversion of the masses of the positron and electron into energy by E=mc2. The other 10.2% of the time, the decay begins with electron capture in the K shell, followed by the 1277 keV gamma decay. The energy resolution of the positron-electron annihilation peak is 7.6%, compared to 4.9% for the 1277 keV gamma peak.
The sodium-22 spectrum appeared as expected except for a high energy gamma peak of unknown origin. This peak occurred at an energy of 1820 keV, which is reasonably close to the combined energy of a sodium gamma and an annihilation gamma (1788 keV). We suspect that this high energy peak is the result of a 1277 keV gamma and 511 keV gamma entering the scintillation at the same time. This is possible given our experimental setup in which the source was place directly on the scintillation counter.
Gamma spectroscopy can be employed to determine the constituents of an unknown radioactive sample. In this experiment, we were given two unknown sources, which we attempted to identify. We did this by comparing the unknown spectrum to the gamma energies of known sources.
This unknown was determined to be a combination of 137Cs and some kind of zinc compound. The cesium contributed Peak A, which is interpreted to be the barium x-ray peak of the spectrum, and Peak B, which is interpreted as the full energy peak. Zinc has a number of isotopes that decay with gamma energies between 1366.4-1560.4 keV which may be the source of Peak C. The values are not exact because adding the two spectra together cause the peaks to shift relative to one another. Also, as seen in the above experiments, the cesium calibration does not work perfectly for other spectra, which may have affected the accuracy of the calibration of Peak C.
We determined the composition of unknown 2 by adding together the spectra of various sources available in the lab until we found a reasonable match.
We concluded that unknown 2 is composed of 137Cs, 60Co, 133Ba, and 109Cd. Peak A is the barium x-ray peak from cesium. Peaks B, C, and D are the result of combining the spectra of the four sources. They do not exactly line up with any single line of the sources since the Compton continuum from each source is in this energy region. Cobalt-60 is the source of the dual peaks E and F.
In this part of the experiment we tried to determine if gamma spectroscopy could be useful in detecting trace amounts of copper in a material of unknown composition. We exposed 16.037g of Copper to an active neutron source to create 64Cu. This isotope has a gamma line at 1345.84 keV and a half life of 12.7 hr. The parent isotope, 63Cu, has a relative abundance of 69.17%, making Cu a good candidate for this experiment.
We observed the expected peak and used the data below to calculate how many 64Cu were produced. These "back of the envelope" calculations were done under the assumption that 20% of the emitted gammas were absorbed in the detector, which has a 12% efficiency.
The signal can be approximated by a Gaussian distribution using the method of least squares.
The above equation describes a Gaussian distribution centered at Eo with variance σ2. The least squares method is used to determine the constant A, as well as the error in A. We found A to be 3427
We can approximate the number of 64Cu nuclei produced while the copper was exposed to the neutron source using the data gathered with the scintillation counter and the properties of the isotope and its parent isotope, 63Cu. We also assumed that 20% of the emitted gammas were absorbed in the detector, which has a 12% efficiency.
According to this calculation, there were 4.8×108 Copper-64 isotopes made while the sample was exposed to the neutron source. Even though this is a very small number compared to the total number of nuclei in the sample (
Gamma ray spectroscopy is an excellent method of investigating the radioactive decay of certain isotopes. Our scintillation spectroscopy setup detected the two gamma decay cascade of cobalt-60, as well as the positron-electron annihilation that characterizes the sodium-22 spectrum. Scintillation spectroscopy has greatly increased our understanding of radioactive decay since spectra have been recorded for many isotopes.
Since there is such a large base of knowledge, gamma spectroscopy is also useful in identifying unknown samples. As shown with our copper experiment, it may possibly be used to detect trace amounts of certain elements. This application should be further explored through more research.
137Cs has a single gamma line at 662 keV and a barium x-ray peak at 32 keV. The decay of 137Cs starts with the emission of an electron to an excited state of 137Ba, followed by a gamma emission in 92% of cases. The other 8% of beta emissions bring the nucleus directly to ground state. The x-ray results from electron capture in the K shell of barium followed by the emission of an x-ray, which occurs 10% of the time.
The 137Cs spectrum shows the characteristic features of a gamma spectrum. The Compton continuum represents all the energies of 662 keV gammas that scatter in the crystal with the scattered photon escaping without depositing its full energy. The backscatter peak shows the 662 keV gammas that scatter off something outside the detector and get sent into the crystal. When lead was placed nearby the source and detector, the backscatter rate increased. The Compton continuum is a continuous range of energies because gammas can scatter at any angle from 0 to π. The highest energy of this range corresponds to the energy of the recoiling electron for a scattering angle θ=π in equation (3). This energy is the location of the Compton edge and the energy of the scattered photon at this scattering angle is the location of the backscattering peak.
Calibration of the channels of the MCA was done for several amplifier settings and is summarized in Table 1. At every amplifier setting used, the features described above appeared at approximately the same energy once the channels were calibrated.
Table 2 shows a statistical analysis of the barium x-ray and full energy peaks for an amplifier setting of CG 2 FG 3. The uncertainties were determined using the peak finding software in the MCA program and converted into keV using the calibrations calculated above.
We calculated the energy resolution to be 8.2% using the full width half maximum of the full energy peak divided by the centroid of the peak.
Using equations (2) and (3) the expected energies of the Compton edge and backscatter peak were calculated. Our measured energies for these features were 7.1% and 12% different from the expected values, as shown in Table 3. However, due to the dispersed nature of these features in the spectrum, as well as calibration error, the uncertainty in the measurement was approximately 20 keV.

Atomic and molecular spectroscopy has provided experimental verification for many of the most profound predictions in physics. There has been excellent agreement between theory and experiment in the study of visible light emissions.
In this experiment, we used spectroscopy to test theoretical predictions on three levels of complexity. We start with hydrogen, the simplest atom, to test Bohr's model of quantized atomic energy levels. We then increase the complexity by studying sodium, an alkali atom. Sodium displays nuclear screening, electron spin, and selection rules, all predicted by quantum mechanics. Finally, we looked at the vibrational and rotational excitations of diatomic nitrogen to infer the structure of the chemical bond between the two atoms.
We used an Ocean Optics SD2000 fiber spectrometer calibrated with mercury and helium. The spectrometer consists of two gratings: the master, which has a spectral rand of 632 nm to 880 nm, and the slave, which has a spectral range of 371 nm to 677 nm. Both gratings are mounted in a crossed Czerny-Turner configuration. The device has a resolution of 3.57×10-7 pixels and is therefore able to resolve features such as the sodium doublet, which has a separation of 0.121 pixels. However, the spectrometer is not fit to study the hydrogen Lyman series, which does not lie in the spectral range of the instrument.
The Balmer formula describes the visible spectrum produced by excited hydrogen atoms. In 1889, Rydberg recognized the Balmer formula to be a particular case of a more general expression in which n1 = 2 and RH is the Rydberg constant.
(G is an empirical constant)
In 1913, Bohr revolutionized our view of the atom with his model of atomic quantization. Bohr suggested that orbital energy is fixed at discrete values, as a function of a quantum number n. Changes in energy states are accounted for by the emission or absorption of a photon, which balances the energy difference.
This relation is described by the Planck formula:
Our measurement of the visible part of the hydrogen spectrum is evidence of Bohr's quantized energy levels.
Figure 1 shows a plot of:
By the Rydberg formula, the slope of the line gives the Rydberg constant. Our estimation of this constant is 109683 + 1000 cm-1 (accepted value RH=109737.318 + 0.012 cm-1).
The straightness of the line is a test of Balmer's hypothesis regarding the quantized energy levels of the hydrogen spectrum. We used Excel to plot a linear trend, which gave an R2 value of 1. This indicates that a straight line is a good fit for the data, thus verifying the Balmer hypothesis.
Sodium is an alkali atom, meaning it has one l = 0 electron in its valence shell. As the nucleus is shielded by the charge distribution of the inner shells, his lone electron "sees" a net potential similar to that of hydrogen. However, for smaller n, the behavior of electrons of small l deviates from hydrogenic behavior. The small l states do not have enough angular momentum to maintain a large radius, causing their orbits to cross inside the closed shells (Fig. 2). When this occurs, there is less of a screening effect and a lowering of the Coulomb energy.
Evidence for nuclear screening is found by fitting our measurement of the sodium spectrum to a Balmer-like relation. We did this using a quantum defect, which mathematically summarizes the effect of nuclear screening. The quantum defect (δ) is a correction to the quantized energy predicted by Bohr.
Following the selection rule,
, the wavelength of a photon emitted to bring an electron from energy state nd to 3p is determined by:
Figure 3 is a plot of
The sodium spectrum also shows evidence of electron spin. In the rest frame of the electron, the proton creates a circular positive current and therefore produces a magnetic field. The spin of an electron can point either up or down with respect to the internal magnetic field. The interaction energy between the magnetic field and the internal magnetic moment of the electron is described as the spin-orbit interaction.
Since spin can only have two values, the spin-orbit interaction energy can also only have two values:
The interaction energy is either added or subtracted from the total energy of each state. This leads to a small energy split for every state with
The spin-orbit interaction manifests itself as a ~0.1% split of spectral lines. Therefore, the presence of doublets in the sodium spectrum verifies the prediction of spin-orbit coupling. These doublets can also be used to infer the magnitude of the internal magnetic field of the atom. The classic example of a doublet is the sodium D line, shown in Figure 4.
We calculated the expected internal atomic magnetic field to be 1.332×1021 T for an expected energy split of 0.00617 eV. Our measured energy split was 0.00104 + 0.001 eV. This gives 2.2432620×1020 T for the internal magnetic field, which is about an order of magnitude lower than expected.
The spectrum of diatomic nitrogen differs from those of hydrogen and sodium greatly. The hydrogen and sodium spectra consist of several sharp peaks whereas the N2 spectrum shows a broad "comb-like" band of emission wavelengths. Diatomic nitrogen is more complex and has both vibrational and rotational degrees of freedom.
According to the Born-Oppenheimer approximation, vibrational and rotational excitations can be treated separately. The vibrational mode is described by the quantum harmonic oscillator model. In the following equation, ω0 represents the fundamental frequency oscillation.
A transition in which n is decreased by 1 forms a spectrum of equally spaced lines. The separation of these lines can be measured and then used to calculate ω0. In figure 5, the coarser set of lines represents the vibrational excitations. These lines were not exactly equally spaced, which we expect to be the result of coupling with rotational motion. We found the fundamental vibrational frequency of nitrogen to be 3.11×1013 Hz.
The bond holding the two nitrogen atoms together can be viewed as a spring. The
strength of the bond can therefore be approximated using
, where m is the reduced mass (7.0015 amu). We found k=11.237 N/m.
The rotational excitations are seen in the fine structure of each of the coarse peaks in the N2 spectrum. These emissions are described by a quantum rigid rotator.
In the above equation J=0,1,2... h is Planck's constant and I is the moment of inertia of the molecule. For two particles separated a distance r with a reduced mass m, the moment of inertia is I=mr2.
We again extracted a frequency for rotational motion, which was found to be 2.52×1012 Hz. This is an order of magnitude smaller than the vibrational frequency. We also calculated the distance between the atoms as 0.19 nm.
It is curious that the same comb-like structure was not seen in the diatomic hydrogen spectrum. This is because the rotational energy is a function of 1/m and the mass of a hydrogen molecule is very small compared to nitrogen. This would result in emissions with wavelengths much larger than our apparatus can detect.
An application of our knowledge of molecular excitations is found in a common kitchen appliance. A molecule will absorb a photon that matches its fundamental frequency.
This causes the energy of the molecule to increase, which increases the temperature of the material. In a microwave oven, microwaves are used to excite water molecules in order to heat food.
The atomic and molecular spectra studied in this experiment verify several famous theoretical predictions in physics. The hydrogen spectrum verifies Bohr's model of quantized energy levels. On a more complex level, quantum mechanical predictions of electron spin, nuclear screening and selection rules are manifested in the sodium spectrum. Finally, the spectrum of diatomic nitrogen shows the validity of the Born-Oppenheimer approximation regarding vibrational and rotational energies.

The voter model is one of the most elementary interacting particle systems. It can be used to describe simple opinion dynamics, as the name implies, as well as certain kinds of dimer-dimer kinetics [1]. However, the voter model is often studied primarily because the relative simplicity of its rules frequently allows it to yield exact solutions. Each lattice point k can be in one of two states. For convenience, I shall denote these by + and −. A lattice point and one of its nearest neighbors are selected at random. The selected opinion then takes on the value of the chosen nearest neighbor.
A number of important aspects of the model jump out of this dynamic immediately. First and foremost is the exitance of two absorbing states, one comprised of all + opinions, the other of all -- opinions. For a finite system, one of these solutions will be the necessary outcome. Furthermore, the system will behave identically under the transformation ± → ∓, implying a Z2 symmetry. Thirdly, despite the stochasticity of neighbor selection, the system will never change a value randomly. Equivalently, the system is at zero temperature. This means that all of the dynamics takes place at the edges of domains of similar opinion. A correspondence thus exists between the behavior of domain edges and random walks that helps the solubility of many VM problems.
Although the voter model is at zero temperature, the critical temperature is also zero, so the standard array of critical exponents may be found [6]. For the remainder of this paper, however, I will focus on the dynamics of opinion switching and persistence in the manner of Ben-Naim et al. [2]
In order to get an initial idea of what solutions to the voter model will be like, I will start with a mean field approach to a large lattice. Being a mean field solution, the exact nature of the lattice does not matter and a complete graph is used instead. Also, set the density of + opinions, c+ is equal to that of -- opinions, c−. Define Pn(t) to be the fraction of sites at time t that have made n flips. Since the system is large, we can approximate the change of Pn(t) with a set of differential equations:
where λ sets the characteristic time of flipping. Note that we implicitly invoked the requirement that c+ = c− in treating the system as one population. Relaxing this will lead to interesting behavior later. Redefining
This is the large N limit of a Poisson distribution, thus
Therefore after a very short amount of time, the initial opinions are effectively lost to the system. The mean-field topology makes this unsurprising.
The case of uneven distributions of opinions can be derived in a similar manner, but the system must be separated into two species on the basis of the initial state of each lattice point. Thus P+n (t) is the probability of a voter with an initial + opinion to flip states n times by time t. Note that this also makes even numbers of changes distinct from odd numbers, since each will come from different pools of initial voters. The set of equations analogous to (1) is then
Before solving the system, note that we can trivially solve
This is again a simple exponential equation to solve, taking the initial condition that
This is the first indication of substantively different behavior of the even and odd changes. For
That A(t) does not tend toward zero may seem strange given the lack of conservation of net opinion during an interaction, but in the limit of large system size that is being considered, the average concentration of opinions is in fact fixed. A single voter may not have a memory of its initial opinion, but the statistical prevalence of the majority opinion initially skews the future toward the majority opinion for all time, keeping the autocorrelation positive.
The mean field techniques provided some insight into the time scales of the voter model and the effect of concentration on the long term behavior. However, large amounts of information were obviously lost as the mean field solution exhibits none of the expected coarsening. Continuing in the manner and notation of Ben-Naim et al. I will now investigate the exact solution of Pn(t) on an arbitrary lattice. This process will show that dc = 2 is the critical dimension above which coarsening does not occur. For the remainder of this paper the initial opinions will be set equal, thus
The state of the system S as a whole must be considered now. Let Sk be the opinion at site k and Sk be the lattice S with
The rules of the voter model make Wk(S) easy to write down, as well. When all nearest neighbors are different, Wk = 1 and when all are the same, Wl = 0. Between these two values, the rate should scale linearly with the number of different nearest neighbors. Thus for z denoting the coordination number,
With these definitions in hand, it is now easy to determine the evolution of the average opinion at site
Remembering that Sk differs from S exactly at k,
Putting equations (10) and (11) into (9) gives us the final relation:
A similar derivation gives the equation for the two point correlation function:
Reflecting briefly on the form of these equations, it is apparent that
with
Since
It is also possible to calculate the average number of flips in opinion,
However, the right hand side has a physical meaning. Since
Much of the physics of what goes on lies in these equations. The convergence to zero of the density of unlike nodes for d ≤ 2 means that the system is taking on more and more order. Furthermore, d = 2 is a critical dimension, above which coarsening is not able to occur. After a simple integration, the long time behavior of the average number of flips is also known.
The mean field time dependence occurs for d > 2, but fails for lower dimensions. This implies that, while the mean field distribution of Pn(t) may be acceptable for higher dimensions, the previous discussion does not have any input on d ≤ 2 distributions. However, Derrida et al. solved for one particular value, the persistence of unchanged opinions, P0(t), for the q state Potts model [5] for d = 1. The details of this calculation are beyond the scope of this discussion, but the idea was to again employ the duality with random walks. When a site is changed for the first time, it must be at the edge of an ordered domain. This relates the problem to that of finding a first passage time of certain random walkers. This can in turn be described by a reaction-diffusion equation which they then solve. After much calculation, they find that the exact power law describing persistence is given by
For the equal initial concentration voter model, q = 2 and θ(2) = 3/8. Uneven concentrations can be handled by defining larger q Potts models and equating certain states. For instance, if c+ = 3c−, a q = 4 state model would describe the dynamics and three states would together form those with initial state + [2].
The dynamics of coarsening of the voter model are a rich subject, and the above treatment only scratched the surface. A mean field solution qualitatively reproduces many of the dynamics of opinion changing for d > 2, but fails for smaller dimensions. This is expected, as the voter model is sensitive to dimension and converges relatively weakly even for d = 2. Neglecting a discussion of more exotic topologies, the universality class, other critical exponents, and a field theoretic treatment [7] [6], there are other questions open based on the work described here. Understanding the two dimensional coarsening process further would be important, and seems likely solvable for small initial concentration. Also, an investigation of the behavior of smaller systems and the impact of fluctuations could be very interesting for its use as a caricature of opinion propagation in the social sciences.

In this project, I determined the site percolation threshold of the Kleinberg small-world model (Kleinberg 2000). Site percolation on a lattice consists of labeling each site on the lattice "active" or "inactive." Percolation can be used to model the spread of epidemics and catalytic surfaces. Applying site percolation to small-world networks could be used to better understand the spread of diseases and innovation. The Kleinberg small-world network (2000) is similar to a two-dimensional version of the Watts-Strogatz small-world model. The main difference between the two models, besides the dimensionality, is that the long-range connections in the Kleinberg model are directed. The main feature that Kleinberg wanted to show is that there is an optimal value to the clustering coefficient, r, for greedy search with local information. The clustering coefficient determines the probability that a long-range bond, say from a to b, is chosen,
Percolation on small-world networks can answer questions about the spread of disease through networks that resemble real human contact networks. In this context the "active" nodes are people who are susceptible to the disease; "inactive" nodes are people who are immune to disease. By determining the effect of different long-distance contact distributions and number of long-distance contacts on the percolation threshold, we can start to answer epidemic prevention questions. One possible question is, "is it more important to reduce the total number of long-range contacts or changing the distribution of long range contacts to make the contacts more 'clustered'?"
Another motivation is my initial motivation: modeling the diffusion of information through high-loss small-world networks. One known issue with Milgram's experiment is that a number of the messages were dropped by intermediate people. In terms of percolation these people would be "inactive" nodes. With both stochastic and deterministic message passing schemes, a message that goes through more steps is more likely to be dropped. Therefore, the average path length of a message that makes it to its destination is shorter than the average shortest path if there were no people who dropped messages. I planned to look into the probability of messages reaching a target at the percolation threshold. As it turned out, just finding the percolation threshold as a function of r and the total number of long connections (N) was difficult enough.
The subject of percolation is very old and the percolation threshold has been determined for practically all 2D (Suding 1999) and 3D (Lorenz 2000) lattices. Lattices are said to percolate if there exists a path from one side of the lattice to the other through only active sites. This group of active sites is called a percolation cluster. Networks, on the other hand, require a different definition of percolation. Percolation on networks requires determining the size of the giant component of the network, after the sites have been labeled active or inactive. Network percolation has been performed on random graphs (Callaway 2000), 1D Watts-Strogatz small-world networks (Newman 1999), 2D Watts-Strogatz small-world networks (Newman 2002), and on Heterogeneous networks (Sander 2002). Due to the similarity between the Kleinberg and Watts-Strogatz models, one limit of my analysis should yield similar results to Newman 2002.
My goal is to determine the percolation threshold as function of the two parameters of the Kleinberg model, the power-law distribution coefficient (or "clustering coefficient") r, and the number of long range connections N. The percolation threshold is defined as the percentage of active sites required to always percolate for infinite size systems.
To find the percolation threshold I used simulations written in C++. The details of my simulations are as follows. I create a square lattice of equal height and width. Each site gets N additional directed long range links. If N is not an integer, then some sites get
I measured the profiles of I(p) for several size systems, Figure 1. The most common method of determining the percolation threshold is to measure pc, say where
My main result is shown in Figure 3. This figure shows how pc varies as a function of r and N. There is a lot to digest in the figure so I will discuss some of the interesting behavior. The first limit to note is the limit of N = 0; this corresponds to a normal square lattice with no long-range connections. The percolation threshold for the square lattice is known to be 0.59274621(13). My simulation calculated pc to be 0.5927. This limit shows, at least partial, functionality of my programs.
This limit is shown in Figure 4. This limit corresponds to long bonds of any length being allowed. This result can be compared with the results of Newman 2002. For both results we see the percolation threshold start at that of the square lattice, 0.5 and 0.5927 for bond and site percolation, respectively. For my results, pc monotonically decreases until N=2, after which pc is a constant value, 0.1897. Newman's results don't show this behavior and if I understand their paper correctly, they don't predict any point where pc should become constant. This discrepancy is because, in that paper, he did not measure pc for N>1 and he used the giant component to determine percolation, whereas I used crossing probability.
My approximation of this limit is shown in Figure 5. The slice of the contour plot is at r=15, a good approximation of the infinite behavior. From the plot we can see that there is a linear decrease in pc until N=2 where pc becomes constant. One interesting thing about this limit is that it can be compared against other lattices because it corresponds to adding some connections to next-nearest neighbors. In terms of lattices, a square lattice has a coordination number (z) of 4, i.e. four neighbors. N=1 and N=2 correspond to z=5 and z=6, respectively. I found that pc for z=5 and z=6 are 0.528 and 0.470. These results can be compared against the percolation threshold of normal lattices. My results for z=5 are smaller than known lattices (ranging from 0.550 to 0.579). The common z=6 lattice is the triangular lattice which has a site percolation threshold of 0.5. In all of these cases, the r= limit have a lower percolation threshold than normal lattices. This is an interesting result, without a clear explanation.
A plot of pc versus r for N=1 and N=2 is shown in Figure 6. For large r, pc approaches some asymptotic value as the "long" links become next-nearest neighbor links, as seen in Figure 5. When r decreases, pc also decreases until it reaches a constant value at r
I studied site percolation on the Kleinberg small-world model. Using numerical simulations I determined the percolation threshold for a wide range of values of the system's two parameters: the "clustering coefficient" r and number of long range links N. The percolation threshold values were corrected for finite-size effects. The percolation threshold contour showed several interesting features. First, I found that for large N, N>2, pc does not change anymore. This result is surprising and warrants further study. I plan to simulate similar systems to see if the result is occurs commonly. Another interesting feature is the limit of small r for N=1 and N=2. In these cases, pc takes on values of
There are a number of "special" values of pc, N, and r. Unfortunately, I have so far been unable to come up with any theories or compelling conjectures for why these values should be important.
Apart from theory, there are some "practical" things that can be gleaned from the main contour plot. If this model was an accurate description of real human networks, then my results could help make several policy positions in regard to epidemics. First, if people on average tend to have more than two long-distance friends, then it is always better to try to reduce pc by increasing r. This means convincing people not to contact very long distance friends. If the average number of long-distance friends is less than one, then it's always better to try to reduce the average number of long-distance friends. If on the other-hand, the goal is to spread innovation, then the exact opposite set of recommendations would be best.

Following President Suharto's resignation in 1998, the electoral reforms passed into law on January 28, 1999 shaped Indonesia's first democratic election in almost fifty years. Under Suharto's repressive rule, five general elections were held between 1977 and 1997, with only three political parties allowed to participate: Golkar, PPP, and PDI (King, "The 1999 Electoral Reforms" 90). These elections were designed to present a façade of legitimacy for Suharto's government; they were neither free nor fair. Under Suharto's corrupt "New Order" government, Indonesians were deprived of their rights and effectively excluded from the political process. Although Suharto held regular elections in order to portray a false image of democracy, he refused to submit to direct election and forbid the organization of opposition parties in rural areas, where most voters were located. Instead, Suharto's term was extended by an electoral college whose members he had individually appointed to ensure his continued power (Case 9). In view of Indonesia's long history of authoritarian control, the 1999 elections signified its tentative transition to democracy. The legitimacy and quality of Indonesia's emerging democracy heavily depended upon the electoral system, rules, and institutions developed by these electoral reforms. To break away from its corrupt and tyrannical past, the first free and fair elections to occur in over 50 years carried significant consequences for Indonesia, ultimately determining which leader would be chosen to guide its course towards individual liberties and freedom of democratic rule.
In contrast to Suharto's corrupt government-controlled electoral college, the existence of an independent body to preside over elections suggested greater legitimacy for the results. How did the establishment of an Independent Election Commission during Indonesia's 1999 electoral reform influence the number of political parties within Indonesia? This is an important question to examine because the number of political parties can affect levels of vote segmentation, ability to produce an effective leader, and political fragmentation within the country. Since Indonesia's Independent Electoral Commission carried the responsibility of administering elections, which was previously a task controlled by the government, this body held the ability to greatly shape the breadth of political parties competing. In transition governments, such as Indonesia's, the quantity of political parties is of great importance for the emerging government's stability and quality, as well as the degree of representation for the median voter's preferences.
According to Paige Johnson in "Anti-party Reaction in Indonesia: Causes and Implications", Indonesia's Independent Election Commission, now known as the KPU, was formed as a key electoral reform in 1999 as a specific reaction to the government's manipulation of elections during Suharto's ruling years (485). The Independent Election Commission was conceived in order to plan and oversee the execution of the elections, collect data on results of the election, and determine the number of seats won by each party (King, "Half-Hearted Reform" 54). It was composed of five government officials and one individual from each of the parties qualifying to participate in the election. In order to qualify for participation, parties had to have an organization established in one-third of the provinces and half of the districts in each of those provinces (King, "Half-Hearted Reform" 51). The presence of party representatives on the commission was designed to prevent the former government-backed party, Golkar, from using the election administration to declare its own party as the winner.
While more than 200 parties requested to compete in the 1999 elections, the Independent Election Commission determined that only 48 met the qualifications for participation (Evans 136). The Commission's resulting 48 parties were significantly greater in number than the previous three designated eligible to compete, supporting my initial hypothesis that the number of participating parties would drastically increase. However, with such a large number of parties competing for seats, the Independent Electoral Commission reached an impractical size of 53 members. An additional component of the electoral reforms required political parties to obtain two percent of the seats in the national legislature in order to participate in subsequent elections. As a result, when 42 of the 200 parties did not meet this threshold, they were plunged into "lame-duck status" according to Dwight King in "Half-Hearted Reform: Electoral Institutions and the Struggle for Democracy in Indonesia" (210). This caused the commission to break into factions and led to a deadlock over the certification of the election results. According to Hermawan Sulistyo in "Electoral Politics in Indonesia: A Hard Way to Democracy", the election results were required by the Law on Elections to be endorsed and signed by at least 75 percent of the competing parties, or 36 of the 48 parties competing in the 1999 elections (82). When the parties that failed to meet the two percent threshold refused to endorse the final results, acting President B.J. Habibie signed a presidential decree recognizing the outcome.
Despite the domineering influence of the Independent Electoral Commission on the number of political parties in Indonesia, there are theories presenting other institutional variables as the cause for such a large number of parties. According to William Clark and Matt Golder, Duverger's theory states that more permissive electoral systems, such as proportional representation, produce more political parties (681). However, Clark and Golder argue that Duverger's views are not solely focused on institutional factors, but take into account societal influences as well. Duverger believes political parties are a "reflection of social forces," with greater social forces leading to the multiplication of parties, depending upon the degree of constraint applied by electoral institutions (Clark and Golder 681). The degree to which party systems reflect existing social cleavages depends upon the type of electoral institution. Permissive electoral systems, such as proportional representation, producing a large number of parties when levels of social heterogeneity are high (Clark and Golder 683). Given that Indonesia is an ethnically and regionally diverse archipelago, this theory may appear attractive for explaining the large number of political parties. However, Indonesia has a mixed electoral system, dubbed "PR Plus," allocating 76% of the seats in the legislature to single-seat districts according to the plurality principle and distributing the remaining 24% of the seats according to proportional representation (King 56). Therefore, while Indonesia is a very heterogeneous country, its electoral system is only partially designed as proportional representation, and therefore this theory cannot fully be applied to its specific case.
In addition to competing theories attributing the quantity of political parties to other institutional factors, the influence of further electoral reforms must also be taken into consideration. Other aspects of the electoral reform package, such as the requirements that parties must have an organization in one-third of the provinces and half of the districts in each of those provinces, certification of non-involvement in leftist organizations, and an ideology that did not conflict with the national philosophy of Pancasila influenced the number of political parties participating in the 1999 elections. These reforms eliminated narrow ethnic or religious parties, promoted parties with mass-based organizational structures, prevented leftist orientation from gaining any representation, and ensured that the national ideology was upheld. These reforms not only limited the number of political parties eligible to compete in the 1999 election, but also ensured that Indonesia's democratic transition did not cause the country to stray from its past.
In order to analyze the influence of Indonesia's Independent Electoral Commission on the number of political parties within the country, I will examine the quantity of political parties participating in elections over the course of the past four decades, which will reveal any change following the 1997 Electoral Reforms. In addition, I will assess the change in inter-party competition following the establishment of the Independent Electoral Commission by studying the number of political parties participating during the elections of 1971, 1977, 1982, 1987, 1992, 1997, 1999, and 2004. To determine the degree to which smaller minority parties gained representation following the 1997 reforms, I will consider the participation of such parties in the subsequent election as compared to their inclusion in previous elections. Inter-party competition will also be explored in an effort to better understand the degree to which the 1997 Electoral Reforms increased competitiveness within the government in comparison to the authoritarian-style pseudo-democracy and its lack of true opposition.
To address the effects of the electoral reform, political parties will be defined as those competing in elections. I hypothesize that the establishment of an Independent Electoral Commission will result in a dramatic expansion of the number of political parties competing for seats within Indonesia's government. Due to decades of profound censorship by the military-controlled government, a vast array of groups will strive for representation of their interests. Previously, political parties served the sole purpose of establishing legitimacy for Suharto's power. By maintaining frequent elections, the government provided a limited sense of public access and participation in the political process. However, the establishment of an independent body, rather than a government-run electoral college, would guarantee the inclusion of parties beyond the previous government-sanctioned three. With the world watching to see how Indonesia handles this opportunity for transition, the Independent Election Commission holds the responsibility of establishing free and fair elections and proving that Indonesia deserves to be regarded as a legitimate democracy rather than a falsely depicted one. The establishment of an independent body to oversee the 1999 election addresses the greatest flaw of Suharto's government: the lack of "an arena of contestation sufficiently fair that the ruling party can be turned out of power" (King, "Half-Hearted Reform" 5). Instead of simply allowing a select few parties to compete under a fixed election, the Commission provides a neutral electoral institution to prevent such government manipulation.
I hypothesize that the creation of an Independent Election Commission will increase inter-party competition. Given that electoral contest will be opened to all parties who wish to request to compete, it would follow that the level of competition between parties would increase as well. With more parties eligible for inclusion in the election, the competition to gain a seat will become tighter. The great importance of this election also influences the political aspirations of parties who wish to gain power within the government during such a significant period of Indonesia's history. An independent body such as the Commission will no longer prevent competition within political parties, as had occurred under Suharto's regime, but will instead encourage a healthy level of contest indicative of a democratic electoral system. Furthermore, I hypothesize that the Independent Election Commission will result in greater representation for smaller, minority parties. Such parties were previously forbidden from competing under Suharto's rule, essentially neglecting the interests of ethnic minorities within the country. With the expansion of electoral participation under the new rules of inclusion, smaller political parties will have a newfound opportunity to compete for the representation of minority interests. The Commission's impartiality will apply rules equally to all parties wishing to compete, allowing those who meet the qualifications to participate, regardless of size or ethnic membership.
As evidenced by Table 1.0, the number of political parties participating in elections from 1971-1997 remained stagnant at three parties, while the number expanded to eight parties receiving at least one percent of the vote in the 1999 elections and thirteen parties gaining one percent or more of the vote during the 2004 elections (Suryadinata 32). This dramatic increase in political parties competing in the elections following the electoral reforms of 1997 affirms the hypothesis that the establishment of an Independent Election Commission would result in a greater number of parties. Despite the fact that the three parties permitted to participate in elections from 1955 to 1997 remain active during the 1999 and 2004 elections, they receive competition in the form of additional parties. The Independent Election Commission took control of party inclusion and participation out of the government's hands, enabling more parties to compete in subsequent elections.
The Independent Election Commission may have successfully enabled more parties to compete in the election, but it failed to account for the high levels of inter-party competition that would unavoidably result. According to Shaheen Mozaffar and Andreas Schedler in "The Comparative Study of Electoral Governance", high levels of distrust between parties in democratizing countries motivates them to create an independent election-management body (17). During a country's transitional regime, opposition parties are particularly suspicious of the electoral process due to the government's former manipulation of electoral structures and processes (Mozaffar and Schedler 9). Therefore, election authorities must ensure that their political neutrality is widely accepted in order to ensure an effective transition. However, the structure of the party system also affects the decision to establish an electoral commission. Within a two-party system, political parties are more likely to choose to relinquish their powers of electoral governance to an independent commission, which allows them to share power while retaining the right to veto. In a more fragmented multiparty system, political parties tend to choose a multiparty commission instead in order to retain a level of control (Mozaffar and Schedler 17).
When governments choose to arrange a multiparty commission to manage their elections, some parties may form exclusionary alliances against others (Mozaffar and Schedler 17). This occurred under the Independent Election Commission in Indonesia when it became apparent that most seats would be concentrated among the top six or seven parties. Thirty-one of the 48 total parties refused to sign off on the election results, preventing the commission from receiving the two-thirds majority needed to certify the election (King, "Half-Hearted Reform" 87). According to King, the party representatives were effectively overturning the decisions of their colleagues to verify the results ("Half-Hearted Reform" 87). In addition, several representatives for the small parties also demanded legislative seats for all parties that had participated in the elections, regardless of how many votes they had acquired, citing the defense that they had helped successfully administer the elections and did not have sufficient time to prepare for their campaigns following Suharto's resignation (Johnson 488). This request was not granted, however, and these small parties abused their positions in the Independent Election Commission by uniting in an effort to block the Commission's decision and buy themselves time to obtain seats through corrupt measures (King, "Half-Hearted Reform" 77). This evidence supports my second hypothesis that the establishment of an Independent Electoral Commission would result in increased inter-party competition. Thirty-one of the smaller political parties that failed to obtain a sufficient number of votes manipulated their positions on the Commission in an effort to secure seats in the legislature from parties that had received more votes. Competition for seats was extremely high, with only six parties receiving a sufficient amount of votes to meet the 2% threshold required to compete in the next election and only 21 parties receiving a seat (King, "Half-Hearted Reform" 77). Since this election occurred during Indonesia's influential transition from an authoritarian regime to a democratic government, the level of distrust between parties was particularly high. In addition, parties were accustomed to governmental manipulation and were willing to resort to corrupt measures in order to secure a position for their party in the legislature.
In order to test my third hypothesis, which speculates that the establishment of an Independent Election Commission will result in greater representation for smaller, minority political parties, I must first examine the preceding election of 1997. Under Suharto's regime, the predetermined three parties were given permission to participate: the United Development Party (PPP), the Indonesian Democratic Party (PDI), and the government-backed Golkar. Since the two opposing parties were participants merely to present an image of electoral competition and legitimacy for Suharto's regime following Golkar's fixed victory, PPP received 22.4% of the total votes and PDI only received 3%, leaving Golkar the majority of votes with 74.5% in the 1997 election, as evidenced in Table 1.0 (Suryadinata 32). Smaller parties had no opportunity to compete in this election, let alone obtain representation. During the 1999 election, 21 political parties may have won seats, but the top five parties accounted for 90% of the votes and therefore almost all of the 462 seats in the House of Representatives (King, "Half-Hearted Reform" 207). PDI-P received 33.7% of the votes, Golkar obtained 22.4%, PKB acquired 12.6%, PPP gained 10.7%, and PAN won 7.1% (King, "Half-Hearted Reform" 78). In total, these five parties obtained 416 of the 462 seats in the House, leaving parties that each acquired less than 1% of the vote with less than 10 seats each and 10 parties with only one seat. Despite the electoral reform, the three parties which were formerly the only electoral participants still received 66.8% of the votes and 331 of the seats, leaving the lesser parties susceptible to their majority control in the legislature.
The lack of representation for smaller parties can be traced to the electoral restrictions enforced by the Independent Election Commission. In order to qualify to compete, parties had to have an organization established in one-third or nine of the provinces and half of the districts in each of those provinces in an effort to exclude ethnically and regionally based parties (King, "Half-Hearted Reform" 51). Suharto's government had restricted participants to three political parties, but these reforms now limited on the basis of "insufficient geographical coverage and depth or penetration of their organizations" (King, "Half-Hearted Reform" 51). This data disproves my third hypothesis, revealing that minority interests continued to be overlooked and smaller parties were unable to gain enough votes to contend with the top five winning parties. For instance, the ethnic Chinese party PBI was eligible to compete in the 1999 election, although it only captured 0.34 percent of the Indonesian vote, securing a single seat (Suryadinata 132). This may be due to confounding variables, however, such as fear of voting for an indigenous political party. Within Indonesian society, ethnic Chinese were often victims of ethnic violence, causing them to stray from political participation as a defense strategy. According to a survey conducted by the Indonesian weekly news magazine Tempo, many Chinese were still afraid to express their political views in 1999 (Suryadinata 127). Nevertheless, smaller parties fortunate enough to gain a single seat in the legislature, such as the PBI, would be incapable of advancing any of their interests when met with the larger parties that dominated the House of Representatives.
However, while each of these requirements did limit and curtail the number of parties participating in Indonesia's elections, the institution of the Independent Election Commission carried out these electoral rules and limitations. Initially designed to prevent the government from intervening in the election's outcome, the Independent Election Commission ultimately failed to perform its obligations to ensure a truly free and fair election as necessary for qualification as a truly democratic nation. Its expansive, largely undefined and misused powers allowed each party to overstep and manipulate its boundaries. As evidenced by the formation of a coalition of 38 parties refusing to recognize the election results, this institutional body was corrupt and far from independent. In its final report, the central supervisory committee of the Independent Election Commission charged the body with overreach and abuse of its authority, stating that the commission allowed "parties greater authority than the law allowed in determining their elected candidates" (King, "Half-Hearted Reform" 87).
According to Larry Diamond in "The Global State of Democracy", for a country to be considered a liberal democracy, a truly independent electoral commission must first exist to ensure the possible removal of corrupt elected officials from office through frequent, free and fair elections (418). Truly free and fair elections are one of the greatest indicators of a democratic government, revealing the power of the public to hold its representatives accountable and restrain their authority. In emerging democracies, the role of an independent electoral commission is particularly vital to the legitimacy of the subsequent government. Electoral governance in such transitional regimes must balance administrative efficiency, political neutrality, and public accountability (Mozaffar and Schedler 8). Credibility of electoral results can be attained when electoral rules and institutions meet each of these challenges. Within electoral governance, the level of rule application is most susceptible to errors due to the magnitude and complexity of tasks and the authorized discretion of officials involved while accomplishing such tasks (Mozaffar and Schedler 9). During Indonesia's 1999 election, whenever new electoral procedures were ambiguous or poorly understood, members of the Independent Election Commission tended to resort to old operating procedures, contributing to the relatively high incidence of procedural violations (King, "Half-Hearted Reform" 86). Although thousands of violations of election laws and regulations were documented, international and domestic election monitors agreed that the 1999 election was the freest and fairest election since 1955 (King, "Half-Hearted Reform" 77).
The results of this research indicate that the establishment of an Independent Election Commission helped to bring a higher degree of legitimacy to the election's results than had been achieved under Suharto's regime. Although the Commission was not without flaws, it is expected that the first elections to occur following the fall of an authoritarian government will contain a degree of attempted manipulation on the part of participants. The Commission did allow the expansion of electoral participation to encompass a greater number of political parties, although competition among participating parties increased as well and minority interests were not represented to a greater degree than before. Continued research is necessary to further assess the role of electoral governance in relation to political parties within emerging democracies. According to a growing compilation of evidence, ineffective electoral governance is an important cause of many flawed elections that have occurred in transitional regimes within the last three decades (Mozaffar and Schedler 6). However, electoral governance has been largely overlooked as an influential variable of democratization in political research (Mozaffar and Schedler 6). The importance of electoral institutions that enforce the rules, carry out the elections, and determine the allocation of seats among parties should not be discredited as inconsequential. Electoral results are due to party systems, political participants, individual campaigns, and voter turnout, but electoral governance plays a greater role than is typically acknowledged.
In addition to indicating the need for additional research regarding the role of electoral commissions, this research reveals the influence of social, economic and political variables that affect the process, outcome, and legitimacy of elections. The institutional changes and electoral rules established in Indonesia by the 1999 electoral reforms contain restrictions; they cannot reverse the damage that decades of authoritarian rule have inflicted upon the integrity and strength of elected representatives. Factors outside the limitations of an electoral institution such as the Independent Election Commission also affect the number of political parties, levels of inter-party competition, and degree of minority representation. Establishing an Independent Election Commission in an effort to ensure the neutrality and legitimacy of electoral results will not yield the preferred outcome of increased legitimacy for Indonesia's government without taking into consideration the factor of human error. Individuals working as members of the Commission do so as agents of its authority, but institutional regulations cannot compensate for their corrupt ambitions of personal advancement. This research indicates the large degree of uncertainty, even within electoral rules and institutions, which cannot be controlled in an effort to democratize a nation. Rather than seeking to contain unexpected circumstances, electoral bodies and the laws they enforce must instead account for such factors in advance in order to avoid the breakdown of well-intended establishments, as occurred in the case of Indonesia's Independent Election Commission.

Over the past years, the media has been accused of having a liberal bias. The Daily Show faces the brunt of the criticism with their "fake news" program that highlights liberal ideas and openly criticizes policies of the Bush administration. In 2004, the Daily Show created its own series to cover the presidential race entitled Indecision 2004. The Democrat and Republican Conventions were each covered in four consecutive episodes in July and September. Assuming that the "liberal bias" of the Daily Show helped filter the content shown during convention coverage, it could be inferred that the Democrats were covered more favorably than the Republicans. However, when isolating and analyzing the convention coverage from the DNC and RNC, the Daily Show's coverage better highlights Republican ideals and message.
The DNC was covered from July 27 to the 30th with the series title featuring the Democrat's "Race to the White House." The RNC was taped later from August 31st to September 3rd, and was entitled "Target New York." Each series contained four episodes covering convention highlights and in-house interviews with John Stewart. By analyzing the sound and image bites of the convention coverage, the noted strength of the Republican party to speak in "one voice" was clearly shown. This study provides an analysis of the speakers shown, topics covered, and subsequent commentary. While the clips of Republicans focus on topics related to the war in Iraq, homeland security, and Bush's promise to keep America safer, the clips of the Democrats fail to produce a unifying theme and message.
The potential effect of the Daily Show's coverage to influence viewers has been studied in the past years through the presentation of sound and image bites. Scheuer and others agree that "television is the dominant medium of a media-dominated age" and effective because it "manipulates our emotions, numbs us with stereotypes, saturates us with the trivial and the superficial" (1; 6). Scheuer also notes how simplicity drives the television message and "because of this language (sound bites), television favors certain political ideas and disfavors others. The electronic culture fragments information into isolated, dramatic particles and resists longer, more complex messages" (10). Liberal ideas are then naturally challenged, as they appear more complex and progressive. "Simplicity, I will suggest, is epitomically conservative, whereas complexity is quintessentially progressive" (Scheuer, 10).
The existence of televised political coverage then enables journalists and commentators to transmit messages to the common public. They are able to take events and sound bites and treat them "as raw material to be taken apart, combined with other sounds and images, and reintegrated into a new narrative" (Hallin, 9-10). The Daily Show adds a humorous component to its reporting of political news, which diminishes the distance between the viewer and the situations being reported on. "Humor can enable people to confront authority, to diminish it, to reduce its distance and majesty, thereby revealing authority holders as imperfect mortals, error-prone humans, ordinary people unworthy of special respect, deference and continuation in office" (Paletz, 8). Hallin went farther to note the feeling of public efficacy derived from political commentary. "Often it was extremely interesting...to hear a politician, or even once in awhile a community leader or ordinary voter, speak an entire paragraph. We had a feeling of understanding something of the person's character and the logic of his or her argument that a 10 second sound bite can never provide. One also had a feeling of being able to judge for oneself" (Hallin, 19). Bucy and Grabe also emphasized the importance of image bites because "nonverbal emotional displays of leaders can serve as a potent vehicle for expression" (659). The study of these sound and image bites in political news coverage, even the "fake news" of the Daily Show enables valid conclusions to be drawn about the effect of potential communications on the electorate.
The goal of the analysis was to find a way to distinguish the content of the Democratic and Republican coverage in a way that was measurable. The best method to do this was to pick out direct points of reference that would occur frequently in coverage of both conventions. The compiled coding sheets are available for reference in the appendix of this paper.
The first thing coded for was the ways to define Democrats and Republicans in the episodes and who made those statements. Examples of this included the Daily Show's opening segment, which defined the Democratic Convention as featuring "black people, trial lawyers...celebrities who believe in their hearts that they're helping but really aren't." The coded definitions were only those that meant to apply to the party as a whole and not individual people or the nominees. By noting only the generalizations for the party, conclusions can be drawn about a party as a whole, rather than one representative of it.
There was also a distinction made between references to the party rather than the convention. The coding sheet marked references to the Democratic National Convention and Republican National convention. The messages coded in this section noted the themes presented at the convention and general remarks made by commentators about the event. The intent of marking down the ways in which the parties and convention were defined was to note the differences not only between the parties themselves, but the ways in which the opposition party was referenced by the other.
The third point of analysis for the convention coverage was the references to the nominated candidates, John Kerry and George W. Bush. This type of analysis provides insight to how the opposition party defines the other candidate while also presenting their own. The main point of a convention is to introduce a Presidential candidate to the nation and so the ways they are defined are crucial.
In addition to noting references made to specific candidates and parties, the coding sheet accounted for blatant attacks made by one party on the other, and in some cases, attacks made by one party on itself. Attacks were only marked when one person made a direct reference to a party with a subsequent statement or negative tone. The attacks were labeled as issue or character based. The reason for making this distinction is to note whether there was a significant difference in the type of partisan attacks.
The speaker clips shown in the convention coverage represent the party's message. The coding sheet included the clip of every speaker who was not a correspondent for the Daily Show at each respective convention. The coding for each speaker included their title, talking point, length of time shown in the clip and the context in which they were shown. The context varied in three mediums: either a collection of video clips chosen by the Daily Show, a speech on the convention floor, or an interview in the Daily Show's studio. All of the time measurements were taken in seconds to be able to make comparisons more accurate.
The last two things coded for were the actual length of sound and image bite coverage of the two conventions. Sound bites were only measured when someone who represented the convention, not the Daily Show's representative, was heard. Image bites were taken when a person was shown, but not heard. The measurements do not overlap and adding them would denote the entire length of time the convention was covered for both parties. Sound and image bites of the convention do not include the studio interviews by Stewart or commentary from the Daily Show correspondents. The intent of this analysis is to show that the message from the clips of actual convention coverage and subsequent commentary from direct party and Daily Show representatives themselves produce a message that can then be interpreted by a viewer.
The results of the content analysis show that there is a more unified message delivered by the Republicans through the Daily Show's convention coverage. The first two references coded for were ways to define the parties and the conventions generally. This analysis was more focused on the rhetoric used by both the parties. In both series of convention coverage, the terms to define Republicans and Democrats came from the Daily Show correspondents and other media figureheads. Every episode in the "Race to the White House" series opened with an announcer proclaiming the convention featured "black people, trial lawyers, unifying anger, organized labor, godless sodomites, and abortion for everybody,..." A serious tone was not associated with the message, but the viewer is introduced to Democratic coverage by their stereotypes. Furthermore, when Stewart is talking about the theme delivered by the Democrats, he mockingly insinuates that they are "laying it on too thick." The viewers are encouraged to view the clips as very rhetorical, so they would possibly then analyze the speeches from a more critical perspective. The statements made generalizing Republicans were much more directed and offensive. Stewart himself made these comments when generalizing the GOP convention to be a lot of "white men." Stephen Colbert also criticized the Republican's management of convention coverage to falsely highlight their small number of minority members, "if you're a non-Caucasian Republican, you got yourself a heaping helping of face time."
The commentary on the Republican's relationship with minorities continued when coding the ways in which the two conventions were described. The opponent's convention was mentioned only once during the Republican convention and not at all during the Democrat's. The attack on the Democratic Convention came from Arnold Schwarzenegger when he described it to be as the same as his movie, "True Lies." All of the other references to the Democratic convention, with the exception of one made by Gov. Bill Richardson were statements made by Daily Show correspondents. John Stewart called the convention a "Dance Party" three times and immediately switched to image bites of delegates dancing in their seats. There was no parallel mention of this jovial spirit during the Republican convention. The constant theme of the message describing the Democratic convention was insisting that the event was a "farce, a scripted stage managed event, it's not news, it's not even fake news." Stewart went further than Colbert to say that both conventions merely resembled "infomercials."
While the commentary for the Democratic convention targeted their energized spirit and mocked the event itself, the Daily Show's commentary for the Republican convention was much more directed towards the Republican character. Steward described the convention atmosphere as a time when "Disneyworld closes for gay couples" and Colbert reminded the viewers that "we're doing the same show tonight -- homosexual white men." Five out of the nine mentions by the Daily Show about the Republican convention mentioned homeland security and the Republican theme of a "safer and stronger America." Despite what could be viewed as negative commentary, "is it tonight that they exploit 9/11 or is tonight empty promises for the future?" the constant associations of the Republicans to homeland security issues could also serve to reinforce their talking points.
The graph further emphasizes how the Democrats did not make any attacks on the Republican's convention, while the Republicans not only attacked them, but also kept their message about themselves positive.
The following table and charts show the differences in commentary related to the Republican and Democrat nominees.
The charts clearly show that the Republicans did not make any attacks on their own candidate, yet the Democrats made three direct attacks on Kerry. Those attacks came from Democratic congressman Zell Miller who spoke at the Republican convention to express his disdain for John Kerry. Miller did not hesitate when describing John Kerry as "more wrong, more weak, and more wobbly than any other national figure" and proclaiming that "Kerry would let Paris decide when America needs defending." The Daily Show included the clips of Miller's speech and spent several minutes discussing why a Democrat would attack his own party. Miller further emphasized the Republicans' talking points against the Democrats as John McCain questioned whether John Kerry had shot his dog. While the table shows both nominees received an equal number of attacks, none of George W. Bush's attacks came from his own party. The nature of their attacks on John Kerry were about his war record and security issues, while the comments for Bush targeted on his character and intelligence. Joe Biden described how he knows the President "doesn't read anything, but I think he watches television." The content analysis showed that while both nominees were described in relation to a war context, the Republican Convention contained more pointed and direct attacks towards John Kerry's record.
The results issues coded for related to the concept of issue ownership amongst the Democrats and Republicans. The coding sheet accounted for the number of positive and negative mentions between certain issues and the different parties.
The most notable result is that the Republicans had 15 positive mentions associated with them and homeland security and only 6 negative comments. The Democrats, in relation, had only 3 positive mentions associated with security issues and 8 negative. The issues of health care and religion were not mentioned negatively for either the Democrats or Republicans. Religion was mentioned positively once for both parties and health care once for the Republicans. Social issues covered topics ranging from abortion to gay marriage. The Republicans and Democrats balanced both their negative and positive associations with social issues so that no party clearly had more significant coverage.
Another main focus of the content analysis was to evaluate the number of speakers clips shown from the convention coverage and evaluate the topics and length of airtime. The graph clearly shows a difference between the sheer volumes of Democratic speaker clips as compared to Republicans.
The Democrats' coverage had 25 speakers, while the Republicans had only 14. The following pie charts highlight greater discrepancies between the types of speakers shown for both types of convention coverage.
Over half of the Democratic speakers shown held some form of public office, while only six of the Republicans did. Seven of the Democrats were related to the President and Vice Presidential nominee and the remaining were media figureheads, such as celebrities, who spoke in support of the Democrats. In contrast, the Republicans had six of their speakers related to their nominee and one from the President's staff. While the number of speakers for those categories may be roughly the same, the Republicans had a smaller number of speakers overall, thus their percentage of speakers directly associated to the President and Vice President was 50%. It is also interesting to note that roughly one-third of the Democrats shown were candidates for the nomination themselves and five of them including Howard Dean, Joe Biden, Joe Lieberman, Wesley Clark, and Al Gore, openly talked about their attempts to run for President. Governor Dean shared stories from the campaign trail for 20 seconds and Former General Wesley Clark shared his war credentials and history for 14 seconds. The stories shared by elected public officials from the Republican convention evoked memories of 9/11 and the war on Iraq. Senator McCain talked about the Iraq War for 20 seconds while Governor Pataki and Mayor Giuliani referenced September 11, 2001 with a banner with those words in the background. No matter the sound bite shown, viewers would still receive an image of 9/11.
The coding sheets for the Republican and Democratic National Convention also show a difference between the number of in-house interviews Stewart conducted during each series. Stewart interviewed four people during the Republican convention and only two during his coverage of the Democrats. However, two of the interviews during the Republican coverage were with media commentators, Ted Koppel and Chris Matthews. The two other interviews were with Senator McCain and White House Communications Director Dan Bartlett, and they focused almost exclusively on the Republican message and President Bush's handling of the war. The two interviews with Democratic Governor Bill Richardson and Senator Joe Biden covered some of the convention message, but also focused on the official's personal story and their advocacy issues. There was no significant discrepancy between the time each interviewee was allotted during coverage.
The length of time an image or sound bite is shown on television can be significant. The following graph shows that the Republicans were shown more than the Democrats, with 330 seconds devoted to image clips, while the Democrats had only 282.
The Democratic sound bites were longer totaling 408 seconds, while the Republicans had 330 seconds. The nominees themselves also received an equal amount of coverage with Kerry receiving 77 seconds of image and sound bites and Bush being heard for 61 seconds with no image clips. The majority of the Republican image bite coverage was pictures of delegates on the floor, while many of the Democratic clips were speakers walking out to the podium on stage. Looking only at the length of actual convention coverage, including video and audio footage of the DNC and RNC, the amount of time was roughly equal for both parties. The Democrats were heard and shown for 13% and the Republicans for 11.8% of the Daily Show's total series coverage for each convention.
It seems slightly farfetched to hypothesize that coverage of the Republican and Democratic National Convention on the Daily Show could provide some advantages for the Republican candidate. Especially in a media environment that is constantly accused of having a liberal bias and favoring progressive ideals. The conclusions drawn above are extrapolated evidence from a small sample size of only eight episodes. The size of the sample could have affected the results because only four episodes are being compared with their four counterparts. The Daily Show is a television show that has been established over years and it seems impractical to make general statements about its content on the basis of eight episodes.
Yet analyzing the content of the sound and image bites of the conventions and their subsequent commentaries show that the Republicans had a more focused and more disciplined message than the Democrats. The number of convention speakers shown for each the Democrats and Republicans support the image of a closed Bush administration. Auletta noted in his article in the New Yorker that "what seems new with the Bush White House is the unusual skill that it has shown in keeping much of the press at a distance while controlling the news agenda" (4). After studying the communications and public affairs office of the Bush White House, Martha Kumar explained that, "Bush has a great focus on the structure of his communications operation -- advocating for him is not the same thing as explaining his decisions or actions"(15). It is this strength, which shines through the coverage by the Daily Show on the Republican convention. Most of the Republican speakers mentioned the war in Iraq, September 11th, and the ability of George Bush to help make America safer. Most of the Republican speakers were related to or staff for the Bush administration. The administration speaks with one voice and they "never get off their talking points" (Auletta, 4-5).
The Democrats, by contrast, through this analysis, are presented by their depth. Over half of the convention clips shown by the Daily Show were of previous candidates for the nomination and most of them even talked about their own Presidential ambitions. Senator Barack Obama and Congressman Dennis Kucinich talked about uniting America and the "hope of a skinny kid with a funny name who believes America has a place for him too." But those hopes for unity fell short of the coverage offered by the Daily Show. Democrats attacked fellow Democrats and the coverage of Zell Miller's speech at the Republican convention was the highlight of one episode. Not one Republican spoke ill of President Bush, and not once did Democrat Zell Miller speak positively about his own party.
The Daily Show commentary played a strong role in influencing the perception of Democrats and Republicans by the viewers. When referencing the nominating candidate, the Daily Show was responsible for the majority of insults against President Bush. Their video montages echoed Bush's ability to control the media with the headline, "George W. Bush: Because He Says So." There is no doubt that reporters "see the White House as a fortress" (Auletta, 2). The Daily Show introduced John Kerry in their montage to be the alternative choice, "Because He's Not George Bush." Despite the clear insult towards the President's authority, the viewers associate the images of John Kerry constantly with George Bush. Kerry and the Democrats are unable to dictate the message.
One possible reason for the inability of the Democrats to control the message is the circumstances surrounding the 2004 campaign. The Republicans were facing a re-election year, so President Bush did not have to go through the vetting stage at the same time as John Kerry. The Democratic convention discussed the trials that took place through the nominating process. Joe Lieberman discussed how there was almost a "three way tie" for third place. Governor Dean shared stories about his campaign and the Daily Show highlighted his famous scream. Although the chair of the convention, Bill Richardson stated that this convention was a "build up to tell the story of Senator John Kerry," the story was lost amongst the breadth of speakers and inner party attacks.
Another subtle influence on the coverage of the two conventions was their locations. The Democratic convention was held in Boston, Senator Kerry's home state. The Daily Show had to stay in travel accommodations and did not have their familiar set available. For the Republican convention, the Daily Show was on its home turf. They had more in studio interviews during their Republican coverage, possibly because more people were available to come onto the show and they had more access to other media commentators in New York like Ted Koppel and Chris Matthews. It is also no coincidence that the Republican National Convention was hosted in New York three years after the attacks on the World Trade Center. The President and other Republicans were able to stand outside the twin towers and further emphasize September 11th as the central focus of their re-election strategy.
This militant focus on message is also shown through the analysis of positive and negative issue associations with the Democrats and Republicans. The Republicans had 15 positive messages associated with them and homeland security. Mayor Rudy Giuliani spoke in front of a backdrop of September 11, 2001 that was in bolded white against a black screen. If the Daily Show wanted to clip any of his speech, they would inevitably be fed the image bite of 9/11 regardless of Giuliani's words. In comparison, three issues, which would normally be associated positively with Democrats, social issues, health care and the economy, did not distinguish them more significantly than Republicans. In fact, the Republicans had more positive mentions in relation to social issues than the Democrats. They also had less negative associations with homeland security than the Democrats. The election of 2004 was arguably a referendum on the Bush administration's ability to "keep America safer" and by noting the message of the Democrats and Republicans in relation to that theory, the Republicans message was more emphasized and thus could be more directly extrapolated upon by the viewers.
The coverage of the Daily Show's Indecision 2004 highlighted the ability of the Democrats and Republicans to present a message through their respective conventions. Although the Daily Show is known as one of the most liberal television programs on TV, the direct convention coverage showed that the Republicans are more organized, more disciplined, and have a more coherent message. This reflects the strategy of President Bush that was also echoed by a senior official weeks after the Republican convention. "We're an empire now, and when we act, we create our own reality. And while you're studying that reality -- judiciously, as you will -- we'll act again, creating other new realities, which you can study too, and that's how things will sort out. We're history's actors . . . and you, all of you, will be left to just study what we do"(Suskind, 2004). The Democrats' attempt to create a coherent message from a nomination field of over seven candidates and the unsure war record of the nominating candidate clearly pales in comparison.
The next question then, is to ask why coverage assumed to be skewed in one direction actually promotes another as a result of its failure to diminish the effects of the Republicans' talking points. It is assumed that "journalists can create importance and certify authority as much as reflect it, in deciding who should speak on what subjects under what circumstances" (Cook, 87), but that is not reflective in John Stewart's coverage of the conventions. The entire Republican Convention reflected a well-oiled machine of speakers and unflinching support for the current President. It would have been difficult for the Daily Show to find footage of the actual convention that showed otherwise. Chris Matthews of MSNBC produced insight into this phenomenon two years later, after Stephen Colbert gave his speech at the annual White House Correspondents Dinner. He proclaimed that "the president is our head of state, not just a politician" and because of that, is he subject to different rules pertaining to the scrutiny of his campaign strategy and message (Matthews, 2006)? Analysis of the Daily Show's presentation of Indecision 2004 exposes how the Republican "voice" will effectively and inevitably break through their political coverage.

The issue of health care in America has not always been at the heart of every political debate. In a study conducted during the 2004 Presidential elections, although health care ranked higher in importance among voters than most other domestic issues, it was only fourth in importance in determining their vote for president, with affordability of health care and health care insurance being chosen as the specific health related issues of greatest concern (Blendon, et al. 2004). However, health care has, at certain times, been extremely contentious and highly politicized, bringing it to the forefront of voters' attention. A current prime example would be the bitter partisan debate between the Democrats and Republicans regarding the reauthorization of the State Children's Health Insurance Program (SCHIP), an extremely volatile issue precisely because it concerns the well being of society's most vulnerable -- children. SCHIP, which will be discussed in greater detail later, has brought forth a barrage of petty party bickering and accusations of politicization from both sides of the aisle. This has led to much media criticism about the futility of such political theatre and rhetoric in addressing the problem at hand. Thus, what sort of effects has the politicization of health care brought to the American people? Have these disagreements paradoxically brought about more thoroughly analyzed policies and led to pragmatic and optimum compromises or has this political theatre and exaggerated rhetoric only served to impede the improvement of healthcare due to both parties' inability to cooperate and break the deadlock in political ideologies? The link between the variables of politicization of healthcare and its effect on the public will thus be studied with SCHIP as its main case study.
SCHIP was enacted as Title XXI of the Social Security Act by the Balanced Budget Act of 1997 primarily to expand insurance coverage to low-income children living in families who earned too much to qualify for Medicaid but were yet unable to afford private insurance. At that time, more than 10 million children lacked health insurance, with about seven million of them living in families with incomes below twice the federal poverty level (FPL). Although 75 percent of those uninsured children lived in a family with at least a parent who had full-time employment, and 90 percent had one parent who was employed either full or part-time, their families were either not provided with job-based health insurance or lacked the finances to buy the insurance offered (Families USA). Thus, Congress appropriated approximately $40 billion to fund the program for 10 years. Although it was optional, within two years of SCHIP's inception, all fifty states had adopted the program to expand coverage for children. Based on steadily rising enrollment in SCHIP, rates of uninsured children from low-income families have declined 2.2 million, from 23 percent in 1997 to 14.4 percent in 2004, and the program is seen as highly successful (Pediatrics). However, in 2006 the U.S. Census Bureau reported that 9.4 million children in the U.S. did not have health coverage-one in eight being uninsured. What this meant was that after six years of improvement in the rate of health coverage for children, the number of uninsured children has again begun increasing. In 2006 alone, although SCHIP was found to provide health insurance coverage to 6.6 million children, there was found to be one million more uninsured children than two years ago (Pediatrics).
This set the stage for SCHIP to become a highly contentious issue and a political hotbed as children have always been perceived as political untouchables. Thus, the reauthorization of SCHIP is a very charged one because it has been a highly popular program that has drawn bipartisan support in the past. In fact, a survey by the Center for Children and Families at the Georgetown University Health Policy Institute has also found that nine in ten Americans say the program is important, with support for it crossing party lines (Lake Research Partners). Democrats are in support of the SCHIP reauthorization bill and have championed the fact that the SCHIP bill would cover 10 million children and be completely paid for by the tobacco tax, adding no strain on the federal budget. However, on the other side of the aisle, Republicans have felt that a $35 billion expansion would be too great of an increase and goes "too far in federalizing healthcare." (Pediatrics) The inclusion of a tobacco tax, purported claims of covering illegal immigrants, middle-class families and adults, and the possibility of the reauthorization adversely affecting the private insurance market are also the opposing arguments that are most often brought up by the Republican side.
The debate that has taken place in Congress over the course of the past three months is indeed a complicated affair that has seen usual legislative processes thrown out of the window. The reauthorization issue first arose in early August when the House passed the CHAMP Act in a partisan fashion 225 -- 204 with primarily Democratic support, in an attempt to renew SCHIP before it expired on September 30, 2007 (Children's Defense Fund). The CHAMP Act would provide health coverage to 4.1 million more uninsured children. Nearly $50 billion would be needed in additional funding over the next five years and will be paid for by a 41-cent increase in the federal tobacco tax and cuts in overpayments to private Medicare plans (Children's Defense Fund). However, the Senate's own Children's Health Insurance Reauthorization Act of 2007 (CHIPRA) would expand coverage to another 3.2 million uninsured children and would cost an increase in $35 billion of the federal SCHIP contribution, from $25 billion to $61.4 billion over the next five years. This package is funded by a $1.00 tobacco tax, a 61-cent increase from before. CHIPRA eventually passed 68 -- 31 (Children's Defense Fund). President Bush has expressed his opposition to both plans and promised a presidential veto if the bill passed both houses. He proposed increasing funds by a comparatively lesser $4.8 billion over the next five years. In addition, Bush also proposed the highly unpopular move of reducing funding for states that have expanded SCHIP eligibility to children in families with yearly incomes more than 200 percent of the FPL (Pediatrics).
On September 24 this year, after nearly two months of negotiations, the House and Senate finally attained a bipartisan compromise to reauthorize SCHIP for an additional five years and cover 3.1 million more uninsured children. The compromise bill is similar to the Senate's, but includes additional provisions adopted from the House bill. This legislation would cost $35 billion over five years, cover a total of 10 million children, and would be paid entirely by a 61-cent increase in the tobacco tax (Children's Defense Fund). It has also obtained the endorsement from 43 governors, and a wide range of constituencies ranging from AARP, the National Council of State Legislatures and the American Academy of Nursing. In the House, the bill passed 265-159-insufficient to override a presidential veto-with 45 Republicans voting with all but eight Democrats in support of it. In the Senate, however, the bill passed veto-proof in a bipartisan manner 67 -- 29. However, President Bush followed through with his promised veto, but signs a continuing resolution temporarily funding SCHIP at former levels till November 16, 2007. Thus, even though SCHIP coverage has not ceased, the ideological deadlock has placed the health insurance coverage of the 6.6 million children currently enrolled at risk (Children's Defense Fund).
Following that, the House then won a procedural vote that allowed it to postpone until October 18 2007 a vote to override the veto of the SCHIP legislation. In those two weeks, congressional Democrats and their allies advanced with a paid media and grassroots campaign to pressure Republicans in vulnerable districts to vote for the override. According to CQ Today, the delay was intended to give Democrats and bill supporters time to "make a 'no' vote as politically unpalatable as possible for Republicans." (Kaiser Network) Public pressure and the media spotlight continue to mount as a joint survey conducted by NPR, Harvard School of Public Health, and the Kaiser Family Foundation show that 65 percent of Americans support increased funding for SCHIP, even after hearing opponents' arguments against it. On October 18 2007, the House voted 273 -- 156 in an unsuccessful attempt to override President Bush's veto of the reauthorization of SCHIP, with 229 Democrats and 44 Republicans voting in favor of the override, just 13 votes shy of the two-third margin needed (Children's Defense Fund). On October 25 2007, the House again successfully passed a revised but weaker CHIPRA bill 265-142, again without a veto-proof majority. This bill was essentially the same as the first and would cover 10 million children, but it was altered cosmetically to pacify Republican demands. It would limit coverage to children in families with annual incomes below 300 percent of the FPL, made it less attractive for parents to switch from private to government funded insurance, phased out SCHIP coverage of childless adults within one year, and also required states to apply more thorough citizenship documentation standards to prevent undocumented immigrants from enrolling in the program (Kaiser Network). The Senate followed suit and passed the revised compromise bill as well, this time without a veto-proof majority. Bush again promised to veto the bill if it was sent to his desk and this time went further to state that he would veto any bill that consisted of a tobacco tax increase. Therefore, in order to obtain more time for bipartisan negotiations, Congress passed and President Bush signed another continuing resolution that funded SCHIP at 2007 levels of $5 billion a year through December 14, 2007. After reaching a stalemate in negotiations right before the two week Thanksgiving recess, the SCHIP reauthorization bill was picked up again when Congress resumed on 3 December 2007 (Kaiser Network).
For the purpose of this study, we will define politicization according to Dictionary.com, which describes it as the bringing of political character or flavor to, as mainly partisanship -- a person with an inclination to favor one group, view or opinion over alternatives. It is a commitment to the incumbency of the ideology of a particular political party. Thus, in this case, the politicization of healthcare would imply that the debate has become mostly a Democrat versus Republican battle, with precedence being given to securing political points and capital rather than focusing on the substantive content of the issue at hand. The specific effects of this form of politicization on the American public that will be analyzed in this study will be the variable of the availability of health care insurance because these have been selected by voters as issues of greatest concern (Klein 2006). The section of the American Public that will be focused upon specifically will be low-income children since SCHIP was created precisely with their well being in mind.
The debacle over SCHIP is not the first time that America's healthcare has been brought into the fray of partisanship. The Clinton universal health care plan of 1993 was another highly politicized health care reform package proposed by the administration of then President Bill Clinton, and created and chaired by his wife, the First Lady of the United States Hillary Clinton (Skocpol 1995). The proposal provided guaranteed insurance coverage for all employees that was to be funded through payroll taxes and provided by tightly regulated, non-profit Health Maintenance Organizations (HMOs) (Health Economics 2007). The Clinton health care plan was estimated to create 59 new federal programs or bureaucracies, enlarge 20 others, require 79 new federal authorizations, and make substantial changes in the tax code, which critics felt would have only served to further complicate the already inefficient bureaucracy (Skocpol 1995). In line with providing universal healthcare, Clinton's plan also offered the unemployed government subsidies for enrollment into the HMOs (Health Economics 2007). Thus, on Capitol Hill, as political strategizing started kicking into gear, astute right-wing Republicans appreciated the fact that their ideological fortunes within the party itself, as well as the Republican partisan interest in undermining the Democrats as a means to securing control of Congress and the presidency, could be served by first demonizing and then completely crushing the Clinton plan (Skocpol 1995). Skocpol (1995) has found that from 1993 to 1994, hundreds of special interest groups spent more than $100 million collectively to sway the outcome of this particular public policy issue, leading to the Center for Public Integrity calling it "the most heavily lobbied legislative initiative in recent US history."
The politicization of this health care initiative also went as far as to prompt Mr. William Kristol of the Project for the Republican Future to distribute a continual stream of private strategy memos urging an all-out partisan warfare in December 1993 (Skocpol 1995). He remarked that "an aggressive and uncompromising counterstrategy" by the Republicans should ultimately kill, rather than amend the plan. Abolishing the plan without offering amendments was a key priority because of the potential of a Democrat-led universal health care plan to secure crucial middle-class votes and revive the reputation of the party, putting a Republican future in jeopardy (Public Broadcasting Service 1996). The timing of the memo coincided with a mounting private consensus among Republicans that a complete resistance to the Clinton plan was in their best political interest, and they did so by labeling it as a "quintessential example of Big Government Democratic liberalism run wild." (Public Broadcasting Service).
President Clinton's had made the assertion of a "health care system that is badly broken" and revealed his proposal in a widely acclaimed speech to Congress on September 1993. Republicans were thus keenly aware that they had to convince middle-class Americans that was both untrue and inaccurate (Skocpol 1995). Therefore, under the cover of Rush Limbaugh and other conservative right-wing anchors of hundreds of news and talk radio programs that had access to tens of millions of listeners, the Republicans through skillful political strategy and a genuine belief in their ideology promoted their cause by portraying the plan as a bureaucratic triumph by welfare-state liberals (Skocpol 1995).
As increasing partisanship ensued, moderate Republicans who had originally favored reaching a compromise began to recant in the face of anti-reform demands within their own party (Skocpol 1995). Interest groups whose leaders were initially not averse to further negotiations over reforms were soon arm-twisted by their constituents and Republican leaders to draw back from cooperating with the Clinton administration and congressional Democrats (Skocpol 1995). The distinction of party lines was also further exacerbated by the media as it increasingly focused its coverage from the content of the competing health plans under review to the partisan clashes and strategy of the different Congressional groups that were vying for control of the reforms (Bok 1998). Therefore, the inevitable end result was an unbridgeable partisan based schism that played a significant role in the defeat of the Clinton proposal. Amidst the debate, there was no middle ground political compromise that addressed the inefficiencies of the health care system reached (Bok 1998). The creation of SCHIP later in 1997 was thus a milestone event in American social history. Not since Medicare and Medicaid were established in 1965 had Congress members worked together to pass a bipartisan bill that endorsed such a large subsidized health insurance program, reducing the number of low-income uninsured children (United States Department of Health and Human Services).
However, partisan politics has not always led to the complete destruction of beneficial initiatives for Americans. In fact, it can be argued that it is precisely such partisanship that promotes healthy democratic debates over issues. With myriad opinions from both sides being contributed through suggested amendments even on the basis of party lines, the final bill can be further refined and improved before it goes to the floor, increasing the probability of it being passed into law. This can be seen in the example of Medicare where in 1995, for the first time in 30 years, a highly public, partisan and ideologically divisive debate occurred (Oberlander 2003). The 1995 Republican Medicare reform bill championed achieving a balanced budget through large cuts in program spending by introducing a political cap on Medicare expenditures (Oberlander 2003). Democrats were opposed to such a change because they believed that this would have led to a shortfall in program finances and crippled the program, thus leading then President Bill Clinton to veto the bill, while congressional Democrats joined in solidarity to criticize the Republican Medicare proposal as abandoning its social contract with the people (Oberlander 2003). However, the 1995 defeat of the bill created the gateway for further political compromises, and finally in 1997, Medicare reforms were passed in a bipartisan manner with the hard cap on Medicare spending that would have activated automatic spending cuts in the program being removed (Oberlander 2003). This was a key concession by the Republicans as Democrats had identified this particular cap as threatening the ability of seniors to access quality medical care and were thus strongly opposed to it (Oberlander 2003). Spending cuts were also less harsh this time around. In hindsight, these political compromises have been recognized as essential in preventing traditional Medicare from being devastated through cuts and restructuring, allowing it to continue serving its target population (Oberlander 2003). Thus in this scenario, we are able to observe how partisan politics might have served the American people.
Various assessments of America's current health care system have led to it being called "financially inefficient, inaccessible, and administratively wasteful" (Matcha 2004). Despite spending more per capita on health care with annual costs exceeding $2 trillion in 2005-an astounding 16 percent of its GDP-America is not getting a corresponding value for its money. It continues to document higher infant mortality rates, lower life expectancy, and an actual uninsured population of 45 million as of 2005-a phenomenon virtually non existent in the rest of the industrialized world (Klein 2006; National Coalition of Healthcare). Furthermore, the problem is slated to get worse with American health care spending projected to continue increasing at similar levels for the next decade, reaching $4 trillion in 2015-a sizable 20 percent of GDP (National Coalition of Healthcare). Currently, California records the largest number of uninsured-18 percent of its residents-with 80 percent of these people being from working families with average incomes (California Teachers' Association). Without the social safety net of health insurance, their illnesses go unchecked until they become emergency concerns, further pushing up the cost of health care (California Teachers' Association). Employers have also borne the brunt of the health care crisis, seeing their insurance premiums rise 87 percent over the last seven years. General Motors now spends more on its employees' health insurance than on its purchase of steel (National Coalition of Healthcare). Clearly, the American health care insurance system is in dire need of a massive overhaul.
In addition, in a recent survey conducted by The Wall Street Journal and NBC, those who voiced pessimism about the future were asked to identify the source of their concern. Next to the Iraq war, the inadequacies of the health care system drew the most votes (Inglehart 2007). This comes as no surprise considering the fact that one in four Americans says that his or her family has had a problem paying for medical care during the past year, an increase in 7 percent over the past nine years (National Coalition of Healthcare). A new high of 30 percent has also said that someone in his or her family has deferred medical care in the past year, even when the medical condition was considerably serious (National Coalition of Healthcare). Corroborating these statistics, national surveys have also repeatedly shown that the main reason of non-insurance has been the high cost of health insurance coverage. In fact, since 2006, annual premiums for family coverage have significantly overtaken the gross income of a full-time, minimum-wage employee who takes home $10,712, illuminating the severity of the problem (National Coalition of Healthcare 2005).
Although Medicaid was established with the mission of providing health insurance coverage to the nation's poor, disabled and the destitute elderly people, it is currently facing a funding crisis that has seen several critical health programs reduced or abolished, and program eligibility adjusted (American Dental Association). Similarly, Medicare, which is a health insurance program for those 65 years and older, has also faced similar problems. Evidently, in light of the variables of health care affordability and health care insurance which polled voters valued most, the administration has failed miserably in allaying the fears of the American public.
Friedman (2000) believes that in health care, there is always the inevitability of politics. She feels that while most other countries have decided on the basic rules of whether health care should consist of universal coverage, be publicly or privately controlled, and the extent of the government's role, America has yet to decide on any of those things. Therefore, partisan politics fills the vacuum and has the potential to be destructive, as witnessed in the partisan wars over Medicare and managed care (Friedman 2000). In her opinion, excessive partisanship would thus result in the two biggest crucial health care issues -- an aging society and the rapidly increasing number of uninsured-remaining unaddressed until they reach crisis proportions. This does not bode well for the American people. However, others have maintained that it is exactly this politicization that encourages healthy debate and thus need not be perceived negatively.
Therefore, what has being embroiled in a larger struggle over ideologies meant for the general state of health care in America? Several trains of thoughts exist. From the above analyses, we can see that politicization of healthcare has its pros and cons, either a beneficial enlargement of the democratic debate or a possible stalemate in policy implementation due to excessive party bickering. The issue of SCHIP must thus be further analyzed to determine its effects on health care. My hypothesis is that SCHIP has become a politicized issue and that this has resulted in negative effects on low-income children.
The research design of content analysis was used in evaluating my hypothesis. A total of 33 newspaper articles from the weeks starting 5 September 2007 to 18 November 2007 (11 weeks) were used to determine if politicization of SCHIP was reported and if it was perceived to have been detrimental for low-income children. I had to decide to stop my collation of articles just before Congress went on its Thanksgiving recess because the SCHIP legislative process is an ongoing one. I also chose such forms of publication for content analysis because I feel that it best represents what permeates the public sphere and influences the perception of the people the most. Specifically, congressional newspapers and publications (Roll Call, The Hill, Politico, Congress Daily, Congressional Quarterly) were chosen for analysis as not only are its reporters most attuned to the workings of Congress, they also provide up-to-the-minute news of the daily legislative and political maneuvers that take place on Capitol Hill. However, editorials were left out as I felt that they represented more personal points of view and lacked the objectivity that I felt was essential in conducting a fair content analysis. Thus, out of all the relevant SCHIP related articles collated, three were randomly chosen from each of the 11 weeks that analysis was being conducted upon. A standardized coding sheet (Appendix 1) was then applied to each of them to measure the presence of politicization in the SCHIP issue, the perspective from both sides of the aisle, and also the reported effects on low-income children. The articles were coded by paragraphs with the options in each question of the coding sheet designed to be mutually exclusive so that reliable data could be obtained. Inter-coder reliability was also established in the coding of these articles.
An example of what might be a disputed coding scenario would be the classification of the term "erosion in Republican territory" in the fight to expand healthcare coverage for children in an article in The Hill on 7 September 2007 titled, "House Dems See Political Win on SCHIP." Ambiguity might result from the decision whether to classify it under politicization in the generic party context (Question 1) or in the context of the political standing of an individual Congressman (Question 2). Thus, in such a situation, I have consistently classified this as politicization in the party context. I have done so is because although the term "erosion in Republican territory" implies that individual Congress members will be adversely affected through the losing of their House seats, the usage of "Republican territory" exudes a more general collective party feel and thus should be coded as the Republican party as a whole bearing the brunt of the negative ramifications.
Several interesting findings have surfaced from the content analysis which provides us with an insight into the evidence of politicization of SCHIP and the possible impacts on children from low-income families.
First, slightly more than half of the 33 articles were found to contain terms that suggest politicization such as "political theatre,"
Sixty-four percent (21 out of 33) of the articles implicitly suggest politicization by stating how the SCHIP process has either inadvertently politically advantaged or disadvantaged the political parties or their respective Congress members (Questions 1 and 2). As seen in the above table, out of the 21 articles that coded positive for these two questions, the option that gathered the most votes at 28 paragraphs mentioned specifically "Republicans voting against SCHIP would lead to disadvantageous consequences." Trailing second with eight paragraph mentions was the option stating "Democrats voting for SCHIP would lead to advantageous consequences."Table 2: Democratic and Republican Perspective of the Optimum SCHIP Bill
The table above also illustrates the different perspectives of what each side has defined as optimum. Republicans were opposed to the SCHIP bill for myriad reasons. Some said the SCHIP expansion would cover illegal immigrants, adults and families earning too high of a percentage above the FPL and was a step towards socialized medicine. Others were opposed to the increase in the federal tobacco tax, claimed that it was too expensive of an expansion, and alleged that it would adversely affect the private insurance market. Thus, although Republicans expressed a general support for the SCHIP bill, they felt that this particular bill was flawed and needed to be renegotiated in a bipartisan manner since it would not provide the best health care for children from low-income families -- the intended beneficiaries of the program. Content analysis showed the top three reasons, ranked in order, against the SCHIP reauthorization bill as being the disputed areas of coverage for children in families earning too high of a percentage over the FPL, coverage for illegal immigrants and lastly coverage of adults. Thirty-six percent of all coded congressional publications mentioned the first two factors, with one in three of the articles that brought up illegal immigrants demanding specifically for tighter provisions verifying citizenship status. Eighteen percent also mentioned providing health insurance coverage for adults as a shortcoming in the SCHIP bill. These were frequently brought up by Congress members on the House floor during scheduled vote debates.
The Democrats, on the other hand, were mostly supportive of the passage of the SCHIP bill. They repeatedly brought up the fact that the SCHIP bill would cover 10 million children, 3.4 million more than the year before, as their main push for the program. Eighteen percent of all coded articles mentioned this factor as the Democratic rationale for supporting SCHIP. The fact that SCHIP would place no strain on the fiscal budget, since it was going to be paid for by an increase in the tobacco tax, was also another factor mentioned by the Democrats in an attempt to garner support for the program.
Lastly, in the five articles that mentioned the specific effects of the SCHIP debate on children from low-income families, two of these articles reported Republican members suggested an eventual negative effect on uninsured children. They felt that if the bill was passed in its current form without the inclusion of Republican demands such as better citizenship documentation and the immediate exclusion of health insurance coverage for adults, low-income children will not be able to reap the full benefits of SCHIP. Another Republican felt that the general politicization of the issue was detrimental to children while two articles reported on how Democratic members felt that the passage of the bill would be beneficial for low-income children. It is also interesting to note that three out of five of these articles that mentioned these effects were published during the period of 4 to 18 October, which was the time period in which the two-week delay of the override vote was scheduled and where politicization was found to be most intense. This correlation may suggest a link between politicization and its effects, whether positive or negative, on the well being of children from low-income families.
The term "politicization" has often been a loaded term with a largely negative connotation, with most assuming that its end result is usually a harmful one. Although my hypothesis assumes such, this paper thus questions if that assumption holds true or if politicization can ever lead to a betterment and improvement of society. Before we further this discussion, it is imperative to be aware that James Madison, Alexander Hamilton and John Jay-the founders of this country who crafted The Federalist Papers-had already envisioned the inevitability of conflicting factions in government. They believed that it was human nature to pursue short-term self-interest often at the cost of long-term benefits and were concerned that factions formed around these areas of immediate self-gratification might eventually demolish the moral foundations of civil government. However, Madison, often referred to as the architect of the Constitution, dismissed the quixotic notion of entirely eliminating factions since it would either destroy liberty or entail everyone having "the same opinions, the same passions, and the same interests." (Anderson 2005) He thus championed an extended republic -- a larger and more diverse society-with "each representative...chosen by a greater number of citizens," believing that while a small republic might be torn apart by factions, the larger number of representatives chosen would "guard against the cabals of a few." (Anderson 2005) He also believed that an extended republic would reduce the likelihood of one faction advancing its agenda to the omission of others. The usage of popular vote would also make it difficult for undeserving candidates to further their personal agendas at the expense of society at large. In addition, the "pluralist" reading of Madison's theory also suggests that the government would be a platform in which myriad interests of the society could be acknowledged, with public policy birthed through conflicts and compromises-the push and pull that often defines legislative politics. It is thus this ideology that frames the current American political system and process (Anderson 2005). Therefore, could politicization-defined as partisan conflict-actually be what the founding fathers of America envisioned for it and a much maligned term?
From the research findings obtained, it can thus be inferred that the politicization of the SCHIP issue did indeed take place. More than one in two of all the coded articles had terms explicitly suggesting that the issue had taken on an overtly political agenda. These were comments either made by the journalists themselves, by Congress members, their spokesperson, or lobbying groups. Rosenbaum (2007) has asserted that this effort at political framing has greatly distorted the matter at hand, leaving the public with a convoluted mass of contradictory information as both sides attempt to present their stance as being most favorable to society.
As a widely received, highly popular program, SCHIP reauthorization should have been a rapid and relatively uncluttered political process. However, the ideological vitriol that has invaded the discussion as seen in the data in the content analysis is also reflected in the politics of real life, corroborating the statistical claims of politicization (Rosenbaum 2007). An example of politicization can be defined as the act of keeping the debate alive in order to score political points rather than trying to resolve it in the fastest manner possible. This is exemplified in the Democrats' exercise of a rare procedural tool to postpone for two weeks the override vote even though an immediate override vote could be scheduled. In this period, the Democratic Congressional Campaign Committee ran 60-second radio spots and "robocalls" targeting Republican House members in vulnerable districts. This inevitably drew sharp criticism from Republicans who accused them of needlessly riding out the political debate. Don Steward, a spokesman for Senate Minority Leader Mitch McConnell (R-KY) put it most aptly when he commented, "The idea that this is about health care is gone...It's about 30-second ads in congressional districts." (Kady 2007) However, Republicans have similarly used the same tactic and have ran radio advertisements against vulnerable Democrats such as Ohio Rep. Zach Space for voting for the House bill (Kady 2007).
The focus on presenting compelling political images or rhetoric, which often skims the surface of the policy by overlooking the substantive issues, is also another form of politicization observed in the SCHIP debate. One Democratic House member has termed the White House as part of the "axis of evil" in attempt to tarnish Bush's public image while Senate Majority Leader Harry Reid (D-Nev) has stood in front of pallets of fake hundred dollar bills to symbolize the millions wasted in Iraq while America's children go without health care (Kady 2007). During a House debate on whether to override Bush's veto, Iowa Rep. Steve King also declared that SCHIP stands for "Socialized Clinton-style Hillarycare for Illegals and their Parents." (Sustar 2007) In their quest for a gripping political storyline, Democrats also recruited 12-year-old Graeme Frost to tout the reauthorization during the two-week postponement. Frost, from the city of Baltimore, Maryland, received SCHIP benefits in 2004 after a massive car accident (Kady 2007). He gave one of the Democratic radio addresses in September, offering a passionate argument to expand the program to more children. However, conservative blogs investigated the Frost family. The family home was watched and the GMC Suburban parked in their driveway was reported upon. In addition, the Majority Accountability Project, financed by a previous top aide at the National Republican Congressional Committee, dug up property records and a New York Times wedding announcement to raise questions regarding the family's net worth (Kady 2007). Democrats, however, responded to these attacks by releasing a point-by-point refutation of the criticisms of the Frost family, showing that the children were on scholarship to private school and the family of six made only $45,000 a year (Kady 2007). Although Republican congressional offices maintain that they were far removed from the investigative reporting work of the bloggers, they failed to distance themselves from the result of that work which accused a valid SCHIP recipient of being ineligible to qualify for a government program (Kady 2007). Thus, from the Republican response it is also evident that the desire to outwit the Democrats has shifted the focus from what should be the key issue of providing healthcare to uninsured children to a partisan based strategizing and manipulation.
Lastly, accusations of politicization, defined by taking an uncompromising and unreasonable partisan stance, made by both Democrats and Republicans, have also been asserted. Democrats claim that the Republicans have voted blindly in lockstep with the Republican President without considering the welfare of the American public (Kady 2007). Republicans have also similarly maintained that Democrats have voted solely along party lines, refused to come to a bipartisan compromise in an attempt to drag out the debate so as to score political points for the 2008 elections, and have thus failed to identify and correct the shortcomings in the bill. This has thus led to SCHIP being unable to obtain a veto-proof passage through the House.
This issue is precisely such a contentious one because ideological differences exist over what is to be defined as optimum for children's healthcare. The top three reasons will thus be more extensively discussed. Republicans say that the bill allows illegal immigrants to benefit from SCHIP because it considerably weakens the requirement of proof of citizenship or nationality. In addition, the inclusion of a state option which permits "express enrollment" for SCIHP benefits without proper documentation of citizenship has been denounced by Republicans who feel that this would be siphoning away resources from the intended beneficiaries, children from low-income families (House Republican Leader). However, Democrats maintain that any change to the citizenship requirement might make it too difficult for inaccessible yet eligible populations to get benefits. They say that the SCHIP reauthorization bill is sufficient as it prohibits payments to illegal immigrants and allows coverage only to citizens and legal immigrants who have been in the U.S. for at least five years. Furthermore, Democrats also oppose the inflexible citizenship requirements which would require even newborns born in the U.S., who are automatically U.S. citizens, to have their citizenship proven before being given SCHIP benefits (Johnson 2007).
Republicans also say that they oppose the fact that taxpayer dollars are being used to fund SCHIP for adults through 2012, when waivers that allow for alternative uses of SCHIP funds end. Instead, they want parents receiving SCHIP to be phased into Medicaid at an accelerated rate. Through waivers, 11 states currently use SCHIP funds to cover parents. Four states cover childless adults, and 11 states use SCHIP funds to cover pregnant women through the prerogative to define a fetus as an unborn child. Democrats are in support of this clause because they believe that allowing states to have the choice of covering low-income pregnant women and providing essential prenatal care would eventually lead to healthier babies, which will reduce the long-term cost of SCHIP (Office of Majority Leader). They also state that the states that have obtained waivers to cover adults have been allowed to do so because they had already done a good job of finding the children that needed coverage. In addition, Democrats are unwilling to attempt a one year transition of parents to Medicaid as it would mean that beneficiaries would have their insurance yanked away from them (Johnson 2007). However, the current compromise bill prevents states from covering any more pregnant adults and requires them to phase out adults who are covered.
The claim that children from middle-income families, earning too high of a percentage above the FPL, are being covered is also a disputed issue. Republicans such as Rep. Thomas Reynold (R-N.Y.) has said that the party had to "stand on conservative principles" and vote against an increased coverage that would allow middle-income families to latch onto a government entitlement meant for the poor (Kady 2007). They favor a hard-cap on income eligibility at or near the 300 percent level (Johnson 2007). Democrats, however, maintain that less than 10 percent of children currently covered by SCHIP live in a family of four earning more than $41,000 annually, and that this will be maintained under the new SCHIP bill (Office of Majority Leader). They also reasoned that different regions in the U.S. have significantly different standards and cost of living and thus there needs to be flexibility in determining the income-cap. A report by the Economic Policy Institute has showed that a family trying to make ends meet in New York in 2004 would need $58,656-three times the federal poverty amount in that year (Vermont Foodbank). However, in latest negotiations, Democrats have agreed to put the income-cap at 300 percent of the FPL.
Republicans are also against a tobacco tax as they say that it would disproportionately burden low-income Americans, be both a regressive and declining source of revenue, and have the negative condition of requiring 22.4 million new smokers by 2017 in order to fund the expansion. However, Democrats cite the Campaign for Tobacco-Free Kids which has found that a 61-cent increase in the tobacco tax would mean that 1,873,000 fewer children will take up smoking (Office of Majority Leader). They are also uncompromising with the psychologically important benchmark of 10 million children being covered under the reauthorized SCHIP.
Findings from the content analysis have shown that three out of five of the articles that mentioned specific effects on children from low-income families have either purported negative effects from the politicization of the issue or advantageous benefits on America's uninsured children if the bill is passed. However, it has to be noted that two articles presented Republican views that it was a positive move to hold off on passing a bill, which they believed was insufficient and inadequate in addressing the health care insurance problem. Thus, with such inconclusive statistical evidence, the crux of the issue, which was first brought up in the introduction, is whether these disagreements have paradoxically brought about more thoroughly analyzed policies and led to pragmatic and optimum compromises or has this political theatre and exaggerated rhetoric only served to impede the improvement of healthcare?
The second revised bill (H.R. 3963), which underwent cosmetic changes, is different from the initial one (H.R. 976) in various ways. First, it permits states to receive federal funding only for children with family incomes up to 300 percent of the FPL. Thus, New York is no longer allowed to extend its program to 400 percent FPL. However, the bill does retain the provision that allows New Jersey, which provides coverage to families up to 350 percent FPL, to continue its program (Senate Republican Policy Committee). It also phases out coverage of adults after a year instead of two and allows states to receive performance bonuses for recruiting the lowest-income uninsured children. In addition, it also clarifies the role of the Social Security Administration (SSA) in verifying citizenship for eligibility purposes. Instead of cross-checking the name and SSN provided for invalidity, applications will now be verified to determine if they are inconsistent with the records maintained by the Commissioner of SSA. This would thus allow the Commissioner to use supplementary information, such as birth place records, to determine the citizenship of the applicant (Senate Republican Policy Committee). However, both bills would provide 10 million uninsured children with healthcare insurance.
Various studies have found that the passage of the bill will be highly beneficial to children from low-income families. The Congressional Budget Office (CBO) has found that the bill will allow 1.3 million children, who would otherwise lose insurance coverage, to retain their SCHIP coverage because adequate funding over baseline levels is provided for states to maintain their current programs. An estimated 78 percent of the children who would have been uninsured in the absence of the SCHIP bill will also have incomes below 200 percent of the FPL (Kenney 2007). Furthermore, an estimated 70 percent of all children who would gain or retain SCHIP coverage, including those who move from private to public coverage, were found to have incomes below 200 percent of the FPL. A substantial number of children targeted under the bill will also have incomes below 100 percent of the FPL while very few will have incomes above 300 percent of the FPL because so few states currently have or are projected to have eligibility thresholds above 300 percent of the FPL (Kenney 2007).
Thus, the politicization of the SCHIP issue might actually have served to benefit their intended beneficiaries. President Bush had initially proposed a $5 billion increase for SCHIP for the next five years, an amount the CBO and the Center on Budget and Policy Priorities both found insufficient to maintain even current enrollment (Kenney 2007). However, because the issue has taken on such a partisan turn with Democratic ideology being fiercely pitted against Republicans', it might have led to the Democrats developing their uncompromising stance of seeing 10 million children obtain health insurance. If the bill eventually passes, their stubborn refusal to budge on this key aspect would thus have advantaged these children. Politicization, which also led to the original bill becoming revised, might have also resulted in positive effects on low-income children. With tighter regulation through the use of refined terminology, it has become more likely that illegal immigrants will not be covered under SCHIP and draw away limited funds. Nonetheless, politicization definitely also has its flip side. It is this same politicization that has caused progress on the SCHIP issue to come to a standstill as both sides endeavor to forward their ideology in an attempt to appeal to voters. Thus, until a true bipartisan compromise is reached, health insurance coverage expansion for eligible children is stalled and millions of low-income children are being negatively affected by remaining uninsured.
Legal immigrants have also borne the brunt of this politicization. Democrats are often viewed as being champions of social equality and more pro-immigration while Republicans are perceived as more nationalistic and anti-immigration. Thus, the fractious politics of immigration has also permeated the SCHIP issue. Immigrants, whether legal or illegal, have now been painted with a broad brush. The SCHIP bill specifies that legal immigrants in the country for less than five years will not be covered under Medicaid and SCHIP even if they meet income eligibility requirements (The Kaiser Commission). Thus, while a number of states with large immigrant populations have provided state-funded coverage, the lack of federal funding makes this coverage vulnerable to cuts during economic recessions. This is an especially salient issue considering that the number of immigrants living in the U.S. has continued to increase. Therefore, children from low-income legal immigrant families have been most adversely affected by the politicization of SCHIP (The Kaiser Commission).
Essentially, if SCHIP is not renewed, 6.6 million children stand to lose their health care insurance coverage instantly. From the perspective of a government's moral obligation to its people, Rosenbaum (2007) states that to reverse the government's role in creating a dependable social safety net for children and to leave financially limited families to fend for their children in the individual market would be a regrettable step in the course of America's social progress. This could lead to enrollment denials for newborns and exclusions for children with physical, mental, and developmental conditions. Thus, in her opinion, exposing children at any income level to the full force of the individual market is an unfair path for any nation to ask its families to take.
In terms of financial implications, Georgetown University's Health Policy Institute has found that the SCHIP bill, if passed, would have provided states with more than $8.9 billion in the 2008 fiscal year, compared to the previous $5 billion, as the federal contribution to defray the program's cost. This, together with the nearly $3.8 billion in unspent SCHIP funding that states can carry over, would have given states over $12.7 billion in federal money this fiscal year for the SCHIP program (Hess 2007). This is in stark contrast to the $1 billion per year increase over five years that the administration originally wanted. The CBO also told lawmakers that President Bush's offer would lead to a loss of 1.4 million eligible children from the program (Hess 2007). In fact, some of the affected parties worry that the deadlock will eventually lead to a simple continuing resolution for a year till just before the 2008 Presidential elections. Inevitably, this would bring about a setback in the number of covered enrollees as compared to what the bill originally proposes (Hess 2007). Joy Wilson, a healthcare specialist with the National Conference of State Legislatures, has said that the biggest and most immediate concern is the uncertainty that states are confronted with as the fate of SCHIP hangs in the balance while Congress battles it out (Hess 2007). If the funds that the continuing resolution allots are much lesser than the SCHIP bill, some states might be forced to drop covered children from their SCHIP roll.
The current continuing resolution in place, which extends the SCHIP program until mid-December, funds these programs at last year's levels. That is at least $1.6 billion short of what is required. A recent report from the Congressional Research Service has also indicated that more than $6.6 billion would be required to ensure no state faces a deficit in fiscal year 2008. Furthermore, if funding remains at current levels, 21 states will also not have the resources to cover their projected SCHIP spending next year, putting the health care of 1.4 million children and pregnant women at risk. In addition, at least nine states could run completely out of funds as soon as March 2008. Therefore, many states have begun taking negative and restrictive measures to prevent that from happening, which has been harmful for low-income children (First Focus). New census data has consistently showed an increase in the numbers and rate of uninsured children, which are often driven by declines in employer sponsored coverage. Thus, in the face of such a trend, SCHIP will not be able to support current program levels or expand to cover the additional six million children that are said to be eligible if enough money were available (The Kaiser Commission). On 12 December 2007, Bush again privately vetoed the second bill with an accompanying statement that said, "This bill does not put poor children first, and it moves our country's health-care system in the wrong direction." (Kady 2007) In response, congressional leaders have said that they would try to extend SCHIP well into 2008 in its current form. The House also voted 211-180 to put off until 23 January 2008 a vote on overriding Bush's veto. Republicans have again cried foul and accused the Democrats of scheduling the override vote strategically to coincide with the week that Bush comes to Congress for the State of the Union address (Kady 2007).
Therefore, much uncertainty plagues this scenario. It depends on how Congress acts over the next few weeks, especially when the continuing resolution expires on 14 December 2007, that will determine if low-income children will see a beneficial expansion of the program or a continuing resolution that will detrimentally limit enrollment. Thus, even though the jury is still out on this issue, the impact so far has been a negative one.
Through the course of my research, several key findings have emerged. First, the reauthorization of SCHIP has been shown through content analysis to have been a politicized issue. However, the term "politicization," although often viewed negatively, has been found to be beneficial for low-income children in certain aspects of the SCHIP reauthorization legislative process. For example, it has allowed for stricter citizenship documentation procedures to be implemented, ensuring that illegal immigrants do not get access to limited SCHIP funds. In addition, politicization has also allowed the Democrats to hold fast to their ideologically significant 10 million children targeted for health insurance. Thus, not only was this idea of "politicization" -- for factions to oppose each other so as to prevent a monopoly of ideas-the intention of America's founders for the democratic political process, it has indeed at times led to a more optimum outcome as seen in SCHIP. Therefore, even though politicization might seem counterproductive, we are able to appreciate the fact that allowing the political system to be inefficient does allow for the checks and balances that are imperative in every well functioning government to exist. Yet, at the same time, politicization has also been shown to have impeded the political process and been detrimental to SCHIP's intended beneficiaries -- children from low-income families. The delay in the passage of the bill has meant that while politicians wrangle over certain minor technicalities, millions of low-income children are going uninsured and lacking vital health care coverage. Therefore, through this realization of the disadvantageous effects that politicization has on SCHIP, we can become more analytical about various policy issues and start pressuring our representatives through our votes and various other feedback mechanisms to come up with effective bipartisan bills that can be most beneficial for society at large. My hypothesis, which states that the politicization of SCHIP has led to solely negative ramifications for low-income children, is thus not supported.
However, there were certain limitations to my project and I faced various difficulties through the course of my research. First, I had a rather small sample size of 33 articles which I had used to analyze the SCHIP legislative process. Thus, this might have been insufficient to draw concrete results from. It is also hard to generalize the findings from the result of my SCHIP content analysis to every legislative issue taking place in Congress. Politicization might exert different effects on different issues. Also, the fact that the SCHIP bill pertained to the well-being of children, who are often perceived as political untouchables because they represent the most vulnerable in society, needs to be taken into consideration. This fact might have made the SCHIP issue particularly highly charged and politicized, especially when framed in the context of the extremely competitive upcoming 2008 Presidential elections. Thus, all these external factors and influences might have culminated and provided the SCHIP bill with a greater incentive to be turned into a politicized issue than usual. Lastly, in the coding of the congressional publications, I found only five articles specifically mentioning the effects of the SCHIP debate on low-income children. Thus, it made it difficult for me, with a small sample size of five, to draw any reliable statistical conclusions. This might have occurred because I had intentionally excluded editorial and opinion pieces in my sample, on the assumption that these were more subjective pieces that might reflect the biases of the author. Thus, in an attempt to be objective and leave out personal predictions, the articles I coded might have refrained from casting personal opinions about the effects politicization might have on low-income children.
Since SCHIP is currently an ongoing issue and I had to stop my analysis midway, it was difficult for me to draw definitive findings about the effects of this politicization. Thus, future research projects can definitely look towards reexamining this issue once it has blown over. Also, I would be interested to know which issues are most often politicized and if my assumptions that they are issues pertaining to political untouchables such as social security, veterans and children are correct. A further area of research could also be to determine if the use of politicization did indeed result in any significant election gains or losses for any of the political parties.

The phrase "Theory of Mind" (ToM) is used in different ways to refer to distinct areas of investigation. There are the general theories that describe how people think (functionalist theories such as the Computational Theory of Mind, and brain-mind identity theories are of this kind). This paper, however, will use the term ToM
The Prisoner's Dilemma (PD) is both one of the most compelling and relevant discoveries of Game Theory4. The PD has been used to model conflicts ranging from nuclear arms races to oligopolistic competition, and a game-theoretic analysis into the PD has even been used to justify preemptive war. The PD that is relevant to our current investigation in ToM, however, is on a smaller, more individualistic scale5. Past experiments that have put two people in a PD (represented by a matrix game) has produced results that differs greatly from what traditional game theory would deem "rational". A significant amount of work into the PD has involved creating new definitions and measures of ratonality, and introducing different interpretations of the conflict. These new constructs have been developed to reconcile the central "dilemma": despite the fact that mutual defection is the most "rational" outcome by traditional measures6, mutual cooperation is preferred from both individuals, and in fact, occurs quite frequently empirically. When Nigel Howard introduced the theory behind Metagames
Howard's metagame approach is general in theory, but we will only consider it here as it is relevant to the PD. Instead of a standard PD, the metagame of the PD involves a Player 1 (P1) having the standard choice between Cooperating (C) or Defecting (D). And a Player 2 (P2), choosing a "meta-strategy", which are strategies that are contingent on P1's choice. Since P1 pickes one of two strategies (C or D) and P2 can respond to each of P1's strategy one of two ways (C or D), P2 has a total of 4 meta-strategies as follows:
Player 2's four strategies may be nicknamed I. defect regardless, II. do the opposite, III. do the same, and IV. cooperate regardless. The metagame PD thus yields the following set of outcomes:
Which in turn yields the following payoffs8:
An analysis of this metagame shows that the only Nash Equilibrium is when P1 chooses D (bottom row), and P2 chooses I (1st column); this results in the mutual defection outcome with the corresponding payoff of (2,2). So this metagame actually does not produce a new equilibrium, and the equilibrium between P1's Defect, and P2's "defect regardless" is maintained.
The metagame approach starts producing unique results once we consider the "meta-meta game". In the meta-meta game, P1 chooses from one of the four meta-strategies described earlier; P2, however, chooses from one of 16 "meta-meta" strategies. A meta-meta strategy is not contingent on P1's decision of C or D, but on one of the four (I, II, III, IV) metastrategies that P1 chooses from. P2's meta-meta strategies thus are as follows9:
The meta-meta game thus yields the following set of outcomes:
This set of outcomes, in turn yields the following payoffs10:
An analysis of this meta-meta game reveals three Nash Equilibriums. When P1 selects "I" and P2 selects "1", the familiar mutual defection occurs (as it did in the meta-game and in the standard PD). When P1 selects "III" and P2 selects "3", however, there is a Nash Equilibrium of mutual cooperation; this new equilibrium also occurs in the intersection of P1's "III" and P2's "6" strategies, respectively.
So what do these new equilibriums mean? And how is Howard's theory of metagames relevant to a Theory of Mind? Firstly, the fact that there is a new equilibrium at mutual cooperation is significant for several reasons. There has always been a lack of theoretically-convincing arguments that advocate cooperation as a "rational" strategy. Howard's theory of metagames not only makes mutual cooperation a stable outcome, it also accounts for elements of psychological game theory that traditional game theory is unable to consider. The fact that the two players involved in this conflict (the PD) are human beings, means that their theories of mind about each other will certainly play a factor in their decision-making. Although the objection that the meta-metagame PD is fundamentally different from the standard PD is valid, this fact does not imply that the findings of the meta-meta game are not relevant. Most important in introducing new frameworks to interpret the prisoner's dilemma is that the original conflict is preserved. Manipulations of the matrix that represents it or the ordinal preferences(payoffs) threaten the integrity of the conflict; Howard's metagames, however, does neither. Two individuals who have ToM's concerning each other could, presumably, interpret the game in such a way that the meta-metagame framework becomes an appropriate model for their respective ToMs for one another. If for example, P1 was employing 1st-order reasoning, he would see four possible strategies to choose from (strategies I through IV). Since P1 is aware that P2 is rational and wants to maximize his own payoffs as well P1 thus might reason "since we both want to maximize our payoffs, I should choose a strategy that would retain the possibility that of the outcome that maximizes our combined payoffs: mutual cooperation"; thus P1 would eliminate strategies I and II. P1 could then reason "I don't, however, want to leave myself open to the possiblity of being exploited. And since III can always ensure a payoff that is at least as good as what IV can ensure, I will select III11 "
Since P2 believes that P1 is employing 1st-order reasoning; P2 could thus conclude that P1 is deciding between the strategies I, II, III, or IV. The strategies 1-16 are thus appropriate representations of P2's choices. P2's thought process might be as follows "If P1 chooses I (defect regardless), I certainly will not employ a strategy where I will cooperate and be exploited", therefore P2 would eliminates strategies (5, 9, 10, 11, 13, 14, 15, 16). P2 could continue and reason "If P1 chooses II (do opposite), I again won't select a strategy where I would end up cooperating, since I would then receive the worst payoff", and so P2 then eliminates, of the strategies remaining (4, 7, 8, 12). P2 then would reason "If P1 chooses III (do same), then I would want to cooperate, since I am really choosing between either CC or DD, and mutual cooperation is better than mutual defection" and so he eliminates (1, 2). Note that P2's two remaining strategies (3 and 6) are the two (and only two) strategies that contain the mutual cooperation outcome as a Nash Equilibrium. Either one of those two choices would thus yield the mutual cooperation outcome (since P1 will, rationally, play III), but it is worth noting that P2 could further reason "If P1 chooses IV (always C), then I would prefer a strategy that defects so as to exploit him and receive the best payoff"; and thus eliminate strategy 6.12
The lines of reasoning I have presented are certainly not the only lines that can be deemed "rational". As Nigel notes, the concept of rationality "breaks down" under different conditions, and so the same definition of rationality can, employed with different logic, yield different results (in fact, this is the very nature of the paradox in the PD). The reason I have employed that specific logic, however, is to demonstrate how the Nash Equilibriums found in the meta-meta game are not only theoretically valid, but intuitively valid as well. The logical progressions of reasoning by a 1st-order and 2nd-order player not only mirrors the elimination of "unintuitive" strategies in the meta-meta game, it also concludes that mutual cooperation is a rational outcome both intuitively, and when the meta-meta game is considered, theoretically.
Using the fundamentals of the meta and meta-meta game for the 2-person PD, we can now extrapolate, and consider a 3-person meta game. A 3-person PD is defined as having the following ordinal payoffs:
Where the payoffs are ordered according to the player whose decision is the 1st of the 3 letters. For example, in outcome 1, the 1st player receives the best payoff, since both the other players cooperated and he defected. The 2nd player (corresponding to the 2nd letter), meanwhile, receives the 5th payoff (CDC) because from his perspective, he cooperated while one other player cooperated and one other player defected. The 3rd player's payoff is similarly defined.
For a 3-person meta game, let the non-meta player (0th-order) be P1, with m1 strategies (in general, m1 = 2, because that is the standard PD). The meta player (P2) then has 2^(2^( p1p3)) strategies (m2), with each strategy having 2^( p1p3) elements (where p1=p3=2, the number of "distinct resolutions", and an "element" is a single "If-then" statement). A "distinct resolution" is defined as an output (for all players, either C or D). Notice that m2 depends on p1 and p3; m2 cannot possibly depend on m1 and m3 (the number of strategies P3 has) because m3 depends on the number of strategies P2 has (m2)13. The distinction between the input pi (a strategy) and the output mi (a distinct resolution) is important not only on theoretical grounds, but on its implications for ToM.
In this 3-person meta-meta PD, the players 1 and 2 have the following strategies:
Where 1C3C → C is a single "element" that reads "If Player 1 cooperates and Player 3 cooperates, then cooperate" and each element is separated by a comma. Roman Numerals I-XVI denote P2's 16 strategies, and we will simply use "C" and "D" to denote both the strategy and distinct resolution of P1. A single strategy from P3, since it is contingent on both the strategies of P1 and P2, is as follows
Since each strategy from P3 has 32 elements, P3 has 2^(32) strategies, far more than that which can be listed here. We do not, however, need to consider all the strategies from each player in order to analyze this game.
Let us consider first how we might arrive at a Nash Equilibrium to this game without exhaustively mapping it out. If one were to take the point of view of each player, one can then "build" an optimal strategy by considering each possibility (each "If", what the other two players might do) and then deciding whether a D or C would yield a higher payoff. In doing so, it quickly becomes clear that the optimal strategy is one that corresponds to the nickname "cooperate if, in doing so, the other players will also both cooperate. Defect otherwise". This nickname makes evident why Nash Equilibriums of mutual cooperation14 exists in n-person (n-1)-meta games. By making one's strategy choice not independent of the relevant element in another person's strategy choice, meta-games possess a facet of "causality". P3, being rational, will thus select a strategy such that if P1 cooperates and P2 cooperates, he too will cooperate (if defecting meant another player would defect as well). By choosing a strategy that defects in every other situation, P3 is making sure that the other players won't have an incentive to defect; there is thus a Nash Equilibrium at the intersection of the dominant strategies that we "build"
Likewise, other (and all beyond DDD) Nash Equilibriums exist where all three players can "coordinate" a C, and where if any one person moves unilaterally away from that equilibrium to a strategy that yields a "D", the other meta-player has a strategy that would produce a "D" as well.
When trying to use our meta-game analysis to model ToMs, we do encounter a few problems. Firstly, P1 has no basis whatsoever to make his decision. By definition, a myopic player is one that is unable to consider how another player's thoughts and beliefs plays a role in his own payoff. A myopic player can therefore only consider his own desires; in order for P1 to make a choice, he must have some kind of payoff immediately available (but since the payoff is contingent on what P2 and P3 decide, which in turn is contingent on what P1 decides, P1 has no basis for making this decision without developing some kind of ToM about the other two players). Another limitation of this model is that it requires that the three players (1, 2, 3) to be 0th-order, 1st-order, and 2nd-order thinkers, respectively. This is a very specific situation, and so this model is not applicable to all n-person Prisoner's Dilemmas (or even all 3-person PDs). A final, important point to consider is that, in applying this model, we have made the assumption that a particuar player's ToM is indicative of the actual order of reasoning another player is employing. The number of strategies the meta-meta player has in this game is not derived objectively from a measure of the number of strategies the normal and meta player are actually considering. Even if a 2nd-order player is actually employing a ToM modeled by a meta-meta player, then, for our theoretical results to be valid, his subjective belief of the strategies the other players are considering must be the objective reality. Our analysis, interpretation, and application of Meta-games to ToM will be therefore be invalid if the ToMs are not themselves indicative of what other players are actually considering.
Future studies into ToM using Metagame Theory should expand beyond the Prisoner's Dilemma. Since the fundamentals of a metagame are, intuitively, very similar to the recursions of logic employed in higher order reasoning, the applicability of metagame theoretical results to ToMs during other matrix games should be investigated. One of the primary drawbacks of Metagame theory is that it is unable to account for the situation where two or more players are applying the same meta-level. If two players in a prisoner's dilemma are both 2nd-order thinkers, metagame theory currently has not prediction for what kind of reasoning would take place. Until these limitations in metagame theory are addressed, it is unlikely that the theory of Metagames will be used to model more situations where ToMs are relevant.

We've all heard of subliminal messages and their power to make you crave certain foods and have certain preferences. However, whether or not subliminal messaging is actually effective is still controversial. The research is mixed on if and when subliminal messaging works. It is important to understand if this phenomenon is really effective and if so at when and in what specific situations it works. There are many different possible uses for this information. First, and most obvious it can be used to form marketing strategeies by businesses and more importantly then used by consumers to understand what causes them to like or dislike certain products. It is also important for motivational and self esteem techniques. It may be possible to alter peoples self esteem at least temporarily or make them more motivated to achieve a certain goal through the use of subliminal messaging.
One study has looked at subliminal messaging in television commercials and its impact on preferences for certain products. Participants were shown commercials with subliminal messages and with no messages. This study showed a small but significant increase in subjects' intention to use the product with subliminal messaging compared to no messaging. (Smith, 1994) This shows that even though small, there is an effect of subliminal messaging on preferences.
Another study examined the effects of subliminal messaging on reducing anxiety. It tested participants of different levels of self-identity and the ability of subliminal messages to reduce anxiety in these different groups. It found that all groups had reduced anxiety with the presentation of a certain symbiotic subliminal messages that said "Mommy and I are one." (Orbach, 1994) This study shows that it is possible to change not only one's preferences but one's mental state as well with the use of subliminal messaging.
A third study we assessed looked at the ablity to detect subliminal messages. When one does subliminal messaging studies one tries to present the stimuli just short of them noticing it. That is you want it as close to the subjects threshold for recognition as possible. However, if the stimuli is presented too short this may decrease the power or effect of the stimuli. This study examined the possibility that participants threshold actually varies and is not stable. Therefore past experiments may have had reduced effect and not realized a subliminal effect because the stimuli was not presented close enough to the threshold. (Miller, 1991) This may be important for many studies that have had only a small or no effect for subliminal messages such as the study by Smith et. al presented above.
In this study the effects of subliminal messaging on self esteem and color preference were measured. We predict that presenting subliminal messages to enhance a color preference and others to enhance self esteem will increase the preference for the color and increase subjective ratings of self esteem. However, we hypothesize that there will be a larger effect with respect to color preference enhancement than self esteem.
Fourteen subjects participated in this experiment. The participants were all undergraduate students. They were not compensated for their participation but received credit for a class for partaking in the experiment.
A pre-survey was given to participants which contained questions from the Rosenberg Self Esteem Scale as well as irrelevant deversion questions about humor. A post-survey was also given which contained comparable questions. The video used was a five minute and twenty second clip from a Seinfeld episode.
The computer used was a Dell Opiplex GX620. A Sharp Notevision was used to project the video onto a projection screen. The video was edited to add subliminal messages by a student majoring in film.
The experiment was between subjects. Half the participants were in the subliminal condition and half were in the non-subliminal or control condition. The two conditions were exactly the same except for the presentation of the subliminal messages within the video for the subliminal group.
Particpants were first asked to take a piece of paper from a bag. The paper contained the number one or two which assigned them to one of the two groups. All surveys were placed at a seat in the room. Then participants entered the room and were asked to take a seat at one of the seats with a survey. One of the experimenters then went around the room with a bundle of pencils and participants were asked to take one to fill out the survey. Participants were told to turn over the survey when finished. Upon completion, surveys and pencils were collected by an experimenter. A short video clip was then shown. For the subliminal group a subliminal message was inserted into the video which flashed the phrase "I am wonderful" five times at intervals throughout the video. A red background was also used for the computer screen during the playing of the video. For the nonsubliminal group no message was flashed during the video and a white background was used during the playing of the video. At the conclusion of the video clip a second post-survey was passed out to participants. A bundle of pens was also handed out and participants were asked to chose a pen. There were 45 pens total, with fifteen blue, black, and red pens each. After completing the survey, the pens and surveys were collected and the participants could leave.
We predicted that the preference for red would be higher in the subliminal group versus the nonsubliminal group. This hypothesis was not supported. Participants in the subliminal group did choose a higher frequency of red pens than participants in the control group. However, these results were nonsignificant with an ANOVA test showing F(1,12) = .000, p = 1.000.
We also predicted that their would be a larger increase in self-esteem for the subliminal group than the control group. There was a difference in groups with the subliminal group having a larger increase in subjective ratings of self-esteem. (See Table 1) However an ANOVA test showed that this difference was nonsignificant, F(1, 12) = .466, p = .508.
Although are results were in the right direction and did show differences between the two groups, none of them were significant. There are many possiblities for why this was the case.
One of the major problems was sample size. We had a total of only fourteen participants, leaving seven in each condition. This could have caused a type II error. There might have been too small of a sample size to produce a significant effect even though the effect was there.
Another problem was the ceiling effect we had with self esteem ratings. Most all the participants started out a such high self esteem there was not much room for improvement. It may have been a problem with the sample population. The students at the university might have simply a higher self esteem than the general population. To correct this we might have used a different sample population that did not consist of only college students. Another solution may be to lower their self esteem in the beginning and then from there try to raise it with the subliminal message.
There were also possible reasons for the color preference portion not showing significant results. There were many factors in the room, noticed later that could have contributed to their color preference as much as the background of the computer screen. While we all tried to wear colors other than black, red, or blue to influence them the room had many of these colors in it. For example on the computer screen itself the border around the video clip was blue. The participants also had black computer screens right in front of them for the duration of the experiment. To resolve this issue participants should be placed in a plainer room with mostly white so that nothing else in the room may influence their color choice. It may have also been that participants started out with a predisposition to like blue or black pens more than red pens. And this may have had an effect on our results as well.
Another limitation we had in our study was the limited access we had to professional editing equipment. The subliminal stimuli may have been to small or to presented for too short a time to have a significant effect. Future studies may want to use more professional equipment to edit the video so that the stimuli can be presented longer and larger while still not being consciously visible by participants.

Through professional work with children of different ages in daycare, babysitting and summer camp settings, it appears that male children are more likely to engage in physically interactive group play. At first glance, it appears that male children are more physical in their types of play, tending to push, shove, touch each other or invade one another's personal space. Psychologists have paid close attention to the different play styles of male and female children in recent years. Scott and Panksepp (2003) looked specifically at the occurrence of rough and tumble play among 3 to 6 year olds. Their findings show a higher frequency of physically interactive play among their male subjects in free play environments, but did not find any significant gender differences in controlled environments. Manwell and Mengert (1934) found a higher frequency of group play among 3 year olds than with 2 year olds, and found boys more likely to engage in physical play than girls. Colwell and Lindsey (2005) analyzed differences in child play behavior with regard to the gender of play partners. Their findings also confirmed the higher incidence of physical play among male children, and showed that boys who engaged in physical play with same-sex children were more liked by peers than boys who engaged in physical play with opposite-sex children. These research findings all suggest that male children are more likely to physically interact in a group play setting than females, and that they are more likely to interact with other boys in this style of play. These studies are significant because they show a higher tendency of males to exhibit group play behavior and physically interactive play, such as rough and tumble play, and both of these factors are considered with the present study's focus on physically interactive group play.
In the present work we evaluated physically interactive group play at a University-based Children's Center in the Midwestern United States. Our research focused on observing the presence of physical group play in male and female subjects. We proposed that male children would show a higher frequency of engaging in physically interactive group play than female children.
The sample consisted of 20 children, aged 3-6 years old. There were 14 male and 6 female students. Five males were in the 5-6 year old classroom, 9 males were in the 3-4 year old classrooms and all 6 females were in the 3-4 year old classrooms. Each researcher randomly selected five children to observe upon arriving at his/her assigned classroom for observation at the center. The research team was unable to obtain background information on the children regarding racial identity, socioeconomic background, or personal or family history. The children belonged to three different classrooms at the Children's Center, one classroom for 5-6 year olds, and two classrooms for 3-4 year olds. The children did not receive any sort of compensation for their participation because they were not directly informed of the observation. The Children's Center informs the parents of the research and observations performed at the Center and the parents sign consent forms allowing for the observation of their children. By means of the consent forms that are kept on file at the center, the researchers received indirect parental consent to observe the children. The study did not present the children with any potential harm or risk. The center follows a philosophy that helps them care for the developmental needs of each child, while creating a fun and nurturing play environment for all. In this experiment, the researchers had no control over the classroom environments, but the different classrooms appeared to have similar quantities and styles of toys and play areas for the children to interact, learn and have fun.
Three investigators observed the children in two classrooms through one-way mirrors in an observation room. The investigators had to walk through the classroom to enter the observation room, allowing for a possibility of being seen by the subjects in one of the classrooms. The subjects did not seem to take any interest more than a quick glance. The researcher in the fourth classroom entered the classroom and sat quietly on a chair near the doorway, in full view of the children. Three of the investigators were non-participant concealed observers, and the fourth investigator was a non-participant non-concealed observer. The children were all observed during an assigned period of "free play" in the classroom. The four investigators each observed five children at one time, for a period of thirty minutes. Each investigator had a watch with a second hand to provide an accurate measure of time for the procedure. The children were recorded as showing a presence or absence of physically interactive group play every 30 seconds for 30 minutes, providing 60 items of data per child.
We operationalized physical play as any sort of physical interaction between the subject and any of the other children in the room, with physical interaction including hugging, pushing, touching the other child while working together and touching the same toy at the same time. For example, two male subjects were coded as displaying physically interactive group play when they were standing together next to a large tray of putty and intertwined their arms as they laughed and pressed their hands into the putty. We did not code for physical interaction between the subject and the adults teachers in the room. The researchers recorded these findings using a number one to represent presence and a number zero to represent absence on a table with 60 increments for observations every 30 seconds over a 30-minute period. Each researcher observed five children over a 30-minute period.
The inter-rater reliability was established between the four observers immediately before the observations began. The researchers all observed one randomly selected child, all individually coding for the presence or absence of physically interactive group play every 30 seconds for five minutes. Inter-rater reliability was established with a 90% rate of reliability. Having demonstrated an overall understanding of the team's operational definition of physical play, the researchers then went on to make their individual observations of different children.
Table 1 presents the descriptive statistics used to analyze the data and Table 2 presents the raw scores of the observations of the children in physically interactive group play. Our hypothesis predicted higher frequencies of group play for the male subjects than for the female subjects. Table 2 shows the range of the scores, with some subjects showing no instances of physically interactive group play (male subjects 1, 2, 3 and 7) while some male and female subjects exhibited much higher frequencies (male subject 14, female subjects 18, 19, 29). The ranges for both gender groups are similar, 38 for the male subjects (0-38), and 36 for the females (10-46). Table 1 displays the variance for each gender group, 113.67 for males with a standard deviation of 10.66, and variance of 191.07 and standard deviation of 13.82 for females. A small variance indicates that the scores all fall close to the mean, and the variances of 113.67 and 191.07 are not very small numbers, thus they do not suggest that the scores all fall close to the mean. The standard deviation scores, 10.66 for males and 13.82 for females suggest that the scores fall in a wide range around the median.
Table 1 presents the frequency and central tendency for each gender's data. For males, the overall frequency was 156 instances of physically interactive group play out of 840 observations, 18.57%, and females were recorded with 158 instances out of a possible 360 observations, 43.89%. Our hypothesis predicted a higher frequency of group play among male subjects than females, but the raw scores show a higher frequency of group play among females than males. The male mean was 11.14, mode of 0 and median of 10. The male subjects had a mode of 0 because 4 of the 14 subjects were not recorded as displaying any instances of physically interactive group play. The female mean was 26.3, mode was not determined and median of 28. The data does not display a mode for the females owing to the small number of subjects (6) and that none of the scores repeated. Looking at Figure 1, it is apparent that the mean observed interactions for the female subjects is more than twice the mean observed interactions for the male subjects. These results show that the female subjects showed a higher tendency to display instances of physically interactive group play, as noted by the greater mean and median than those of the male subjects. These results fail to support our research hypothesis that males would show a higher tendency of physical group play, and as analyzed by the different forms of descriptive statistics, the results actually appear to refute our hypothesis.
The purpose of this study was to examine the differences in frequencies of physically interactive group play of 3-6 year old male and female children enrolled at a research-oriented preschool. We proposed that through observation of these children during free play in their classrooms, we would observe a higher frequency of physically interactive group play among the male subjects that the females. The findings of this study clearly did not support our hypothesis, showing that female children showed higher average rates of physical play than males. These findings do not necessarily discredit the findings of the previous research studies used as background research, due to the many limitations of this study.
Our results showed a higher frequency of physically interactive group play among females, contrary to the findings of Scott and Panksepp (2003) which showed a higher frequency of physical play among males. Our research assessed the frequencies of physical group play though, while Scott and Panksepp (2003) looked only at the occurrence of rough and tumble play, which may attribute to the differences in our findings. While our research did not record for rough and tumble play specifically, it was included in the coding of physical play, allowing for the possibility of rough and tumble play to be coded along with pro-social and gentle physical group play. Our results do not support the findings from the research by Manwell and Mengert (1934) that boys are more likely to exhibit group play behaviors. Colwell and Lindsey (2005) showed higher instances of group play among male children, which is not supported by the findings of our research, but this study also focused on the gender of play partners, a variable that was not coded for in this observation study.
The external validity of this study was limited by the small sample size (n = 20) and the lack of random assignment of the subjects. The subjects in the sample all attended the same preschool program with a play-based and research oriented philosophy in a middle-upper class university town in the Midwestern United States. Future studies should try to incorporate larger sample sizes from different populations to improve upon the external validity of the study.
It was not known how long the children had been attending the preschool, how long they had been in the classroom with their peers. It is entirely possible that some of the subjects had been in the same classrooms together for different periods of time, allowing them to become more familiar with each other and less inhibited to engage in physically active group play with one another. While the results of the present study do not support the research hypothesis, they provide the background for further research into the different factors that may affect levels of familiarity that allow children to comfortably engage in physically interactive group play. Future studies should identify the levels of familiarity between the different subjects, by identifying how long they have known each other and if they have any social interactions outside of the classroom. Other issues to consider are cultural background, an analysis of family behaviors; whether they encourage or discourage physical play at home, and a look at different gender interactions. These studies could identify different factors that affect children's comfort and likelihood of engaging with peers in a physically interactive style.
As a classroom-based observation, the researchers were unable to manipulate the setting, such as the number or gender of the children, the objects found in the classroom or the way the teachers interacted with the children. The different classrooms and different teachers are confounding variables that may have contributed to the children's different behaviors. Perhaps one classroom has more group-play style toys, encouraging the children to work together more and therefore providing the opportunity for more physical play, where another classroom has more books and art supplies, providing opportunities for solitary play. The observations took place during a brief time slot assigned by the preschool, during the free play period at the beginning of the day while the children were arriving. During the observation, different children were arriving to the classroom, and many children appeared to be talking animatedly to the teacher, discussing what had happened since their last class together. Because of the early hour of the observation, the children may not have been behaving in the same way that they would at a later hour, or in a free play session in a different setting outside of the classroom, such as the playround. This sample consisted of children aged 3-6 who come from different family backgrounds. Perhaps the children experience sadness or mental anguish at their parents departure and are less likely to engage in intimate, physical play with peers in the very beginning of the day, but they may become more comfortable as the day goes on and show higher levels of physical play. Future studies should incorporate time of day and location into the research design to try to explain these confounding variables.
While the findings of this study failed to support the research hypothesis, they still pave the way for future research ideas. Although there exists a plethora of research findings that show male children are more inclined to physically interactive group play, the present study found females were actually more likely to engage in physical play. Future research should address the confounding subject variables, such as the subject's family background and culture, length of time children have known each other and time of day. In addition, the research design should accommodate for different situational variables, identifying what style of toys are found in the environment, using both indoor and outdoor environments, and noting the presence or absence of adult figures.

Creativity has been described as "one of the most enigmatic subjects in cognitive psychology" (Kim, Cramond, & Bandalos, 2006). It is difficult to define and tricky to measure, but seemingly easy to recognize (Amabile, 1982). It may also have a host of benefits for those who posses it; previous research has suggested that creative interventions can have a significant impact in many different areas, from increasing life satisfaction and sense of purpose in the elderly, to supporting teacher effectiveness (Davidovitch & Milgram, 2006; Krawczynsi & Olszewski, 2000; Cook, 1998; Dawson & Baller, 1972).
Though many today would agree that creativity is significant and that it should be nurtured, not many would be able to agree on exactly what creativity is or how to go about supporting its development. The nature of creativity is highly subjective, and there is little agreement among researchers as to a specific definition (Johnson & Carruthers, 2006). Some define creativity simply as "the ability to produce work which is both novel and appropriate" (Lubart, 1994). Others focus on the making of connections between previously unrelated concepts (Mednick, 1964). Still others focus on the evaluation of creative products, defining a creative product as something which is judged to be "creative" by people who are knowledgeable in the field (Amabile, 1979; Amabile, 1982a; Amabile, 1982b; Amabile, 1983; Amabile & Gitomer, 1984)
Assessment is therefore difficult; different tasks are developed according to different conceptions of creativity, and research projects will typically use only one type of measure (Johnson & Carruthers, 2006; Kim, Cramond, & Bandalos, 2006). Because these measures test different abilities, it is difficult to make connections between separate studies in order to draw conclusions about what types of creative processes might relate to realistic creative output and lifetime achievement.
Some of the oldest conceptions of creativity are based on the cognitive processes thought to be necessary for creativity to be present. Within this process based approach, two types of thinking are stressed: convergent thinking, which is the ability to combine different pieces of information based on the elements they have in common; and divergent thinking, which is the ability to create many different ideas based on a single topic.
Mednick, one of the earliest proponents of convergent thinking as an indicator of creativity, defined creative thinking as: "forming of associative elements into new combinations which either meet specified requirements or are in some way useful" (Mednick, 1964). He therefore based many of his assessments on verbal association tasks, such as the Remote Associates Task in which participants are provided with a grouping of three seemingly unrelated words and asked to think of a fourth word which links the previous three; and a response task, in which participants are provided with a single word and asked to think of as many words that are associated with the original word as possible (Houston & Mednick, 1963; Mednick, Mednick, & Jung 1964).
This type of thinking indicates an ability to focus on correct responses while disregarding irrelevant responses in order to come to a correct answer (White & Shah, 2006). This focus and concentration on important aspects of a problem may be important in order to produce a creative piece of work (Sosik, Kahai, & Avolio, 1999). However, the same focus that allows a person to disregard irrelevant information may also prevent them from considering "off the wall" solutions that are often thought of as being creative (White & Shah, 2006).
Divergent thinking, on the other hand, encourages the individual to think of as many ideas as possible -- even if they seem odd or unworkable at first glance. Guilford (1950) suggests that an important factor in creativity is fluency, which is the quantity of creative production. Although rapid and prolific production alone will not guarantee a creative final product, if two people are producing the same quality of work, and one of those people is able to produce more, that person will be more likely to come up with a significantly creative idea (Guilford, 1950). Testing of divergent thinking often involves asking the participant to engage in brainstorming activities, such as responding to the prompt: "What are some possible uses for a paperclip" (Plucker & Runco, 1998). While at first responses will usually be somewhat mundane ("hold paper together"), after these typical responses are exhausted something original might come to mind ("arrange clips to spell 'tip' and leave at a restaurant when the service has been poor").
While Guilford argues that more production will simply lead to a greater likelihood of a creative response, one danger of the divergent thinking conception of creativity is that often, quality is ignored in favor of quantity (Cropley, 2006; Guilford, 1950). While it is possible for a person to happen upon a creative solution through divergent thinking alone, it is more likely that some convergent thinking will be necessary in order to produce an idea that is accurate and meaningful (Cropley, 2006).
Assessment using both of the previous models of creativity is based on the process that results in a creative product. However, neither model focuses much on any realistic examples of creativity. While understanding the cognitive processes that drive creativity can aid researchers who are searching for ways to mold creative young minds, often these process assessments of creativity suffer from lack of a direct link to a creative product (Amabile, 1982b).
Self-report of previous creative achievement is one way of assessing creativity through actual accomplishments. Carson, Higgins, & Peterson (2003) suggests that because certain processes associated with creativity -- such as divergent thinking -- are also present at a higher than average level in some disordered populations who do not tend to engage in creative production, these processes may not be enough by themselves to identify creative individuals. Therefore, in order to determine which people are truly creative achievers they suggest gathering information about their lifetime accomplishments.
Carson, Peterson, & Higgins (2005) conducted several experiments designed to test their Creativity Achievement Questionnaire (CAQ). This questionnaire was developed in order to assess creativity on the basis of past creative accomplishments (Carson, Peterson, & Higgins, 2005). They were able to show that the CAQ was a good predictor of creative ability and future creative behavior, and that the CAQ correlated with personality traits associated with highly creative individuals, and with divergent thinking abilities.
While this type of questionnaire is valuable for its ability to identify creative individuals, it is difficult to use the results in order to determine what creates, drives, and nurtures creativity. Scoring is objective and leaves little room for interpretation. Amabile (1982b) suggests that a definition of creativity that focuses on environment and motivation is necessary in order to avoid the "translation problem" that many other assessment techniques face. Amabile does not attempt to quantify creativity using mathematical formulas and detailed coding, rather she admits that her scoring techniques are subjective, but asserts that they are still reliable.
In Amabile (1979) 95 undergraduates at Stanford University were asked to create a collage with the theme of "silliness" for an experiment on different moods. As a part of 8 separate experimental conditions, subjects were told that their work would or would not be evaluated, and then were either given no specific instruction on how to complete the collage, instructions relating to technical goodness, or instructions relating to creativity. In the "instructions" conditions, subjects were further divided into groups that either received very specific instructions or broad general instructions.
15 judges with prior art experience were asked to rate the collages on 16 different artistic categories, one of which was "creativity", which they received brief descriptions of before judging. The judges did not work together and were not given any special training before rating the collages. Reliability for the dimension of creativity was .79, demonstrating that it is possible for judges to recognize and rate creativity using subjective criteria for realistic products.
This method of assessing realistic creativity is very useful in studies of motivation and environment, but is weak in the area of defining personal characteristics that a creative person might possess. Like the other methods of assessing creativity, it is limited in scope and seems to be missing a part of the equation.
Though many methods of measures of creativity have been identified, because of the conceptual differences noted above, it is unclear how these measures may be related to each other. Do laboratory tasks such as the Remote Associates and the Unusual Uses predict real life creativity? How do laboratory tasks that attempt to mimic real life creative production relate to traditional measures of creative processes?
Previous theories have suggested that both convergent and divergent thinking may be important for creative production (Cropley, 2006; Finke, 1996). In Finke's model of "creative realism," the creative process begins with an "exploratory phase" in which divergent thinking plays a primary role, and then moves into a "generative phase" that involves a greater deal of convergent thinking (Finke, 1996). This type of model would result in creative inventions that were both spontaneous and practical (Finke, 1996).
In the present study we seek to combine several of the traditional methods of assessment in order to create a more complete picture of the factors of each that contribute to creativity. We tested a large group of undergraduates using several convergent thinking, divergent thinking, realistic product, and self-report measures of creativity. We predicted that high scores on the measures of divergent thinking would have a positive relationship with higher scores on realistic creative products and on the self-report measure of creative accomplishment. We further hypothesized that high scores on measures of convergent thinking would also be related to high scores on realistic creative products and the self-report measure of creative accomplishment.
55 college students from the Introductory Psychology Subject Pool of a large, Midwestern university were included in this study. One student did not complete the study; therefore the final sample size was 54. Of these, there were a total of 31 males and 23 females. Participants were between the ages of 18 and 22 (mean = 19.17) and were from ethnicities which represented the demographics of the university.
Students participated in the study in groups ranging from a single student to 30 students at a time. All measures were included in a booklet provided at the beginning of the session, and were completed with a pen or pencil. The tasks were administered and timed by the principal investigator, and the entire session lasted approximately 1 hour. Before beginning, students were asked to read and sign a consent form detailing their rights as subjects, and were given background information about the study by the principal investigator.
After signing the consent form, participants completed eight measures: the Hasher Inhibition Task, a Haiku Task, the Remote Associates Task, the Unusual Uses Task, a Children's Adapted Remote Associates Task, a Circles Task, the Creativity Achievement Questionnaire, and a General Information Questionnaire. Following completion of the General Information Questionnaire, students were given a debriefing form and thanked for their contribution to the study. Participants received 1 credit of research for their Introductory Psychology class.
The first task administered to the participants was adapted from a reading task developed to test ability to ignore distracting information (Connelly, S.L., Hasher, L., & Zacks, R.T., 1991). Participants were asked to read 8 short stories while ignoring irrelevant stimuli present within the stories. In 4 of the stories the irrelevant stimuli consisted of italicized words that did not relate to the story, while in the other 4 control stories, the irrelevant stimuli was a row of italicized x's.
The stories were presented to each participant, individually, on a sheet of unlined paper. Participants were able to read silently, at their own pace. After every person in the group finished reading, 4 comprehension questions were asked in multiple choice format. The questions for the irrelevant word stories each had two unrelated answers, a foil answer relating to the irrelevant words which were supposed to be ignored and the correct answer. The questions for the irrelevant x's stories each had three unrelated answers and the correct answer. Participants were scored based on the number of correct answers in the extra word stories versus the number of correct answers in the control stories. A higher score indicated a higher level of inhibition.
A haiku is a structured poem that is written in 3 lines. The first line contains 5 syllables, the second line contains 7 syllables, and the third line contains 5 syllables. This type of task is thought to represent realistic creativity (Amabile, 1983). Participants were given 3 minutes to create an original haiku about "summer". The creativity of the poems was scored from 0-2 in the categories of: Metaphor Imagery, Form, Movement, and Rhythm, for a possible score between 0 and 10. For example, in the category of "metaphor", a poem was given a score of "0" if no metaphors were present, "1" if it contained a simple metaphor (no depth) or a simile, and a "2" if it contained a profound metaphor with depth and quality which developed through the poem. A higher score in each category represents a greater mastery of that category. A higher total score indicates a higher level of creativity.
The Remote Associates task (RAT) is a measure of convergent thinking abilities (Mednick, 1962). Participants were presented with 46 word trios and were given 7 minutes to complete the trios with a fourth word that could be related to the previous three. An example of this is the trio "call, pay, line" in which all of the words can be related to the fourth word "phone". It is thought that the ability to mentally screen irrelevant answers in order to produce a single correct answer is a necessary skill for the RAT, and that this skill reflects convergent thinking abilities. A greater number of correct responses was associated with a higher level of convergent thinking.
The Children's Remote Associates Task (CRAT) is a measure of convergent thinking abilities which has been adapted from the original Remote Associates task (Mednick, 1962) and the Mattel children's game Tribond Jr. for children at an upper elementary school level. Participants were presented with 57 word trios and given 7 minutes to complete the trios with a fourth word that could be related to the previous three. An example of this is the trio "band, first, lemon" in which all of the words can be related to the fourth word "aid". It is thought that the ability to complete this task is related to the ability to complete the original Remote Associates task. A greater number of correct responses was associated with a higher level of convergent thinking.
The Unusual Uses task is a measure of divergent thinking abilities (Guilford, 1950). Participants were given 2 minutes each to think of as many uses as possible for a brick, and another 2 minutes to think of possible uses for a bucket. Answers were scored on the criteria of Fluency (number of unique answers), Flexibility (number of unique categories and number of categorical shifts between answers), and Originality (uniqueness of the response in relation to the sample of responses). Higher scores in each of the three scoring areas are associated with higher levels of divergent thinking.
The circles task is a measure of divergent thinking adapted from the Torrance Test of Creative Thinking (Torrance, 1974). Participants were given 7 minutes to create designs using 16 blank circles. Answers were scored on the criteria of Fluency (number of unique answers), Flexibility (number of unique categories and number of categorical shifts between answers), and Originality (uniqueness of the response in relation to the sample of responses). A higher score on the circles task indicates a higher level of divergent thinking.
The Creativity Achievement Questionnaire (Carsons, Higgins, & Peterson, 2001) was designed to rate the level of creative accomplishment of the respondent. This task was not timed. Participants were asked to check each statement that applied to them in each of 7 different creative categories. Statements such as "I do not have any training or experience in this field" were scored as a 0, whereas statements such as "My work has been reviewed in a national publication" were scored as a 7. Total scores for each category were created by adding the score (0-7) of each checked statement for that category, then multiplying the number of times the respondent had achieved the top level in that category by 7 and adding that to the original score. A higher score indicates a higher level of creative achievement.
The last item administered was a General Information Questionnaire. This task was not timed. This questionnaire was designed to provide background information on each of the participants. Subjects were asked to provide their age, their academic major(s)/minor(s), and current ADD/ADHD/learning disability status. They were also asked to check off interests and hobbies from a provided list, and to write down any other hobbies not listed.
The first hypothesis predicted that higher scores on the divergent thinking tasks would correlate with higher scores on the measure of realistic creativity and of self-report measures of creative achievement. To test this hypothesis, we ran a correlational analysis on the Unusual Uses task, the circles task, the haiku task, and the Creativity Achievement Questionnaire (CAQ). The results of the analysis are summarized in Table 1. We found that our prediction was partially supported.
As shown in Table 1, no significant relationship was found between the fluency sub-scores and the flexibility sub-scores on the haiku task. Participants who provided more answers, or demonstrated more categorical shifts, on divergent thinking tasks did not tend to score higher on the task of realistic creativity. This did not support our hypothesis. Table 1 also shows that flexibility was not significantly related to self-report of previous achievement, which did not support our hypothesis.
There were mixed results on the relationship between fluency and previous achievements. Table 1 shows that there was a significant relationship between fluency on the Unusual Uses task and the CAQ, but there was no significant relationship between fluency on the circles task and the CAQ. These results are inconclusive and do not support our hypothesis.
However, there was a significant relationship between the originality sub-score and the haiku task. Originality on the Unusual Uses task was significantly related to the haiku task, r = .303, p = .034. There was also a significant relationship between originality on the circles task and the haiku task, r = .333, p = .015. Participants who exhibited a higher level of originality on divergent thinking tasks tended to score higher on the realistic assessment of creativity. These results lend support to our hypothesis.
There was also a significant relationship between the originality sub-score and reports of previous creative achievements. The originality sub-score on the Unusual Uses task was positively correlated with the CAQ, r = .534, p = .000. The originality sub-score on the circles task was also positively correlated with the CAQ, r = .309, p = .024. A higher level of originality on divergent thinking tasks was associated with greater previous creative accomplishments. This provides support for our hypothesis.
In order to control for the effects of other variables on the haiku task, a multiple-linear regression was performed with the originality sub-scores of the Unusual Uses task and the circles task as the predictor variables, and the haiku task as the outcome variable. The results of this analysis are provided in Table 2. There was a marginally significant relationship between originality on the two divergent thinking tasks and the Haiku task, p = .055. A step-wise linear regression showed that originality on the Circles task contributed significantly (R = 10.3%) towards predicting haiku scores, p = .000.
The second hypothesis predicted that scores on measures of convergent thinking would be related with scores on measures of realistic creativity and self-reports of previous creative achievement. To test this hypothesis, a correlational analysis was performed on the Remote Associates task (RAT), the Children's Remote Associates task (CRAT), the haiku task, and the CAQ. The results of this analysis are summarized in Table 1. The relationship between convergent thinking and realistic creativity produced mixed results. Table 1 shows that while no significant relationship was found between the CRAT and the haiku task, a significant relationship was demonstrated between the RAT and the haiku task. These results are inconclusive and do not support our hypothesis.
Convergent thinking did not seem to be significantly related to self report of previous creative accomplishment. As shown in Table 1, no significant relationship was found between the CRAT and the CAQ, or the RAT and the CAQ. Participants who exhibited higher levels of convergent thinking tended to have fewer previous creative accomplishments. These results do not support our hypothesis.
The purpose of this study was to examine the ways in which different available
measures of creativity might be related to each other. Measures of convergent thinking, divergent thinking, realistic creativity, and self-report were examined. The results showed that the originality sub-score for both of the divergent thinking tasks was positively correlated with the realistic creativity task. The originality sub-score was also positively correlated with self-report of creative accomplishment.
Our findings suggest that one of the main forces behind creativity could be the factor of "originality". Guilford (1950) considered "novelty", in the way of producing uncommon but serviceable ideas, to be a primary element for defining a creative person. This implies that in order to be creative a person must be able generate many ideas, which would increase the likelihood of coming across a novel one, but must also be capable of the focused thinking that creates answers that are functional (Cropley, 2006).
Interestingly, neither the category of "fluency", nor the category of "flexibility", was significantly related to higher scores on the realistic assessment of creativity (Table 1). This suggests that divergent thinking alone probably cannot account for creative end-products. Rather, there is likely another process taking place which works with originality. Previous researchers have theorized that convergent thinking is that second process (Cropley, 2006). In this model, divergent thinking allows for the brainstorming of original ideas while convergent thinking allows for the critical appraisal of these ideas (Cropley, 2006).
In this study, the relationship between convergent thinking and realistic creativity was inconclusive. The results showed that while the original Remote Associates task (RAT) was significantly related to higher scores on the haiku task, the Children's Remote Associates task (CRAT) was not. Though the RAT and CRAT themselves were significantly related, the correlation was not overwhelmingly strong, and it is possible that this new task simply did not measure convergent thinking as well as the original. If this was the case, it is possible that given only the correlation between the original RAT and haiku scores we could conclude that convergent thinking was in fact related to realistic creativity.
The use of an adapted version of the RAT was a significant limitation of this study. The purpose of using the CRAT was to test it for later use with a younger population. However, the results of this study may have been stronger had a time-tested measure of convergent thinking been used. Future research in this area should focus on identifying another method of testing convergent thinking that can be paired with the RAT.
Another limitation of this study was the fact that both the RAT and the CRAT had over 40 items each for the participants to answer. The researcher running the study noticed that many participants seemed to become bored with the task towards the end of the allotted 7 minutes. This appeared to be more of a problem with the RAT, which was a more difficult task. When coding the data, it was noted that a number of the participants did not even reach the last page of the RAT. While it is possible that this was due to time constraints, given the information about participants seeming to "not try" towards the end of the task, it seems that boredom was a very likely culprit. If this is the case, the number of RAT trio's answered per participants could be lower than normal, artificially lowering the correlation between convergent thinking and the haiku. This problem could be corrected for future studies by limiting the number of items on these tasks.
As predicted, different measures of creativity relate to each other in different ways. The originality sub-score for divergent thinking measures has a clear relationship with realistic creativity, while convergent thinking measures seem like they may have a relationship with realistic creativity given better assessments. This finding suggests that future research could focus on defining a unified assessment of creativity. This type of assessment would acknowledge that no one definition of creativity can account for the many dimensions involved, and a more holistic account could be an excellent resource for future creative investigations.

The process of learning has sparked a great deal of research in the field of biopsychology. The mechanism by which an animal acquires a memory, stores it, and later retrieves it is a procedure that has yet to be fully explained. Throughout history, paradigms such as operant conditioning and classical conditioning have provided possible models to more effectively study learning. The topic of how one learns has been narrowed even further; specifically, interest has been generated in the connection between stimuli and their fear-induced responses (Maren, Aharonov, Stote, and Fanselow, 1996). The fear response produced by an animal can both hinder and improve the survival and quality of life experienced (LAB LECTURE, 2006). In humans, fear arising from phobias or post-traumatic stress disorder (PTSD) may impede his/her ability to function on a daily basis. For example, if a patient has a phobia of heights, and he or she is hired by a company with an office on the hundredth floor of a building, a conflict is inevitable. Conversely, the ability to produce a fear response may actually be vital to one's survival; therefore, fear is not completely maladaptive. For example, the same patient's phobia of heights will prevent him or her from doing a potentially life-threatening activity like sky-diving. Understanding the mechanism by which a fear response is acquired and expressed will be invaluable to the advancement of pharmacology and psychology.
More specifically, scientists have become extremely interested in the cellular mechanisms involving the formation and storage of new memories. Recently, research has indicated the importance of the amygdala in fear conditioned learning (Maren et al, 1996). Studies have shown that both lesions in the amygdala and pharmacological inactivation produce insufficient responses in both the formation and demonstration of fear conditioned learning (Maren et al, 1996). Also, synaptic plasticity has been established as a critical feature of the learning process in the brain, specifically, long-term potentiation (LTP) or the receptor-dependent strengthening of synapses (Maren et al, 1996; LAB lecture, 2006).
In this experiment, the connection between stimuli and the fear-induced response was studied using a Pavlovian fear conditioning model (Maren et al, 1996). In this model, a stimulus that does not normally elicit a response (conditioned stimulus, CS) was paired with an unconditioned stimulus (US) that normally elicits an aversive response. By the pairing of these two stimuli, the CS began to predict the US, and when the CS was presented alone it produced a fear response. The amygdala is the loci of the association of the US and CS; therefore, the significance of NMDA receptors in the amygdala during aversive learning conditioning was tested (Maren et al, 1996). The research intended to show that by the administration of D, L-2-amino-5-phosphonovalerate ("APV; a selective NMDA receptor antagonist"), LTP, and therefore, expression of the fear response, was disrupted (Maren et al, 1996). The glutamate receptor, N-Methyl-D-Aspartate (NMDA), was hypothesized to be vital to the generation of LTP in the amygdala for the expression of fear conditioned learning (LAB LECTURE, 2006).
Forty-Eight (two subjects per group) male Long-Evans (hooded) rats were used. The rats were supplied by Harlan and weighed from 175-199 grams when received. The rats were handled daily two weeks prior to surgery, and were maintained on a 12 hour light/dark cycle (light: 7:00 am; dark 7:00 pm). The dummy cannulae were changed daily after the surgery until the time of the drug infusion.
The unconditioned stimulus (US) was a mild (0.6 mA) one second footshock in the chamber which was given during the training session. The conditioned stimulus (CS) consisted of the environmental context; i.e. the chamber in which the rats were placed. The chamber was a small box with a transparent door. The floor of the chamber consisted of a metal grating of rods through which the shock was transmitted, and under the grating was a metal pan to collect any droppings. On one side of the chamber there was also a dim light. Rats were also subject to the odor of a vinegar solution which was used to clean the chamber in between trials. The unconditioned response (UR) was the unlearned reaction to the shock. This reaction entailed a startled reaction and specifically jumping in response to the US. The conditioned response (CR) involved the learned response of freezing. Freezing behavior in rats specifically consists of somatic immobility. Freezing is also associated with an increase in heart rate, and respiration. The independent variable for this experiment was the difference of drug administration between groups (APV treated rats v. control rats). Whereas the dependent variable was the rat behavior which was recorded (specifically freezing) for analyzing fear conditioning. Rearing, grooming and locomotion were also recorded and therefore additional dependent variables.
After anesthetization with sodium pentobarbital, rats were mounted on a stereotaxic surgical device. This device allows for the precise location of various brain regions to be mapped and for accurate implantation of guide cannulae. An incision of the skin was made in the scalp and the skin was held back by hemostats. A drill was used to make holes in the skull in order to implant the cannulae. Using coordinates found in a stereotaxic atlas, guide cannulae were implanted bilaterally into the basolateral amygdala. Screws were placed in the skull for support, and the screws and cannulae were held in place by a coating of commonly used dental cement which covers the open surface of the skull. Dummy cannulae were inserted into the guides to avert any type of obstruction. The dummies were changed daily, as the rat was given a minimum of one week to recover from surgery.
Drug or saline was infused using a 10 µL Hamilton syringe which was attached to an infusion pump cannula which extended 1 mm further than the guide cannula. After removal of the dummy cannulae, the infusion cannulae were inserted into the guide cannulae. This ensured that the drug was locally administered to the amygdala, the brain region which is of primary concern. In the control rats, 0.5 µL 0.9% sterile saline was infused at a rate of 0.1 µL/min for five minutes. In the experimental group, the same volume of aminophosphonovalerate (APV) in 0.9% sterile saline was infused at the same rate. The infusion cannulae were removed one minute after the completion of the drug administration and replaced by clean dummy cannulae. One minute was given in order for the drug to diffuse, after which the rats were taken to the chambers for the training session.
Each rat was placed in a chamber, and three behaviors (freezing, grooming, and rearing) were independently scored, one by each team member. The session lasted four minutes, and recordings were made based on a time-sampling procedure. No footshocks or other stimuli were administered for this session. Recordings were determined at ten second intervals for a total of twenty four recordings for each behavior scored. After the session, the rat was removed from the chamber and returned to its home cage. The chamber was then cleaned using a weak vinegar solution, and the collecting pan was removed and cleaned. This process was then repeated for the second rat.
Each rat was then placed in a bucket near the infusion pump where infusions of APV or saline were administered (see Drug Infusion above). The infusions were only carried out during week one. Rats were placed in chambers and a computer program MED-PC IV, which delivered the footshocks, was operated. Footshocks were initiated at one for four minutes. No recordings were taken. After the session was completed, each rat was returned to its home cage and the chamber was cleaned as described above in the Pre-training procedure. This process was also repeated for the second rat.
These same procedures were repeated again two weeks later in the second session. The procedures were equivalent to the procedure the first session described above except for the absence of drug infusion for the second session.
Before training the freezing behavior of both groups of rats did not differ significantly and was very low. However, freezing behavior increased greatly in the first post-training session for both the saline group (2.6→87.67%) and the APV treated group (4.86→44.97%). APV only increased freezing behavior by about half of the increase seen in the saline controls. Two weeks later in the second post-training session, the freezing behavior decreased to a great extent in both saline (87.67→21.53%) and APV groups (44.97→15.08%), yet the freezing behavior of APV treated rats was still lower than that of the saline group. After the retraining session, both groups increased dramatically in the amount of time spent freezing (96.53 % for saline, and 89.76% for APV), and there was not a great difference between the two groups (Figure 1).
In the pre-training session, there was no considerable difference between the groups grooming behavior (9.9% for APV and 7.81% for saline). In the first post-training session, the levels of grooming in the APV treated group increased slightly to 12.85% whereas that of the saline group significantly decreased to 0.69%. Two weeks later in the second post-training session, the levels of grooming in both groups returned to levels exceptionally close to the levels seen in pre-training (10.76% for APV, and 8.68% for saline), and there was no significant difference between the two groups. However, in the post-retraining both groups declined vastly: 0.69% of the APV group and 0% of the saline group showed grooming behavior (Figure 2).
Before training, rearing behavior did not differ significantly between the two groups (23.44% for APV and 26.25% for saline). After training rearing declined in both groups, yet it declined much more in the saline treated group (26.25→4.17%) compared to the APV group (23.44→11.46%). In the second post-training session, rearing behavior increased again in both the saline group (4.17→15.97%) and the APV group (11.46→18.75%). Again, rearing behavior was less in the saline treated group. In the post-retraining session, rearing for both groups declined remarkably: 1.22% in APV treated rats and 0.69% in the saline group (Figure 3).
In the pre-training session, no significant differences were seen between the control group and the experimental group (33.3% for APV and 31.25 for saline). In the post-training 1 session, locomotion declined in both groups. In the APV treated group locomotion decreased by almost one third (33.3→12.5%). The decrease observed within the saline treated group was markedly greater as no locomotion was recorded in the post-training group (31.25→0%). In the second post-training session, locomotion did not change from the first pre-training session for the APV treated group (12.5%). However, locomotion increased greatly in the saline treated group (0→45.83%). The locomotion observed in the rats in the post-training 2 (45.83%) session was also much greater than the pre-training session (31.25%). After retraining, locomotion decreased in both the APV treated group (12.5→5.56%) and the saline controls (45.83→2.78%) (Figure 4).
By analyzing the results, we found some of the information to be expected based on our background knowledge of the biology of the brain, but there were also some surprising outcomes. As anticipated, both rats had low levels of fear behavior in the pre-training trial because the chamber had not yet been paired with the aversive stimulus, the shock (4.86 APV rats and 2.6 SAL rats for freezing percentage). With classical conditioning, the conditioned stimulus must be paired with the unconditioned stimulus in order for an association, and in this point of the learning, that had not been done yet. Along with the absence of freezing and other fear responses during the pre-training, the rats also showed similar levels of grooming and rearing, because they were not preoccupied with fear of the chamber yet.
The division in behaviors became apparent in post-training week 1 when rats in the APV group spent a considerably less amount of time freezing than the SAL group, and thus more time grooming and rearing (freezing percent: 44.97 vs. 87.67). This finding is very similar to Maren's study on APV infusion, where the experimental group showed that the drug "severly blunted the acquisition of immediate postshock freezing" (Maren et al., 1996). There are many explanations for why the APV rats showed less fear response after being shocked at the same intensity as their SAL counterparts. Perhaps the drug interfered with the acquisition of the fear learning at the time of training by blocking the NMDA receptors that are crucial to learning. If the rat could not encode the experience properly, then it could not consolidate or recall it at a later time because the association wouldn't have entered into memory. Possibly by blocking the receptors during the experience, the drug also disrupted the expression of fear, shown by less freezing than the control group. A way to test if it is both acquisition and expression is to place the rat back in the cage when it is drug free. If it does not show the fear response still, it is clear that it did not acquire the learning in the first place. If the rat did instead demonstrate freezing, that would show acquisition happened but the drug simply interfered with exhibiting the response.
As with any experiment, a few of the results were somewhat unexpected and warranted a closer look as to the reason they did not match our predictions. We thought that the testing in post-training week 2 would yield similar results with respect to comparing freezing levels between the rats. Nevertheless, both the APV and SAL groups showed a sharp decrease in freezing behaviors during this testing (APV 15.08% and SAL 21.53%). Since the SAL group was so low, it brings up the question of whether or not we actually blocked long term memory of fear in the APV group. This result suggests that similar factors could have played a role in decreasing freezing in week 2, even though one group received drugs and the other didn't. For instance, perhaps the time span between the training and testing was too long for the rats to remember the chamber. If they could not pair the environment with the shock, they would have no reason to freeze. It is possible that if the time span was the same, but the intensity of the shock was greater, then more long term potentiation may have been able to take place, and thus better learning.
A small number of rats were also scored on their locomotion during the task, but the SAL group showed much more movement during post-training week 2 than the APV group, which was the opposite of what was predicted. If the SAL group demonstrated a greater fear response because of better acquisition of the association, then they should be freezing, not exploring the cage. Given that the sample size was so small compared to the other results, however, it is likely that the variation is misleading because of the limited information available. If all groups had been able to score on locomotion, a more accurate representation might have been possible.
Considering possible problems with the experimental design also prompts new ideas for future experiments. While there was some concern about the amount of time between training and post-training trials, maybe upcoming tests could study the effect of spaced training over a few weeks instead of a mass training in one session. Spaced trials could produce longer lasting memory and better recall. The results also brought to our attention how important it is to follow lab protocols. During the post-training week 2, when SAL groups also decreased freezing (an unexpected result), it is possible that not all groups put their rat in the chamber at the same time. By remaining in the environment during a time with no shocks, it is possible the rats learned that the chamber was not frightening after all. In addition, this could have interfered with the pairing we were trying to do, because the rat may not have learned as easily that each time he is in the chamber he will get shocked.
In the future, it would be interesting to test the effect that various intensities would have on the rate on learning. I would predict as the intensity if the shock increased, the level on sensitization in the rat would increase, as well as long term memory of the situation. Additionally, other drugs which target NMDA receptors could be used, and at varying amounts. Furthermore, drugs that facilitate NMDA receptors could also be used in an experiment such as this. In another study examining fear conditioning in the amygdala, the rats were infused with corticosterone, which enhanced memory of fear learning (Roozendaal et al., 2006).

It is generally assumed that women have more trouble than men in the classroom, specifically in subjects like mathematics, science and engineering. Gender stereotypes, at both a conscious and non-conscious level, exist in our society and may be contributing to females' behaviors, interest, motivation and performance in the classroom. Each individual male and female uses their own self-perceptions of their gender's success in particular domains in making their academic and occupational choices. Women tend to acknowledge their success in English and difficulty with mathematics (Eccles, Jacobs and Harold, 1990). As a result, college-aged women are not pursuing these male-dominated fields because of their lack of self-confidence, inequitable classroom environment, need of support from instructors and negative interactions with peers.
In college, women are even more vulnerable than men to lower self-esteem and therefore need attention, support and encouragement (Allen and Niss, 1990). Women are usually the minority in mathematics, science and engineering classes, causing their thoughts, opinions and therefore their success to be taken over by their assertive male classmates. In addition, women are more hesitant than men and need a comfortable and unbiased environment to succeed.
Montgomery and Barrett (1997) found that faculty are interacting and responding more positively
Not only are women lacking support from their instructors, but also from their peers. Research shows that women feel resented by their male classmates and isolated in the classroom. As a result, women hide their talent and success in the field to avoid hostility (Montgomery and Barrett, 1997). My observational study strived to better understand how unconscious gender stereotyping affects math performance in the hopes of offering suggestions to students and instructors to help alleviate gender bias in the classroom and create a comfortable and fair learning environment for both males and females.
An observational study was conducted to examine if conscious and non-conscious gender stereotypes affect females' mathematics performance. The study investigated the effect of gender stereotypes through the student's behaviors and participation in class. I hypothesized that females would participate less in class than their male peers because of gender stereotypes.
The constructs of interest were operationally defined. The classroom observer observed the number of students of each gender who asked a question, voluntarily answered a question and answered a question after being called upon. These constructs measured the students' participation in the class where "asked a question" demonstrated the most motivation in the most vulnerable situation, followed by "voluntarily answered a question" and lastly by "answered a question when called upon". In terms of these operational definitions, I expected men to ask more questions and voluntarily answer questions more often, while both men and women would answer questions when called upon.
The observation study took place in a Math 115: Calculus I classroom in the Dennison building at the University of Michigan in Ann Arbor, Michigan. The observation took place on November 17, 2005 from 8:40-10:00 a.m. All 22 undergraduate college students and one graduate student instructor in the class participated in the observation study. The sample of 22 students consisted of 10 male and 12 female students. The graduate student instructor was female.
The classroom observer contacted the graduate student instructor earlier in the semester asking for permission to observe her Math 115 class. The students were informed of the visitor ahead of time but were unaware of the purpose of the study. They were told to behave in the same manner they would any other day in class. On the day of the study, the classroom observer attended the Math 115 class and counted the number of male and female students present. A tally system was used with an "m" for male and "f" for female, in order to count the number of times a student of each gender performed a construct being observed. The classroom observer tallied the constructs: students asked a question, voluntarily answered a question and answered a question after being called upon.
The classroom observer scheduled a feedback session with the observed graduate student instructor to share a report of the study: the previous research, hypothesis and results as well as a discussion on positive feedback, suggestions and preventing gender bias in the classroom.
There were a total of 22 participants; 10 male and 12 female students. The construct "asked a question" had m = 0.182 responses and sd = 0.395 responses; the construct "voluntarily answered" had m = 0.682 responses and sd = 1.524 responses; the construct "answered when called upon" had m = 0.146 responses and sd = 0.351 responses; and the total of the constructs had m = 1.00 responses and sd = 1.773 responses.
An independent samples t-test was conducted to investigate whether the categorical variable gender and the continuous variable measured by the constructs of the students' participation in class are associated. Assuming there are equal variances, the independent samples t-test showed if there was or was not a statistical difference between two groups.
The independent samples t-test found that gender and each of the constructs and gender and the total of all the constructs are not statistically different. With 20 degrees of freedom and assuming there are equal variances, I can conclude that there are no significant differences between gender and the construct "asked a question" since t(20) = 1.303, p > 0.05. I can conclude that there are no significant differences between gender and the construct "voluntarily answered" since t(20) = 1.498, p > 0.05. I can conclude that there are no significant differences between gender and the construct "answered when called upon" since t(20) = -0.435, p > 0.05. Lastly, I can conclude there are no significant differences between gender and the total of the three constructs since t(20) = 1.491, p > 0.05. Therefore, the analysis does not support my hypothesis because gender did not have an effect on any of the constructs or the total of the constructs. I accept the null hypothesis at a 5% significance level and conclude that gender had no effect on student's participation in class.
Figure 1: Student's Participation in Class, below, displays the total tallied results of the observation study. However, the sample of observations is small, therefore it is difficult to make strong conclusions. Note that in the chart, for the "asked a question" construct, the chart indicated the number of questions asked by male and female students during the 80-minute class time. But in addition, the classroom observer noted the number of questions asked by students before and after the class*.Figure: Student's Participation in Class
While the sampling of the constructs in the observation study of the Math 115 class was not sufficient, the results do appear to be consistent with previous research on women's performance in mathematics. The results measuring both male and female participation in class in terms of asking a question, voluntarily answering and answering when called upon, found that men are more involved and vocal in the classroom. In addition, it appears that female students felt more comfortable asking the instructor questions privately before or after class in a more personal and intimate environment, thus avoiding the vulnerability associated with speaking in front of the entire class. The classroom observer also noted that rather than asking the instructor questions, many female students asked questions to their male classmates at their tables, who responded in a positive manner. This observation contradicts Montgomery and Barrett's (1997) findings that women feel begrudged by their male peers.
Figure 1: Student's Participation in Class shows that 12 male and three female students were tallied under the "voluntarily answered" construct. However, the classroom observer noted that one specific male in the class made-up most of these voluntarily responses whereas the three women who spoke voluntarily were different students. This created a bias in the data sample since one male student was an outlier and therefore overrepresented the male genders participation.
In conclusion, these observations suggest that women are participating less than men in the mathematics classroom and receiving less encouragement from their instructor. Therefore, it is important that instructors create a comfortable classroom environment, free of sexist attitudes. Montgomery and Barrett (1997) made several suggestions for instructors to encourage women to raise their confidence in their mathematics skills including acknowledging women's individual achievements and encouraging them to apply for workshops, internships and graduate school in their field. Female guest speakers also serve as role models for women and demonstrate female success in male-dominated fields (Montgomery and Barrett, 1997). In addition, if females are hesitant to participate in class, instructors should request answers from specific "quiet" tables with a mixture of males and females. In the observation study, the classroom observer found that this instructor technique was effective and helped the female students.
There were many flaws in this study in terms of participants, setting and operationalization of constructs, which should be considered and accounted for in future research. First, there may have been a bias because the instructor in the classroom was female. Future research should investigate whether female students receive different support and encouragement from male instructors. The setting of the observation study was also less than ideal since the college course was taught so early in the morning, beginning at 8:40 a.m. The lack of participation from students may have been influenced more by students' exhaustion than by gender, especially since the classroom observer found that students' participation increased as class continued when students appeared more awake and alert. In addition, the data sample was much too small to perform any real efficient statistical analyses. Therefore, it would be necessary to observe the same class on multiple occasions in order to obtain more data to make reliable and valid statistical conclusions.
In the future, researchers should examine if racial stereotypes have an effect in the mathematics classroom. Researchers should divide the sample into White males, White females, non-White males and non-White females and examine which group is the least comfortable and does not receive support and encouragement from instructors and peers, thus putting them at greater risk for failure in the mathematics field. It would be interesting to see if the trends of race are similar to the sexist bias that exists in the mathematics classroom.

Two men. One a baby boomer, one from "Gen X". What important national events have happened during their lives? Which will they term most salient? Psychological research argues that significant political events experienced during young adulthood (a time of intense identity formation) will be considered most important and will have the greatest impact on how an individual views future events. (Duncan & Agronick, 1995) Due to this effect, it is possible to posit the existence of political "generations" formed around the shared experience of particular historical events at a certain age. To explore this idea, this paper will present research data from interviews with two American men of different generations. First, the methods used for the interviews will be described. Next, a brief biographical sketch of each participant will be presented. Third, responses from the interviews will be analyzed to determine to what degree each of the subjects exhibits the proposed construct of political generation. Finally, possible limitations of the study will be explored and overall conclusions summarized.
To carry out this study, two male participants from different generations were selected. One was born in 1957 and the other in 1972. More details on both subjects will be given in the biographical section. The subjects were both interviewed by phone, with the researcher taking notes by hand. The former interview took place on November 11, 2006, and lasted about 75 minutes. The latter was conducted on December 1, 2006, and took about 50 minutes.
Each subject was asked the same questions in the same order. The first section of the interview asked the subject to identify "the most important public event that [had] occurred during [his] lifetime." Follow-up questions asked about other "national events or series of related events [that have] been the most personally meaningful." Subjects were asked to consider the effect of the event on them, its overall importance, and if the event had any lessons. This series of questions had the end result of having each subject select, describe, and evaluate several relevant events from his own life.
The second section of the interview asked about current political views. Both participants were asked, in order, about nine issues. These topics were: 1. Women's rights and gender-related issues. (equal employment, equal pay, abortion, reproductive rights.) 2. Sexual orientation and gay rights. (same-sex marriage, benefits for same-sex couples.) 3. Race relations and affirmative action programs. 4. The balance between fighting terrorism and preserving constitutional rights. 5. The war in Iraq. (justification, success, plan for the future) 6. Financing the government. (taxes, government expenditure, and national debt.) 7. Healthcare. 8. Immigration. (benefits, downsides, possible guest worker programs), and finally, 9. Economic globalization and international trade. Asking about these nine topics produced a fairly comprehensive view of each subject's political beliefs. What follows is a brief biographical summary of each participant's life.
The second participant, "Jack", is a 34-year-old male (b.1972) who presently lives near Dallas, Texas. He was born in California, moved to Michigan as a baby, and moved back to Northern California at age seven where he resided through college. Jack was raised in an upper middle-class household, as his father was a successful executive at Ford Motor Co. Although Jack has three siblings, they are much older than he is, so for his adolescent and teen years Jack was effectively an "only child." Jack's siblings currently all live in separate states across the country. Jack attended college at the University of Southern California where he obtained a bachelor's degree in history. Subsequently, Jack enrolled in a history master's program at the Kansas State University, but he left the program after about a year to pursue job opportunities. Jack held one job with a recycling company before settling at Ford Motor Company, where he worked in marketing and sales roles for nine years at locations in Texas and Michigan. About a year ago, Jack changed career fields when he took his current position near Dallas. Now he works in sales for a medical technology company, selling medical software and devices to doctors and hospitals. Jack is unmarried and has no children.
Frank and Jack not only come from different generations, but they also have different backgrounds, different educational degrees, and different jobs in distinct industries. A combination of all these factors may influence their political beliefs. However, the theory of political generations predicts that the political events these men consider significant should not be influenced by any of these different factors, but merely by the age they were at which the event occurred. Events taking place from age 17-25 are considered most relevant by Mannheim (1952) in his theory of generational entelechy, while Winter (2006) generalizes the "critical period" to about ages 15-23. The two participants in the study conform to the political generation hypothesis in varying degrees. They are interesting contrasts because Frank shows strong evidence of belonging to a political generation, while Jack does not.
When questioned about important political events in his life, Frank immediately said, "Well that's easy. The fall of the Iron Curtain." This refers to the wall built to separate East and West Germany. It was seen as the divide between communist and non-communist countries. Dismantling of the wall began in 1989 when Frank was 32. Although this is outside the critical period age range, Frank characterizes the event as a symbol of the destruction of the Soviet empire. And the Soviet empire is something which affected Frank very much during the critical period of his life. The event was personally meaningful because "I grew up as a child fearing the opposing superpower, the U.S.S.R." In school, he faced drills where the students were trained to hide under their desks or run into the hall in the event of a nuclear bomb. The prospect of nuclear war was a part of Frank's life from a young age. As he grew older, Frank recalls many images of Russia as the enemy. Later, when the wall came down, Frank naturally viewed it as a "major historic event." At the time, he remembers feeling that the world was experiencing change, that it would be different. The U.S. would not have its old enemies, and there would be a different political and economic climate. In fact, Frank worked for the defense industry at the time, and a work joke was how business would decline because "peace broke out."
The sense of the entire world changing is significant because Stewart (2003, p.4) writes that "if events are discontinuous with a previous period in an individual's life, they are likely to have a bigger impact." Because this event marked a significant change in Frank's view of the world, it is likely that other people his age also felt the same way and would identify with each other from this shared experience of change.
Today, Frank understands the main lesson from this event as the failure of communism. And the question for the future is, "Will people want to try it again even though it failed?" Frank thinks "most things tend to go in cycles" (perhaps a product of his agrarian upbringing) and points to countries today like China where he says communism still exists, only with an overlay of capitalism. He also believes the fall of the U.S.S.R. is still resulting in change, and he cites a "ripple effect" with how it has affected Europe, the U.S., the emergence of China as a power, and even events in the Middle East. Frank's young adult experience of a world where the Soviet Empire was a feared superpower still influences how he sees world events unfolding today. Even though the fall of the Iron Curtain took place when he was a bit older, it was the overall experience of the Cold War that was the defining event of Frank's young adult life, and surely others his age would concur.
Besides the effect of communism, every other event Frank listed also occurred within the critical period of his life. They were all memories of presidents, which probably reflects the increased importance of who was leading the country during the conflict with the Soviets. Frank mentioned the fall of Nixon as a significant moment. (He was 17.) He remembers feeling disgrace over Watergate and a sense that it was "embarrassing for the whole country and in the eyes of the world" to have an American president resign. Again, it is not surprising that Frank considers this event important because it too marked a significant discontinuity in history. Something happened which had never happened before nor has happened since -- Nixon is the only president to resign. Frank explained how Ford was appointed and pardoned Nixon. Frank believes the pardon cost Ford the subsequent presidency but served the greater good because it "helped heal the country." Based on this critical event, we might expect Frank's later political views to contain a tendency to abhor unethical behavior and commend doing the "right thing," even if that might be unpopular.
Frank's last significant event was the election of Carter, who served when Frank was 20-24.
Frank lived through events like the Gulf War, the prosperity of the '90s, and 9/11, but none of these events made his list of significant moments. He fits the model of a political generation very well because all his significant events come from the critical period in his life.
Jack, on the other hand, listed events that do not fit neatly into the age range of his critical period. The significant event that immediately came to mind for him was 9/11. This attack on the U.S., where terrorists leveled the World Trade Center towers in New York City by flying airplanes into them, happened when he was 29. He describes how he "knew people could dislike us," but the magnitude of the event surprised him. His main lesson from the event came from the reaction of Americans. He remembered reading about World War II with the Japanese internment, and the "mass hysteria, jingoism," and as he describes it, " "
Interestingly, later in the interview Jack links 9/11, which occurred past his critical period, to events that happened just before his critical period -- the events of the Cold War which were so vivid for Frank. Jack lists a few events he "didn't completely grasp": the reign of Gorbachev and the fall of communism. Because he lived near San Francisco, he remembers the 1984 Olympics in Los Angeles (he was 12) and the sense that it was "the USA vs. the Soviets." Here he is experiencing the Cold War through sports -- a medium a child can understand. He also remembers seeing a movie on TV called "The Day After". This portrait of a possible nuclear war was shown when he was 11. Jack links these earlier, vaguer memories of the Cold War with his experience of 9/11 to conclude that "society's based a lot on fear." In his early life it was the U.S. vs. the U.S.S.R. with the threat of nuclear war, and today it is the U.S. vs. the terrorists with the threat of ideological war. Although Jack links the early and later parts of his life, he has not yet mentioned an event from his critical period.
Jack described the Challenger explosion, which occurred in 1986 when he was 14. Although he remembers that the space shuttle accident "shook [him] at that age," the occurrence is slightly outside the critical years. Finally during the last part of the historical events section Jack related some events that happened during the proposed critical period for forming a political generation. There were two such events in his life -- the earthquakes in California in 1989 and 1992 (age 17 and 20), and the L.A. riots that also happened in 1992. The two earthquakes, especially the earlier one which led to the collapse of the Bay Bridge and the destruction of freeways in California, were a "humbling" lesson for Jack about nature's power and our ultimate small size in the universe. The L.A. riots were personally significant because they happened while Jack was at school just down the street at USC. Jack describes the riots as a "major life lesson" about the "anger and frustration that a certain segment of the population can experience."
It is interesting that although these events both affected Jack, they are events of a more local, personal nature. They both occurred close to home and may not have affected other people around the country or around the world quite as much. The major world events that Jack describes (and links in his mind) occurred just before and just after his critical period. This suggests that people of Jack's age may not have specific political events that shaped them into a cohesive political generation.
Expanding the concept of political generation to current politics, several aspects of Frank's experiences from his critical period are still evident. He tends to take a more international view of political issues. This could be due to his young adult experiences of watching political events unfold on a global scale and tracking their effects in different countries. When asked about women's rights, Frank is in favor of equality but acknowledges that it does not exist, especially on the global level. He points out that parts of America and Europe are more equal, yet in many countries women still face harsh inequalities. Regarding the current prison abuse scandals, Frank gives credit to the U.S. "that this stuff is revealed," and he compares this to the Tiananmen Square massacre which the Chinese government tried to cover up, events in the former U.S.S.R., and events in current Russia. On the Iraq War and terrorism, Frank suggests that "the world views Americans as not patient [where] all that enemies have to do is persist and it will cause controversy in the U.S." About the federal deficit Frank says, "clearly we'd love to run a surplus instead of a deficit, but we're competing in a global market." He argues that the role of the government should be to support high value exports and jobs, and "promote industries within the U.S. that help the balance of trade." This is an internationalist view. Overall, Frank took a more international view than Jack.
The impact of Frank's critical period is also evident in a few other comments. He agrees with the need for taxes, but doesn't want a system that "redistribute[s] wealth in a socialist way." This reflects his views on the failure of communism. With regards to President Bush violating Geneva conventions on treatment of prisoners, Frank says he doesn't think the President believed he was violating conventions. Also, Frank (unlike Jack) still supports the Bush administration's Iraq War even though sometimes he feels it wasn't justified. He says, "I don't have all the data that I assume our government had." These relatively more supportive views of the administration may reflect Frank's desire for the president to be a "good guy," not a disgrace like Nixon. Overall, there are several aspects of Frank's current politics that show an influence from events during his critical period.
Jack, on the other hand, did not seem to experience major world events during his critical period. When Jack was about ages 15-23 (1987-1995), the world was in a relative era of peace and prosperity with the abatement of the Cold War and the economic boom of the late '80s and early '90s. "Gen X" is still defined as a generation, but not really by political events. In fact, Ortner argues that " "
It is worth noting that although Jack does not seem to belong to a politically-influenced generation, his views today do also seem to be somewhat influenced by personal events from his own critical period. One might argue then that the critical period can still be important on an individual basis even when it does not apply at a national, generational level. For example, experiencing the L.A. riots exposed Jack to violent racial tensions. This may well be a factor in his current support of affirmative action. He says, "I do think society owes a certain debt to people we brought to this country who didn't have the benefit of a white, Anglo-Saxon background." Jack also believes that the federal government does a poor job of providing programs and says that "any time the government tries to get involved in some sort of national program, invariably it's a bumbling mess." Views like these could be partially inspired by the jarring explosion of the Challenger, itself a government project gone terribly wrong. The online encyclopedia Wikipedia states that the panel investigating the accident "found that NASA's organizational culture and decision-making processes had been a key contributing factor to the accident." ("Space Shuttle Challenger disaster", 2006) These critiques of the government, which came out during Jack's critical period, may have influenced his later views.
To summarize, Frank's accounts of his current political views do show effects from the political events he experienced as a young adult. Also, all the important national events he names fall within his own critical period. These observations support the concept of a political generation. Jack, on the other hand, does not seem to belong to a clear political generation based on the significant national events he describes. It is likely that the generations immediately before and after him feel a stronger sense of political generation. (Indeed, we see this with Frank, who is of the preceding generation.) Jack's current politics seem to be influenced more by other factors proposed to define his generation than by specific political events; however, he still shows some influence from the critical period in his own life if not from one marking his generation. This discovery is a compelling argument for the overriding importance of the critical period -- it is always of some significance, at the very least on an individual scale.
Of course, these conclusions are not without their limitations. Despite the clear differences between the subjects, they are each only a single representative of their generation. A much broader sample from would have to be taken to draw any definitive general conclusions about political generations. Furthermore, it should be noted that the men actually agreed on many subjects, and even with their differences -- with Frank being more internationalist and Jack more concerned about equality and class differences -- it was not as though Jack never mentioned internationalism nor Frank class. These were merely broad trends of difference. Another possible limitation is that there may be third variables at play. Frank's internationalist view might owe partly to his recent MBA education or his world travel. Also, although not detailed, there were several instances where Frank expressed distaste for a sense of entitlement, free-loaders, and handouts. These views could be influenced by Frank's hard work on the farm as a child, and it just shows that there are other variables that might influence political views besides world events occurring during one's critical period.
Still, even with all this, the data available from this study points to two important conclusions. First, the interviews provide evidence (from Frank) that the concept of a "political generation" is valid. Second, they also show (via Jack) that the critical period may be of such importance that it matters on an individual level even when generations do not experience national political events together. Thus, in conclusion, both the broad concept of the political generation and the particular idea of the critical period are constructs that deserve continued attention in future research.

In this paper, I intend to examine three interviews conducted by myself and two other classmates of three separate people who were not born in the United States using the topics of immigration that we have recently learned in our class. The participants come from very different parts of the world, but there are some common topics that have affected all of them throughout the course of their lives. Such matters include the part that gender has played both in their home countries and while they have been here, the role of their families in the structuring of their identities, how generational differences influence their transition into American society, and how transnationalism has affected their concepts of who they are. This paper will attempt to discuss all of these issues in the lives of all three participants and come to a conclusion about what immigrants face by coming to America, no matter where they originally came from.
The first part will outline the concepts that I plan to use to analyze the interviews with textual support from the articles in the course pack and the book by Mahalingam (2006). I will use my own interview as a starting point to see how well they relate to an actual immigrant's experience. Then, I will introduce the other interviewees and compare their life histories to my participant's life history to see if any common themes emerge along the lines of the different theories. I will follow that with a discussion of what the consistencies or inconsistencies in the stories mean and how my life relates to that of the immigrant participants. I will conclude by mentioning the limitations of the study and giving a summary of the main points.
An immigrant's experience is always colored by the gender they identify with. For a large portion of the men, they may undergo a negative change in social status due to economic and social discrimination from the dominant culture, so they attempt to control everything that they can in their lives in order to regain some of that prestige (Espìn, 2006). Immigrant women usually are the targets of the male's attempts to dominate their lives in addition to dealing with outside prejudice of their own, so they are doubly affected by their gender in the new social setting. Also, women, more so than men, have to live up to their culture's idealized cultural identity, which serves as both a source of strength and stress for them, and therefore might play a critical role in gendering the immigrant experience (Mahalingam & Haritatos, 2006).
My interviewee's experience with gender has to do with the apparent gender differences in career expectations in his family. He and his brother have always been pushed along a certain occupational path while his sister has never had the same kind of pressure placed on her. She seems to have more freedom to decide what she wants to do with her life, and as my participant pointed out, "she's the lucky one." Contrary to the cultural gender roles described in Mahalingam and Haritatos (2006), however, his sister does not seem to be the one that is expected to carry on the family's cultural traditions. She does not know how to read or write Bengali, so it appears that it is actually up to the men in the family to pass down their written heritage to their children. The difference seems to stem from the fact that the two boys immigrated here after they had at least finished middle school in Bangladesh and the girl had only attended up to second grade, so it was a matter of exposure to the language that might have prompted the inconsistency with gender theory. Still, it is interesting to see a time when the men are entrusted with the values of the culture after learning about how important women can be in that domain.
The role that a person's family plays in how they handle the stresses of immigration and develop or rework their identity is often the most important one. An immigrant's identity is formed around the decisions that their family makes in how to raise them and teach them their cultural background, and quite often the discrepancies between the two cultures makes for some interesting ideological conflicts (Dion, 2006). Matters such as the level of involvement that the parents should have in their children's lives and how closely they want to adhere to the values of their home country become major topics, and it is up to the parties involved to navigate these influential identity issues to form a coherent sense of self in their new society. Whether the whole family is together in the new country or just one member has moved on, family members often affect a person's level of cultural adaptation in such a way as to influence their identity formation (Sakamoto, 2006).
The influence that my participant's family had on him seemed to be especially concentrated on his choice of a career. Everyone from his grandmother to his aunts to his parents wanted him to become a doctor, so that is exactly what he is going to school to be. He claims that "they (his family) pushed [him] to be a doctor, but it wasn't like it was that strict." However, he also mentions that "when [he] was born, [his] mom decided [he] was going to be a doctor," so it is obvious that he was pushed from a young age to go into medicine. He internalized his family's expectations for him and incorporated their vision of what he was supposed to be into his identity, and now it has become an important part of who he is.
Another aspect of his life that has been shaped by his parent's cultural teachings is the emphasis he places on the language and religion of his home country. They are the two things that he would like to pass down to his children, and it reflects the influence that his parents had in making sure he maintains the traditions of his homeland. During this same process however, he has gotten used to speaking English almost exclusively and embraced some of the cultural trappings of his new culture, such as watching American football. In this way, he has been able to "achieve bicultural and bilingual competency that became an integral part of [his] sense of self" (Suarez-Orozco & Suarez-Orozco, 2001). His technique of integrating the two dominant cultures in his life shows how supportive his family has been in allowing him to explore his host culture while keeping him grounded in their native beliefs, and it has led to his success so far in living up to both cultures' standards.
A topic that goes along with the issue of family dynamics is the apparent discrepancies that exist between different generations of immigrants. The conflict seems to occur when the older first generation parents try to impose their traditional values on their unwilling children in the more Americanized second generation; when the kids realize they actually have more power in the family due to this connection, they rebel against their parents and refuse their traditions (Zhou, 2006). The children of immigrants usually have more social opportunities open to them than their parents, who had to struggle in the new culture just to survive. The new freedom afforded to them by the relative inclusiveness of American society leads to a clash of family values.
The reason that immigrant children become acclimated to the new culture so quickly seems to be the exposure they get while in school. They are quickly introduced to social groups and trends as they are passed down from peers, and it becomes a defining experience for them in how they handle the often overwhelming situation (Suarez-Orozco & Suarez-Orozco, 2001). Since the parents can not relate to such an experience, the children are left to make sense of this new information by themselves, and this generational gap in knowledge may lead to family conflict and an eventual breakdown in the family relationship (Zhou, 2006).
However, the relationship that my participant described with his family did not seem to follow this generational conflict model present in Zhou (2006). I know that he has had more direct exposure with American life than his parents since he arrived here before them, but nowhere did he mention that he had to somehow help them along through the acculturation process nor clashed with them on cultural matters. He points out that they were all exposed to American culture in Bangladesh because of cable and satellite TV and had a chance to get accustomed to it, but I do not think that it explains why there is little generational conflict. Watching TV does not equate to the experience of actually being involved in American culture, so I believe it was his technique of integrating the two cultures in a rather successful manner that prevented any such family discord from occurring.
The international connections that immigrants make between their home country and the US carry a lot of different implications for both cultures. Such connections can be made by either the immigrant traveling back to their home country or through some form of verbal communication. Whatever the case may be, the political, economic, and social contacts they have influence everything from trade policies between the two nations to the mental well-being of the immigrant, so the transnational nature of immigration is an important issue to understand. Communication with one's family members back home seems to have the most observable effect on these domains (Murphy & Mahalingam, 2004), and it appears that technological advances in the efficiency of electronic messaging with one's relatives will have a huge impact on the psyche of people who come here and on the entire global community.
My informant's transnational ties were most evident in his traveling back to Bangladesh almost every summer since he has moved to America. He has maintained a close connection with his aunts and uncles that live there, and he is often updated on the social climate of the country while he is talking with them. This family-oriented connection with his home country has shaped a lot of his personality because it allows him to maintain some of the traditions and values that he grew up with, and it provides him with a psychologically satisfying connection in his life. He can practice his home culture when he is back with his extended family in their native land, and he is aware of what is going on in their political arenas even if he does not vote in their national elections. His family is well enough off that he does not need to bring back money for them to survive, but the other transnational contacts he has supports Murphy and Mahalingam's (2004) inferences about the societal implications for foreign nations and the immigrant's mental health.
These theoretical concepts of immigration will be the framework for studying the interviewees' responses, and I believe they will be effective in drawing out the most significant topics from the conversations. Any major similarities or differences between participants will be explored in these four areas, and I think that they all have major implications for a large variety of people. Each one provides a different part of the immigrant experience, and by putting them together into one analytic argument, I will be able to effectively understand what is most important for people who come to America.
Interviews were conducted with each participant at a place and time convenient for them as determined by the interviewer. Two of the interviews were taped with a tape recorder and later transcribed on the computer; Vidal, the third informant, preferred to not be recorded and requested that the interviewer type what he said directly during the course of the interview. All of them signed consent forms for participating, and no direct compensation was given.
Benji, the participant I interviewed, is twenty-one years old, and he is currently studying biology at the University of Michigan. He moved to America from Bangladesh about seven years ago with his older brother. He originally lived with his aunt in Pittsburgh for two years, but then he moved to New Jersey when his mother, father, and younger sister settled there after immigrating also. He identifies himself loosely as a Sunni Muslim, and he is very devoted to his faith. He plans on going to medical school in the near future.
Vidal is a fifty-seven year old immigrant from Istanbul. He immigrated in 1980, right after he married his wife, and he has two college-aged daughters. He is very proud of his family, especially of his daughters, and he recognizes how much support he has received from them over the years. His whole family is Jewish, and he makes sure that his girls practice their faith as much as they can since he did not get a chance to back in Istanbul.
Mila emigrated from Macedonia in about 1970 and settled in Detroit almost immediately. Although the interview does not explicitly say how old she is, I would estimate her age to be at least sixty-five. She originally came with her husband and their son, but they divorced in 1988 when she noticed that he had become "a different person." She did not know English when she arrived in America, but she learned it by watching television and talking with the kids that she babysat for fifteen years. She now has three grandchildren of her own, and she maintains very close ties with the local Bosnian community.
Many of the stories that the participants told of their immigration and subsequent relationship with American culture appeared to follow the gendered pattern of most immigrants. Benji and Vidal were the first to come over from their immediate families to establish a connection here in America, but Mila had to wait for her husband to get settled here first before she could follow two years later. Also, she was expected to be the "good daughter" and "faithful wife" during her marriage while her husband went carousing around the city with other women, and she felt like she had to maintain that image for the sake of her family (Mahalingam and Haritatos, 2006). She did not know English and could not get a good-enough job to become financially independent, so she had to wait to get a divorce until her son was old enough to take care of himself and support her as well. Benji and Vidal had no such restrictions placed on them, and they were in control of any decision that was made about their lives.
However, one aspect that is contrary to Espìn's (2006) theory about gender is that Benji and Vidal seemed to suffer no loss in social or psychological status because they were able to remain in the middle-class level of society. Vidal got a job on Wall Street that could easily support his family, and Benji's father was so successful that he is now self-employed. Therefore, they did not find themselves wasting their talents in jobs that most immigrants are reduced to, such as the housekeeping done by Mila. Also, they had both been taught English in their home countries, so they were able to maintain their economic and social integrity with the help of good connections and a strong knowledge of the dominant language, which allowed them to stave off any detrimental impacts to their character.
The importance of a good family support system in their home country and here in America was evident in the interviews. All of the interviewees thanked the close-knit families they belong to for either allowing them to come here or supporting them in some way since they have arrived. They realize how lucky they are compared to other people from their home country, and they understand that it was all of their family members that allowed them to experience the opportunities of the United States. Furthermore, all of them relied on members of their extended family, such as aunts, cousins, and grandparents, who were already here to help them with housing and employment. With these kinds of resources available, it is apparent they had many people watching out for their well-being, which they could not have survived without.
In addition, they all recognized the importance of passing down their native language and some aspects of their home culture, especially their religion, to their children. All of them mentioned how they have tried or will try to instill the traditional values of their culture in the younger generation so they know where they came from, which seems to be a consistent theme present in many immigrant families according to Dion (2006). Their experiences revealed good connections with the most important people in their lives, and the importance they have placed on these relationships seems to have prompted them to pass down as much of their familial and cultural teachings as they can.
The most obvious difference that is apparent in the interviews is the age of the participants and the amount of time since they have emigrated. Benji arrived here just seven years ago and is still making his way through college; Vidal and Mila have been here for at least twenty-five years and have adult children of their own. With this kind of generational gap, the information that the older participants provided about their pasts might be less reliable than Benji's. For example, Vidal says of those first months and years: "I forget about them now, you know because things are so different, things are easy. The girls don't know how it was for us because we even forget sometimes." This kind of selective memory to block out unpleasant thoughts is more prevalent among those who have lived farther past them, so the age difference is a very salient feature of the interviews.
An aspect of the interviews related to this age gap that is consistent with Suarez-Orozco's (2001) finding on generational differences is the acculturating influence of school. Benji was the only one to go through any kind of formal schooling here in America, and he is the one that has immersed himself the most in the American way of life. Vidal was connected with American society through work, but he was not as exposed to the social trends and groups as much as Benji was. As a result, his socialization into the dominant society was limited to his family and work contacts. Mila did not attend any kind of school and only learned English after coming here, so she tends to relate exclusively with her own ethnic group. This type of result shows how powerful formal education can be in introducing immigrants to the society, and it illustrates how cultural gaps can form between people of different generations.
The transnational nature of the participants was marked by their frequent trips to their countries of origin. Benji and Mila admitted that they travel to see their relatives in their respective locales at least once a year, and Vidal said that he takes his family back to Istanbul several times a year. These kinds of vacations may be good to keep the family ties strong, but none of them seem to want to ever move back to their home countries. They are satisfied with visiting when they can, and even Benji, who planned on returning to Bangladesh after finishing medical school, acknowledged that he is unsure if that is what he still wants to do.
However, the extent of the participants' transnational ties appears to end at social and family-related travel and communication, as defined by Murphy and Mahalingam (2004). Mila takes some money back to relatives when she can afford it, but they do not seem to rely on her income in order to live. All of their immediate families are at least middle class, either here or in the home country, so there is no need to support them economically. None of the participants are actively involved in their countries' politics either, so the extent of the connection with their homelands is on a psychological, personal, and communicative level.
The theories that have been used to examine these interviews have provided me with a good basis on which to make some inferences about the various topics. For example, on the issue of gender, the differences between the males and the female in their immigration experiences reveals how much more difficult it can be for women coming to America. Mila was dependent on her husband when she moved to this country, and she remained dependent on him for half of the time she lived here. Even after she left him, she had to rely on another man, her son, who could take care of her while she tried to navigate the public domain in her search for work. Her lack of cultural skills greatly hampered her social and economic mobility, and her experience is probably echoed by thousands of women with the same background.
The reason Vidal and Benji were able to overcome their lowered status as immigrants and contradict the traditional theories about male immigrant experience is two-fold. First, it has to do with the cultural capital they arrived with; second, it has to do with the level of responsibility placed on them in their immediate families. The first one was addressed earlier in the results section, but the level of responsibility is important also because it gave them a strong impetus to succeed in America. Vidal was supporting his wife with his high-paying job on Wall Street, so he knew that he could not let the negativity get to him if he was going to start a family. Benji's family relied on him to get a good education, and he did not want to feel like he disappointed them by not attending a good college. Both of the men were able to overcome the obstacles that were placed in their way by remaining dutiful to the people that supported them.
As much as issues of gender were ever present in the interviews, talk of a person's family seemed to pop up with most of the questions. There were many instances of a participant saying how much their wife or brother was instrumental in the transition period between countries and how they could not have survived without them. I think that these experiences described by the interviewees about how supportive their families were have to do with the stereotypically close-knit nature of family units outside the United States. The cultures that the participants came from are known to emphasize loyalty and honesty with people in their bloodline, and that kind of mentality carried over to the people who have traveled here. It is hard for most Americans to understand this connection, but it is a major part in the lives of people from foreign countries.
Related to the issue of family relations is the transnational conduct of the three participants. First of all, the purpose of the interviewees' adherence to some of the ideas of transnationalism seems to be for their own sense of family commitment. They know that they will probably never live in that country again, so they use every chance they get to see their family members and enjoy their time together. However, I also believe that they wish to maintain those ties in the event that they ever need to use them again. They were able to receive so much assistance from the people back home during their time of adjustment that they know they can rely on it if it is ever needed again. They just have to preserve those connections, and keeping in constant contact with the ones they love will accomplish that.
Although generational differences are usually analyzed in the context of families, I believe that the huge age discrepancy in this study warrants a closer look. With that in mind, the fact that Benji is so much younger than Vidal and Mila makes for an interesting comparison of their views on life. They each arrived in America under completely different historical circumstances, and it would be very hard for them to relate to each other in what they experienced socially. Their subsequent levels of cultural involvement are directly related to that because the opportunities that Benji had available to him when he arrived far outweighs what the other two had, even after accounting for Vidal's connection to the working world. It comes down to how issues of discrimination and race relations have changed in this country, and the various generations of immigrants each have a unique experience to speak of.
These life history interviews have had an impact on my life and my perception of immigrants because they show me how hard some people have worked to live in this country. They labored for long hours while they were separated from family and friends just to live comfortably, and they had to change their entire perception of the world around them in order to function properly in society. Although there are much worse examples of immigrants who had a harder time here, these interviewees still made sacrifices in a couple different domains to be successful, and it teaches me to be thankful for the social position I am in.
A limitation of the study was that the same questions were not asked of all three participants. Some questions were phrased differently or may not have been asked clearly enough, and this probably lead to some confusion or misunderstandings on the part of the participant. Another limitation was the different styles in collecting and transcribing the data. One person had to type the dialogue directly onto the computer, so I am sure she omitted parts of answers when she could not type fast enough or she did not deem them worthy of inclusion. Even for the people that transcribed, there might have been problems with the tape recorder or with a section that was hard to hear, so it is not guaranteed that all relevant parts of the interview made it into the final transcript.
The immigrant experience is a diverse and complicated one. There are so many factors that go into each person's unique understanding of the world around them that it is hard to capture what they think and feel with just a few key topics. However, I believe that showing how gender, family relations, generational differences, and transnationalism fit into this expansive field provides a good start for exploring any of these ideas more in-depth in subsequent analyses. We might further explore the gendered nature of immigration and how we can stem some of its negative effects, or we can see what the traits are of particularly successful and supportive families. We could design studies to better examine immigrants from different eras and see how their cultural skills changed, and we can investigate what parts of a transnational identity are most effective for the immigrant's personality. No matter how we study these, they are all important in appreciating people who come to America.

Depression is a psychological disorder which has detrimental effects on not only the individual, but also the individual's family and society as a whole. Research indicates that in any given year, approximately 18.8 million American adults will suffer from a depressive disorder (Robins & Regier, 1990). Further, according to Kessler (2002), one in six adults in the United States will meet criteria for major depression in their lifetime, while one in four will be diagnosable with either major depression, minor depression or recurrent brief depression. Though depression is often underestimated as simply a blue mood or a state of intense sadness, this assessment is far from accurate. According to the National Institute of Mental Health (2007), depression is a disorder effecting not only mood, but also the physical health and thoughts of the individual.
Symptoms of a depressive disorder are serious. The diagnosis of major depression requires that an individual have experienced a depressed mood most of the day, nearly every day, or a complete loss of pleasure in all or most of the activities the individual used to find pleasurable for al least two months. Beyond these detrimental requirements for the diagnosis of depression, further symptomology can range from sleep and weight loss to thoughts and attempts of suicide (APA, 1994). Depression lasts for weeks, months or even years (NIMH, 2007), and has in recent research been shown to be a disorder of high chronicity. According to Kessler (2002), more than 80% of individuals experiencing a past episode of major depression will have recurrent episodes. This evidence of high chronicity is one of a few variables which lead to the shocking ranking of depression as the world's number one most burdensome disease according to the World Health Organization Global Burden of Disease study (Murray & Lopez, 1996).
The burdensome nature of this disease warrants further investigation into the possible correlates of depression, which may consequently provide avenues for intervention research to target. One such correlate involves family relationships. There is an abundance of research that suggests that unhealthy family relationships are related to depression. Some of the domains that have been extensively studied include family conflict, support, and cohesion. A two-year longitudinal study on peer and parental support mechanisms in adolescent girls by Stice, Ragan, and Randall (2004) found that poor support from parents was associated with greater levels of depression. Another longitudinal study on support and conflict found that lower levels of support, and greater levels of conflict were related to depression both concurrently and one year later (Sheeber, Hops, Alpert, Davis, & Andrews, 1996). A third study on Turkish university students found low family cohesion to be a significant correlate of suicide probability among college students, even after a multitude of variables such as mood and academic performance were controlled for (Gencoz & Or, 2006). Higher levels of family support and cohesion were also found to be related to lower levels of depression and suicide ideation in a study of African Americans attending a historically black college (Harris & Molock, 2000).
Other constructs similar to conflict and cohesion, such as perceptions of parental alienation and trust, have also been shown to be associated with depression. A study by Pavlidis and McCauley (2001) assessing the differences in mother-child interactions between clinically depressed and non depressed adolescents found that a main difference between depressed and non depressed groups was the lower perception of trust and higher perception of alienation in the adolescent-mother relationship for the clinically depressed group. These results correlated well with observational recordings for individuals without externalizing behavior problems, but did not correlate well for individuals with externalizing behavior problems. This study was useful in that it used different methodologies for assessing parent-child interactions, including survey and observational research, thereby increasing the scope of the findings. Another study by Essau (2004) which looked at 1035 adolescents randomly selected from 36 schools in Germany found similar results, where scores on parental alienation were significantly higher, and scores on parental trust were significantly lower for depressed adolescents compared to non-depressed adolescents.
Papini and Roggman (1992) studied parental trust, alienation, and communication in terms of attachment theory in middle school children. Attachment theory suggests that a healthy emotional relationship between the parent and child could provide a supportive framework within which a child can develop and master new challenges associated with developmental and environmental transitions (Papini & Roggman, 1992). The study by Papini and Roggman (1992) focused on the transition from elementary to middle school, and found that students with higher levels of parental attachment (corresponding to higher trust and communication levels and lower alienation levels), experienced lower levels of depression.
In accordance with attachment theory, the present study seeks to determine the relationship of two parental attachment variables, trust and alienation, to depressed mood among college students. The transition to college constitutes a significant change in environmental conditions, and requires a great deal of emotional maturation, where, according to attachment theory, a supportive parental relationship may help ease the transition. Accordingly, higher levels of parental attachment may be related to lower levels of depressed mood. In addition to attachment, this study seeks to address the relationship between how academically successful parents perceive their children to be, and the level of depressed mood in college students. The constructs of self-competence and self worth and their relation to academics have been widely researched, where an individual's own perceptions of their performance has been correlated with motivational and academic achievement variables (Wong, Wiest & Cusick, 2002). In addition, student perceptions of parental attachment have been correlated with academic achievement variables (Wong, Wiest & Cusick, 2002). However, few studies have correlated parental perceptions of academic achievement with depressed mood. As such, this study will provide a good next step in understanding the correlates of depression by extending the current literature to include parental perceptions of the academic competency of their children in addition to attachment variables.
All in all, I will be testing the following 3 hypotheses in order to determine the relationship between parental attachment variables and academic perceptions, and depressed mood in college students.
H1: Perceived alienation in the parent-child relationship will be positively correlated with depressed mood.
H2: Perceived trust in the parent-child relationship will be negatively correlated with depressed mood.
H3: Higher levels of perceived parental pride and approval in academic accomplishments will be correlated with lower levels of depressed mood in college students.
In this study, participants were obtained through non random convenience sampling and snowballing methods. Initially, researchers contacted their friends and acquaintances who lived in close proximity to them either in person or through email, and requested their participation in the study. Additional participants were obtained by asking participants to provide the name and email address of one or more of their friends who matched the criteria of the study and would be willing to complete the survey. These individuals were later contacted by email to request participation. In this way, purposive sampling techniques were used in order to specifically target certain sub-populations that were required by members of the research team.
The research team conducted power analyses using G-Power, an internet based statistical software, to determine the number of participants that would be required for the study, and determined that for each independent samples t test hypothesis with a medium effects size of d=0.5, a sample of n=102 (n=51 per group) would be required for an alpha level at α = 0.05 and statistical power (1-β) of 0.80. For each correlation hypothesis with a medium correlation coefficient of r=.3, a sample of n=82 would be required for an alpha level at α = 0.05 and statistical power (1-β) of 0.80. The alpha level of 0.05 that was used in this study indicates the probability of detecting a Type 1 error, which occurs when an effect is found where none really exists. The statistical power of 0.80 indicates the probability of detecting an effect that is actually present. It is defined as 1-β, where β refers to the Type 2 error, which occurs when a significant effect is not found when it is actually present.
In order to obtain representative samples to evaluate the hypotheses of the entire research team, which included the subgroups of Christians, Jewish individuals, Caucasians, Minorities, on campus residents, and off campus students, a total sample size of approximately n=350 was estimated to be required. In order to test the three correlational hypotheses presented in this paper, a sample of n = 82 was necessary according to the power analysis. However, due to the convenience sampling method and the absence of an incentive for participation, researchers were ultimately able to obtain a sample of n = 65 participants from the target population of college students.
Tables 1.1-1.7 show the demographic characteristics of the sample obtained by this study. The sample consisted largely of Juniors (20%) and Seniors (47.7%) in college, with the remaining 21.5% of participants being underclassmen, and 10.8% graduate school students (see Table 1.1). In terms of the racial distribution, almost half (47.7%) of the sample was Caucasian, while 12.3% was African American, 15.4% Asian American, 4.6% Asian, 1.5% Bi-racial, and 18.5% other (see Table1.2). The majority of participants who classified themselves as "other" were of Arab descent. In this sample, 98.5% of the participants were single, while only 1.5% were married (See Table 1.3). The sample was female-biased, where 66.2% of the respondents were female, and only 33.8% were male (see Table 1.4). The grade point averages (GPA) of the respondents were skewed towards the upper end of the spectrum, with the lowest reported GPA being a C, and the vast majority of GPAs (83.2%) fell in the A+ to B range (see Table 1.5). In terms of a family history of depression, 78.5% of the participants did not report any family history of depression, while 20% did (see Table 1.6). Lastly, in terms of actual depressed status based on the Beck Depression Inventory (BDI) cutoff scores, the vast majority of participants (73.8%) fell into the "not depressed" range, while 21.5% of participants fell into the mild to moderate depression range, and 4.6% fell into the moderate to severe depression range (see Table 1.7).
In order to measure depression in college students, the Beck Depression Inventory (BDI) was used. The BDI is a measure created to assess the severity of depression among both clinical and normal samples. The measure contains 21 items specifically designed to relate the experience of depression to 6 of the required symptoms for DSM-IV-TR diagnosis. Each item contains four statements that indicate increasing severity of an individual symptom of depression. The 21 items are measured on a 4 point scale ranging from 0 to 3. The range of scores possible is from 0 to 63, where higher scores indicate a greater severity of depression. For details on scoring, see appendix C. The clinical cutoff points can vary depending on the base characteristics of the individuals tested, but a general classification considers a total score of 0-9 to indicate no depression, 10-18 to indicate mild to moderate depression, 19-29 to indicate moderate to severe depression, and 30-63 to indicate severe depression. The BDI has been found to be reliable with internal consistency estimates ranging from .73 to .92 with a mean of .86 (Beck, Steer, & Garbin, 1988). The BDI has also been found to exhibit concurrent validity, and has been correlated with other measures of depression including the Hamilton Psychiatric Rating Scale for Depression (.73) and the MMPI Depression Scale (.76) (Groth-Marnat, 1990).
Perceived alienation and trust was measured by the Inventory of Parent and Peer Attachment (see appendix B), a scale that uses 28 parent items to measure the constructs of trust, alienation, and communication in parent-child relationships. The alienation subscale asks questions about anger and interpersonal isolation (ex. I don't get much attention at home), while the trust subscale seeks to find out about understanding and respect in the parent-child relationship (ex. My parents respect my feelings). In total, there are 12 items that correspond to parental trust and 12 items that correspond to parental alienation in the IPPA, with 3 items inversely overlapping between the two. The IPPA uses a 5 point Likert scale that ranges from almost never or never true to almost always or always true with corresponding numerical values ranging from 0 to 4. The 12 items on parental alienation yield a possible range of scores from 0 to 48, where a higher score indicates higher levels of perceived parental alienation. The 12 items that are included in the alienation subscale are items 6, 7, 9, 10, 11, 15, 17, 18, 19, 20, 23, and 24 on the IPPA (edited) measure used in this study (see appendix B). The 12 items on parental trust also yield a possible range of scores from 0 to 48, where higher scores indicate higher levels of perceived parental trust. Items 1, 2, 3, 4, 9, 12, 13, 16, 17, 19, 21, and 22 on the IPPA (edited) measure in the appendix were used to test perceived parental trust. For details on scoring, see appendix C. The test retest reliability for the IPPA has been found to be high, ranging from .86 to .93 over a three week period. The IPPA has also been found to correlate with measures of related constructs such as the cohesion subscale of the Family Environment Scale (a relationship of .56 was found), demonstrating high concurrent validity (Essau, 2004).
The next construct, perceived parental approval and pride in academic accomplishments, was measured by 4 self-designed, non-validated questions that were measured on a five point scale that corresponded to the scale used by the IPPA. These four items were embedded within the IPPA (edited), and were included as items 5, 8, 14, and 25 (see appendix). The items aimed to measure how much students believed that their parents were proud of them in terms of academics (ex. My parents are proud of my academic performance in college). The possible range of scores for parental pride and approval in academic accomplishments was 0 to 20, with higher scores indicating higher levels of perceived parental pride and approval in academics.
In order to recruit participants for this study, each of the five researchers, who were enrolled in an advanced psychology research lab at the University of Michigan, approached a minimum of twenty college students over the age of 18. Some of the participants were previously known to the researchers, while others lived or worked in close proximity to them. Due to the time limitations, the length of the questionnaire, and the lack of resources to pay for participant recruitment or incentives, the researchers decided to use a convenience sample. This sample was chosen based on proximity and affiliation, so as to allow participants ample time to complete the questionnaire while ensuring its timely return back to the researcher. Additional participants were obtained thorough snowballing, where the researchers requested the names and contact information of other individuals known to the initial participants who matched the criteria of the study and who would be willing to participate.
When recruiting participants, each researcher approached the participant either in person or through e-mail and asked if they were willing to fill out a 45 minute questionnaire on depression and associated characteristics. The researcher also told them that they would have 24 hours to complete the questionnaire and could return the completed questionnaire to the researcher at a convenient location or by e-mail.
For in-person recruitment, agreeable participants were given a brief introduction to the study, revealing the general areas that would be covered in the questionnaire. Participants' questions and concerns were then answered, after which the researcher asked the participants to read and sign the consent form (see appendix A). After the consent form was signed and collected, the researcher requested that the participant honestly answer the questions in a comfortable setting. Researchers then made an arrangement for the collection of the completed questionnaire. Questionnaire collection was done by arranging a time and place convenient to the participant to meet either later that day or the following day. The researcher then thanked the participant for his/her participation.
For e-mail recruitment, all initial introductory information was included in the email, along with additional reassurances on confidentiality, where participants were told not to write their names on the participant consent forms if they were going to be returned through email, and that their emails would be deleted immediately after their questionnaires were printed. Participants were instructed to either highlight or underline their responses and to email the questionnaire back to the researcher who contacted them within 24 hours. Participants were also notified that by replying to the email with an attached completed questionnaire, they were agreeing to all the terms put forth by the informed consent document, including voluntary participation and the confidentiality of the responses.
This study presented three main hypotheses. The first hypothesis, that perceived parental alienation would be positively correlated with depressed mood was supported by the data. This finding is largely consistent with previous literature which shows that perceived alienation scores on the IPPA are significantly higher for depressed individuals compared to non-depressed individuals (Essau, 2004; Pavlidis & McCauley, 2001). The present study used a correlation rather than an independent samples t test design (which was often used by previous studies) because the researchers had limited access to clinically depressed individuals. In this sample, only 17 individuals had scores which ranged in the mild to severe depression range according to the cutoff scores of the BDI, and 14 of these individuals were experiencing only mild depression. The small available sample would have likely made an independent samples t test design analysis produce statistically insignificant results. However, by using a Pearson correlation, the results were highly significant, with a correlation coefficient of r = 0.697. In terms of the percentage of variance, in this study, perceived parental alienation helps explain nearly 48.6% of the variance in respondent's scores on the Beck Depression Inventory.
The second hypothesis, that perceived parental trust would be negatively correlated with depressed mood was also strongly supported by the data. This finding is also consistent with the previous literature, which found that perceived parental trust scores were higher in individuals who were not depressed compared to those that were depressed (Essau, 2004; Pavlidis & McCauley, 2001). The correlation coefficient of r =-0.541 results in a percentage variance of 29.2%. This means that perceived parental trust helps explain 29.2% of the variance in responses on the BDI. Perceived parental alienation and trust were measured by the same instrument, the Inventory of Parent and Peer Attachment developed by Armsden and Greenberg (1987). The IPPA Parent Attachment score includes three subscales, parental alienation, parental trust, and parental communication. The former two were assessed in this study. Some previous literature has correlated overall attachment scores, including all three subscales, and found that higher parental attachment levels were associated with lower levels of depression. This finding is also consistent with the findings of the present study, even though the communication subscale was not used.
The final hypothesis, that perceived parental pride and approval in academic accomplishments would be negatively correlated with depressed mood was also supported, though to a lesser extent. The correlation coefficient of r =-0 .319 shows a medium rather than a large correlation. This is also evident when comparing Figures 1 and 2, which show the correlations of BDI score to parental alienation and trust respectively to Figure 3, which shows the correlation of BDI score to parental pride and approval in academic accomplishments. The points n Figure 3 are far more dispersed, and the pattern and direction of correlation are much more difficult to see than those in Figures 1 and 2. In terms of the percentage of variance, parental pride and approval in academic accomplishments only helps explain 10.1% of the variance in responses on the BDI. There is little prior research that has correlated these two variables directly, but conceptually, the relationship makes sense. Parents of college students are likely to place a moderate to heavy emphasis on academics, particularly in a reputable institution such as the University of Michigan. Not living up to parental expectations may then be related to experiencing higher levels of depressed mood.
Overall, this study contributes to the literature on parental attachment and depression by providing additional support for the relationship between the two variables. In addition, this study makes a theoretical contribution by analyzing the relationship between parental pride and approval in academics and depressed mood. It shows that, for college students, how parents perceive their academic competence may be important to their psychological health. However, the theoretical contributions of this study need to be discussed with caution. There were a great many limitations to the present study that largely reduce the validity and generalizability of the findings to any other population. These limitations span the entire research process, from conception to completion. The first limitation of the study involves the hypotheses. Barker, Pistrang and Elliott (2002) point out that research should be able to teach the researcher something new, not merely confirm relationships already known to the researcher. The hypotheses presented in this study were largely based on previous research or had strong conceptual and logical grounding. Thus, from the very start, the findings were already known, thereby violating the very intention of research -- to discover something new. Another major limitation of the hypotheses is that they were all correlational in nature, which means that causality could not be established from the study. In fact, in terms of the variables used in the study, the directionality of influence is a major area under question. It is possible that low parental attachment would lead to depression, and it is equally possible that depression could lead to low parental attachment as a result of emotional withdrawal (Essau, 2004). As such, the fact that directionality cannot be assessed by this study is a major limitation of the research.
In addition, another limitation of the study involved the sampling procedures and participants. Participants were obtained through non random convenience sampling and snowballing, which led to a biased sample. The sample was female biased, and included more upperclassmen than underclassmen. In addition, the racial distribution was largely unrepresentative as almost half the sample was Caucasian. Not only was the sample unrepresentative, the final sample obtained was also very small (n=65) due to limited time for data collection. Thus, due to the small, unrepresentative sample, the findings of this study have very low generalizability to any population other than the sample studied.
The measures used in this study were also somewhat flawed. The BDI is an established measure of depression, but has limitations in how well it corresponds to the DSM criteria for diagnosing depression. For example, the BDI only covers six of the nine criteria put forth by the DSM to diagnose depression (Groth-Marnat, 1990). The IPPA, which was used to measure parental trust and alienation, has been found to be a valid and reliable measure of attachment. However, in this study, only two of the three subscales were used, thereby calling the validity of the items used into question. In terms of the measure for parental pride and approval in academic accomplishments, the four self designed questions had no established validity or reliability, and no internal consistency analyses were conducted within the study. Consequently, the measure may not be reliable. In terms of validity, the items used appear to have face validity, but may not have content or construct validity. The issues that may diminish the validity of the self-constructed questions include not having adequate coverage of the construct being measured, poor, unspecific, or confusing question wording, and implicit premises guiding the construction of the questions.
Other limitations involved participant response issues, including acquiescence, where participants may have had a tendency to agree more than disagree with statements. This was partially combated by using a few item reversals. A more pressing concern may have been social desirability, where participants may not have wanted to seem depressed, or may not have wanted to reveal unhealthy family circumstances. Other participant response issues included participants not finishing the questionnaire, indicating the same answer for all items on a scale, and not reading the instructions. Not reading the instructions may have invalidated the results as some participants may not have used the correct time frame for responses which were mentioned in the instructions. A final limitation of the study was the lack of methodological pluralism, as only one scale was used to measure each construct. Ultimately, in spite of the limitations, the relationship between family and depression is important to understand as family is a central part of almost every individual's life. Knowing what interaction patterns are associated with depressed mood can help provide avenues for intervention research to target which may help improve the quality of life for many individuals.

Winona Laura Ryder (formerly Horowitz) is an Academy Award-nominated and Golden Globe-winning actress. She was born in 1971 in Olmsted County, Minnesota and was named after the nearby city of Winona (Wikipedia, 2007). She was born to author and mother Cindy Horowitz whose family was from Romania and father Michael Horowitz whose family immigrated to America from Russia (Wikipedia, 2007). Winona also had a younger brother Yuri, an older half-brother Jubal, and an older half-sister Sunyata (Wikipedia, 2007). Her father was an atheist and her mother a Buddhist (Tiscali Film Biographies, 2007). Ryder was brought up in a bohemian, intellectual, alternative lifestyle and her parents were friends with many notable people. Her middle name Laura was borrowed from her parents' good friend Laura Huxley, the wife of the famous Brave New World author Aldous Huxley (Wikipedia, 2007). Her godfather is psychedelic guru and former Harvard Professor Timothy Leary, and other family friends included beat poets Allen Ginsberg and Lawrence Ferlinghetti whose famous works were often on the subject of drug experiences (Tiscali Film Biographies, 2007).
When Winona Ryder was seven years old, her family relocated to a commune called Rainbow in northern California where they lived with seven other families on 300 acres of land (Tuscali Film Biographies, 2007). The remote property had no electricity or television, thus Ryder was an avid reader when she was a young child, especially relating to the novel The Catcher In The Rye (Wikipedia, 2007). However, her mother, who later became a producer of educational movies, often showed films for the children in the commune barn, sparking her daughter's interest in acting (Tiscali Film Biographies, 2007).
When Ryder was ten years old, her family moved to the Bay Area in northern California, where they enrolled her in public school (Wikipedia, 2007). In her first week at school, she was battered by fellow classmates who thought she was an effeminate, homosexual boy. Both as a result of that trauma and her difficulty acclimating after commune life, Ryder was home-schooled (Tiscali, 2007). During this period, her parents allowed her to enroll at the American Conservatory Theatre and she expressed her seriousness about a film career (Tiscali, 2007). Success came early for Winona Ryder as she took on many roles at a young age, including: Lydia in Beetlejuice, Veronica in Heathers, and Myra in Great Balls of Fire to name a few.
Ryder became romantically involved with Johnny Depp for three years and when the relationship came to an end, Ryder pulled out of her role in Godfather 3, citing exhaustion and illness brought on by overwork. Then because she was suffering from chronic insomnia, she booked herself into a psychiatric clinic for five days. After several more movies made, Ryder dropped out of the movie Lily and the Secret Planting in 2001 and visited Dr. Jules Lusman for chronic pain from scoliosis and an elbow injury from Mr. Deeds. Lusman, known for his dealing with celebrities and prescribed an opiate-based painkiller, had been under investigation for over-prescribing such drugs. In December, Ryder was caught shoplifting at Saks Fifth Avenue on Wilshire Boulevard for items valued at $5,560 after having used scissors to cut off the security tags. When she was searched, the police found she was carrying Demerol, Endocet, Vicodin, and Vicoprofen. She was sentenced with 480 hours of community service and three years' probation, and was ordered to pay $3,700 in fines and $6,355 in restitution (Tiscali, 2007).
I hypothesize that Winona Ryder has the psychological disorder of kleptomania. Kleptomania is a compulsive conduct disorder whose symptoms include recurrent failure to resist impulses to steal objects, which are not needed for personal use or for their monetary value (Wikipedia, 2007). The diagnostic criteria defined in the DSM-IV emphasizes that although individuals with kleptomania experience a sense of tension immediately before committing a theft, and experience pleasure, gratification, or relief afterwards, the stealing is not committed to express anger or vengeance, is not in response to a delusion or hallucination, and cannot be explained by another psychological disorder such as a manic episode (DSM, 1994). All of these symptoms are necessary for diagnosis.
The exact cause of kleptomania is unknown, but there are many theories and speculations. Because of the lack of psychological well-being (particularly mood disorders, substance abuse disorders, and eating disorders) in many kleptomaniacs, it has been theorized that stealing behavior is an attempt to self-treat underlying depression and/or driven by a desire to account for an actual or anticipated loss (Scott, 2003). It was obvious by the career consequences following the tumultuous breakup of Ryder with Depp that this loss affected her psyche. It was also indisputable that Ryder was battling drug abuse or possibly dependence at the time of the theft, which appears to be a risk factor. Also, psychoanalytic theory traces kleptomania to childhood neglect. Ryder's upbringing in the commune Rainbow may have left her with a sense of abandonment from her biological parents, which also may be an explanation for her proposed kleptomania.
The classic kleptomaniac has been described by Goldman (1991) as a 35-year old woman who has been caught several times for stealing things that she does not need and can easily afford. As a 30-year-old woman at the time of the incident, Winona Ryder fits this description rather well. This paper hypothesizes that Winona Ryder developed kleptomania and thus needless stealing behavior as a result of her predisposing cause of childhood neglect and precipitating causes of both the deterioration of her relationship and opiate abuse or dependence.
The research done for the purpose of this case study was all archival -- or using previously compiled information to answer research questions. The well-known shoplifting incident involving Winona Ryder in 2002 sparked my interest; however, I had to evaluate the difference between regular thefts and shoplifting with the psychological disorder. I first referenced kleptomania in general to explore the difference between shoplifting or theft and kleptomania. For these references, I used the following sources: Wikipedia, the Diagnostic and Statistical Manual-IV, C. Scott's article entitled Kleptomania (2003), and M.J. Goldman's article entitled Kleptomania: making sense of nonsense (1991). The Diagnostic and Statistical Manual-IV was particularly important in deciphering the diagnostic criteria necessary to diagnose kleptomania, which indicated that a large factor was the lack of need in stealing behavior. I then used the following references to obtain information about Winona Ryder's personal history and background: Wikipedia and Tiscali Film Biographies. Finally, I used Runyan's article (1981) in order to evaluate other interpretations of Ryder's stealing behavior. Further research is suggested in order to determine whether or not Winona Ryder is definitively afflicted with kleptomania.
The first aspect of the hypothesis to be evaluated is the predisposing factor of childhood neglect in the case of Winona Ryder. The effects of childhood neglect are considered to have long-term effects on psychological development. It is also true that co-morbidity in impulse control disorders under which kleptomania is classified. These disorders are classified by an individual's willingness to reap short-term benefits at the expense of long-term losses and are considered related to Obsessive Compulsive Disorders, likely due to the fact that kleptomaniacs experience recurrent thoughts paired with urges to steal (Wikipedia, 2007). Ryder's alternative upbringing at the Rainbow commune suggests that although she may have ultimately been cared for, the care was not necessarily by her biological parents and the style of upbringing may have been otherwise stressful. Ryder recalled later that in the commune "everyone walked around naked," and "It wasn't a nightmare, but it was no Utopia as a child, in northern California it gets really freezing in the winters. We had no electricity, no running water. Everyone was looking after everyone's else's kids and sometimes I just wanted my own family" (Tiscali, 2007). This quote suggests that she also may not have always been afforded necessities that most children in the United States have readily available.
This impulsive behavior is also consistent with the fact that Ryder was clearly dealing with a drug abuse or dependence problem at the time of the theft. It was reported that she had filled up to 37 prescriptions written by 20 doctors, using six different aliases, in a three-year period (Wikipedia, 2007). This information leads one to believe that there were many psychological issues that may have been co-morbid with her kleptomania -- a pattern that is typically seen with the disorder. A likely stressor that may have brought on depression in Ryder was her breakup with Depp. Ryder took the demise difficultly, allowing for it to interfere with her career (as she dropped out of Godfather Three), and also forcing her to check into a psychiatric clinic (citing insomnia). Unlike many cases of shoplifting, the stealing associated with kleptomania is not usually planned or motivated in the slightest by need or in some cases desire. In kleptomania, it seems that the act of stealing is more important than the items themselves. Because Winona Ryder was a successful actress who was making approximately three million dollars per movie at the time of the crime, it is obvious that need was not a factor in her decision to steal. In addition, approximately 80% of kleptomaniacs are women, making gender a risk factor for kleptomania.
Although we are unable to definitively determine how Winona Ryder felt immediately before and after committing the theft, we can presume that the tension/pleasure diagnostic criteria are plausible. It is definitive that she was unable to resist urges to steal as evidenced by the monetary extent of items stolen. Ryder's reaction to the incident also did not indicate that her stealing behavior was committed to express anger or vengeance, nor can it be explained by a manic or other psychotic episode. The defense claimed that the incident was a "misunderstanding" and that she was just carrying items between the store departments. This story ultimately failed and does not discount the possibility of a kleptomania diagnosis.
Because the diagnostic criterion for defining kleptomania is rather vague and has high generality potential, I would suggest more research on the topic of Winona Ryder. For example, a diagostic test could be given to Ryder. Diagnosis is typically done using the Structured Clinical Interview for Kleptomania (SCI-K), the semi-structured Minnesota Impulsive Disorders Interview (MIDI), or the Irresistible Impulse Test in legal proceedings (Scott, 2003). Archival research seemed to indicate that the Irresistible Impulse Test was not given to Ryder, although she faced severe criminal charges. Both Winona Ryder's predisposing and precipitating factors strongly point in the direction of the diagnosis of kleptomania, yet it is difficult to prove the hypothesis without utilizing further research.
Other explanations or interpretations are possible as well. For example, it could be that Ryder ingested so many intoxicating prescription opiates at the time of the incident that her judgment was altered, or her body control lost. A weakness of the diagnosis of kleptomania is that reliability and validity may be affected by the fact that kleptomania is often co-morbid with other mental health illnesses, including: mood disorders, anxiety disorders, eating disorders, obsessive-compulsive disorder, and alcohol and substance-abuse disorders. However, this creates a predicament because in order for kleptomania to be diagnosed, an individual's stealing behavior cannot be explained by another disorder. Runyan (1981) makes the point that for every action, there are many possible explanations of which possibly all are true, none are true, or some are true. For example, in this case, it is possible that the effect of opiate contributed to Ryder's stealing behavior, but did not necessarily cause it. There are always complicated interactions that determine decision-making and to ascribe explanations from an outside perspective is extremely difficult.
It is also possible that she did not lose control of her impulses, but expected that her celebrity status would skate her through any trouble that she may encounter following her actions. If Ryder's thefts were a conscious choice motivated by desire then her condition may not be kleptomania after all. In kleptomania, it seems that the act of stealing is more important than the items themselves. Persons with kleptomania have been known to leave stolen goods unopened, throw them away, donate them to charities, or return to the stores to apologize and atone for their behavior. In addition, because many kleptomaniacs know that their condition is out of their own control and is in conflict with their own self-image, they may call stores before they arrive to warn the clerks of their condition (Scott, 2003). It would be interesting to evaluate whether or not Ryder fits into this criteria or not, but it is impossible to know since she was caught in the act. The absence of this information is definitely a shortcoming of the case study.
Various treatment options exist for kleptomania, although it is generally viewed by professionals as one of the more persistent mental health illnesses. Treatment is generally designed to address underlying psychological issues that are contributing to stealing behavior. This may explain the absence of future stealing episodes by Ryder. For example, if she sought mental health help for her possible depression over Depp, it is possible that her symptoms would disappear as well. For example, medical records of Ryder from the perspective of archival research are unavailable, and thus it is impossible to tell if she sought help from Selective Serotonin Reuptake Inhibitors, for example. I feel as if Ryder's history and textbook predisposing and precipitating causes over-rules other possible explanations.

Gender identity development is one the most crucial milestones that a child attains. An individual's gender identity can be an important mediator of his or her life experiences and in his or her development of a self-concept. Thus, one's gender identity can shape whom one interacts with, whom one becomes friends with, what type of toys he plays with, what type of courses he takes in high school, and eventually what type of job he chooses (Noppe, n.d.). Past research has proven that gender identity development, especially the ability to label oneself as either "boy" or "girl," starts at a very young age. This ability sometimes starts as early as 18 months (Salkind, 2002). However, the most critical years are in early childhood, between the ages of two and six. It is during this time frame that play styles and behaviors are found to crystallize around the identity "boy" or "girl" (Salkind, 2002).
One of the most famous researchers in this subject area was Lawrence Kohlberg (1966), who viewed gender development as a three-stage process. As cited by Salkind (2002), Kohlberg said that children first learn their gender identity ("I am a girl"), then gain gender stability ("I will always be a girl"), and finally they will understand gender constancy ("even if I wore boxers, I will still be a girl"). Kohlberg stated that the final stage is achieved at around six years of age. Thus, one can say that the younger child's cognitive understanding of gender is not as complete as that of the older child (Thompson & Bentler, 1971). For instance, Miller (2007) states that young preschoolers generally fail gender constancy tasks, and that success on these tasks emerges around the ages of 4 and 7.
Using this idea of gender identity development and its three stages, we wanted to see if there is a difference in play in terms of gender between age groups. In other words, in early childhood, do older children prefer to interact with same-gender peers over cross-gender peers when compared to younger children? Or does this age difference not matter? Does the development of gender identity and the attainment of gender constancy influence whom one interacts with? Since research and theories have shown that gender identity develops and crystallizes by the age of 5, we hypothesized that the older children (5- to 6-year-olds) would spend less time interacting with their opposite-gender peers as compared to the younger children (2- to 3-year-olds). We also hypothesized that the older children (5- to 6-year-olds) would interact proportionately more with same-gender peers as compared to the younger children (2- to 3-year-olds), whose gender identity has not yet fully developed.
The participants for this study all attend the children's center on a university campus in a Midwestern town which serves approximately 180 children. The children's center is designed to serve as a resource of the university students for their research needs. University students who are interested in childhood development can use this setting for observation, participation, and research. The center aims to provide a consistent, safe, secure, stimulating, and enjoyable environment for the children and parents are made aware that it is used for numerous research studies. The children's center philosophy places emphasis on a play-based and "developmentally appropriate curriculum" and it facilitates social behavior which develops the children's self-concepts.  Additionally, the center's philosophy has a strong focus on social, cognitive, and physical development through large and small group activities. Since parents send their children to this center with the knowledge that their students will be participating in such observations and studies, we did not have to directly attain the informed consent of the parents. Since making our presence known would have induced an observer influence on the children's activity and levels of interaction, we also did not directly attain the assent of the children being observed. The data collected maintained the anonymity of each child observed. No names or identifying characters were noted. The children were not compensated for taking part in this observational study.
Twelve 2- to 3-year-olds and eight 5- to 6-year-olds were chosen using convenience sampling from their respective classrooms for observation based on availability. The participants were chosen so that there were equal representations of both genders in each group. Thus, there were six males and females in the 2- to 3-year-old group and four males and females in the 5- to 6-year-old group. The majority of the children in the center were White-Americans with a few Asian-Americans and Black-Americans. The socioeconomic status of group is presumed to be middle class to upper middle class since most of the children's parents work for the university.
The five researchers of this study started their observations by having a test day to check the reliability of their measures. All five researchers observed the same child for 7 minutes and 15 seconds (with 30 second intervals for observation and 15 seconds for coding). Using this information, we established that we had .85 inter-observer reliability. After this, we split up into two groups. The first group consisted of three researchers who each studied four 2- to 3-year-olds together. The second group consisted of two researchers who each studied four 5- to 6-year-olds together. We observed the students during classroom playtime hours (as specified in the school time schedule) in their naturalistic setting. We watched students either through a two-way mirror (window in the door) or inside the classroom, if invited in by the teacher. The observation was unobtrusive and followed all the guidelines of the center. We used a stop-watch to measure 30 second intervals of observation and 15 second intervals to record our data. Researchers coded for both verbal and physical interactions as well as the same-gender or opposite-gender interactions (0 = behavior absent, 1 = behavior was present). We spent a total of seven minutes and 15 seconds on each child. We then took 30 seconds to choose another child. Each researcher observed 2 male children and 2 female children. If working with other researchers, they split the children so that each child would not be observed more than once.
In each child that was observed, we measured and coded for two types of interactions: verbal communications and intentional physical contact. We operationalized verbal communication as any type of vocalization (e.g., speaking, whispering, laughing, screaming, singing, humming, sound imitations, sound effects, and crying). This category did not include involuntary sounds like sneezing and coughing. We operationalized intentional physical contact as touching, touching through an object (e.g., exchanging toys), and acts of aggression (e.g., pushing and slapping). It also included throwing an object (water included) at someone. However, this definition did not include unintentional physical contact like accidentally bumping against someone or accidentally tripping into someone. We coded if each behavior was present or absent (0 = no behavior present and 1 = behavior present) under two sublevels: same-gender and opposite-gender peers. Thus, for verbal communication, we coded if a child had verbal communication with a same-gender or an opposite-gender peer. For example, if a 5- to 6-year-old girl was whispering to another 5- to 6-year-old girl while holding her hand, we would code that there was same gender verbal and same gender physical interaction (1 for both) and no opposite gender verbal or opposite gender physical interaction (0 for both). Also, regardless of many times a behavior occurred in each of the 30 second time frames, it was still coded as just being present (1) or not (0). Thus, if the child touched another child six times in the span of 30 seconds, we still coded it as just 1 for physical interaction.
The purpose of our study was to see if older children in early childhood spent more time interacting with same-gender peers as opposed to cross-gender peers in comparison with younger children. We used mean, as a measure of central tendency, and range, as a measure of variability, to organize our results. As can be seen in Table 1 and Figure 1, we found that in contrast with our hypothesis, the older children (5- to 6-year-olds) had more interactions with opposite-gender peers ( 1.88 = mean of interactions per child for the older children and 1.0835 = mean number of interactions per child for the younger children). Additionally, we found that both 2- to 3-year-olds and 5- to 6-year-olds interacted more with their same-gender peers than opposite-gender peers (see Table 1). However, as mentioned before, 5- to 6-year-olds had significantly more opposite-gender interactions than the 2- to 3-year-olds.
For verbal communication, we found that there is a greater difference between same-gender and opposite-gender verbal interactions in 2- to 3-year-olds (see Table 2 and Figure 2). The 5- to 6-year-olds verbally communicated with their peers at approximately the same level regardless of gender (2.88 = mean number of same-gender interactions per child and 2.75 = mean number of opposite-gender interactions per child). The 2- to 3-year-olds, on the other hand, verbally communicated with their same-gender peers more than opposite-gender peers. 2- to 3-year-olds, on average, had .67 verbal communications with opposite-gender peers and 1.83 verbal communications with same-gender peers. Thus, 2- to 3-year-olds had a difference of 1.16 verbal communications per child between same-gender and opposite-gender peers, while 5- to 6-year-olds, only had a difference of 0.13 between same-gender and opposite-gender peers. In terms of variability, 2- to 3-year olds had a range of 6 for same-gender verbal communications per child and a range of 5 for opposite-gender verbal communications. On the other hand, 5- to 6-year-olds had a range of 9 for same-gender verbal communications and a range of 2 for opposite-gender verbal communications. Thus, 5- to 6-year-olds had a wider range of data for same-gender verbal communications than the 2- to 3-year-olds, while the 2- to 3-year-olds had a wider range of data for opposite-gender verbal communications than the 5- to 6-year-olds.
For intentional physical contact, results showed that overall 2- to 3-year-olds had more intentional physical contact than the 5- to 6-year-olds (see Table 2 and Figure 3). For instance, 2- to 3-year-olds had a mean of 2.33 physical interactions per child with a same-gender peer while they only had a mean of 1.5 physical interactions per child with an opposite-gender peer. Along the same lines, 5- to 6-year-olds had a 1.25 mean number of physical interactions per child with same-gender peers and only 1 mean number of physical interactions per child with opposite-gender peers. Additionally, both age groups had more intentional physical contact with same-gender peers than opposite-gender peers. As can be seen in Figure 3, the difference between intentional physical contact between same-gender and opposite-gender peers for the 2- to 3-year-olds (.83 interactions per child) is greater than the difference for the 5- to 6-year-olds (.25 interactions per child). In terms of variability, 2- to 3-year olds had a range of 5 for both same-gender and opposite-gender physical interactions. On the other hand, 5- to 6-year-olds had a range of 2 for same-gender physical interaction and a range of 5 for opposite-gender physical interaction. Thus, 2- to 3-year-olds had a wider range of data for same-gender physical interactions than the 5- to 6-year-olds, but both had the same amount of data dispersion for opposite-gender physical interactions.
The purpose of our study was to see if older children in early childhood spent more time interacting with same-gender peers as opposed to cross-gender peers in comparison with younger children. Our results were both consistent and inconsistent with our hypothesis. The results were consistent because older children showed slightly fewer physical and verbal interactions with opposite-gender peers than with same-gender peers. However, our results were inconsistent because younger children interacted more with their same-gender peers than opposite-gender peers in both types of interaction when compared to the older children. Salkind (2002) found that play styles and behaviors crystallize around a child's gender identity between the critical ages of two and six and our results matched his findings because both age groups seemed to interact more with their same-gender peers than their opposite-gender peers. This could mean that the gender-identity is crystallizing from the ages of two to six. However, our results show that there might not be a linear progression of a more crystallization of gender-identity as a child gets older since the younger children interacted more with same-gender peers than the older children. Thus, unlike the findings of Kohlberg (1966), Thompson and Bentler (1971), and Miller (2007), all of whom stated that the cognitive understanding of gender and gender constancy is more finalized at six years than at two years, our results showed that a child's gender-identity is not more developed at 6 years than it is at 2 years.
Our study also had numerous limitations. For instance, some of the children did not remain within eyesight at all times of observation. Consequently, if a child had been interacting more with either the same-gender or opposite-gender peer, then we could not code for it since we could not see it. This could have clearly skewed our results because these children could have been partaking in behaviors that would have supported our hypothesis while they were hidden (e.g., 5- to 6-year olds interacting with same-gender peers). Also, since we could not hear their conversations, sometimes it was difficult to decipher if the child was verbally communicating with the teacher, another peer, or themselves. Thus, if the child had been verbally communicating with a peer, we might not have coded for it since we probably assumed they were communicating with the teacher instead. Thus, the children could have had more same-gender or opposite-gender verbal communications than was coded for. Also, because not all of the children were present at the time of observation, there was a skewed ratio of boys- to-girls in the classroom, which may have influenced our results. In such an environment, a child did not have the equal opportunity to interact with same-gender and opposite-gender peers. Had the classroom had an equal ratio of boys and girls, then our results might have been different because then the child would have been more likely to encounter both genders. For instance, if the 2- to 3-year-olds had more boys in their classroom then girls, then the girls would have had a higher probability of interacting with a boy. Also, due to the skewed ratio of boys and girls, the same child could have been observed by multiple researchers, which would have influenced our results. If there were only two girls present in a classroom, then the researchers would have all been observing the same child. And it could have just been that these two children were more or less inclined to interact with opposite-gender peers, and thus the results would not be representative of a larger population. There was also observer bias. Because we knew that we were looking for interactions with peers, we may have inadvertently chosen children who were more active, thus biasing our results. Similarly, we may have looked for specific behaviors without realizing it, which would have also biased our results.
Thus, based on our study, it is evident that younger children do have a sense of gender identity since younger children did interact more with their same-gender peers than opposite-gender peers. Parents, educations, and child-care professionals should also be more cognizant of the fact that these younger children do have a developed sense of gender-identity when interacting and communicating with children. For instance, educators could use this information in their lesson-planning to form same-gender groups for younger children to ensure their comfort or to form more opposite-gender groups to encourage interaction between genders depending on their classroom goals and objectives.
Our research also produced some additional and unforeseen results. We found, for instance, that younger children (2- to-3 year olds) interacted more through physical interaction, and that older children (5- to-6 year olds) interacted more through verbal communication. These results could be the product of the fact that children have attained a higher degree of verbal development at 6 than they have at 2. Thus, children could be using the physical and verbal capabilities that they have while interacting. For a future study, we think it would interesting to look into these results and test to see if a child's physical and verbal developmental level has an impact on the type of interactions he engages in. It would additionally be interesting to see if these physical and verbal capabilities have an affect on gender-interactions (same-gender and opposite-gender). Moreover, because of the high number of limitations of our study design, we would recommend that the study be repeated before the results are generalized. If this study were to be repeated, we would recommend that the researchers observe children at more than one center, observe classrooms with equal gender distribution, ensure random selection (by using either single-blind or double-blind studies), and make sure that the children can be observed in all areas of the classroom.

Essential hypertension (HTN) is a "silent killer" that affects 60 million adults in the United States causing arteriosclerosis, cerebrovascular accidents, myocardial infarctions, and end-stage renal disease (Hildreth & Saunders, 1992). The cause of hypertension is unknown (Williams, 1992) and current epidemiological studies indicate that it is more prevalent in African-Americans than all other racial groups (Burt et al., 1995). Moreover, African-American women (AAW) have the greatest gender differential in the incidence of uncontrolled HTN (60% for AAW and 49% for Caucasian women). HTN can be successfully controlled with prescription medication (Kaplan, 1995) and with the DASH diet (Appel, 1999; Appel et al., 1997; Sacks et al., 1995; Scisney-Matlock, Kachorek, Glazewski, in press). Unfortunately, compliance with medical regimes is extremely difficult for at least half of all individuals with the diagnosis of HTN (Branche Jr., Batts, Dowdy, Field, & Francis, 1991). The purpose of this project was to examine how explanatory style (ES) predicts compliance with antihypertensive medication regimens and with other general health promoting behaviors in both African-American and Caucasian women. Additionally, this paper will examine the link between ES and the central nervous system, specifically, blood pressure (BP) as it changes over time.
Attribution theory, first introduced by Fritz Heider (1958) and popularized by Jones and Davis (1965) and Kelley (1967, 1972), posits that people rationalize why events happen to themselves or others. The function of this is to maintain a sort of psychological homeostasis. When we realize that new information is incongruent with what we have come to accept and internalize through past experience, we try to interpret the events surrounding this information or condition in order to arrive at an appropriate explanation for what we're feeling (Schachter & Singer, 1962). Explanatory style, coined by Abramson et al. (1978), and redefined by Peterson and Seligman (1984), is the verbal language people use when trying to mentally organize and make sense of (what Piaget would call "assimilation") new information or a disturbing condition, such as chronic health problems like HTN. ES can be broken down into two different "styles," if you will -- pessimistic and optimistic. These two ways of thinking about the world can be empirically measured via an assessment tool called the Attributional Style Questionnaire, or ASQ.
The ASQ has been used in numerous experimental studies (Seligman, Abramson, Semmel, and von Baeyer 1979; Peterson, Semmel, von Baeyer, Abramson, Metalsky, and Seligman 1982). It is a means through which researchers can examine how people react to, behave in, and understand or interpret various hypothetical situations, ranging from failure on an academic test to being fired from a job. It presents subjects with twelve events (6 bad and 6 good), in which they are asked to rate the cause of each event on a 7-point Lickert scale across three different dimensions. The causes of events can be explained in 1) internal or external terms, as in "I got second place in the race because I wasn't sufficiently trained to compete" (internal cause for failure) versus, "I got second place in the race because the wind was blowing very hard" (external cause for failure). Reasons for events can also be verbalized and internalized as 2) stable or unstable, as in "Because I got second place this time, I will get second place in the next race too" (failure and success are stable), versus, "Although I got second place this time, that doesn't necessarily mean that is how I will do next time" (failure and successes are unstable, or variable). The last way events can be explained is through 3) global or specific terms. An example of this perspective would be a statement such as "I never get better than second place in any running event" (global failure -- it extends into other domains), versus, "I haven't got better than second place in this particular race" (in which failure to do better is limited to a particular event or set of circumstances).
Ratings are averaged across the good and bad events separately. A composite score is derived by combining all scores from the three aforementioned dimensions. Internal consistencies of the composite scores have alpha coefficients of .70 or more. Initial test-re-test reliabilities were quite high over several month periods (R's .60 and higher), (Peterson, Maier, Seligman, 1993). Since then, reliabilities have been bolstered (.70 to .85 range) by increasing the number of events for which causes are offered (Peterson and Villanova, 1988). Potential biases due to answering questions in a "desirable" or particular fashion were ruled out by Schulman, Seligman, and Amsterdam (1987) who found that even subjects given explicit instruction in what the test was trying to measure could not produce these answers, suggesting that the measure is not transparent. Lastly, meta-analysis and the application of ES to depression models have shown that different types of people, at least as studied in the 132 experiments, are not differentially susceptible to tests of laboratory helplessness. If there is a group that shows more helplessness outside the laboratory, perhaps it is "because these people have experienced more uncontrollability" in their life's events, or, simply because of their ES, as opposed to being differentially sensitive to testing effects (Peterson, Maier, Seligman, 1993, p.108; see also Sweeney, Anderson, and Bailey, 1986 for meta-analysis of relationship of ES to depression).
Understanding people's explanatory or attributional processes is especially important in health behavior for numerous reasons. First, people are especially motivated to seek causal attributions under periods of high uncertainty (Gerard & Rabbie, 1961), such as when medical information is unclear, or when physiological symptoms are unfamiliar or difficult to evaluate objectively. Second, illness itself causes general distress and/or inconvenience. People might draw "inaccurate or distressing inferences" about what is wrong with them when they feel poorly. These "misattributions, in turn, lead to inappropriate action or inaction" (Stone, Cohen, & Adler, p. 489). Third, people might put off medical attention, other personal responsibilities, and/or other people close to them because they do not want to let surface or make obvious physical symptoms that they are ashamed of. This could result in both less social support in the face of illness, and/or delay medical treatment. These are some of the many possible reasons for poor health quality-of-life and how attributions (ES) or perceptions about causality are directly linked to health practices. But before scientists can examine the mechanisms through which this occurs (via enhancing knowledge, changing cognitive representations, misattributions, et cetera), the effect of ES on outcome variables such as BP and compliance must be established. This is the goal of this paper. Possible mechanisms for this link will be addressed in subsequent publications.
Inducing people to adhere to public health recommendations purports to put the patients in a submissive position relative to health professionals, but this is not really the case. Patients are active participants in the process of seeking medical care; patients have the choice of deciding whether or not to continue with and keep medical appointments, deciding whether or not to accept what professionals have suggested to them, actually enacting those suggested changes, and consciously deciding everyday whether or not behaviors will be (continuously) carried out at the appropriate hours. Engaging patients in their own health treatment is of utmost importance because managing illness lies in prevention -- personal commitment to lifelong self-care activities and medical care when necessary. It becomes especially difficult at times because deciding to partake in "healthy behaviors," may entail a temporary increase in deprivations or physical suffering. In addition, they can negatively affect people's general sense of well-being and quality of life (Stone, Cohen, & Adler, 1979; Anderson, Hogan, Appel, Rosen, & Shumaker, 1997).
In deciding what course of behavioral action to take (or not take for that matter), people are essentially deciding whether or not they want to succumb to their illness and the course of the disease, or if they want to speed up recovery and limit/prolong further physiological deterioration. These thought processes are captured by the ASQ because this assessment tool gives scientists a realistic picture of what people's perceptions about the chronicity of problems and their pervasiveness really are like in everyday situations. People who view their personal behaviors (preventative health behaviors, compliance behaviors with medical regimes, et cetera) as unimportant tend to have explanatory styles which encompass the dimensions of internality, stability, and globality. Women with hypertension and this pessimistic ES make statements such as, "I have brought this high blood pressure upon myself, and it isn't going to go away so I might as well plan on having it affect the rest of my body -- I guess I can plan on a heartattack soon." People who explain negative events (such as diagnosis with hypertension) and their failures to produce desired lifestyle changes in this manner (in this pessimistic style) also tend to do so in the future, despite their inclination and/or knowledge that such thinking and such acts are harmful to their physical health.
For instance, Wiliams and Brewin (1984) studied people's reactions to test failure and found that people who explained their failure with stable terms reduced their expectancy for future success at the test. Hence, this measure has been used as a means of operationalizing persistence. In this particular case, subjects' pretest expectancy for and incentive for success was either impeded by or facilitated by their attributional style. Concrete evidence of the effects of attributional style on performance of particular behaviors have also been researched by Peterson & Bossio (1991), Nolen-Hoeksema, Girgus, and Seligman (1986), Steele and Southwick (1981), and Anderson (1983). It has been also examined in regards to coping with stress (Major, Mueller, and Hildenbrandt 1985), and in regards to reactions to physical injury or disease (Abrams and Finesinger, 1953). In short, peoples with an optimistic explanatory style have been shown to fall ill less frequently, survive longer when confronted with terminal illness, recover from illness more successfully, engage in less risk taking behaviors less frequently, are less likely to die a sudden and accidental death, and deal with stress more effectively.
These diverse research findings all converge in their finding that a pessimistic ES is related to poor health, depression, and what has been coined "learned helplessness", or general passivity. When people believe that their efforts are useless, they are apt to fail to comply with recommendations made to them by both concerned friends and family as well as healthcare professionals such as doctors and nurses (Janis and Rodin, 1979). Thus, whereas most patients are active recipients of health membership and care, people with a pessimistic ES are not at all involved with decision making for healthcare because they believe that their efforts go unrewarded or are simply ineffective.
A "key concept in the reformulated theory (of learned helplessness) is..the habitual tendency to offer the same sorts of explanations for diverse bad events" (Peterson, Maier, Seligman (1993). Such maladaptive perceptions, such as the thought that bad events (be it diagnoses, poor prognosis for disease improvement, an adverse reaction to a medication, et cetera) will undermine and effect all preventative behaviors (eating healthy, taking medication regularly, keeping appointments with health care physicians) will inevitably affect the course of disease because people won't take precautionary measure requisite to better maintenance of their respective disorders. In the case of hypertension specifically, not taking blood pressure measurements regularly, not adhering to instructions for prescription medications, and not eating a low-fat, low-salt diet can exacerbate the disorder, leading to uncontrolled BP and various medical complications. Therefore, effective cognitive-behavioral therapies which will encourage behavioral change are solely needed to demonstrate to these individuals that preventative and maintenance health care is effective at enhancing the quality of physical life.
Never before have researchers examined how attribution theory might explain perceptions about the causes of hypertension and high blood pressure reduction behaviors. This paper will concentrate on how an optimistic ES leads to active engagement in preventative health behaviors, thereby increasing compliance with medical regimes and having and overall lower BP range. The active participation of people with an optimistic ES would also support the notion that persons with a pessimistic ES are not engaging in healthy behaviors because of the passive or helpless nature of this cognitive style of thought. These goals were explored with the following three hypotheses in mind:
H1: Women who attribute the cause of their hypertension to global, stable, and internal causes (pessimistic ES) will be those who consistently have higher blood pressure and are in the high blood pressure classification category (Systolic BP >160 mmHg, and/or diastolic BP >100 mmHg).
H2: Women with an optimistic explanatory style will be more likely to comply with medical regimes such as taking their antihypertensive medication regularly.
H3: Women with an optimistic ES will have higher scores on their health promoting lifestyles scales, suggesting that they engage in more general preventative self-care behaviors, as compared to women with pessimistic ES, across all four group assignments.
Approximately 200 women with Stage I or Stage II HTN, that is, two SBP 140-159 mmHg and DBP 90-109 mmHg at least two weeks apart were recruited (National Institutes of Health, JNC-V, 1993). Patients with uncontrolled hypertension were the target population for this study because patients with higher BP are at greater risk for cardiovascular accidents. They are also an important population to target because Burt and associates (1995) reported that 36% of patients treated for HTN have BP above the 160-95 mmHg threshold, with only 29% being able to maintain BP below the 140/90 threshold. These women were recruited from a large Midwestern University hospital. While HTN is diagnosed equally in both men and women, the wider disparity in the control rate between AAW and WAW (White American Women) compared to men of any racial group suggests that a sample limited to women is necessary if health care professionals are to better understand why this large discrepancy exists. Additionally, AAW ages 34-54 have the highest incidence of uncontrolled HTN (Burt et al., 1995), so it was imperative that the sample be at least half AAW since they suffer disproportionately and die prematurely from the effects of uncontrolled HTN. A stratified sampling approach with Solomon-four groups was used to diminish potential threats to validity of the effects of the intervention from exposure to survey instruments. See Figure 1 below.
These women ranged from ages 30 through 65 years of age. Hypertension is rarely diagnosed in persons under thirty, and for persons over age 65 there is often a secondary diagnosis or medical complication which might be the result of hypertension, or have caused hypertension. This study aims to address issues pertinent only to essential hypertension, not hypertension due to any number of secondary causes (e.g. end-stage renal disease, stroke, or insulin-dependent diabetes).
At time one, women in both of the experimental groups (Groups 1 and 2) were given MEMS© bottles and had their function explained to them. MEMS bottles are plastic containers which people are instructed to place their medications in. (These women were already taking antihypertensive medications prescribed by their primary health caretakers.) Each time the lid is removed, a microchip recorded the date and time at which the pill was taken. The participants utilized the MEMS bottles to take their daily dose(s) of antihypertensive medication for the next thirty days (until time two). They answered questionnaires at this time as well as at the end of the study (time three), ninety days later.
Additionally, women in experimental group one were given cognitive messages designed to increase their knowledge about HTN and the need to take medications as prescribed. These messages were developed so that participants would have a mechanism through which they could "use their own reflections (about their diagnosis of HTN) to appraise, evaluate, and internalize 'how' -- perceptions, preferences, and possibilities -- and 'what' -- beliefs, barriers, and intentions -- a person thinks about specific self-care activities associated with prescribed medication" (Scisney-Matlock, 1995). Content for the messages was developed from interviews, written responses to questions from survey research, and included a comprehensive literature review. The 18 goals that the cognitive messages encompassed were displayed in a wheel-shaped form, so that the messages rotated in and out of a window, one by one, so that the content of only a single message could be viewed and read at a time. (See facsimile WHEEL attached). The WHEEL provided a means through which women could learn about and be exposed to different information relevant to HTN. One dimension aimed to increase general "knowledge" about HTN, another to enhance "skills" to live with it, and the third and last dimension was labeled "attitude," designed with the intent of shaping women's perceptions about having to comply with medical regimes.
Thus, each of the three WHEELS had the same 18 goals on each (9 on each side), but addressed three different aspects of each and every goal: knowledge, attitude, or skill. Women were instructed to read at least one tailored message once a day, and to respond (via a pager provided for them) whether or not they did so by the end of the day. Having the subjects respond via pager not only verified the performance of the intervention activities, but it also provided immediate reinforcement to strengthen the importance of compliance with medical regimens. Subjects were allowed to keep these tailored cognitive messages for the duration of the study, but returned the MEMS bottles after one month of use.
Women in both of the control groups (groups 3 and 4) did not receive the MEMS bottles or cognitive WHEELS messages. They did not have any responsibilities outside of their coming to the clinic to fill out questionnaires as indicated above in Figure 1. Members of all groups however, had their BPs measured on each of the three occasions so that changes in BP could be assessed across time and across group assignment.
Again, the goal of this paper was not to determine how race, knowledge, or cognitive messages are related to ES or BP even though measures of these constructs/variables were included in the design of this study. These will be the topics of other articles. Here we set forth to determine how ES (both pessimistic and optimistic style) mediate preventative health practices and high blood pressure specifically.
Attending physicians at the outpatient internal medicine and general medicine clinics referred possible candidates to the researcher. All candidates were generally of sound mental health and able to read and write English. A protocol was read verbatim to the potential subjects, detailing what their involvement would include. After reading and signing the consent document, each subject was randomized into one of the four groups by randomly selecting from a large pile of numbered envelopes. Subjects who withdrew from the study were replaced. Any subjects who met the criteria displayed in Figure 2 below were eligible for participation:
Demographic data were collected on individual characteristics such as age, race, marital status, menopausal state, support for hypertension regimes (meal preparation and acquisition of medications), educational level, occupation, income, and employment status. Medical information was extracted from lab reports and medical files to determine history of concurrent medical conditions and medications taken. Other measures utilized in this study included health promotion lifestyle profile measures (Walker, Sechrist, & Pender, 1987), and the Explanatory (or Attributional) Style Questionnaire discussed in the Introduction section. MEMS© bottles were used to quantitatively assess whether or not participants were actually taking antihypertensive medications as prescribed.
Histograms, box plots, and descriptive statistics were be used to identify aberrant data points, and to characterize the distributions of the measures and the four groups to which participants were assigned to. There was little missing data, so simple methods of mean substitution and complete case analyses led to equivalent results. T-tests for independent samples were conducted to make sure that the randomization produced equivalent groups.
To address the first hypothesis, planned contrasts (ANOVA) were conducted to compute BP changes from Time 1 to Time 2, and again at the end of the study. It was hypothesized that women with an optimistic explanatory style would have lower BPs at both the beginning of the study and throughout its duration, regardless of group assignment.
To test the second hypothesis, one-way repeated measures ANOVAs were run to determine if explanatory style mediated whether or not medications were taken by participants in experimental groups 1 and 2, and to determine at which times it did so.
To assess the accuracy of the third hypothesis, that type of explanatory style would significantly effect health promoting lifestyle behaviors (as assessed by the questionnaire), repeated measures ANOVAs, 1) total score, and 2) explanatory style type were used as a between-subjects factor and time of measurement (pretest, one month post, and three months post) as a within-subjects factor was used to assess the impact of the intervention on explanatory style and on health promoting behaviors.
Age, race, education, and marital status were not included as covariates in any of the aforementioned analyses because they weren't found to predict changes in BP or explanatory style, and therefore were not regarded as confounding factors throughout the duration of the analyses.
In running the analyses for the first hypothesis, results were as anticipated. Namely, women with pessimistic explanatory styles had higher BPs than women with optimistic explanatory styles both at the beginning of the study and at its very end. See Graph 1 below to examine changes in mean Systolic BPs and Diastolic BPs for women with optimistic and pessimistic explanatory styles across time. Graph 2 below depicts changes in BP by group assignment.
It appears as though it is very adaptive to view failures to engage in preventative and health maintenance behaviors as specific, external, and unstable. When failure to achieve a desired goal or result occurs, this way of explaining the "failure" does not appear to thwart future attempts to do so. Therefore, optimistic ES is adaptive in that it results in increased compliance relative to people who view their illness and their failure to comply with medical regimes/ to engage in health preventative behaviors in global, internal, and stable terms. The implications for this are great. Explanatory style is not just a way of thinking about or explaining events. It is because of the way that one's thoughts are organized and generalized that actions are literally carried out (or not). Thus, even if someone understands or has the knowledge that s/he should do something, that thought will not necessarily be translated into behavior until that person believes that their effort will be fruitful/effective.
The fact that women with a pessimistic ES have higher BPs than women with optimistic ES means that women with pessimistic ES can anticipate having more cardiovascular accidents, strokes, and even premature death than they should perhaps be at risk for. Their way of thinking might ultimately cost them their life because they can not convince themselves of the utility of preventative and health maintenance behaviors which have been established to prolong life.
The CBI implemented in this study was effective at lowering BP to all women who read the cognitive messages, suggesting that even women with pessimistic ES can benefit from reading the messages. Certainly, because they engaged in more health relevant behaviors than they were before having read the tailored messages, they can expect more benefits than those women who did not read the messages (regardless of explanatory style type).
Awareness of people's ES type will cue physicians into types of unhealthy behaviors that can be anticipated, so that professionals can refer patients to alternative sources of support (nurses, psychologists). These social support providers can work with patients to reinforce medical regimes prescribed by primary caretakers. In the case of hypertension specifically, this might include the distribution of the cognitive WHEELS messages. Ultimately, this tailoring of health services to individual needs will lead to enhanced quality of life in patients because of their improvement in and maintenance of symptoms of disease. This will also be of great benefit to medical and health professionals because as a result of symptom management and illness prevention, they can expect fewer hospitalizations and medical complications/development of new comorbid disorders (be it depression, diabetes, or stroke) due to lack of active participation in the health care process.
It appears as though explanatory style is an important component when trying to understand, predict, and enhance compliance with medical regimes. Because a negative explanatory style actually affects people physiologically in negative ways (e.g. keeping blood pressure at elevated levels despite attempts at intervention), it is important that health care officials are able to know who these people might be so that they can be given extra medical attention and counseling on how to change their explanatory style to be more optimistic. The CBI was effective at helping women to remember to take their medications regularly, and therefore this intervention provides hope/suggests a promising means by which people with pessimistic explanatory styles might be exposed to a new and healthier way of looking at the management of their disease. Such interventions might be tailored to other diseases and health complications in the near future to help facilitate compliance with any number and type of medical regimens.

Students of environmental conservation are well aware of the canon, "Think Globally, Act Locally," but to what extent is the global aspect actually realized? People should be more inclined to act locally more than globally, but what causes an individual to think globally? In other words, how far (literally) do our own environmental values span? Using and adding to research on the psychology of choice, and the effects of framing and protected values, this experiment tests the scope of people's environmental concerns. Here, I test how international versus national environmental issues effect people's choices.
Tversky and Kahneman (1981: 453) argue that rational decision-makers will prefer prospects that offer the "highest expected utility." However, this rationality may be thwarted when prospects are framed differentially, in terms of gains and losses. When prospects are framed in terms of gains, the chosen option is more likely to be risk averse. In contrast, when prospects are framed in terms of losses, the chosen option is more likely to be risk-taking. Tversky and Kahneman (1981) argue that people do not simply become irrational, but rather they make bounded decisions based on the most readily available, although limited, information.
Protected values may confound these framing effects. Protected values are firmly held ethical beliefs about one's duties and rights as a human being. Such values shape individual behavior because people are concerned not only with the consequences of the action, but also with the very nature of their own participation (Baron & Spranca, 1997). People with protected values cannot trade their personal values for others, and if they do, they are unhappy with their decision. Baron and Spranca (1997) suggest that protected values prohibit certain actions. Tanner and Medin (2004), in contrast, argue that protected values influence people to act, out of moral obligation; active moral obligation is stronger than moral prohibition of actions.
Tanner and Medin qualify their argument by suggesting that, at least in terms of environmental values and behavior, "prohibitions against action may be salient when the act can be seen as causing harm. In contrast, obligation to act may be more salient when the act can be seen as promoting something good" (in press: 4). Thus, there may be a relationship between the framing of the action prospects/options, such that harm (negativity) induces omission, while good actions (positivity) induces action. This argument holds, however, only when protected values are present and strong. Tanner and Medin suggest that when protected values are lacking, or even weakly present, tendencies are, as Tversky and Kahneman (1981) argue, malleable such that the way a prospect is framed may sway one's answer.
This experiment, then, will test an individual's reaction to different types of framed national and international environmental conservation situations. Several predictions are made, based on the above research.
1) If protected values do not exist, participant choices will be influenced by the way in which the scenario prospects are framed:
2) If protected values do exist, participant choices will be less influenced by the way in which the scenario prospects are framed:
This experiment was conducted through the use of e-mail surveys. The surveys were not anonymous, as respondents sent their responses to my e-mail address. I received 236 responses from people of various backgrounds and educational interests/career paths, and from across the United States.
To establish the presence/absence and intensity of protected values, participants were given a set of three national environmental statements and three international environmental statements. For each statement they chose one of three opinions:
Selecting option c is evidence of an environmental protected value. The statements were
as follows:
Immediately after selecting their opinions about the environmental statements, respondents proceeded to the environmental scenario section. Half of the participants (Group A) received scenarios with only negatively-framed prospects, and half of the participants (Group B) received scenarios with only positively-framed prospects. All participants received the same 3 international scenarios and 3 national scenarios. Both types had one scenario based on each of the following environmental issues: non charismatic endangered species, charismatic endangered species, and forest resources. To reduce bias, the actual survey did not denote the framing, scope, or categorical nature of the question (e.g. "international, forest resources, positive frame" will not be mentioned).
For this experiment, the framing (positive/negative), scope (national/international) and category (environmental issue) of each scenario are the independent variables, and the choice made by the respondent -- the effect of the framing, scope and category -- is the independent variable. This paper describes only the results of framing and scope, but addresses environmental issues in the discussion.
You were born and raised in the Midwest. It rains a lot in there, and most people have lush lawns and gardens that almost take care of themselves. Your fondest childhood memories are of you and your friends rolling down the grassy hill in your back yard, picking cherries and plums from your fruits trees, and smelling the roses and lilies that bloomed for months right outside your front door. Upon getting a new job, you move to Arizona, where the climate is a bit different from where you grew up. People have rocks instead of lawns and cacti instead of roses! You long for the beauty and comfort of a grassy landscape in front of your home, but you know that Arizona is very arid and grass is not a natural landscape there.
As a member of the city council, you must decide whether or not to begin constructing a new business plaza in town. The site slated for development, and you know that the businesses will bring in more employment and much needed revenue for the city.  However, is a remnant forest grove, with both historical and ecological value. The city is voting on which construction agency to hire.
After 35 years on the job, you have finally retired. You and your spouse are planning on moving to your mountain cabin to enjoy canoeing, bird-watching, and watching movies on your big screen television. Since you bought the cabin 10 years ago, however, the road leading up to your cabin is becoming more and more congested. Elk, moose, and grizzly bears are not only being hit by cars, but they are also losing their breeding and migratory habitat to new cabin and road construction. The county is considering have the inner-mountain roads removed so that forest habitat can re-grow. There are two plans from which the county is choosing from to solve this problem.
While on a kayaking trip in Baja California, Mexico, your tour group unexpectedly comes upon a school of endangered fish. The fish are endemic, sensitive to water fluctuation, and do not migrate out of the peninsula to spawn, or for any other reason, for that matter. The fish are small and barely visible from the top of the water, but your guide tells you that they are spawning all over the area. You and your group must decide which way to go in order to minimize disrupting the fish with the boats and paddles. Which path do you decide to take?
Although you have been diligently saving money for an upcoming backpacking trip through Europe, after watching a documentary on PBS about chimpanzees, you have begun looking for an organization to donate money to for the species' cause. You talk to a friend, who recommends 2 organizations, each with varying degrees of reputation.
You have just moved in to your new home and are shopping for new furniture. You find a store that is having a sale on beautiful wooden dining room set from Asia. The colors and style match your kitchenware perfectly. The price is far below what you want to spend, but you know that the wood is endangered and comes from threatened habitat, home to many endangered plants and animals.
After the scenarios are completed, participants will be asked to justify their decisions, through a single explanatory question: Why did you choose the option you chose?
Table 1 shows the percentage of respondents holding protected values which are national in scope, and those which are international in scope. The average percent of respondents which held at least one national protected value was 33%; the average for international protected values was 47%. This suggests that the scope of protected values can expand well beyond national borders. While not tested against actual behavior, it is likely that protected values are higher for international environmental issues simply because it is easier to "care" about issues which are remote from one's one life.
Table 2 shows the percentage of respondents, independent of protected values who selected option 1, 2, or 0 (zero signifies surveys where respondents refused to choose an option; this will be discussed in the justifications section). For 5 of the 6 positively framed scenarios, and 4 of the 6 negatively framed scenarios, respondents chose the predicted options, 1 (risk averse)and 2(risk taking), respectively.
In the analysis of the data, I categorized protected values into three groups:
For respondents with no or low protected values, results were the same for all three categories (TABLE 3):
As predicted, a framing effect was found for respondents with low or no protected values: for 4 out of 6 negatively framed scenarios, there were more respondents who chose option 2, the risk-taking choice. The framing effect was absent for scenario 3 (national) and scenario 5 (international). The framing effect was not stronger in national scenarios than in international scenarios, as was predicted.
Also as predicted, for 5 out of 6 positively framed scenarios, there were more respondents who chose option 1, the risk-averse choice. The framing effect was absent for scenario 3 (national). The framing affect was much stronger in the positively framed group than the negatively framed group, however, the framing effect was not stronger in national scenarios than in international scenarios, as was predicted
Table 4 shows that, as predicted, when protected values are high, there is no framing affect (although a slight effect is seen in scenario 2). In fact, scenarios 3, 4, 5, and 6 show an effect in the opposite direction; more people chose option 1.
For respondents with high national protected values, a framing effect is seen only in scenario 2 (which, interestingly, is a national scenario). (TABLE 5) Scenarios 1, 3, and 5 show a tendency to select option 1 rather than option 2, and scenarios 4 and 5 resulted in an equal selection of option 1 and 2.
For respondents with high international protected values, negatively framed scenarios did not produce a framing effect. In fact, in scenarios 3, 4, 5, and 6, respondents selected option 1. There was a weak framing effect towards option 2 in scenario 1 and 2. This illustrates that the presence of high international protected values severely decreases the framing effect for international, but not national, scenarios.
For total protected values, all positively framed scenarios tended to show the same framing effect; That is, option 1 was chosen more than option 2 (TABLE 7). Thus, protected values did not seem to lessen the framing effect for positively framed scenarios.
For international protected values, again there continued to be a framing effect for all scenarios; that is, option 1 was chosen more than option 2. For scenarios 2 and 6 the framing effect was even stronger; that is, a larger percentage of respondents with international protected values selected option 1 than the percentage of respondents without international protected values (TABLE 8). The table illustrates that the presence of high international protected values decreases the framing effect for both international and national scenarios.
For national protected values, positive scenarios 1, 4, and 5 showed the same framing effect; that is, option 1 was chosen more than option 2. For scenarios 2 and 3 an equal number of respondents chose option 1 and option 2. For scenario 6, more respondents chose option 2 than chose option 1 (TABLE 9).
Baron and Spranca (1997) suggest that people with protected values often will not even consider scenarios that conflict with their values. People can exhibit anger, bewilderment, and indignation, and subsequently choose neither of the options and (Tanner and Medin,1997). Table 10 shows that only a few (no more than 4% of respondents for any one scenario, positively or negatively framed; also shown in Table 2) refused to select an option. Protected values, however, were not always high for these individuals. This was particularly true for the respondents with positively framed surveys, where the majority of had low protected values (granted, this ranges from 2-4 individuals).
Respondents who refused to select an option to the scenarios stated one of two justifications: Either there was not enough information given in the scenario to justify selecting an option, or they simply could not justify selecting an option where there was a risk of ending the life of a living organism (the latter response was particularly true for the negatively framed options).
This paper did not address the three types of environmental scenarios; non charismatic endangered species, charismatic endangered species, and forest resources. There are, however, some potential predictions about how people choose to answer the scenarios based on the specific environmental issue.
The intensity of the framing effect will highest for charismatic endangered species scenarios, lower for non-charismatic endangered species scenarios, and lowest for forest remnant scenarios. Similar to analysis conducted on national/international protected values and how they correlate with national/international scenarios, I could run an analysis that tests whether or not specific protected values linearly match scenario choices. For example will a protected value about non-charismatic wildlife result in lower or absent framing effect for non-charismatic wildlife scenario, but not the other two options? What is the scope of an issue-specific environmental protected value?
Framing effects and deeply engrained protected values are rarely considered in environmental conservation education and research, yet acknowledging that people respond differently to the way in which a question is framed is obviously important. This research showed that positively framed scenarios were much more resilient to the presence of protected values than were negatively framed scenarios, and thus suggests that survey question design can be biased if questions are framed in particular ways. Specifically, if environmental surveys use close-ended questions where respondents are forced to choose from the given options, results can falsely illustrate overwhelming support or overwhelming opposition.

Bias based on sexual orientation and identity has been associated with a wide range of negative outcomes. For example, sexual minorities (i.e., individuals whose sexual behavior, psychological orientation, or identity is at least somewhat same-sex oriented) report significant levels of psychological distress in response to sexual identity-based hate crimes as well as heterosexist and homophobic comments (Szymanski, 2005). Experimental evidence suggests that even ambient heterosexist comments and actions -- not targeted directly at the observer but nonetheless occurring in his or her surrounding environment -- causes stress to sexual minority individuals (Burn, Kadlec, & Rexer, 2005; Konik, 2005; Silverschanz, 2007). And it has been proposed that all people, regardless of identity or orientation, are affected negatively by ambient prejudice such as sexism, heterosexism, and homophobia (Cortina, Magley, Williams, & Langout, 2001; Franck, 2002; Silverschanz, 2007).
There are numerous ways to measure attitudes towards sexual minorities, including scales of internalized homophobia (sexual minorities' shame and self-hatred based on negative societal messages about being non-heterosexual; e.g., Martin & Dean, 1988, as cited in Peterson & Gerrity, 2005) and measures of people's beliefs and attitudes towards sexual minorities more generally (e.g., Herek, 1994; Morrison & Morrison, 2002; Morrison, Parriag, & Morrison, 1999; Raja & Stokes, 1998; Steffens, 2005). However, explicit prejudicial attitudes may not be the only contributors to discriminatory behaviors and bias. More subtle underlying attitudes and beliefs about the nature of sexuality and what constitutes a normal and healthy adult relationship may also affect such outcomes. One approach to studying these underlying issues is to examine the effects of normative societal expectations about sexual behavior and identities. This area of study may be defined as heteronormativity, a construct that is often cited in gender studies as an important contributor to heterosexism, homophobia, and sexism (e.g., Blasius, 2000; Grace, 1999; Lancaster, 2003; Phelan, 2001, all cited in Kitzinger, 2005; also see Tee & Hegarty, 2006).
Heteronormativity has been defined as enforced compliance with culturally determined heterosexual roles (Nielsen, Walden, & Kunkel, 2000), and assumptions about heterosexuality as "natural" or "normal" (Kitzinger, 2005). Gender is closely related and often intertwined with such definitions, for normative heterosexuality cannot exist without rigid, binary expectations of behavior based on gender (e.g., Jackson, 2006). For example, heteronormativity has a circular relationship with hegemonic masculinity, both creating and resulting from the strict boundaries delimiting acceptable male behavior (e.g., Jones, 2006; Martino, 2000).
Jackson (2006, p. 114) explains that heteronormativity is "mobilized and reproduced" in ordinary, everyday social discourse and behavior, and Yep (2003, p. 13) proposes that it is caused by an "anxious" urge to protect illusory beliefs that heterosexuality is natural, normal, and inevitable. Rubin (1975), Rich (1980), and Jackson (1999), previously explored the "obligatory...compulsory...(and) compulsive" pressures toward heterosexuality (all cited in Yep, p. 18). Yep builds on this work to elucidate the violence that heteronormativity inflicts on women (by requiring that they serve men in marriage and motherhood), men (by defining rigid expectations of hegemonic masculinity), sexual minorities (by defining them as "other" and thus not fully human), and individuals who experience multiple forms of prejudice and marginalization due to the intersections of racial, ethnic, gender, sexual, and other identities. Consistent with Yep's portrayal of heteronormative violence, Lind (2004) points out that sexual minorities living in poverty are especially disenfranchised; they have limited access to resources, and the social services that would otherwise be available to heterosexually-headed households are denied to them due to definitions of marriage and family in welfare law.
Another negative consequence of social norms for sexual minorities is repeatedly confronting assumptions about heterosexuality, which results in having to make daily decisions about coming out or "passing," both of which involve particular stressors (Land & Kitzinger, 2005). Similarly, at social functions such as weddings, where the social capital of heterosexuality is celebrated and revered, sexual minorities are either invisible or "othered" (Oswald, 2000). In two recent studies (Harwood, 1998; Hylton, 2005), lesbian social work students reported that their colleagues and instructors assumed they were heterosexual unless they explicitly stated otherwise, that sexual minority concerns were not given as much importance as other aspects of social justice, and that privileging of and assumptions about heterosexuality led to feelings of isolation and invisibility among these lesbian students. In another qualitative study, Nielsen, Walden, & Kunkel (2000) explained that male undergraduate students who purposefully violated traditional gender norms encountered homophobic jokes and protestations by interlocutors of their own heterosexuality, while female students transgressing normative gender roles received unsolicited advice about adhering to femininity if they hoped to attract a mate. As construed by Yep (2003), and demonstrated by numerous qualitative investigations, the "violence" of heteronormativity is perpetuated on several levels: institutional, interpersonal, and even intrapsychic (i.e., the self-hatred of internalized homophobia in sexual minorities).
Although heteronormativity has not been explicitly examined in quantitative social science research, studies have examined closely related constructs. For instance, authoritarianism is directly associated with support of traditional, hierarchical power structures that demand rigid adherence to traditional gender roles (Duncan, Peterson, & Winter, 1997; Whitley & Ægisdottir, 2000). Authoritarianism has also been specifically linked to negative attitudes towards sexual minorities (Haddock, Zanna, and Esses, 1993, as cited in Ducnan, Peterson, & Winter, 1997; Tee & Hegarty, 2006). In turn, negative attitudes towards sexual minorities have also correlated with lower levels of openness to experience (Cullen, Wright, & Alessandri, 2002), and inflexible beliefs about fundamental, categorical differences between people based on sexual identity or orientation (Hegarty & Pratto, 2001).
In sum, heteronormative attitudes and beliefs define the boundaries of normative sexual behavior (e.g., people should partner with others of the opposite sex), and relate to proscriptions against behaviors and feelings that violate these norms. In addition, heteronormativity is theorized to rely on underlying assumptions about binary, essentialist beliefs about sex and gender. However, current literature includes neither a measure of the construct of heteronormativity, nor a quantitative investigation of the relationships among heteronormative attitudes, attitudes towards sexual minorities, and other relevant personality constructs. The aim of this study was to develop a measure of heteronormativity, the Heteronormative Attitudes and Beliefs Scale (HABS), and to examine relationships among heteronormativity, authoritarianism, and attitudes towards lesbians and gays. For the purposes of this study, heteronormativity will be defined as beliefs about (1) sex and gender as binary and biologically determined and (2) the assumed normality of sexual relationships between males/men and females/women.
I hypothesized that the HABS would be comprised of two factors, which would map onto the two components of heteronormativity described above (binary/biologically determined sex and gender, and normative behavioral expectations). I further hypothesized that scores on this new measure would correlate significantly with authoritarian attitudes and prejudicial attitudes towards lesbians and gays.
Eighty-four undergraduate students participated in this study, including 49 students from an introductory-level abnormal psychology course and 35 from an upper-level psychology course on gender and sexual identity (mean age = 20, range = 18-22 years). Approximately ¼ of the participants (24%) were male, ¾ were female, and none of the participants identified as transgender. In addition, 11% (n = 10) of the participants were sexual minorities and 21% (n = 18) identified as racial or ethnic minorities. Minority group categorizations were based on open-ended questions about race/ethnicity ("How would you describe your racial or ethnic identity?") and sexual orientation ("How would you describe your sexual orientation?"). Racial and ethnic minority participants included students identifying as Asian, Indian, Middle Eastern, or Chinese (n = 9); Latino/a, Hispanic, or Puerto Rican (n = 5); African-American or Black (n = 3); and Native American (n = 1). Sexual minority identity responses included "mostly straight" (n = 2); questioning, open, undefined, or bisexual (n = 6); and gay (n = 2). In addition, 9 students identified as a "straight ally" or "open heterosexual" and were categorized as heterosexual.
The Heteronormative Attitudes and Beliefs Scale (HABS) was developed from a set of 38 items which were selected, adapted, or designed to load on two hypothesized factors: (1) essentialized and binary beliefs about gender and sex and (2) normative behavioral expectations for men and women in romantic or sexual relationships (see Table 1 for items). Several items pertaining to binary and essentialized beliefs about gender and sex were adapted from Tee & Hegarty (2006). Approximately ½ of the items were positively worded (i.e., higher levels of agreement = higher levels of heteronormativity) and the remaining items were negatively worded (i.e., higher levels of agreement = lower levels of heteronormativity). All items used a 7-point Likert scale ranging from strongly disagree to strongly agree. Following the wording in Altemeyer's (1998) measure of RWA, the midpoint response for each item was "exactly neutral."
Two additional measures were administered in order to establish the validity of the HABS. First, RWA was measured using a 20-item version of Altemeyer's (1998) well-established questionnaire. In Christopher and Mull's (2006) study, Chronbach's alpha for this measure was .95, and D.G. Winter (personal communication, February 23rd, 2007) reported observing alphas well over .90 across multiple studies. Altemeyer's previous versions of this measure have also demonstrated excellent reliability and validity (see Altemeyer, 1988 and 1996). Following Duncan, Peterson, and Winter (1997), participants rated each item using a 7-point scale ranging from strongly disagree to strongly agree. A standardized mean composite score was computed to assess overall RWA tendencies. Second, a 10-item version of Herek's (1994) Attitudes Towards Lesbians and Gays (ATLG) was used to measure heterosexist prejudice directed towards lesbians and gay men. The ATLG limits its evaluation of sexual minorities to two discrete groups (lesbians and gay men), thus neglecting to directly measure attitudes about other sexual minorities, such as bisexuals. However, this measure is well-established in the literature as accurately predicting heterosexist attitudes and behaviors (e.g., Stover & Morera, 2007). Standardized mean composite scores were calculated to assess both attitudes towards lesbians (ATL) and attitudes towards gay men (ATG).
Undergraduate students in the two psychology courses were asked during class time to volunteer for this study. Willing participants stayed to complete the survey. Negatively-worded items on each measure were reverse-scored before calculating scale reliabilities, and these scores were standardized before computing the mean scores that were used in correlational analyses, group comparisons, and regressions.
A Varimax rotated factor analysis of the 38 items in Table 1 revealed 2 factors with Eigenvalues of 9.42 and 5.86, accounting respectively for 24.79% and 15.42% of the variance (the scree plot suggested a 2-factor solution). All items with factor loadings of at least .5 (absolute value) were considered for inclusion in the final scale; the items with the highest loadings on one factor and relatively lowest loadings on the other factor were retained in the analyses that follow. This resulted in a 16-item measure of heteronormativity, comprised of two scales with 8 items each, with balanced negative/positive wording. The scales, labeled Binary Gender and Normative Behavior, reflected the two predicted components of heteronormativity. Chronbach's alpha for the Binary Gender Scale was α = .92, and reliability for the Normative Behavior Scales was α = .78. Sample items loading on the two scales included the following:
As predicted, the HABS correlated significantly with RWA, attitudes towards lesbians, and attitudes towards gay men (see Table 2). This was true for both the Binary Gender sub-scale as well as the Normative Behavior sub-scale.
These findings reveal preliminary support for the validity and reliability of the HABS. Because heteronormative attitudes are theorized as contributing directly to prejudicial attitudes towards lesbians and gays, it was hypothesized that these two measures would correlate significantly. And because RWA and attitudes towards lesbians and gays have been highly correlated in previous studies, it was hypothesized that RWA and heteronormativity would also be highly correlated. Results supporting these hypotheses suggest that the HABS measure and its two subscales have adequate concurrent validity with these closely related constructs. Discriminant validity was not directly tested in this study, and I need to consult with someone more knowledgeable about statistics in order to discern whether the regression analyses reported above suggest that the HABS measure is at least somewhat orthogonal to the constructs of RWA and attitudes toward sexual minorities.
Caution is warranted in the interpretation of group differences in heteronormativity among participants of this study. On the one hand, sample sizes are likely too small to detect significant group differences by gender and sexual minority status. On the other hand, significant group differences observed by racial and ethnic minority status are based on small numbers of people who do not likely represent the diversity of opinions and perspectives within racial and ethnic minority groups, and prediction of outcomes by racial and ethnic minority status becomes nonsignificant when controlling for age and course enrollment. In addition, no intra-group analyses for racial and ethnic minority groups (i.e., African Americans, Latino/as), sexual minority status, or gender were possible due to the small within-cell sample sizes. For these reasons, neither the significant differences by racial/ethnic minority status and sexual minority status, nor the lack of significant differences found when comparing scores by gender, are discussed here.
This study lays a foundation for future investigations of heteronormative attitudes and behaviors using the HABS. Several limitations that could be addressed in subsequent studies are described here. For example, neither religion nor political orientation were measured in this study, and a greater diversity of demographics such as race, ethnicity, age, and education level would help in determining whether the properties observed in this study are generalizable to particular populations. In addition, in order to make sense of any group differences by sexual, racial, or ethnic minority status, these populations should be oversampled to allow for intra-group as well as inter-group analyses.
Discriminant validity was not explored in this study and could be examined in the future by investigating intercorrelations among HABS scores, RWA, and constructs expected to relate to RWA but not necessarily to heteronormative attitudes. For example, attitudes about the natural environment correlate significantly with RWA (e.g., Schultz & Stone, 1994) but would be expected to be somewhat orthogonal to attitudes about sexuality. Future studies could also assess test-retest reliability to determine the stability of individuals' heteronormative attitudes and beliefs as well as the affects of potential interventions aimed at reducing heteronormative assumptions.
During the Winter 2008 semester, I plan to replicate the study described in this paper in order to gain a better understanding of the properties of this new measure. Study participants will be recruited from undergraduate courses, hopefully from courses in different departments such as Women's Studies, Sociology, and Psychology.
In addition to pursuing a better understanding of heteronormativity and the statistical properties of the HABS, future research could use this measure to examine potential causes and consequences of heteronormative attitudes and beliefs. For example, understanding how heteronormativity and sexual orientation relate to one another may help us to make sense of mental health outcomes for lesbians, gay men, bisexual and transgender women and men, and even heterosexuals who subtly or blatantly transgress the rigid expectations that characterize a heteronormative society.
In my dissertation study, I am using the HABS in my investigation of two main research questions. First, how do sexual orientation and gender relate to personality constructs such as right-wing authoritarianism (RWA), tolerance of ambiguity, openness to experience, and heteronormative attitudes and beliefs? And second, how do tolerance of ambiguity, openness to experience, RWA, and heteronormative attitudes and beliefs relate to psychological well-being? With respect to the second question, my dissertation study is also investigating whether sexual orientation (as defined by psychological orientation, behavioral orientation, and self-defined identity) predicts different relationships among the variables. I will also assess fluidity, or the degree to which psychological, behavioral, and identity aspects of sexuality vary, and outness, or the extent to which individuals disclose these three aspects of sexuality to others. Because research participants' understandings of the concepts under investigation may vary, I will ask participants to provide the labels and terms they use to define and describe their experiences and identities in addition to asking closed-ended questions about demographics and sexual orientation. After I complete the data collection phase of this project, the following relevant hypotheses will be tested:
As described above, the problems and consequences of heteronormative attitudes are many; normative expectations of gender and sexual experiences likely affect all those who transgress such norms. And arguably few, if any, individuals always satisfy all of the normative expectations of gender and sexuality. Developing this measure is a preliminary step towards a better understanding of the significance of heteronormativity in our daily lives. And with increased understanding, we may yet, as Yep (2003) proposes, be able to challenge the boundaries of (hetero)sexuality -- to undermine some of the ways in which heterosexuality is created and maintained, to destabilize some of the ways in which gender hierarchies uphold heterosexuality, and to understand sexuality as only one layer in our complex and intersecting identities.

The purpose of this study is to explore a variety of potential causes of suicide (with the pre-existing data set that is presently available). Suicide is an action that results in death, and it is therefore important to understand its causes so that future occurrences can potentially be reduced. From this study, I hope to answer the following research question: Are individual or social variables the primary predicators of suicide rates? Drawing on the functionalist tradition that relies on social structure to explain individual behavior, I hypothesize that social variables (marital status, national citizenship, and religious affiliation) are better predictors of suicide than individual variables (race, alcoholism, and insanity). Through this research, I hope to contradict those who support biological, psychological, and behavioral explanations as predictors of suicide.
Current theorists that attempt to explain the factors that contribute to the existence of suicide differ primarily in terms of their explanations regarding the impact of social institutions on individual behavior. Functionalists and conflict theorists often argue that social structures are the primary determinants of individual characteristics and behavior, while theorists who emphasize agency focus on the way in which individuals give meaning to the world that they inhabit. Therefore, the discussion of suicidal determinants is often grounded in the broad theoretical debate between the role of agency and social structure.
Directly related to these opposing theoretical frameworks used to explain the causes of individual action are the current explanations of the act of suicide itself. Weishaar and Beck (1992) and Baumeister (1990) suggest that suicide is often used as a mechanism to cope with hopelessness, the feeling of persistent failure, frequently visible among older adults (though not a feeling felt exclusively by this group). Frankl (1984, 1971) and Weisman (1991) in contrast argue more as theorists who emphasize social structure and its impact on individual behavior by supporting the statement that, "an absence of meaning recognition can promote suicide" (Frankl 1984, 1971). He suggests that meaning in life is, "generally discovered in creative pursuits, in life's experiences and relationships, and in attitudes taken toward both positive life experiences and the 'tragic triad' of pain/suffering, guilt, and death." One's relationships and ability to positively deal with the "tragic triad" are related to the degree to which one is integrated into social institutions and networks. The failure to adequately integrate oneself in society can therefore contribute to one's lack of meaning in life, and contribute to one's decision to take one's own life.
As complex phenomena, the causes of suicide can be explained through a variety of perspectives including brain characteristics, genetics, psychological traits, and social forces, in addition to any combination of these. Those who take a more sociological stance argue that suicide results both from a lack of social integration (anomic suicide), and sometimes from too much social integration (resulting in altruistic suicide). From this perspective, the extent to which one is engaged in societal institutions is the primary determinant of suicidal behavior. It is true that, "people suffering from diagnosable mental illnesses complete about 90 percent of all suicides," (Pseudo Lit. 2 page 3) which some argue supports theories that individual genetic predispositions for certain psychological disorders and depression are the primary causes of suicide. Sociological theorists in contrast may use this same statistic to argue that high suicide rates among the mentally ill occur simply because society has failed to adequately integrate this group of people into social institutions, causing these individuals to feel a sense of normlessness.
Though the current research on the causes of suicide is broad, it remains relatively inconclusive. Previous studies have focused predominantly on biological, psychological, and behavioral variables, and have neglected to conduct comprehensive scientific research on social factors as significant predictors of suicide. Because of this, I will conduct a study that involves the secondary data analysis of comprehensive pre-existing data, in an effort to explain the impact of individual and social variables as potential determinants of suicide.
The data used in this study are secondary data that are being reanalyzed for the purposes of this research. The sources of this data are tables A and B on handout number one, tables C and D on handout number two, and tables E and F on handout number three. Table A examines the relationship between race (which for the purposes of this study is classified as a biological variable) and suicide. Table B examines the relationship between insanity, a psychological variable, and suicide. Table C examines the relationship between alcoholism, a behavioral variable, and suicide. And, tables D, E, and F examine the relationships between marital status, national citizenship, and religion, all social variables, and suicide respectively. A variety of statistical procedures will be used to analyze this data (looking for simple correlations), in an attempt to answer this studies' primary research question.
In table A, the relationship between race, the independent variable, and suicide rates, the dependent variable, is shown. The data shows a comparison in the rates of suicide (number of suicides per one million people) among Austrian provinces with different German population sizes. According to table A below, race (as measured by the number of Germans in a province) does not seem to be correlated with rates of suicide. This is because in the province with the smallest German minority the suicide rate has an average of 86, the province with the important German minority has an average suicide rate of 140, the province that has a majority of Germans has an average suicide rate of 125, and the purely German province has an average suicide rate of 106. If race were correlated with suicide rates one would expect suicide rates to either consistently increase or consistently decrease as the proportion of Germans in a given province declined.
In sections A and B of table B, the relationship between insanity, the independent variable, and suicide rates, the dependent variable, is shown. In part A of table B, the data shows a comparison between the rank order of nine countries' insanity rates (number of insane per 100,000 inhabitants) and the rank order of their respective suicide rates (number of suicides per 1,000,000 inhabitants). The data show (through the randomness of section A of table B) that there is no correlation between the number of insane in a particular country and the number of suicides that occur in a particular country. Section B of table B shows that there is no correlation between average suicide rates and the number of insane in the four given regions as well. If insanity were correlated with suicide in section B of table B, one would expect average suicide rates to either consistently increase or decrease as insanity rates declined.
In table C, the relationship between alcohol consumption, the independent variable, and average suicide rates, the dependent variable, is shown for different countries and regions in Germany. The data show that there is no correlation between a regions' per capita alcohol consumption and average suicide rates.
In the first section of table D, the relationship between marital status, the independent variable, and suicide rates, the dependent variable, are shown for each sex at seven different age groups. For both men and women, the data show that marital status is correlated with suicide rates because in every age group except one (ages 16-25 for men), the numbers of non-married/widowed who commit suicide are higher than the numbers of married who commit suicide. The second section of table D shows the degree to which suicides of unmarried men and women of the same age group are higher when compared to those of the married. One significant result is that non-married men above 75 are 37 times more likely to commit suicide when compared to their married counterparts, though non-married women in this same age group are only 4.5 times more likely to commit suicide when compared to their married counterparts. So, in the oldest age group, non-married men are much more likely to commit suicide than non-married women (both in relation to their married counterparts).
In table E, the relationship between (European) country, the independent variable, and suicide rates, the dependent variable, is shown. The data show that suicide rates (measured as the number of suicides per million inhabitants) remain relatively constant overtime within each of the eleven European countries shown. But, there is quite a bit of variation in suicide rates between countries, so country does seem to be correlated with suicide rates.
In table F, the relationship between religion, the independent variable, and suicide rates, the dependent variable, is shown for different countries. The data show that for all countries included, Protestants have higher suicide rates when compared to both Catholics and Jews. Therefore, religious affiliation does seem to be a good predictor of suicide rates.
Overall, the results show that the societal variables of marital status, national citizenship, and religious affiliation are all good predictors of suicide rates. The results also suggest that the more individual variables of race, alcoholism, and insanity are not good predictors of suicide rates. These findings are significant because they show that even a seemingly personal decision, such as committing suicide, is not determined by individual characteristics (race, alcoholism, and insanity), but rather by social institutions.
As described in the results section, table D indicates that marital status is correlated with suicide rates. The general trend is that both men and women who are married have significantly lower rates of suicide when compared to their unmarried counterparts. Those who are married may have lower suicide rates because the institution of marriage may provide a social support system that helps one satisfy certain goals, traditions, norms, and values. It is the integration in the social institution of marriage that may be the determining factor in whether or not one commits suicide in a time of crisis.
Despite this general trend, there is one instance where married individuals have higher suicide rates than their unmarried counterparts. Among men aged 16-25, the rate of suicide for married men is 10.51, while the rate of suicide for unmarried men is 5.69. This exception suggests that for young men (those aged 16-25), marriage may constrain and be a burden to one who may otherwise be experiencing the freedom that often comes with youth. Married men between the ages of 16-25 may therefore feel deprived of their single years, which may lead to depression, and possibly suicide. Table D also indicates that in the oldest age group (above 75), unmarried men are much more likely to commit suicide then unmarried women (both in relation to their married counterparts). It may be that men who live over 75 feel out of place in society where men typically die much earlier than women. Though married men over 75 encounter a similar situation, the marital institution may provide the social support to mitigate the negative affects of living so long (for men). Similarly, unmarried women over 75 are not an anomaly, which may be why they are much less likely to commit suicide (at least in relation to men in the same position).
The empirical evidence from table E suggests that suicide rates remain relatively stable within countries overtime, but between countries there is significant variance, indicating that national citizenship is related to suicide rates. Suicide rates within countries may remain relatively stable overtime due to the presence of distinct national cultures that promote or prohibit certain behaviors, values, norms, and beliefs. Suicide rates between countries may vary depending on the content of a particular countries' national culture. Saxony and Denmark consistently have the highest rates of suicide, which indicates that there is something about being a citizen of these two nations that predisposes one as more likely to commit suicide when compared to one from any of the other nine countries included in this study. I argue that those countries with high suicide rates have national cultures that promote individualism more, leading citizens of these countries to be more independently motivated, and less integrated into society.
As indicated in the results section of this paper, religion also seems to be a good predictor of suicide rates. The evidence suggests that Protestants have higher suicide rates when compared to both Catholics and Jews. I argue that this is because Catholics and Jews tend to live in more tight knit communities where traditional rituals impose many restrictions on the lives of inhabitants. With close knit communities and restrictions comes an intense sense of social support, something that the evidence suggests Catholics and Jews experience more than Protestants. The presence of social support networks in Catholic and Jewish communities may explain why these religious groups experience lower suicide rates among their people.
The findings in this study relate back to the broader theoretical debate surrounding the causes of suicide by reinforcing the idea that social structure, as opposed to agency, affects individual behavior most. Though this study allows one to conclude that it is social variables that predict the occurrence of suicide rather than individual variables, this study does have some limitations. This study relies on the premise that correlation equates to causation, which is scientifically unreliable to assume. Secondly, the way in which race is defined in this study is somewhat problematic. Race does not simply encompass the proportion of Germans within a given population, but is a much more complex concept, that in a better study would have been operationalized in a more complex way.
Given the data presently available to conduct this study, the data seem to show relatively conclusive results, providing further support for our hypothesis.

Suicide is the third leading cause of death for youth aged 10 to 24 (Centers for Disease Control and Prevention, 2002). Women of all ages are three times as likely to attempt suicide in their lives as are men (Centers for Disease Control and Prevention, 2004). The U.S. Centers for Disease Control list family and community support as one of the key mechanisms for preventing suicide among those at risk (Centers for Disease Control and Prevention, 2004). Considering the above statistics and the federal government's attention to collecting them and offering suicide-prevention advice, we can see that suicide has been recognized as a social problem to which the government has allocated public funds to prevent. The CDC goes on to recommend, however, that clinical care for mental and substance abuse disorders is also key to preventing suicide, thus suggesting that individual psychological factors can be at fault. In this paper, I explore the relationship between suicide and individual versus social factors. Specifically, I ask whether micro level psychological factors or macro level societal factors are more significant determinants of whether a person will commit suicide. I hypothesize that social conditions, not psychological status, are more predictive of an individual's decision to take his own life. Such research is valuable and relevant as part of a larger effort to determine what causes people to take their own lives and an attempt to mediate those provoking factors.
Because of the grave social and painful individual implications of suicide, it has been the subject of much study among scholars. Many scholars believe that a persistent feeling of hopelessness is the driving force behind suicide in adults. They define hopelessness more specifically than its colloquial usage. Minkoff, in his work, describes it as, a feeling that "nothing will turn out right, nothing will succeed, important goals are unattainable, and worst problems will never be solved" (Handout, 2005). Weishaar and Beck also focus on the role of hopelessness in suicide. Many other scholars, including Beck, Pearson, Brown, Conaghan, Davidson, Heisel, Hill, and Uncapher further emphasize the role of hopelessness in causing suicide among specific populations, such as psychiatric patients and the elderly.
Related to theories centering on hopelessness is what are called the "escape theory" and the "meaning-centered approach". Both of these theoretical approaches discuss a particular manifestation of hopelessness as center to suicidality. Escape theorists such as Baumeister and Hewitt posit that suicidal behavior results when a person perceives herself as failing or not meeting perfectionist goals. This state of negative self-awareness leads to "cognitive deconstruction" (Handout, 2005) in which the person's judgment and inhibition falters and they choose suicide to escape their painful state of disappointment and self-perceived failure. The meaning-centered approach similarly focuses on an individual's perception of her life. Frankl and Linehan in particular propose that a person's will to live depends on their ability to find meaning in new life events. Those who can find meaning even in negative events or suffering are thus less prone to suicide.
The medical profession has even gotten involved in the debate. Some scientific research indicates that predisposition to suicide may be a genetically inherited trait. However, psychiatrists and psychologists, such Americans Karl Menninger and Edwin Schneidman, still tend to focus on the role of hopelessness and inability to see solutions to problems as driving factors behind suicide (Handout, 2005).
All of the above literature focuses on traits of individuals that may cause suicide. There, however, a substantial body of literature that claims the individual factors causing suicide are negligible when compared to the social forces influencing an individual's perceptions and actions. To understand such a seemingly counterintuitive position, we must first review the sociological literature on individual agency and social structure.
Two basic schools of thought dominate the structuralist camp. Many sociologists, including prominent scholars Emile Durkheim and Karl Marx, argue that social structures are so powerful that they completely control an individual, and leave no room for individual's actions or characteristics to determine her life. More recently, Berger and Luckmann have argued that Durkheim and Marx's ideal is too restrictive and that, in the beginning, individuals have agency in giving meaning to the world they see around them. After this first act of free choice, however, these meaning become institutionalized and then form the social structure which goes on to limit the future agency of the individual. In discussing suicide, structural sociologists would look at the influence of social institutions and practices on an individual's decision to commit suicide.
On the other side of the coin are sociologists who believe that individuals have considerable power in shaping their world and perceptions. These scholars, known as symbolic-interactionists, do not ignore the influence of society. They recognize the power of social forces such as stigma, situation, and peer groups and the limitations placed by society on individuals. They do believe, however, that individuals retain some agency and ability to effect change in their part of the world. Proponents of this theory include George Herbert Mead and others of the Chicago school.
At this point, I will leave my discussion of previous literature and turn to my own research. Later, in the discussion section, I will support my conclusions by citing and integrating the above scholarship to provide a complete picture of the driving forces behind tendency toward suicidal behavior.
The data set I work with has been compiled by examining public census records throughout the German provinces and in surrounding European countries in the 1880s. This data set is very similar to that used by Emile Durkheim in his landmark study on suicide in Europe. My dependent variable is number of suicides per a given number of inhabitants as indicated in the results tables. My independent variables are: race, insanity, alcoholism, marital status, country of residence, and religion. I will analyze the relationship between each of the independent variables and the dependent variable by listing the average results and visually comparing these averages. I choose to use publicly available data because it is most cost efficient while at the same time most complete because it was collected by an official government agency with the time and resources to create a comprehensive data set.
Analyzing Durkheim's results we can see many interesting trends. In his comparison of suicide and race we see little correlation between the two. Purely German provinces and provinces with a small German population had approximately the same difference in suicides per million as did purely German provinces and majority German provinces. The suicide rate per million increased slowly for purely German, majority German, and important German minority provinces, but then dropped dramatically for provinces with a small German minority. There does not seem to be a correlation between race and suicide rates.
In his comparison of number of insane per 100,000 and average suicides per million inhabitants, there does not appear to be a correlation either. As the number of insane goes down incrementally from 215 to 84, the average suicides increase on the whole from 107 to 153.
Similarly, looking at consumption of alcohol in liters per capita and average suicides per million inhabitants, we see that suicides hover around 200 per million for inhabitants who consumed between 7.2 and 13 liters of alcohol in 1884-1886, then shoot up to 234 suicides for those who consumed 4.5 to 6.4 liters between 1884-1886, then fall again to 147 for those who consumed 4 or less. From a cursory non-scientific examination I see no correlation between the number of liters of alcohol consumed per capita between 1884 and 1886, and the average suicides per million inhabitants.
Examining the data concerning marital status and rates of suicide, however, we can see a much stronger correlation. Single men aged 16 to 65 are, on average, 3.16 times as likely to commit suicide as unmarried men the same age. This shoots up to eleven times as likely for men aged 66 to 75, and thirty-seven times more likely for men over 75. Women aged 16 to 65 are approximately 2.7 as likely to commit suicide as unmarried women the same age, while women 66 to 75 are 11.12 times as likely to do so. Clearly, these rates indicate that unmarried people face a substantially increased risk of suicide, usually on the magnitude of three times but sometimes as high as thirty-seven times.
An analysis of the suicide rates per million inhabitants of different European countries yields a similarly consistent result. Over a twelve year period in a survey of eleven countries, no country ever moved more than two rank positions away from its original rank. For example, Italy remained the country with the lowest number of suicides during the entire twelve year period, while Denmark and Saxony always held their positions as the those country with the top two suicide rates. Each country in the survey basically ranked the same in terms of number of suicides in comparison with the others throughout all of the years examined. This indicates some correlation between citizenship and tendency toward suicidal behavior.
Finally, we see a notable correlation between the average number of suicides in a given period in Austria, Prussia, Baden, Bavaria, and Wurttemberg. Protestants averaged 171 suicides per million persons in those regions, while Jews averaged 111 and Catholics only 96. The relatively large discrepancy in these numbers indicates some sort of relationship between religion and suicidal behavior.
These results demonstrate that macro level social factors are better predictors of suicidal behavior than individual forces. That is, a person's nationality/race, presence of insanity, and addiction to alcohol can all be considered characteristics inherent to a person-characteristics that are inseparable from her as a human being. Contrarily, marital status, country of residency, and religion are social factors-conferred by social institutions or social designations, and not an inborn trait of a person. This confirms Emile Durkheim's conclusion that individuals are less likely to commit suicide when they feel a part of a cohesive group and have strong social bonds, such as when they are part of a married couple, part of a strong, patriotic nation, or part of a community-oriented religion such as Catholicism or Judaism. The results of Durkheim's work suggest a structure-centered, not agency-centered, model of understanding individuals and suicide. That is, he recognizes the power of social and inter-personal circumstances in determining an individual's behavior, even suicide, and gives substantially less weight to personal characteristics. His research demonstrates that macro-level factors are more influential in determining behavior than are traits we consider to be an individual's active choice or predilection.
This can be linked with other research that emphasizes the role of feelings of hopelessness in suicide, as ability to interact with others and have continuing new life events maintains higher levels of hopefulness and life-meaning. Members of communities can take advantage of the social bonds and group support discovered by Durkheim to dispel hopelessness, lack of purpose, and feelings of failure by maintaining positive interaction with others.
This study should be extended, even only on the variables already used, to contemporary times. Given the increase in religions in Europe as well as the more public existence of atheists and agnostics, one could study not only the effects of impersonal, Protestant v. community-based, Catholic or Jewish religions, but the effect of religion (Christian, Judaic, and Muslim) versus no religion. Suicide and insanity could be extended to study suicide rates and those who receive mental health services and those who don't. Tagging onto the debate over information technology, another interesting avenue would be to study rates of suicide among those who frequently use computers and the internet versus those who do not. Given the high rates of suicide among youth mentioned in the introduction of this paper itself, extending the study to suicide rates of youths from different socioeconomic, cultural, and familial situations would be a very interesting and relevant application of the survey. Government and non-profit sponsored programs geared at increasing social integration may be a particularly good area for expansion. In addition to providing individual psychological counseling services and identifying practical ways to physically protect suicidal patients, these programs can develop partnerships with community organizations. Such collaboration can provide a meaningful way for individuals receiving individualized services for suicidality to be linked with a organization that can facilitate positive social and community-building experiences. The data analyzed above clearly indicates the importance of feelings of social integration and belonging in decisions to commit suicide. Connection of both public and private sector suicide counseling and prevention services with neighborhood-based, socially-oriented community organizations may be the critical link in creating an expanded, effective safety net for citizens vulnerable to suicidal behavior.
Clearly, much work remains to be done in the field of suicidality. A multitude of social variables intersect to determine one's susceptibility to suicide. Continuing down the road of individualized psychological services is not sufficient to stem the problem as we now confront it. Taking a new direction in which social services recognize the feelings of loneliness, alienation, and lack of social integration underlying suicide may provide new avenues for early identification of suicidal tendencies and the prevention of suicidal behavior in citizens of all ages and social groups.

Abortion is a very controversial topic for conversation today -- in the 21st century -- where it has achieved mainstream visibility in the political sphere. The controversy often revolves around opposing claims of rights and polarized feelings of support for the baby's right to life and thus opposition to abortion or support for the choice of the woman carrying the baby and thus support for abortion. More than a medical option (though not a consistently legal medical option across the States), abortion is an issue involving widespread disagreement over its appropriate level of legislation. Pro-life activists often argue that because it involves the question of life, abortion is a moral issue. In 2004, many voters admitted this issue was singularly used to direct their choice for President of the United States. These voters were subsequently classified as moral voters.
So this begs the question: how far is morality interconnected with abortion? Are these two concepts causally related? In my study, I will be examining whether or not religious attitudes affect personal feelings on abortion. My research question is: Are religious people more likely to be against abortion than non-religious people. My hypothesis is that religious people are more against abortion (pro-life) than non-religious people. I believe religious people view abortion as a cruel and negative thing because they think it is taking the life of a baby whereas non-religious people tend to think of abortion not in terms of the life of the baby but in terms of the mother carrying the baby. They believe abortion is a necessary freedom of choice.
I am interested in how individual opinions of abortion are affected by religious views, so my unit of analysis will be individuals. The two concepts I am studying are religiosity and feelings on abortion. Religious people are my main interest group, so non-religious people are my comparison group. My independent variable is the level of religiosity of my unit (individuals) studied, and feelings on abortion is my dependent variable. My ideal study population would be a representative population of all United States citizens, with a wide distribution of gender, age, class, region, and race. My actual study population is University of Michigan students because they are who I have access to speak with.
To best collect my data for these variables, I chose to use semi-structured individual interviews by approaching individuals in different locations on campus and asking their feelings on abortion and how religious they are. I measured religiosity by asking different questions I thought indicated the level of how religious people are. I measured feelings on abortion by asking their overall view on it and then narrowing to specific circumstances if necessary. I chose this type of interview instead of a structured paper questionnaire because I decided I wanted to involve the respondents more. I wanted them to be able to expand on the extent of their abortion views and religiosity, without being confined to answer sets created by me. Indeed, an advantage of this method is that they could give more explanation so that I could understand their position better than I would have from a survey question. Abortion can be a complicated topic, so it is important to clarify with people the type of abortion they are envisioning and the circumstances they believe are appropriate for it to be legal. Having an open-ended question format also allowed them to demonstrate the salience of the issues in their minds. When they introduced a topic on their own, I could trust it was on the top of their mind whereas if they had circled it as a survey question option, they would not necessarily have cared about it without my drawing attention to it. Disadvantages of this method revolve around the sample. Conducting interviews is time consuming, so I had a small sample size which was also biased from my selection (see below). Another disadvantage came from the coding involved in creating quantitative analysis. Recognizing which parts of their open ended responses classified under my categories was a subjective act that could not always be completely uniform.
My initial plan was to have a sample size of 40 by talking to 5 people in each of 8 locations, however, only 5 locations ended up being successful, so I finished with a sample size of 25. I wanted to draw a sample of students from a wide range of academic backgrounds (because academic study often leans in a political direction) and social identities (because they impact your lens on the world), so I tried to collect data from areas of campus where different academic concentrators would be and in common university areas where a broad mix of students would be. The Union MUG and the Ugli proved to be the most successful for finding respondents. I eventually found enough outside the fishbowl, but I was held up by being told by an employee that I was not allowed to approach people inside. I wanted to talk to people in Beanster's in the League because the Jewish population is known to hang out there (so this would get different faiths for my study) and Espresso Royale on State St because more art and music students hang out there, but when I visited each of these places, everyone was studying, and I did not have the heart to try and interrupt them. Instead, I talked to people eating at the League food court because I had had success at the other common university locations because I approached people who were eating (or reading The Michigan Daily). I successfully talked to students (hopefully science majors) in the Chem building foyer, but I chose not to talk to anyone in the Business school lobby (as originally planned) because I realized the students were mostly MBA students whose age would not match my other respondents'. My last attempt was to talk to people at East Quad (which is known for having a different student population due to the Residential College residing there), but no one was outside because of the cold weather and no one was inside in the lounge (I assume because of finals).
Besides the location factor, I tried to get a sample mixed by gender and race -- which I was successful in obtaining. However, my sample could have been limited (beyond the lack of academic variety discussed above) by my hand selection. I did not talk to anyone I knew because I did not want their knowledge of my positions to color their response. I did not talk to anyone around someone else because I did not want another person's presence to impact their response. While I tried to control bias by establishing a comfortable interviewing environment, my sample was limited because most people were around others -- for study purposes or just socializing. The people in my sample size are those who were out in the public during my collection time, were by themselves and looking open to disruption, were hand selected by me, and who agreed to participate. (I had a few people tell me they did not have time to answer questions or did not want to.)
Besides the drawbacks of my sample, I must note that my study population is far from representing the general US population. I got a racially diverse population, but my small sample size made it impossible to get representatives from each gender in every race. Most respondents were between the ages of 18 and 22. Most respondents came from the middle class or above because you have to have money to come to an institution of secondary education, especially the University of Michigan (which has a tuition similar to that of private schools), and even more so if you have to pay out-of-state tuition. 60% of University students are from the state of Michigan, so my respondent population was skewed with more people from the state of Michigan, a Midwest state, than other states. Because the Midwest is known to be more conservative (especially on moral issues), the sample could have made people more likely to be against abortion, but the University of Michigan is known as a more liberal school, so political leaning might have been balanced, though I cannot say my study represents the beliefs of the majority of the US population.
My informed consent statement was: "By consenting, you have read the information above and consent to participate in the study." My project was confidential versus anonymous because I identified each respondent with a number so that his or her responses could be traced back for an individual analysis if necessary. It remained ethical because I never asked for any personal identification. My study posed no risks except potential personal emotional struggle if respondents had experienced hardship surrounding abortion or religion. One respondent did appear very uneasy when I asked him questions as he took full minutes in between his responses. Other respondents admitted that their current religious affiliation was something they had been struggling with since coming to college. My study could have been more harmful if given to vulnerable subjects such as pregnant women since the subject manner directly relates to them.
I devised an Interview Guide (see appendix) to assist when I individually talked to my respondents for about five minutes each. My operational definition of abortion was: the medical surgery to stop a pregnancy. (This means that self-imposed abortion or attempt to miscarriage is not what I operated from, but was noted if brought up by respondents.) My operational definition of religion was: the belief in supernatural powers in the creation of the universe or influence over destiny. (This means that spirituality was not being considered, yet noted if respondents interpret "religion" that way.) My purpose was to simply discover respondents' feelings on my two concepts and their personal demographic identifications.
I first asked the general, "How do you feel about abortion?". Some respondents appeared confused or asked if I meant "pro-life or pro-choice." In each situation, I replied, "Just your general view." I hoped to see if respondents would note specific situations where the legality of abortion should be assured or circumstances when abortion should definitely be illegal. I left space on my form to record if they noted these things on their own initially. If they did not, I asked if they thought there were specific circumstances for legality or illegality. If they appeared confused, I suggested that some people had said rape.
I used the same procedure for measuring their religiosity: asking how religious they are then narrowing to specifics (attendance at religious services/independent religious study/faith) to get a greater understanding if they were not initially specific. I sometimes clarified with "so you would say you are somewhat religious" to make sure that how I would classify them later was acceptable to them. Because I asked their religiosity directly after I asked their views on abortion, I designated room on my form to note if respondents made justifications about their paradoxical identifications between the concepts. Only a few respondents did. Finally, I asked for their self-identification in the following categories: year in school, gender, age, socioeconomic status, race, and region where they are from. Some respondents were confused because they thought I was asking, "You're in school?" at first. Others said they struggle in designating where they fall along racial or regional lines. I directed: "however you self-identify." An interesting observation if that most respondents thought gender was assumed and gave me a weird look when I asked for their identification (even though sociological theory says that gender should not be assumed because it is socially constructed).
In conducting my interviews, I tried to appear as unbiased as possible. When respondents were talking, I would look down at the paper where I was transcribing to avoid judgment being shown on my face. I maintained a friendly, approachable tone no matter what their responses were. As noted above, I tried to approach respondents in non-threatening environments (when they were alone/not busy) so that they could be as open and truthful as possible. I had created my guide to be semi-structured, but it was interesting because respondents gave fairly structured responses as if they were waiting for me to run down a list of questions. Perhaps this is due to a natural assumption of a structured format when asked to participate in an interview. Consequently, I did not gather a deep understanding of why they held their views on abortion.
My study is limited due to the characteristics of the sample (see above) and the religious identification of respondents. A problem is that I was not consistent in how I asked for specification on legality/illegality issues during the first few interviews. Next time I would be sure to have a set next question in response to their answers. As far as sample, next time I would want to include Jewish and Muslim people, as well as any other different religion to see if other faiths confounded the results. My results are not necessarily reliable because different results might be obtained from talking to people of different faiths -- or simply different individuals. I do, however, believe that my results are valid (though not generalizable to my ideal population) because of the environment of comfort I established in my data collection. Some respondents noted how abortion is an ethical issue surrounded by controversy and disagreement. One guy would not commit to his views until I frankly asked him to. Some respondents admitted that they are not what they would consider "true Christians." The order of my questions, with religiosity after views on abortion, might have caused self-questioning of religious commitment more so than if not asked to commit to a stance on a moral issue. This potentially amounted to respondents feeling the need to give disclaimers, but I still believe honesty was involved overall.
My dependent variable (feelings on abortion) was measured by "abortionlevel." Most respondents (16) felt its legality should depend on the situation, while 1 person thought it should never be legal and 8 felt it should always be legal.
The "otherexception" variable also measured the dependent variable by showing a scale of exceptions for when abortion should be legal, from most negative feelings toward abortion to most positive feelings. It was helpful because respondents were more spread out in their answers:
The "religionlevel" variable measured my independent variable (religiosity) through a scale of responses that were self-identified and/or interpreted by me, the interviewer. 48% of respondents were somewhat religious while 40% were not at all religious:
The confounding variable was age. I later recoded responses to compare "under 20" to "20 or older" to create an approximately even sample for subgroup comparison:
To best compare the relationship between the independent and dependant variables, they were recoded into new variables and compared through crosstabs and correlations. The variable "new abortion measure" was created from an "otherexception" recode of "strict on exceptions" versus "liberal on exceptions." The "religionlevel" variable was recoded so that "not religious" was compared to any identification of religion: "religious in some sense." (see codebook in appendix)
The crosstab shows that the majority of religious people (53%) and the majority of not relgious people (60%) were both more liberal on abortion. My hypothesis was that more religious people would be more against abortion than non-religious people, and this is shown to some extent (47% v. 40%), but the relations of the independent and dependant variables had only a weak correlation at -.066. My hypothesis was proven correct, but only slightly and not to a significant amount:
I also measured the effect of confounders. By layering the "abortion measure" v "religion recode" table with "younger v older" (the age variable recoded), I found that abortion views differed on whether respondents were under 20 or older:
75% of the "not religious" respondents who were under 20 had liberal views on abortion, while only 50% of the "20 or older" did. Of the religious respondents, 62.5% of those under 20 held stricter views on abortion while only 28.6% of the older religious people did. This shows that the aggregate relationship between religiosity and views on abortion differs when you single out age. My overall findings showed that religious people had slightly more negative views toward abortion whereas here, only the younger religious people had more negative views toward abortion. The difference was due to age, not religiosity.
My study population is limited because the demographics of students at the University of Michigan do not relate to the demographics of the country as a whole, because my sample was not representative of all religious views, and because my sample was small in size. My results do show a support for my hypothesis, but only slightly. More religious people were against abortion than were non-religious people, but only through a weak correlation. Age was also shown to be confounding. To better generalize these results, I would have to interview more people with more varied social identities. I wish I had been uniform in asking their specifics for the extent of legality from the start. If I were going to do more research on this topic (besides having a larger and more representative sample size), I would like to explore political party identification as another possible confounding variable.

Family harmony and marriage maintenance have both been highly valued in Chinese society. Especially for traditional Chinese families, to maintain the intactness and seeming harmony of the family, couples in an unharmonious relationship have often been encouraged to stay in the unhappy marriages rather than choosing a divorce, which is even considered humiliating under extremely conservative social contexts. This is especially true for western China which is less developed and for some minority ethnic groups which are restricted by their religions. Gansu, as both a western and a multi-ethnicity province,1 is a crucial context to investigate the determinants of the attitudes toward unhappy marriages. Moreover, to examine the demographic, socioeconomic and ideational determinants of the attitudes toward unhappy marriages is a critical step to further understand how people's values of the familial attributes and marital patterns are associated with their respective socioeconomic status and the process of modernization in China.
(1) Data: Collected in 2007, the Developmental Idealism in China Survey is a cross-sectional survey of individuals who reside in the rural or industrial areas of seven counties in Gansu with three villages or communities in each county. Using the multistage and clustered area probability sampling, the survey interviewed 633 respondents during November and December of 2007. This survey mainly includes questions about respondent's basic demographic information and idealism on development, family and marriage.
(2) Dependent variable: The dependent variable in this study is based on the answers to the question "Overall, which do you think is better for most people around the world today – choosing divorce or staying in an unhappy marriage?" The answers are treated as a dichotomous variable consisting of "divorce (=1)" and "unhappy marriage (=0)".2
(3) Independent variables
Three types of independent variables are used for this study – demographic characteristics, socioeconomic characteristics and modernity factors. Firstly, a group of demographic characteristics variables are used including respondents' sex ("1"> male and "0" female) and age. Including sex in the analysis is because there has been severe gender inequality in traditional China and women's marriage selections have been restricted more strictly. It might be crucial to investigate the gender disparities of the attitudes toward unhappy marriages. To include age in the analysis is because when people get older, their related social networks are becoming more complicated and widespread and it is more likely that their marriage selections may be restricted by a "dual pressure" – not only pressures from the parents, but also pressures from the children. Therefore, it may also be important to examine the age evolution of the attitudes toward unhappy marriages.
Secondly, a group of socioeconomic characteristics variables are included. Respondents' education (below elementary: reference group; "1" elementary and "0" otherwise; "1" junior high and "0" otherwise; "1" senior high and "0" otherwise; "1" vocational/technical and "0" otherwise; "1" associate and "0" otherwise; "1" bachelor/above and "0" otherwise) is included because with higher education, people may become more modernized and tolerant of diversified personal selections including marriage selections. Respondents' marital status ("1" ever married and "0" never married) is included since these two groups may respectively hold insider and outsider perspectives which may result in their different attitudes toward unhappy marriage. Respondents' religion ("1" with religion and "0" without religion) is included because the marriage attitudes of religious people may be restricted by their beliefs. Income (less than 1000 Yuan: reference group; "1" 1001-4000 Yuan and "0" otherwise; "1" 4001-15000 Yuan and "0" otherwise; "1" more than 15001 Yuan and "0" otherwise) and self-assigned social class
Thirdly, a set of ideational variables indicating modernity factors are included based on the assumption that modernity is consistent with more tolerant and diversified attitudes, as well as more respect for personal freedom and equality. These variables are included to test whether the more modern and tolerant the respondent is, the more likely a divorce would be preferred in an unhappy marriage. This measure of modernity includes six variables: living with parents ("1" living separately and '"0" living with parents), unmarried mother ("1"> acceptable and "0" unacceptable), cohabitation ("1" premarital cohabitation and "0" no premarital cohabitation), elderly parents living alone ("1" living alone and "0" living with adult children), arranged marriage ("1" choose own spouse and "0" parents choose spouse) and female premarital sex ("1" premarital sex and "0" waiting until marriage).4
Table 1 presents the summary statistics of key variables of interest in the study and Table 2 demonstrates the distributions of attitudes toward unhappy marriages across different demographic and socioeconomic variables. As can be seen in Table 2, generally males, those ever married and those having no religions are more likely to prefer choosing a divorce. Moreover, although below the education of senior high diploma and vocational/tech diploma people are more likely to prefer staying in unhappy marriages, above that level people are generally more likely to prefer choosing a divorce. Especially, there is a consistent trend for income and social class that the higher income people have and the higher social class people assign to themselves, the less likely they prefer choosing a divorce in unhappy marriages. Therefore, several of my above interpretations are primarily supported by these descriptive statistics. However, rigorous regression analysis is quite necessary to provide more reliable examinations.
In order to further investigate the determinants of attitudes toward unhappy marriages, I conduct three binary logit analyses. Model 1 only includes respondents' demographic characteristics, Model 2 further includes respondents' socioeconomic characteristics and Model 3 includes respondents' modernity factors besides variables included in Model 2. As can be seen in the log-likelihood ratio tests shown in Table 3, Model 2 is a significant improvement to Model 1 and Model 3 is a significant improvement to Model 2. Therefore, aside from demographic characteristics, both respondents' socioeconomic characteristics and modernity factors are crucial determinants of their attitudes toward unhappy marriages.
As can be seen in both Table 3 and Table 4, different from what is shown by descriptive statistics, the odds of preferring for a divorce are only about 0.696 as large for males as they are for females. If respondent's age increases by one year, the odds of preferring for a divorce decrease by 2%. In terms of education, below senior high diploma and vocational/tech diploma, the odds of preferring for a divorce has been increasing from 2.193, 2.589, 3.216 to 3.354 compared to that of a education lower than elementary school; however, above senior high diploma and vocational/tech diploma, the odds starts to decline from 3.354, 2.152 to 1.035, but these later effects are found insignificant in this study. In addition, the odds of preferring for a divorce are only about 0.265 as large for those ever married as they are for those never married. Both the religion and income effects are found to be insignificant in the regression results. Interestingly, self-assigned social class plays a significant role in deciding the respondents' preference in an unhappy marriage, but its estimated effect is converse to the results of descriptive statistics. The odds of preferring for a divorce are about 1.724 times as large for those middle-class respondents (self-assigned social class between 3 and 7) and 2.933 times as large for those high-class respondents (self-assigned social class between 8-10) as they are for those low-class respondents (self-assigned social class between 0 and 2). Moreover, among the six modernity factors, only the effect of the variable indicating whether married children live separately or with their parents is statistically significant. The odds of preferring for a divorce are about 1.908 times as large for those preferring living separately as they are for those preferring living with parents.
In this study, I investigated how one's demographic, socioeconomic and modernity factors influence his/her likelihood of preferring a divorce to staying in an unhappy marriage. The results of this preliminary study indicate that: 1) women are more likely to prefer a divorce in an unhappy marriage; 2) the older one is, the less likely he/she would prefer a divorce; 3) for educational levels below college education, the higher education one has, the more likely he/she would prefer a divorce under an increasing likelihood; 4) those ever married are very less likely to prefer a divorce; 5) the higher one's social class is, the more likely he/she would prefer a divorce under an increasing likelihood; 6) the more modern one's values are, the more likely he/she would prefer a divorce in an unhappy marriage. Therefore, all of the demographic, socioeconomic and modernity factors affect one's attitudes toward an unhappy marriage.

Interest in improving the condition and position of women, especially in poor countries have focused on the ideas of status improvement and empowerment. Though much of sociological literature on this topic focuses on trying to clarify the ambiguity about the definition and elements of these ideas, the more universally and broadly used definition in various "plans of actions" focus on the contexts of women's access to knowledge, economic resources and political power, as well as their personal autonomy in the process of decision making (United Nations, 1994).
Sociological research on the processes and individual and social outcomes of educational attainment is probably one of the most studied areas in the field. A growing body of literature also focuses on the outcomes of women's economic autonomy on changes in their roles and positions. But evidence of the impact of collective action processes (community groups), a key strategy for implementing the plans of action for empowerment and status attainment of women at the grassroots level, has not been well documented, apart from program evaluations. Most of what has been documented focuses on the efforts of group based micro-credit programs and its outcomes on changes (both positive and negative) on the position and condition of women in poor countries (Hashemi, et al. 1996; Schuler, et al. 1997).
Even after decades of focusing on "gender and development", and women's empowerment, do poor women have increasing control over their lives? Are they able to make decisions and control the implementation of them? What kinds of changes in behavior among men and women in poor settings are affecting their attitudes towards gender roles? How are these attitudes bringing changes in the status and the empowerment of women vis a vis men at the household, community and national levels?
Though there are many questions that still need to be answered, this study only attempts to look at the overall effects of participation in community groups on attitudes about the participation of women in household decision making, using survey data from a rapidly changing social setting in Nepal. It also tests the theory about the linkages between educational attainment and attitudes about women's participation in household level decision making in a society with low levels of education.
Nepali society is multiethnic and multicultural consisting of many linguistic sub groups, social beliefs and practices, as well as variations in gender relations within the ethnic groups (Bista, 1972). A predominately Hindu society, the social structure is marked by patriarchy and strict gender hierarchies, where gender roles are characterized by greater access, control and decision making over resources (social, economic and political) by men at the household, community and national levels, though these roles vary by ethnic groups (Acharya and Bennett, 1981).
To examine the links between educational attainment and group participation on egalitarian gender attitudes about decision making at the household level, this study is guided by the framework of the family mode of social organization (Thornton, Fricke, et al. 1994), its linkages to modernization theory (Inkeles and Smith, 1974) and the conceptual framework of the development paradigm (Thornton, 2001; Thornton, 2005).
The family mode of social organization focuses on social change brought about by the changes in the extent to which activities of daily social life, formerly controlled by the family, came to be organized by a host of social structures and institutions such as schools, industries, other employment opportunities, dormitories, mass media, etc. that began to influence many dimensions of life (Thornton, Fricke, et al. 1994:89). The study setting in Chitwan too has undergone tremendous social and economic changes with the introduction of new social organizations and services (as well as an influx of international aid to support poverty alleviation) that have made considerable impact on family structure, fertility behavior, marriage patterns and the attitudes of people on different elements of life (Barber, 2004; Ghimire, et al. 2006). Apart from the growth in access to education, in the past two decades, as per the policies of the government of Nepal1, people in Chitwan have also been introduced to new organizations and processes such as non family collective and social action that have sought to stimulate and support social change and economic growth in the society. Participation of men and women in such non family organizations and greater exposure through education is likely to bring changes in the socio-cultural traditions and behavior within the family, including influencing changes in decision making at the household levels.
The modernization theory posits that societies industrialize and Westernize, with "Third World" countries like Nepal, at the lower end of a continuum of social change that tries to emulate the systems and processes of "developed" countries on the higher end, like United States and Western European countries. The movement along this continuum can be marked by growing individualistic attitudes and behavior towards religion, the family, social stratification, women's rights, politics, mass media, consumerism, and other topics (Inkeles and Smith, 1974:25). Government plans and policies have also emphasized "modernization" of different sectors in the country. Though the pace of change has been slow, elements of the modernization theory are likely to impact upon behavior and attitudes. Modern education in particular exposes men and women to the ideologies of individualism and independence, leading to more egalitarian attitudes. In their 1974 study of tracking social changes in six different countries based on the modernization theory, Inkeles and Smith hypothesize that "...the liberating influence of the forces making for modernization would act on men's attitudes, and incline them to accord to women status and rights more nearly equal to those enjoyed by men" (ibid:26).
The developmental paradigm is another powerful framework to explain and guide social change, which has been applied at the individual, organizational and societal levels. Thornton (2001 and 2005) explains how the power of the development idealism lies in what it posits as "...its ability to change structures, ideas, attitudes and behaviors, and that it is largely a value system indicating basic human rights, the nature of good life and the means to achieve the desired ends" (Thornton, 2005:240). This has guided the concept of socio-economic and political change or progress ("development") in poorer countries in Asia and Africa -- both by governments of the countries and international aid institutions. Furthermore, Thornton also states that "the power of developmental paradigm provided a powerful framework for ordinary people because they defined some of the attributes of the good life, specified causal approaches to reaching the good life and provided statements of fundamental human rights" (ibid:234). Therefore modern education and processes for social and economic change are elements that are likely to have been strongly influenced by the development paradigm, in aiming for the "good life" and for more egalitarian attitudes as a basis of fundamental human rights.
Within the context of these theories and frameworks, this study proposes two hypotheses:
Some specific mechanisms are also suggested that would operate to produce the relationships hypothesized above.
Studies have shown how formal education has been linked to social change and a wide range of social transformations such as economic growth, demographic transitions, political growth, and status attainment (Caldwell, 1982, Axinn and Barber, 2001; Beutel and Axinn, 2002). In a setting marked by patriarchy and strict gender hierarchies, men and women gaining education in schools not only exposes them to non family social contexts, but also widens their knowledge and increases their skills, and exposes them to the diffusion of the spread of new (or modern) aspirations or attitudes towards family roles, potentially more egalitarian gender roles. This is in line with the modernization theory that suggests how western education exposes women and men to ideologies emphasizing independence from the extended family and egalitarian conjugal relationships (Caldwell, 1982). This potentially holds true in Chitwan too as the education system and context in Nepal is heavily influenced by that of India, where western educational traditions and curricula were established by the British during the colonial period. In their 1997 study Malhotra and Mather also point out that increased education and increased opportunities for non family and paid employment are also likely to cause structural changes in the economic roles of family units, of women, or even of children but their key focus in the study is how "... the relationship between education, work and women's control of household decisions is conditioned by the larger social context" (ibid: 601).
With their increased skills, knowledge and self confidence, their potential for accessing credit or earning income, and their ability to influence change at the community levels, acknowledgement of and respect for the changing role of women relative to men could lead to changes in the attitudes of men and women too, about both having a more equal share in household decision making (though as Malhotra and Mather suggest, this could be conditioned by the large social context).
Since the advent of development aid in Nepal more than two decades ago, the principle implementation strategy of most development agencies in the country has been through group formation and group-oriented activities (Biggs, et al. 2004). For both government and non government programs and projects, community groups have become a popular vehicle for reaching people with services -- family and reproductive health, rural infrastructure, agriculture and livestock, management of natural resources (water, forests), access to credit, etc. Such groups were established at the local levels to focus on service delivery and on social inclusion of poor and marginalized groups in the planning and development processes, and for the empowerment of the group members. Evidence about the effect of such group activities have focused on the impact of credit programs in women's social and economic empowerment and also on changes in their fertility behavior (Hashemi, et al. 1996; Schuler, et al. 1997).
Research about the links between behavior and attitudes (Cleland and Hobcraft, 1985; Thornton, Alwin, and Camburn, 1983) provide a useful framework to better understand how non family behaviors can potentially influence attitudes. Participation in community group activities exposes men and women to non family settings, activities, ideas and processes that lead to a departure in the family mode of social organization. They are exposed to new knowledge and ideas (new techniques and practical skills), have increased access to information and services (accessing credit and other government and NGO resources, natural resource management, their legal and human rights) and increased self esteem, self worth and self confidence, which in turn could culminate in a desire for increased social attainment and participation in decision making processes at the household and community levels. Such non family and non traditional behavior would potentially lead to new attitudes about gender roles at individual, community and organizational levels.
More recently, many groups in Nepal have developed into higher level networks, federations, cooperatives and NGOs, which are getting involved in policy advocacy and securing the rights and interests of their members. One of such example is the Federation of Community Forests Users of Nepal-FECOFUN (Biggs, et al. 2004). But groups at the community level have also successfully been able to lobby for schools or health clinics in their neighborhoods (Barber et. al, 2001)
Thus men and women who have participated in groups and have some levels of education are likely to have more egalitarian attitudes about decision making at the household level compared to those who have not participated in group activities ever or have had lesser or no education.
The setting of the Chitwan Valley in south central Nepal is ideal for this study due to the settlement history of the area and the dramatic changes it has gone through since. In the mid 1950s the government of Nepal allowed for clearing of the densely forested area, eradication of malaria and allowed re-settlement of migrants (particularly from the mid and high hills) into the valley, who were attracted by the fertile soils. Therefore, there is a mixture of different ethnic groups that are living within the study area, all of whom have distinct socio-cultural identities, languages and religious affiliations allowing a study of the ethnic diversity of the country within a microcosm. The study area has seen rapid changes in socio-economic conditions and physical environment of the valley within a short span of time especially since the mid 1970s due to the linkages of several key transportation routes. This brought important changes in access to education, health and other government services (agriculture and livestock, natural resource management), employment, wage labor participation, as well as considerable focus of international donors and NGOs to support the government's efforts in improving service delivery through a multitude of development programs.
For the purpose of my analysis, I use data from the 1996-1997 Chitwan Valley Family Study (CVFS). The CVFS selected a systematic probability sample of 171 neighborhoods in western Chitwan (Barber, et al.1997) where neighborhoods were defined as a geographic cluster of 5-15 households. The structured individual survey component interviewed every resident between the ages of 15 and 59 in the sampled neighborhoods leading to a sample size of 5,271 respondents. The respondents were asked a variety of questions regarding their family background, personal characteristics, experiences, childhood community context, marriage and marital relationships, their attitudes about various aspects of social life and their participation in groups. Immediately following the individual interview, information was collected about the respondent's age, residence, marital status, children, contraceptive use, living arrangements, schooling and work experience using a semi-structured life history calendar. The questionnaires were administered by field staff in face-to-face interviews conducted in the homes of the respondents with a response rate of 97%2.
For the purpose of my analysis, only respondents between the ages 25-54 are used leading to a sample size of 2,851. Respondents between ages of 15-24 were dropped as they are more likely to participate in youth clubs rather than community groups and are also more likely to be in the process of completing their high school education. Respondents of the age group 55 and above were also dropped due to the relatively small number of cases. The CVFS classified the communities into five major ethnic groups -- the High Caste Hindus (the largest group), Low Caste Hindus, Hill Tibeto-Burmese, Terai Tibeto-Burmese, the Newars and the rest were classified as "others" (Axinn and Barber, 2001). The last category was also dropped for this analysis due to the small number of observations.
The CFVS collected a number of measures of attitudes related to women's roles at the individual and household levels. This study focuses on only one of those measures related to what I take to represent "egalitarian attitudes" within the household. This was measured using responses to a statement that was presented -- "A man should make most of the decisions in the household". The responses to this question was fairly normally distributed on a scale of 1 "strongly agree" (33.46%), 2 "agree" (27.32), 3 "disagree" (38%) and 4 "strongly disagree" (0.84%).
As this study examines the effect of educational attainment and participation in groups on gender attitudes of men and women in Chitwan, two independent variables are tested. The first variable measures "participation in groups ever" through asking respondents whether they had ever been a member of any groups or associations such as a User's Group, Mothers Group, health groups, Rotary Club, etc. This is a dichotomous variable with 18% (n=514) of the respondents having ever participated in groups. A follow-up question was also asked to identify the types of groups respondents had ever been members.
The second variable I use, "educational attainment" is a partially continuous variable where respondents were asked the highest grade in school or year of college they had completed (from no education to 10 years and more). Over 52% of respondents have had no education and almost 16% have had 10 years or more of education.
Several control variables are included in the multivariate models that are likely to affect egalitarian attitudes that are specific to the context of the study area. As mentioned earlier, Nepali society is broadly polarized in terms of gender roles and attitudes and this study attempts to better understand some of the factors that might contribute towards reducing this polarization. Controlling for gender of the respondents (coded female=1, male=0) makes it possible to study egalitarian attitudes without the influence of gender, though the differences in attitudes among men and women and the factors affecting those differences also merits in-depth analysis.
Differences in age and in birth cohorts also exert differing influences on social roles and expectations in Nepali society especially in the case of a strictly hierarchal society and more so in the case where within a span of 50 years many socio-economic and political changes have been introduced in the study area. Therefore birth cohort (coded 1 and 0 for six dummy variables) is also used as a control. The latest and the earliest cohorts have been dropped from this analysis while cohort1 (age group 25-34) is used as the reference group. Due to variations in the status and conditions of women in different ethnic groups caste/ethnicity is also used as a control with five categories and the Upper Caste Hindu used as the reference group.
A set of parental characteristics ("mother ever educated", "father ever educated", "mother worked for pay" and "father worked for pay") are also used as controls since previous research has indicated the impact of parental attitudes and behavior on children (Axinn & Thornton, 1996; Thornton, 1991). These are also dichotomous variables. Recent research has also provided evidence of the effect of community level changes on individual behavior (Axinn & Barber, 2001) hence I use two measures that represent community level characteristics that could likely influence individual behavior and attitudes. The "presence of development groups" and "women's groups" in the neighborhood when the respondent was 12 years old, are two measures that are also used as controls in this study.
Descriptive statistics for all the measures used are presented in Table 1 while the breakdown of membership in different kinds of groups is presented in Table 2. The effect of different types of groups on egalitarian attitudes that have been more meaningful and have had more policy implications but it is beyond the scope of this study due to limited numbers in the different categories of data.
Tables 1 and 2 about here
Ordinary least square (OLS) regression procedures are used to estimate the multivariate models of egalitarian attitudes. This technique is applied using the dependent variable as a continuous variable with the assumption that the higher the value on the scale (from 1-4), the more egalitarian the attitude, thereby ordering it from lower to higher even though there is no numeric precision. Nested models are tested, introducing sets of different control variables (individual levels, parental characteristics, childhood neighborhood characteristics and finally interaction effects) to see how the effect of group participation and educational attainment on egalitarian attitudes is affected by the control variables and the interaction effects.
Additionally, logistic regression is used to estimate egalitarian attitudes and to test if the results are consistent in their direction with the OLS estimates. A summary measure of the dependent variable "male decides" is examined, with the categories of "strongly agree" and "agree" collapsed together to specify non egalitarian attitudes (coded 0-60.79%) while "disagree "and "highly disagree" are collapsed together to specify egalitarian attitudes (coded 1-39.21%). The same controls are examined in this analysis as well.
The estimates of the effects of group participation and educational attainment on egalitarian gender attitudes related to decision-making at the household level are presented in Table 3. I test nested models and the incremental F tests for the control variables -- gender, birth cohort and caste variables are statistically significant indicating the fit of those variables in the regression model. For parental and childhood neighborhood characteristics it is not significant but since, theoretically, I expected the latter variables to also have had some influence on egalitarian attitudes of the respondents, I leave them in the subsequent models.
Table 3 about here
In the second step, a set of individual level controls -- gender, birth cohort and caste/ethnicity are introduced and presented in model 2. The third step presents the estimates of adding controls that are related to parental characteristics in model 3 where two variables were tested -- whether mothers and fathers of the respondents ever went to school, and whether they ever held paid jobs when the respondents were young. The fourth step of the analysis presents additional estimates from the effects of two neighborhood characteristic when the respondents where 12 years old -- whether their neighborhood had any development activities and whether there were any women's groups.
Compared to the reference category, the more recent birth cohort (ages 24-35), cohort 3 is less egalitarian as expected, but the results are not statistically significant indicating little differences within the different birth cohorts. This could be due to the fact that the most educated group lies within the most recent cohort (ages 15-24) which has been dropped for the study. Model 4 provides evidence that 11.03% of the variance in egalitarian attitudes can be attributed to a person's participation in groups and education levels controlling for a number of factors.
The last two models 5 and 6 (Table 3) present estimates of the effect on egalitarian attitudes testing for interaction effects of gender and educational attainment, and gender and participation in groups. Since gender differences are quite wide in Nepal, I estimated that interaction effects of gender on education and on group participation might prevail but the results show that such effects are not statistically significant. Yet even then, when I ran separate models to estimate the effects for males and females (results not shown) it was interesting to find the effects were greater for males than for females, indicating that educational attainment and group participation had stronger effects on male egalitarian attitudes.
Table 4 presents the results of the logistic regression where the results, though not directly comparable with the OLS regression, do indicate results in the same direction.
Table 4 about here
For every person who participates in group activities the odds of being egalitarian is 45% higher compared to those who did not participate in group activities, controlling for their education. For every additional year of education a person has their odds of having egalitarian attitudes multiplicated by 1.12. For instance, compared to someone with no education, a person with six years of education has an odds ratio of 97% higher of having egalitarian attitudes. It is interesting to note that only six years of education increases the odds of being egalitarian by almost 100%. Also being a Lower Caste Hindu, Hill Tibeto-Burmese or a Terai Tibeto-Burmese have lower odds (0.55, 0.55 and 0.49 times respectively) of having egalitarian attitudes compared to the High Caste Hindus. Parental characteristics and childhood neighborhood characteristics are not significantly associated with gender attitudes and birth cohorts are also not significantly associated.
The key research question for this study was to better understand the kinds of behavior at the individual and community levels which would bring in more egalitarian attitudes at the household levels. I find that men and women who have ever participated in groups and those who have more years of education have more egalitarian attitudes about decision making and that this is statistically a highly significant association when controlling for a number of individual, parental and childhood community characteristics. The results for the effect of group participation is quite relevant as it boosts policy efforts of service delivery, creating demand and advocating for rights and interests of poor and marginalized sections of the society, towards social inclusion and egalitarianism.
More relevant to policy feedback and program implementation would be evidence of the differential impact of the status of group membership (general member or an executive member) and the differential impact of participation in different types of groups. This was beyond the scope of this paper due to limited number of observations per different types of groups and no data about the types of membership. A second wave of this study can also provide opportunities for measuring changes in egalitarianism over the years and with some additional data a better understanding of how participation in group activities can lead to social attainment and egalitarianism.

In the fall of my senior year in high school, two of my closest girlfriends and I left the large, Midwestern, city we lived in and drove an hour south to tour an highly elite, private liberal arts college. This school was consistently ranked by the "US News and World Report" in the top 5 liberal arts schools in the country, and widely considered the "best" school in the region. Being smart and career-minded young women with close ties to family, friends, and boyfriends in the city we lived in, we all hoped to like it.
While on the tour with 10 or so other high school juniors and seniors -- and a few parents -- also visiting the school, I asked the tour guide about rape on campus. From my experiences in high school and feminist politics I knew/believed that sexual assault was widespread and pervasive, however this particular school had experienced a well-publicized rape the previous year, and I was curious to see how she would respond to my question. The tour guide responded by telling me about rape whistles, showing me the lights that lit nearly all corners of the small, isolated campus. She then told me, in a dismissive tone, that, besides, the only sexual assault cases they had involved alcohol.1
This incident left a powerful impression on me. At the time, I thought that was a ridiculous response. I remember sarcastically commenting to my friends that as there was no alcohol on campus we would all be safe. At the time, my assumption was that the tour guide's response was her way of providing a quick answer to a difficult question, while simultaneously managing to reassure the parents on the tour that their children could be safe at that school as long as they chose to avoid (being around) alcohol. It was not until later that I began to think about the implicit devaluing of (the experiences of) the women/girls who chose to drink and/or be around others drinking; girls who chose to participate in campus party culture.
This paper is a study of those girls -- the ones who choose to participate in campus party culture -- and their perceptions of the risk of sexual violence. The larger project from which the data for this paper is drawn explores the co-constructions of gender and partying through the mechanisms of peer culture, same and cross-gender relationships, bodily experiences of alcohol and drug consumption, sexuality, the pleasures and dangers of partying -- including the risk of sexual violence, and their variations along different identity dimensions (such as race, ethnicity, class, sorority status). The focus of this paper is on one particular aspect of gender construction in the context of partying: the co-constitutive and dynamic engagement of late adolescent, binge-drinking women with the "slutty" discourse and its implications for the risk (and experience) of sexual assault.
Through analysis of data from 31 in-depth interviews, I show some ways in which the binge-drinking college women who I interviewed engage with a discourse of "sluttiness" in the sexualized campus party culture. I begin by demonstrating the pleasure women associate with partying, and that partying is constructed with a particular element of sexuality; providing one possible answer for why women participate in the traditionally masculine domain party culture with well-publicized risks. I then show that the popular conception of a "slut" as a woman who has sex with many different men in a relatively short period of time without a necessary or strong emotional attachment to the men involved (Stombler, 1994; Armstrong et. al, 2005),3 is not the definition of slut at work in the culture of the women I interviewed. I conclude by discussing the implications of binge-drinking college women's engagement with the "sluttiness" discourse for, a) the real and perceived risk of sexual violence, b) the mental and psychological well-being of (the many) women who are sexually assaulted, and c) the "othering" of women who are labeled as "slutty."
There is an extensive, and overwhelmingly quantitative, literature documenting the prevalence of sexual violence on campus and its frequent relationship to partying (Abbey, 2002; Adams-Curtis & Forbes, 2004; Armstrong, Hamilton, & Sweeney, 2005; Boswell & Spade, 1996; Corbin, Bernat, Calhoun, McNair, & Seals, 2001; Handler, 1995; Martin & Hummer, 1989; Stombler, 1994; Ullman, Karabatsos & Koss, 1999). Prevalence rates of sexual violence on campus, very often accompanied by alcohol consumption, constitute college women as a "high risk" group that experiences sexual assault at substantially higher rates than the general population. One research team (Sorenson, Stein, Seigel, Golding & Burnam, 1987) found that college women are at three times greater risk of sexual assault than the general population of the same age and gender.
In 1987, Koss developed the Sexual Experiences Survey (SES) to study rape and sexual assault on college campuses. It is now a widely used and validated measure. Items on the SES ask about specific unwanted sexual behaviors. Included in these items is a question about unwanted sexual contact while under the influence of alcohol.4
In a nationally representative sample of 6,159 college women using the SES, Koss (1987) found that approximately 54% of college women had experienced at least one instance of a sexual assault of any kind, 15% had been raped and 12% experienced attempted rape. Later replication studies have produced similar findings (Finkelson, & Oswalt, 1995 Schwartz & DeKeseredy, 1997).
It is important to acknowledge that any attempt to quantify incidence of rape or other sexual assault must be viewed with a critical eye. There are many social and psychological imperatives to deny, minimize, or "re-frame" one's own experiences with interpersonal violence to fit a more culturally or socially acceptable script (Campbell, Sefl, & Aherns, 2004; French, 2003). In addition, the phrases "sexual assault,"
Given the astounding prevalence of sexual violence on college and university campuses, many researchers have devoted their careers to explaining this phenomenon. The vast majority of the work has focused on quantifying individual level variables such as how much the victim and/or the perpetrator had been drinking, did the victim or perpetrator have physical, sexual or emotional abuse history, to what extent did he/they adhere to rape myth beliefs, etc (Adams-Curtis, & Forbes, 2004; Armstrong, 2005; Littelton & Axsom, 2003; Carmody & Washington, 2001).5
Academic work on campus sexual violence has been primarily, although not exclusively, quantitative. Despite an early and on-going emphasis on individual-level factors such as those listed above for sexual violence, by the late-1980s work on sexual violence on college campuses began to incorporate insights from qualitative methodology and from sociology. This methodological, and to some extent disciplinary, analysis brought analysis of situational or contextual aspects, such as the organization of fraternities, (Boswell & Spade, 1996; Martin & Hummer, 1989) on sexual violence on campus.
In Martin & Hummer's important article on fraternities and rape (1989) they argue that the structure of fraternities, of which hyper-masculinity, excessive drinking and partying, "scoring" with women, and devout loyalty to the brotherhood are important elements, creates an atmosphere (particularly at parties) that facilitates and encourages rape. They conclude that sexual violence on campus is not likely to be reduced until the culture and social organization of fraternities change. In the mean time, they call for more monitoring of fraternities, particularly their use of alcohol, by campus officials.
Armstrong et al. (2005) expanded this existing analysis to construct a theory of campus sexual assault that is simultaneously rooted in both the structural organization of the University and the specific practices of (groups of) individual students. They call their approach to theorizing campus sexual assault the "social organization and peer culture approach". Their analysis expands upon previous research in two important ways. First, it acknowledges that fraternities are large contributors to the rape-supportive structure of a University's social life, but they are not the only culprits. University policies such as the manner and extent to which alcohol use is restricted, the organization of non-Greek housing (dorms, primarily) and the limited interaction between first year and more advanced students, all contribute to a pro-rape college culture.
Secondly, Armstrong et al. (2005) emphasize the role of individual actors, and groups of actors, in shaping a college culture that enables party rape. Their analysis of the peer culture of college women, along with that of Handler's (1995) study of sorority women, provide some of the only attempts to explain the active participation of women in dangerous party scenes. Based on ethnographic observation and interviews with college women at a large Midwestern University, Armstrong and her colleagues argue that the peer culture of college women creates a situation in which choosing not to participate in the (specifically Greek) party scene would mean "social death." Their research, in addition to that of Stombler (1994) and Holland and Eisenhart, (1990) suggests that attention they get from men (which Young et al. (2005) and others have argued can now be gained from participating in heavy drinking) is so important to college women's social status and sense of self-worth that they are willing to risk the potential threat of sexual assault to get it. In other words, within the peer culture of these young women, the risk of sexual violence that is associated with partying is worth it.
This study expands on the existing research in three important ways. First, the focus of this study is on binge-drinking undergraduate women. It is not primarily a study on first year students and/or sororities, which is where much other research focuses. Second, this study uses in-depth qualitative interviews to better understand the meanings and co-constitutive processes people attach to sexual violence. Third, this study focuses on the engagement of binge-drinking undergraduate women with the "slutty" discourse and attempts to manage the risk of sexual violence within the context of campus party culture. All of this is done under the a framework of co-constitutivity and gender construction.
The data for this study are 31 in-depth, semi-structured interviews that took place on the University campus from May to July of 2005 (May through July). I conducted interviews with women aged 18-24 who were undergraduate students at a large, elite, Midwestern University and who self-identified as frequent binge drinkers and participants in campus "party culture."
Greater racial/ethnic diversity was represented in this project than across the campus at large. Of the 31 interviews, 18 (58%) women identified themselves as white or Caucasian. Eleven (35.5%) women identified themselves as Asian or Asian-American, and two (6.5%) identified themselves as Black or African-American.7
Socio-economically, participants were less diverse than the general campus population. Participants were asked to record their estimation of their family household income.8 Only two participants indicated that their families' annual household income was less than $50,000, and only six (including those two) reported annual household incomes of less than $75,000. Twenty-six participants indicated that their mothers' had at least a college education (10 had graduate degrees). Twenty-five participants reported that their fathers' had at least a college education with 17 of them having graduate degrees. Two participants (not the two with the lowest household incomes) reported that their fathers' were not involved in their lives.
The sexual orientation of the women I interviewed was also very homogenous. They all identified as straight or heterosexual. The only variation was one woman who wrote "heterosexual (more nonspecific)."
The participants in this project differed from those in many other studies of campus sexual assault in two ways; age and sorority status. While many studies of campus sexual assault focus on the experience of sorority members (see, for example, Martin and Hummer, 1989; and Boswell and Spade, 1996) the participants in this project were not largely members of sororities. Only four participants (12.9%) were Pan-Hellenic sorority members.9 Interestingly, this is likely an over-representation of sorority members when compared to the campus as a whole, as only 8.5% of first year students anticipated joining a fraternity or sorority (http://www.crlt.umich.edu/gsis/StudentProfileData2004.pdf.).
I recruited participants for this via flyers that I posted in three coffee shops on the University's central campus, and in two large classroom buildings. Due to the sensitive nature of the study, flyers were posted both on designated posting spots and in the stalls of women's restrooms in all locations. The flyers announced a University sponsored study on women's experiences with partying and requested the participation of people who were interested in talking about their experiences with partying (including their perspectives on the risk of sexual assault while partying) and who met the following criteria: 1) female undergraduate students at the university between the ages of 18-25, 2) had at least 4 drinks in a setting at least three times in the last two weeks or 15 times in the last 6 months, and 3) participate in party culture (as defined by participants). Tear-off tabs with my name, email address and phone number were located at the bottom of the flyer. People who were interested in participating were invited to contact me by phone or email. In part because recruitment began just as the traditional academic year was ending and I was concerned about having a difficult time recruiting participants and also because I wanted to be respectful of peoples' busy lives, potential interviewees were offered $25 for being interviewed.
I soon realized that despite my earlier concern, I was not going to have trouble recruiting people to be interviewed. This is largely due to the active academic schedule (a large number of classes offered) of the spring and summer terms. Thus although the population of undergraduates is substantially lower in the spring/summer than it is in the fall, there are still a sizeable number of undergraduate students who stay on or near campus over the summer.
I conducted all of the interviews and followed the outline of an interview schedule with questions about experiences with partying, campus party culture, social relationships and interactions, identity, and risks of party culture, and sexual violence. In keeping with the tradition of qualitative interviewing, all interviews covered all topics, but not every question was asked to every interviewee and interviewees were often asked questions that emerged from the flow of the interview. I began every interview by having participants fill out an informed consent form that described the project and explicitly stated that participants would be asked about their perceptions of the risk of sexual assault (as did the recruitment flyer).
Most of the interviewees appeared excited about being interviewed about their experiences with partying. They commented that they thought it was important for such a common, every-day experience to be studied and analyzed. Binge drinking and using other drugs is unconditionally stigmatized in many segments of society, and was illegal for many of the interviewees (those under age 21). However, partying is such a common and culturally accepted behavior for college students at Universities such as this one that I believe most interviewees felt comfortable talking about their experiences with me. Additionally, we did the interviews in a school building, a place where many students talk with their friends about their social lives and experiences with parties, which might have contributed to their feelings of comfort with the topic.
My location as an interviewer was similar to what Patricia Hill Collins (1990) describes as an outsider within. I was approximately 10 years older than most of the interviewees. I had recently had a baby, who some of them saw me interact with as they were waiting for the interview to begin, and I was marked (via my rings) as married. These identities (older, married, a mom) marked me as a "grown-up" and as distant from the youth that they embodied and that is central to their identities as college students. As such, they were salient in the interview setting. Some similar, if somewhat removed, identities were also salient. Most obviously, I am a woman, as were all of the interviewees.10 Both the interviewees and I were students, albeit I was a student in a doctoral program and they were undergraduate students. Additionally, and perhaps most salient to this project, I also was a frequent binge drinker and participant in party culture as an undergraduate student, although the setting was different both contextually (a small, politically progressive, liberal arts school) and temporally.
This past experience, in addition to academic literature and professional work, provided the "context of discovery" for this project and shaped the questions I explored with interviewees. The interview questions largely remained the same throughout the course of the interviews, with some minor adjustments made to reflect the language of the participants. For example I began asking interviewees about their experiences and relationships with the men they partied with. I changed that language after the first few interviews when I received puzzled looks from interviewees who said, "oh, you mean my guy friends." I realized that the word "men" to these college women implied an older "grown-up," distinct from their age/developmental identity of late adolescence. I then changed my language in interviews and used the word "guy." I also followed the interviewees guide as to whether they referred to them selves as women or as girls.
After we had gone through my questions, I asked participants if they had any questions for me after completing the interview. Many of them reiterated that they were glad the issue was being studied and asked what I thought I would find. I responded that I believe participating in party culture is an experience or behavior that shapes our senses of identity, particularly our identity of ourselves as women. Related to that I expected that our perception of the risk of sexual assault also impacted the way we understand our experiences with partying and our identities and sense of ourselves as women.
Following the completion of the interview, participants were asked to complete a demographics form requesting general background information such as age, year in school, socio-economic class, race, location and type of high school, and sorority membership. They were also offered a list of resources giving contact information to mental health, substance abuse, and sexual violence services. At the very end of the interview I handed participants an envelope with their incentive money, $25 in cash, and told them they could contact me with any additional questions. I also told them that if they had requested I would send them a summary of my findings from the project.
Interviews lasted between 45 minutes and one hour and 45 minutes. Twenty-eight of the 31 interviews were recorded on a digital voice recorder and transcribed for analysis. Extensive field notes were taken on the three interviews that were not recorded, and then transcribed for analysis.
I analyzed the data using a method of open and focused coding (Emerson, Fretz & Shaw, 1995). I initially read through transcripts and field notes from all 31 interviews. On a second read-through of the data I began thematic coding, using NVIVO software.
I began coding with a list of themes suggested from the literature and from my personal and professional experiences. While using these pre-existing themes as a starting-off point, I simultaneously sought out themes that were emerging from the data, making coding an iterative and dynamic process. Anticipated themes included slutty, safety, risk/danger, and heavy drinking. Emergent and unanticipated themes included issues of control, the pleasure of feeling/being sexy within the party scene, and slutty-as-drunk.
Not far into the process of coding, I decided sacrifice breadth for the hope of comprehensible depth (Hurst, 1999), and thus to focus my first paper out of this data on the connections between the "slutty" discourse and perceptions of the risk of sexual assault. To do so I narrowed my general research question from "how is gender constructed among frequently binge drinking women within the campus party scene?" to the more focused questions of "how do college women construct the discourse of 'slutty' "
Armstrong et al. (2005) and Young et al. (2005) argue that despite the well-known dangers that research and conventional wisdom associate with partying, undergraduate women gain pleasure from participating in party culture. My data supports that claim. When asked to name some reasons why women drink every single interview mentioned -- without prompting -- that partying is fun.11
Partying is a normative and highly visible institution at this University. For the women who were interviewed for this study, it is the context in which much of their social lives take place. They describe partying as a strategy for meeting people, a way to lower their inhibitions and allow them to interact freely with people (particularly "guys") they do not know well, and as a vehicle for bonding with their friends.
Sexuality was both a component of partying and source of fun for the women I interviewed. That there is an element of sexuality associated with partying is such a naturalized assumption that it is difficult to find explicit evidence of it. Barring direct statement of the conflation of partying and sexuality, I found evidence of it in two places; 1) the efforts to which women described going in order to create the desired sexy appearance for "going out," and 2) the frequent linkages made between partying and "hooking up,"
When describing how she and her friends get ready to "go out"
The description Sara provided of "getting ready", or at least her feelings behind it, varied somewhat from most of the other interviews. It was reminiscent of both Bartky's (1990) and Fenstermaker and West (2002) conception of femininity, and the accompanying heterosexuality, as an achievement or accomplishment to which others hold women. She described a similar process of getting ready to go out, but attributed the way she was constructing her appearance to expectations place upon her. "If I'm going clubbing, then it's like a skirt or like a tight shirt or something like that...I feel like they expect you to dress more like provocatively. And it's kind of there's like definitely a sexual element to it." Anne expressed a similar sentiment:
Among the women I interviewed, whether explicitly acknowledged or implicitly suggested, partying is constructed as a combination of alcohol and (hetero)sexuality. Much of their mental energy regarding partying is focused on whether they are going out to try to meet someone, if they will "hook up" with someone that night, and how to read the sexualized attention they receive from men. The following words from Mary demonstrate the foregrounding of sexuality within party culture:
The discourse of heterosexualized interaction and partying is an important, and neglected, site of gender (re)production. My data show that women who participate in party culture conceive of (and thus construct) guys as being more focused on "hooking up" than women.
Julie explained this by stating that many guys "have an agenda," as Clarissa said, with a laugh, when guys go out they "plan on getting drunk and plan on getting laid."
Armstong et al. (2005) theorize that college women who participate in the party scene interpret their knowledge and familiarity with the "givens" of party culture as a skill of which they should be proud. Similarly, I found that being able to navigate the party scene, in which it is expected that men are partying with the intention or goal of "getting laid" implies a level of sophistication. This can be seen in the apparent nonchalance of Dana, a long-time (since early high school) participant in party culture and a graduating senior:
Her repeated and unprovoked insistence that the behavior of "guys on the prowl" does not offend her is notable; suggesting that not being offended by the sexually aggressive behavior present in party culture may be a skill in which she is accomplished, and of which she is proud.
Taken together, the individual data from these interviews demonstrate that within the social/peer context of those who participate in party culture, the relationship between partying and sexuality is naturalized to such an extent that sexuality appears to be inherent within party culture. Within this context, sexuality and partying can be understood as co-constitutive. They are separate institutions whose meaning is constructed through interaction with each other. The constitutive link between sexuality and partying is strong, and it provides fertile ground for the "slutty" discourse.
The discourse of "sluttiness" is important to conceptualizations of female sexuality. Sexuality scholarship indicates that women and girls of a variety of race, nation, and class backgrounds, actively work to construct their own identities and sexualities in opposition to being "slutty" (see, for example, Martin, 2005; Schalet, Hunt, and Joe-Laidler, 2003; and Stombler, 1994). "Sluttiness" is the negative referent against which respectable and controlled sexuality is compared.
College women who participate in party culture must negotiate the powerful, pervasive, and stigmatizing cultural assumption that "drunk girls are slutty." Consistent with the literature on sexuality, the most common way the women I interviewed engaged with the "sluttiness" discourse was to define themselves in opposition to it. Calling upon the stereotype of sorority girls as "sororowhores" or "sorostitutes", and distancing themselves (or groups to which they belong) from behavior attributed to girls "like that" was a common method of engagement with the "slutty" discourse. Examples of this run through my data. The following illustrates a few versions of this stereotype:
That the women I interviewed would participate in the construction of women -- particularly sorority women -- who participate in party culture as "slutty," was an unsurprising finding. Also unsurprising was that the women I interviewed explicitly distanced themselves from that which they understood as "slutty." What was surprising, was that data from these interviews suggests a necessary (for this population) component to slutty -- "too much" or "excessive" drinking.
Although "slutty" has typically been defined as a woman who has sexual relationships with many different men in a short period of time, and/or engages in sexual activity with men without emotional involvement, data from this paper show a different definition in operation among the binge drinking college women I interviewed. The women I interviewed interact with and construct a definition of "sluttiness" that is based on a combination of a woman's sexiness and "too much" or "out-of-control" drinking. Sexiness (which, as the previous section of this paper demonstrated, is constructed as always present in both party culture and the women who participate in party culture) combined with "too much" drinking creates "sluttiness." In another form: (DRINKING) + (sexiness) = Sluttiness.
This construction of sluttiness as conflated with drinking or partying too much can be seen throughout the data. Anne's words demonstrate engagement with the "slutty" discourse in two ways. She both participates in the construction of partying as constitutive of "sluttiness," and others "those" girls who are in the crowd that parties a lot who -- unlike she and her friends -- are "sluts."
Helen articulated a more internalized and self-referential perspective regarding the relationship between alcohol and "sluttiness" that was rooted in her Korean background.
In contrast to Anne, Kim, and Laura, in this quote Helen is not explicitly distancing herself from other women in her peer group who she defines as "slutty." When differentiating herself from women with stigmatized identities and sexualities, Helen has two reference groups; the "slutty party girl" one that she shares with Anne, Kim, and Laura, and (perhaps particularly Korean) prostitutes. Helen is participating in constructing an additional group of women -- prostitutes -- as "other."
Importantly, as a Korean-American college student, Helen is shaped not only by images and messages from her Korean cultural background, but also by those from her active participation in campus party culture at a prototypical "American" college. As such, and as is demonstrated in the following quote, she must engage with the "slutty" discourse that is dominate within campus party culture. Her words here show her interpretation, and construction, of partying primarily, and (assumed) sexual behavior secondarily, as constitutive of "sluttiness."
An informative account of the perceived relationship between women (particularly sorority women) who party, sluttiness, sexuality, and vulnerability can be seen in Jen's words. In a few phrases she makes clear the connections the elements of campus party culture that create the context of danger for women who party. Her words also illustrate the way in which women who party -- and are thus themselves vulnerable to the same stereotypes being applied to the sorority women -- distance themselves from the "sororowhore" image and "other" those who fit that description.
The following excerpt is Jen's response to one of the questions I asked every interviewee "What do you think, in general, of women who party?"
The richness of Jen's words lend themselves to more in-depth and focused analysis. Jen framed initial response to my question of what she thought of women who party in a negative tone. She did not call up positive images of fun, attractive women bonding with close friends, being social, or relieving stress. Instead she operated from what social work refers to as "a deficit perspective," calling forth a particular -- and negative -- vision of women who party as irresponsible women who do not have control over themselves. Jen explicitly stated that although she disrespects this lack of control in anyone, she holds a particular place of disrespect for women who lack control "because they are more vulnerable."
Her disrespect of vulnerability -- particularly in women -- can be seen mid-way through the excerpt of her interview when she sets up a juxtaposition between women who are "responsible" (the ones she respects) and those who are not (the ones she does not respect). According to Jen, women who dress provocatively (except for her) put themselves in danger because they might drink too much. This contrast is targeted less toward women who do not dress provocatively than she does to women who do not drink. She specifically says "I don't disrespect girls who dress provocatively if they are responsible." And she defines responsible by "limiting how much you drink, knowing who is getting your drinks, and having friends or someone you trust do it [get the drinks]."
It appears that she believes dressing provocatively (or "sexy") by itself is not necessarily risky. She also appears to believe that there ways to drink that are not necessarily risky. It is the combination of both drinking (particularly "too much" drinking) and dressing provocatively that she believes is truly productive of vulnerability for women. Too much drinking combined with too much sexiness (dressing provocatively and participating in party culture) is dangerous for women.
Jen uses -- exactly -- the same language to construct vulnerability among women who party that I used earlier to describe the construction of "sluttiness" engaged in by (for example) Helen, Anne, Kim, and Laura. For Jen, DRINKING + sexiness = Vulnerability. This is an interesting and important similarity. "Sluttiness" and vulnerability are constituted by the same elements. As such, they are largely conflated within campus party culture. Perhaps more accurately, within campus party culture "slutty" is code for "vulnerable."
When I asked Jen to clarify what she believed drinking and dressing provocatively made women vulnerable to, she responded without hesitation, "Having sex." This choice of words is very telling. Throughout the process of interviewing women for this study I heard many references to women being vulnerable, or "getting into bad situations" or having "something bad happen." When pressed for clarification they most often looked at me as though they were somewhat irritated at having to say the words, or perhaps even think them at all, then eventually said "sexual assault", or, sometimes "rape." Jen, however, said "having sex."
It is possible that she really meant "having sex" was what women who dressed provocatively and had "too much" to drink were at risk for. Mainstream substance abuse prevention in schools, and in public service announcements on television, radio, billboards, etc., does teach people that good judgment is a casualty of having too much to drink. Perhaps she was thinking that the ability to make good judgments is what normally prevents women from having sex, and therefore loosing the ability to make good judgments would prevent women from being able to decide not to have sex. This seems highly unlikely.
It seems much more likely that what Jen was thinking when she said women are made more vulnerable to "having sex" was "being raped." For Jen, "vulnerable" is code for "rape-able." By simple extension of logic, if "slutty" is code for "vulnerable" and "vulnerable" is code for "rape-able," then "slutty" is code for "rape-able." To return to equation form: If Slut = Vulnerable, and Vulnerable = Rape-able, then Slut = Rape-able.
Melissa explicitly made this connection in my interview with her. When asked what she thought, in general, of women who drink, she responded by saying:
As with the earlier excerpt from my interview with Jen, this section of my interview with Melissa is incredibly rich, and draws connections between phenomena often only suggested through subtext and insinuation. Melissa unhesitatingly calls upon dominant discourses regarding the causes of sexual assault, namely that men can be so "tempted" by attractive ("temptingly" dressed) women that they "can't stop themselves" from raping women, or that women are responsible for being raped if they wear short skirts (are dressed like a slut.) The causal link between appearing to be slutty and the risk of sexual assault was also acknowledged by Cassie, who described consciously strategizing to make her self less sexy in attempt to be safer.
As Cassie and Melissa articulated so well, there is a culturally constructed image that women who appear "slutty" are at risk of rape. "slutty"
As I have shown throughout this paper, in the data from interviews with Helen, Anne, Kim, Laura, Jen, Melissa, and Cassie, the women I interviewed actively constructed "slutty" girls as "other" from themselves. They defined themselves and their activity in opposition to that (and those girls) which they perceived to be "slutty." I believe they did this in part to feel safe from the risk of sexual assault, and safely distant from the women who were at risk for or were actually sexually assaulted.
Sexual assault is both an individual and cultural a phenomenon; one that Catherine MacKinnon has likened to terrorism. In the introduction to Feminism Unmodified, (1987) she wrote the following:
The language used by the women I interviewed is, fittingly, somewhat modified from that used by MacKinnon. They use code language of "vulnerable" and "slutty", but the process and the result is the same as described by MacKinnon. Jen says that she disrespects women who drink excessively and who wear provocative clothing such as short skirts and tight, low-cut tops, because that makes women "more vulnerable." And Melissa talks about girls who are "slutty, wasted, and... prey for a predator." Jen and Melissa, as MacKinnon suggested, have been using the information available to them through dominant discourses to understand who is at risk for rape, and they are attempting "not to be her."
Another way in which women attempt to define themselves away from being at risk of rape is to engage with the "blaming the victim" discourse in which victims are blamed and held responsible for the crimes that are perpetrated against them. The following words describe the process Amy went through after learning that a friend had been raped.
The end of this quote from Amy is an excellent example of the "blaming the victim" discourse. When Amy says the she has not changed how she parties because she "know[s] what's going on around her" and thus "wouldn't put [herself] in that situation," she is framing herself in contrast to her friend who was raped. Amy is thus implicitly blaming her both for putting herself in a situation where she was vulnerable to being raped, and for being raped. Again, Amy uses code language by stating that she wouldn't put herself "in that situation"; avoiding using the word rape when she refers to herself.
Erin provides another example, another perspective, of how women blame each other for being victimized.
In slightly different ways, both Erin and Amy are attempting (to use MacKinnon's analogy) to make the random appear systematic. They are, as are most of the women I interviewed, attempting to delineate the types of people, circumstances, and behaviors in which rape does or does not happen. If, as Erin says "we choose to put ourselves in those situations" then we can avoid rape by choosing not to put ourselves in "those" rape-producing situations. We can also explain why some women (who are/look/act differently than we do) are raped. This process creates an image of rape as systematic and not random. And it allows some women to feel safe from the risk of rape.
One of the reasons that the women I interviewed judged other women and/or blamed them for being sexually assaulted was that these other(ed) women did not adhere to unstated (and remarkably uniform) rules of safe partying. Women utilized many different strategies for maintaining safety, or as they would say "avoiding bad situations" in the context of partying. They identified strategies such as monitoring their alcohol intake, holding on to their drinks (to avoid "roofies" or date-rape drugs), and regulating what they (and their friends) wear to certain parties or bars. These were all common strategies. However, two "rules of safe partying" emerged from the data as nearly universal. The first was to party with friends and/or people they knew. The second was to watch out for each other.
Despite a generation of sexual assault prevention and education that has stressed the far greater likelihood of being raped by a close friend, family member, or acquaintance, the myth of the predatory stranger rape has not been dispelled. The women I interviewed consistently said they felt safe because they partied with their friends and people they knew, but many said they were afraid when walking home after dark. These two contrasting views of safety and danger are presented below. First, Aditi expresses her fears of walking home alone at night.
Next Soon-yi articulated the more commonly voiced perspective that described safety as being around friend(s):
Although in a different context, Anne articulated a similar sense of safety because she parties among friends. She gave the following response to my query regarding her thoughts about whether women who party are at increased risk for rape and/or sexual assault
Both Anne and Soon-yi explicitly say that they do not feel at risk of sexual assault because they only party with their friends, and/or they do not party with strangers. They appear to make the assumption that only strangers -- never friends -- might assault them. If they do not party with strangers then they will be safe from sexual assault. Two things are interesting about this perspective. One is that it is (if one believes the statistics) inaccurate that people are at more risk from strangers than friends or acquaintances. Instead, many studies have demonstrated that women are at much greater risk of sexual assault at the hands of people they know and/or trust, than they are of stranger rape (Koss, 1995; DeKeseredy, 1995.)
Begging the question of what consequences might arise from women utilizing a strategy to stay safe, and assuming a safe space, that in reality could conceivably put her at higher risk of assault. As a member of a tight-knit club on campus, Jen gave me an example of that potentially problematic sense of safety in (small) numbers. She told me she does not feel at risk from sexual assault.
Jen believes that no one within her "smaller community" of people who are in this club would assault a woman in the club, because the social cost too him would be too high. She appears to assume that if one of the guys in the club assaulted one of the girls in the club, that other people in the club would find out-and that they would care and/or do something if they did find out. In addition, her assumption is contrary to Koss's (1985) work in which she found that 42% of people who identified as being a victim of sexual assault on her survey had never told anyone else.
The second interesting issue regarding safety and partying with friends is the way in which the women I interviewed conceived of "friend." For the purposes of partying and feeling safe while partying, the women I interviewed seemed to conflate the concept of "friend" with anyone who fell within a network of friends who are friends and/or acquaintances with each other.17 And for the women I interviewed, that network of friends often seemed to cover most of the people who participated in campus party culture -- quite a wide network. People who were considered not safe to party with generally were older, or not students, or "townies" who are not affiliated with the university.
In addition to strategizing for safety by partying with friends and acquaintances, nearly every woman I interviewed talked about using a strategy of "watching out for each other." This most often took one of two forms, either watching who friends talked to at the party or bar, and preventing friends from leaving the party or bar with someone deemed inappropriate. In the first case, friends would watch out for each other by monitoring who friends were talking to at a party or bar, and for how long. This allows friends to intervene into a conversation if it appeared that another friend was talking with someone they did not want to be talking with. This was seen as an act of assistance in terms of being able to get away from men the women I interviewed (and their friends) did not want to talk to at a party or bar. It was also seen as a safety precaution that would help keep friends from getting in to "a bad situation" with a strange (unknown) guy. As Erin said, "I have really good friends that would stop you or stop each other from talking to some sketchy guy in the corner." Very often the women I interviewed had elaborate systems of communication with their roommates and friends to signal their need for assistance in extricating themselves from a conversation at a party or bar. These signals included eye signals, minor head gestures, waves, and the very common grabbing of a friends' hand and pulling while saying "will you come to the bathroom with me."
The other form that "watching out for each other" took was intervening if a friend appeared to be leaving the party or bar with someone she was not "supposed" to leave with. This could include ex-boyfriends, ex-"hook-ups," or -- the worst possibility -- strangers. This intervention was sometimes described as "the buddy system" in which people who went to the party together agreed to leave the party together -- unless something (the right guy) came up. Monica describes this phenomenon, including the hierarchy of who it would be appropriate for a friend to leave a party/bar with in the following quote.
Despite the quotes here from Erin and Monica that suggest an effective method of "watching out for each other" that helps keep women who party safe, this strategy and ones like it are ultimately not very helpful or effective. The watching out for each other strategy falls apart quite frequently. As often happens when combined with (often heavy) drinking, a friend could step out of sight to get a drink, or go to the bathroom, or pursue their own social/sexual agenda at a party or bar, and then not be around to fulfill the social requirement to "watch out" for another friend.
Similarly, as I said earlier, partying with friends and staying away from strangers is likely a similarly ineffective strategy for avoiding rape and sexual assault. Research strongly suggests that friends (and family) are often the very people one must "watch out for" the most, for they are the ones most likely to commit sexual assault.
In addition, both of these "safety" strategies that were most often offered up as options by the women I interviewed, place a great deal of blame and responsibility on the victim. This is logical, if problematic. If one is to blame for a situation occurring, it implies that there was something, and is something in the future, that the person could do to prevent it from happening again. Unfortunately, these strategies largely ignore the impact of larger social structures -- likely because changing the larger social structure appears much, much more difficult than making small individual level change. The result is that women are (sometimes unintentionally) held to blame for their own actions, for the actions of the people who assaulted them, and for the larger social structures that support gender-based inequality and violence.
This has tremendous costs to women who are sexually assaulted. When the predominant reaction to rape and sexual assault is to blame the victim for allowing (or asking for) it to occur, a hostile and unsupportive environment for victims exist. In 1999 Schwartz and Leggett published findings of a study showing that the single best predictor of whether a rape victim develops traumatic symptoms (symptoms that meet the DSM-IV definition of Post Traumatic Stress Disorder) following the rape, is the reaction of her social support network -- her (usually female) friends. Women whose friends and family suggest that they might have been in some way responsible for causing the assault, or at least for not preventing it, are far more likely to experience emotional trauma than are people who received instant support and encouragement from their friends and family. There are consequences for women of living in a world where women denigrate other women and blame "slutty" women for being raped in attempt to define themselves out of danger.
This is not a new problem. In writing of the tendency of women to distance themselves from victims of rape and sexual assault by othering victims, Catharine MacKinnon writes: "The problem is, combining even a few circumstances, descriptions, conditions, and details of acts of sexual abuse reveals that no woman has a chance." She eloquently, if somewhat pessimistically, argues that the lines in the sand that women draw to distinguish themselves from rape victims are ineffective, because the slightest change to those lines causes the distinction between safe and not safe to disappear. As Dorothy Allison wrote in 1984, "No one is safe, because we have not made each other safe."
Four important findings emerged out of the data for this study. The first is that sexuality has been socially constructed to appear as a natural and inherent and invisible component of sexuality. This information allows for better understanding of what "partying" means, and the ways in which participants in party culture receive pleasure from participating in it.
This initial finding is related to the second, and perhaps most important finding. This finding has to do with a clarified and specific conceptualization of "slutty." Through analysis of data from my 31 interviews it became clear that the popular conception of a "slutty" as a woman who has sex with many different men in a relatively short period of time without a necessary or strong emotional attachment to the men involved (Stombler, 1994; Armstrong et. al, 2005), is not the definition of slut at work in the culture of the women I interviewed. For the women I interviewed, drinking is a necessary component of "sluttiness." In fact, for those women involved in party culture, with the presumption of sexuality as a natural and obvious component of party culture, drinking -- particularly drinking too much -- is what makes a woman slutty.
This study also shows that the binge-drinking undergraduate women who participate in party culture, construct and are thus constructed by, the image of sluttiness as both a result of drinking "too much" and a risk of sexual assault. For the women I interviewed, drinking is sluttiness, and sluttiness is the risk of sexual assault. "Slutty" is code for rape-able.
As a reaction to the risk of sexual assault associated with "slutty" women, slutty women and those perceived to be slutty women are constructed as "other." The women I interviewed engaged with the "slutty" discourse by actively distancing themselves from the "slutty" party girls -- particularly the sorority girls. They also distanced themselves from rape victims by creating unspoken rules for partying which, if obeyed, would theoretically protect women from rape. Those who did not follow the rules were stigmatized.
Finally, there is a high cost to the practice of blaming rape victims for being raped, and for creating flawed mechanisms (such as unspoken rules for partying) in which women can imagine they are safe from the risk of rape and sexual assault. The consequences of "othering" women who appear at risk for sexual assault creates a tremendous burden for women who are actually sexually assaulted and high rates of psychological and psychic distress.
This study has direct and important implications for feminist theory and practice. Perhaps most obviously, this study shows the desperate need for feminist theorizing on the experience of partying. The area is on rich theoretical grounds, with potentially insightful and expansive connections to be drawn to gender identity, sexuality, sexual violence, the body, and the classic pleasure/danger debate. There is a deafening silence around issues of partying in feminist and sexuality theorizing. Theoretical activity in that area could enrich and expand the scope and reach of feminist theory.
The implications of this study for feminist practice and intervention are equally rich. This study shows a disturbing conflation of the stigmatized label of "slutty" and perception of the risk of sexual assault. Relatedly, this study expands our understanding of the processes, causes, and consequences of women's judgment of women who are, or appear at risk for, sexual assault. This process of othering should be deconstructed and incorporated in all anti-sexual violence prevention and training. It seems possible that including making people aware of the active role they play in stigmatizing victims of sexual assault, and the reasons that they do so (to attempt to feel safe from risk of sexual assault) might reduce stigma, ultimately resulting in a more supportive environment for victims of sexual assault.
Finally, this research suggests the need to disentangle the concepts of partying and 'sluttiness'. We need to work to promote a cultural image that being drunk does not mean being always already willing for sex. There is no reason that this message should not be as important in promoting healthy sexuality and sexual assault prevention as the recent anti-date-rape-drug campaigns that remind women to always know where their drinks are. It is evidence of the stigma associated with partying, and use of alcohol and drugs, that feminist scholarship has not taken up these issues. I believe active feminist engagement with issues of partying, sexuality, and sexual violence could make the world safer and more fun for us all.

Progress through an academic system can be seen as a series of transitions in which each transition in the academic life-course is conditional on the outcome of the previous one, and individuals and their families (consciously or unconsciously) rationally estimate the likely outcomes of future transitions based on the transitions they have already made. For instance, the choice to attend academic rather than technical high school precedes the decision whether or not to enroll in college, and less advantaged students might estimate lower future performance in school as well as lower future returns to academic high school in comparison with technical high school. Social (dis)advantage is thus a cumulative process of tracking experienced across many transitions (Breen and Goldthorpe 1997). The relationship between educational attainment and social origins has been shown to depend on the distribution of schooling in the population as well as on the effect of social origins on the likelihood of making a particular transition to schooling (Mare 1981). Although almost every country has experienced a major expansion of education in the last 50 years, class differences in transition rates have remained relatively stable over time even while gender disparities have declined (Breen and Goldthorpe 1997). Much of educational stratification research has assumed continuous or near-continuous educational paths as normative, although it is known that educational disruptions can have substantial negative outcomes.
But what happens when individuals who seem to be on one trajectory or have already left school wish to increase their educational attainment? Elman and O'Rand (2004) visualize three pathways of educational attainment: one of early education to advantaged persons (cumulative advantage), early school exits resulting in low wages for the least advantaged, and a third pathway of late education to those in the middle, who attend school in later life without experiencing the benefits associated with earlier attainment. As the proportion of a population participating in higher education changes, more adults may re-enter the school system. Indeed, education in later life is becoming more common in industrialized (Hamil-Luker and Uhlenbert 2002) as well as some developing countries (Qian 1994-5).
The proportion of the Chinese population with tertiary education has always been small relative to the entire population, but has fluctuated widely and is now rapidly expanding. Chinese education policy in the twentieth century has been subject to major swings accompanying shifts in the state's view of national development (Tsang 2000). In the 15 years after the founding of the People's Republic in 1949, the number of graduates of four-year universities increased by almost 10-fold as the central government concentrated on urban development, despite a slow-down during the Great Leap Forward (1958-1960) as the educational focus shifted to basic education in rural areas (Hannum and Xie 1994). In the mid-1960's, the debate about national development centered on whether to encourage egalitarianism or focus resources whether they would be most effective (i.e. higher education that would produce experts who could stimulate the economy.)
Poor economic progress during these years led to a return at the close of the Cultural Revolution to an emphasis on science and education in order to achieve modernization. In the reform period, status differences are to be tolerated as incentives, and a technocratic elite is essential to improving material resources for all. Since the early 1980's, the Chinese post-secondary educational system has still been characterized by a limited supply, due to comparatively low government investment in education compared with similar developing countries (Heckman 2003). The education system has been under enormous pressure throughout, with tens of examinees, including high school students, perennial examinees, and employed persons, for every enrollment slot in higher educational institutions. Just after the Cultural Revolution ended, competition was especially intense, as urban youths who had been sent down to rural areas returned to their cities and were allowed to take the college entrance examination from 1977-1981. During the reform period, an expansion in irregular and formal adult education has helped to relieve some of the pressure (Qian 1994-5, Xiao and Tsang 1999).
Traditionally stratification and education researchers have held that very few persons who had already entered the work force passed the examination, while those who few who did obtain higher education did so through irregular means such as television or correspondence schools (Qian 1994-5). However, higher education to adults in China has rarely been examined. Research on delayed education has focused on the youths who were sent-down from urban to rural areas during the Cultural Revolution (Xie and Jiang, forthcoming, Zhou and Hou 1999, Deng and Treiman 1997, Giles, Park, and Zhang 2003). The sent-down youth were exempted from age limits on the exams between 1977-8 and 1981. Coming from another perspective, Li and Walder (2001) have pointed out the importance of the timing of party membership, as promising young people who are selected for party membership increase their chances of receiving a college education mid-career in a system of "sponsored mobility" so that the relationship between party membership and higher education is ambiguous.
Figure 2 plots individuals' ages at graduation by level for the highest strata of educational attainment, university graduates, sorting by age at completion of primary school, then middle school, then academic high school, then university (SURFL data, see below). Many university graduates had not completed certain educational levels which usually would be considered prerequisites for college, such as middle school or academic
high school. More interesting, even for these high achievers there is almost no normative age to complete a given level, with significant variation even at the primary school level accounting for some, but not all of the variation in timing of college graduation. Still, disruptions at earlier levels are substantial, so party-sponsored mobility cannot fully explain the variation in college timing either.
For the most part, researchers on social stratification in China have treated higher education earned in various years as equivalent, and have very rarely even collected data on year of graduation, with the implicit assumption that education is mostly a continuous process, although this is clearly not the case. Although Li and Walder (2001) discuss adult education, Meng and Gregory (2002) are perhaps the first to point out that education cannot be assumed to be continuous in the Chinese context. Focusing on the income effects of missed schooling and non-traditional primary and secondary curricula during the Cultural Revolution, they look at university entry age, defined as the respondent's age in 1978 for Cultural Revolution and year of turning 18 for other cohorts. They find a U-shape for probability of obtaining a bachelor's degree, with those 18 at the time of first examination having a probability over .12, a probability around .02 for 25-year-olds, and above .07 for those 31 at the time of their first examination. For "semi-degrees," a combination of junior college and correspondence degrees, the probability of enrollment by age is actually higher for those 29-31 in 1978 than for 18-year-olds, but the effect of age is smaller. The effect of missed schooling is found to be strongest for those who were between 23 and 26 at the time exams resumed in 1978 and had missed the most years of junior and senior high school combined. They also found that marriage before the first possible examination had a strong negative effect on enrollment for women. The groups who experience the most delay in education were those whose parents were less-educated and had lower occupational status, and this effect of having less-educated parents was stronger for women. Finally, the effect of missed schooling, especially high school, was more important than examination delay.
While Meng and Gregory (2002) assume disruptions to have occurred at uniform times for all respondents, Giles, Park, and Zhang (forthcoming) calculate city-wide birth cohort disruptions to education from individual start dates at various levels and assign those to individuals within the area to avoid correlations between individual characteristics and disruptions. They claim youth returning from having been sent down sat for the competitive merit-based college entrance exams between 1977 and 1981, while Meng and Gregory used 1978 as the date of the first exam. They calculate the average delay in starting various educational levels, as well as average attainment of those levels, by birth year for five cities, as well as correlations between disruptions and attainment by level. For the time being, we do not attempt to model the timing of disruptions, although we include a dummy variable for sent-down youths.
These studies have treated delayed education and age variation in education as purely a result of the Cultural Revolution. This paper argues, however, that the phenomenon of interrupted education is not limited to the Cultural Revolution; that the trend is in fact continuing among those coming of age during the reform period as well. Figure 3 shows 1990 and 2000 census data for college degrees from regular institutions of higher learning (bachelor's and graduate degrees) by age in 1990. For those in their mid-twenties through mid-forties in 1990, almost half of the college degrees were earned between the 1990 and 2000 censuses. Persons age 18-22 during the Cultural Revolution years of 1966-1976 would have been 32-46 in 1990. As expected, the 1990 census shows
a large drop in the number and percentage with degrees around age 46, with the 2000 census showing that much of this decline has been compensated for with late degrees. But the proportion is the same and the number of late graduates is much larger for those less than 32 in 1990; the percentage of 35-year-old males in 2000 with degrees was almost twice that of 25-year-old males in 1990, although that cohort is too young to have missed school during the Cultural Revolution. Clearly, college education after age 25 is commonplace for younger groups as well. Furthermore, given that 1971-5 are the years when the fewest students graduated due to school closings, we would expect that those 18-22 in 1971-5, or 35-41 in 1990, would have the highest levels of delayed schooling. However, the highest levels of delayed schooling appear to occur for those in their mid to late twenties in 1990. Paradoxically, educational attainment for men over 45 in 1990 is slightly lower in the 2000 census than in the 1990 census; this paradox is further amplified in the cohort percentages. The reasons for this discrepancy are not clear.
Similarly, late junior college degrees are noticeable until the late fifties, close to traditional retirement age, consistent with the comparatively lower financial and academic burden and easier enrollment process for older graduates who would have less time to enjoy the higher returns of a university education. Respondents born in 1965, too late to have missed years of schooling in the Cultural Revolution, earned about half of the junior college degrees they held by age 35 after age 25, and it is reasonable to think that this cohorts' attainment will increase by the 2010 census. Also striking in view of the gender structure of university degrees is the indication that junior college degrees are approaching gender parity for younger cohorts. One source of confusion shows up for
females over 40. Although the 2000 census counts more junior college graduates than the 1990 census at every age, the 2000 census counted far more people at those ages, resulting in a crossover of the percentages educated in those years. Although readers should add imaginary wide error bars to these graphs due to the difficulty of enumerating China's population, measurement error cannot be imagined to be large enough to explain
away the findings. Figure 5 sums the cohort numbers and percentages with university or junior college education to find the overall cohort figures for higher education. The problems with the percentages for older respondents in the 1990 census are compounded in this overall data.
There is a noticeable gender gap in educational attainment in China, but this problem is mitigated by late degree attainment. Figure 6 shows the ratio of degrees held by cohort and sex in 2000 to 1990. Although fewer total degrees or late degrees are awarded to women, a slightly higher proportion of women's degrees are earned late. This is striking in view of Meng and Gregory's 2002 finding that women who were married at the time the college entrance exam was first available to them were much less likely than unmarried women to enter college. Around 90% of women are married by age 27.
This study uses the 1999 Study of Urban Residents' Family Life, representative of adults in Shanghai, Wuhan, and Xi'an, which are all large cities. The survey design consists of a matched-pair sample of elders and one of their adult children living in the same city, as well as a general sample aged 16-95, yielding an overall sample of 4,444 respondents. Questions speak primarily to the individual's economic condition, family, time use, satisfaction with China's economic and societal development, and attitudes about family formation and roles.
Because our survey data come from large cities, we face bias resulting from selective migration into cities by more talented individuals and selective migration away from cities by less talented graduates. Educational attainment and income vary widely by region in China (Xie and Hannum 1996). Few graduates live in rural areas, although many live in towns. Migrants and non-migrant graduates may also differ in occupation or other characteristics. Due to the lack of information on age of graduation in nationally representative surveys, and like many other studies, we merely note this problem in passing. However, perhaps because of small sample size, and perhaps because of migration, city to city comparisons were not consistently significant (not shown).
Like studies of income returns to education, our survey contains two categories of higher education, which we call "universit"y and "junior college". For some reason, the SURFL specifies that transfer students are not included as university students. Unfortunately, the SURFL, like other studies of higher education in China (Giles, Park, and Zhang, forthcoming, Meng and Gregory, 2002) combines a wide variety of higher education options, from television and correspondence schools to formal 2 and 3 year degrees (Qian 1994-5) into the junior college category, which Meng and Gregory call a "semi-degree." These degrees may or may not vary in terms of knowledge gained and labor market outcomes. In addition, a small proportion of university graduates also hold junior college degrees or graduate degrees. Because we are interested in the hazard of first entrance into any kind of higher education, we combine all kinds of higher education, and in the case of multiple degrees, we use the respondent's year of starting the first tertiary degree. However, we do not count a respondent who has a graduate degree but never enrolled in junior college or university.
Most obviously, early and late graduates enroll at different points in their life course. Overall, 621 respondents attended higher education; 419 attended junior college and 214 attended university. The mean age of graduation from university is 24.6 with a standard deviation of 3.9, while the mean age of graduation from junior college is 27.8 with a standard deviation of 8.1. Figure 7 is a histogram of age at graduation for junior college and university graduates. The age structure for junior college attendance is wider, and most later graduates attended junior college, while university graduates are more
tightly clustered at earlier ages. The oldest junior college graduates worked between two and ten years between graduation and retirement, allowing little time to reap the rewards of their investment. Overall, 40% of university graduates and 50% of junior college graduates finished after age 25, with 8% and 30% graduating after age 30, respectively.
As seen in the census data, there is a strong gender gap in higher educational attainment in China. Overall, men are almost twice as likely to graduate, regardless of age or graduation timing. This effect is compounded as women graduates are less likely to be party members and less likely to be cadres even if they are party members. Only about a third (33.8%) of the sample of university graduates is female consistent with the proportion of female graduates in the 2000 census data (34.9%). In the reform period, the Chinese Communist Party has sought to recruit talented and highly educated individuals, as well as facilitating the education of those who already belong to the party (Li and Walder 2005, Hauser 2003). About 40% of the on-time grads and 60% of the late grads are party members.
The gender gap mentioned above is compounded by its interaction with party membership; only one third of female college graduates in the sample are party members, while more than half of male graduates had joined the party. Of 191 college graduates, about half (95) were cadres in the jobs they had held the longest, with 30 below and 37 at first (keji) level, 24 at the second (chuji) level, and 4 at the third (juji) level or above. Only 32% of women were cadres, with none at the third (juji) level or above, although 58% of men were cadres.
Wu and Xie (2003) show the importance of occupational sector change in change in returns to income. Obtaining higher education might be one important pathway to exit the state sector or change careers, given China's high job stability. As expected, late graduates were more likely to have changed sectors: 85% of on-time and 63% of late graduates last worked in the same sector as their first job. Job sector itself should also have a cause and effect relationship with educational attainment (Xiao and Tsang 1999).
Delayed education has traditionally been associated with the send-down experience and the Cultural Revolution. But only 20% of late grads (compared to 6% of on-time grads) were 18-24 in 1971, the year of lowest college enrollment during the Cultural Revolution. Having been sent-down to a rural area to work for a period of years has a more straightforward relationship to educational delay. Only 29% of graduates after age 24 (compared to 10% of on-time grads or 17% total) had ever been sent-down. Social origin is likely to have a quite strong relationship with educational attainment. Parents from advantaged occupations may have social connections useful in encouraging their childrens' education. More educated parents may be able to help mitigate disruptions in formal education and help their children prepare for examinations. However, educational attainment may not be a completely straightforward outcome for parents' either. In addition, mothers' and fathers' characteristics are strongly correlated.
Stratification researchers have been most interested in the effect of delayed education on income returns to education. A simple plot of income for respondents below and above age 25 at graduation (not shown) reveals the expected answer: 'on-time' graduates have a wider distribution of incomes and are more likely to be in the highest income quartile, while graduates after age 25 have a more compact income distribution.
Given the differences among early and late graduates, it would be useful to know what factors are associated with enrollment in higher education at three age groups. I employ multivariate logistic regressions in a discrete-time hazard analysis to estimate the relationship between respondents' characteristics and the timing of college. The unit of analysis is the person-year of exposure to enrollment in higher education. The dataset contains multiple observations for each respondent such that the dependent variable for each year prior to enrollment is coded 0, and each year after enrollment is coded 1. For respondents who had not enrolled in higher education by 1999, the year of data collection, the dependent variable for all person-years is coded 0.
I dropped respondents who had not graduated from at least one secondary school level, as they were likely at very low risk of enrollment (lower middle school, vocational high school, or academic high school). This left 413 respondents who had ever been junior college students and 209 who had been university students, as well as 515 respondents who had attained some secondary education and thus theoretically could have taken the college entrance examination. Respondents who graduated before 1949 are also excluded, with the idea that factors associated with college enrollment might have changed after the revolution, and because they do not tend to be in the 1999 labor market.
Factors associated with the hazard of entering higher education are presented at three age groups: enrollment by age 25, by age 34, and by age 43. In each period, respondents who graduated from college in the previous periods are removed from the sample because they are no longer eligible to enroll for the first time. This results in a changing sample for each group; descriptive statistics are shown in Appendix A. The proportion male declines across periods as men disproportionately enroll, but the proportion with party membership increases due to late enrollment. While 43% of the sample enrolled in higher education by age 25, if they had not enrolled by 34 they had only a 6% chance of doing so by age 43. The small number of respondents in the sample with higher education in this sample, as in any sample available to the researchers, results in a general problem of small sample size. For this reason, we have chosen to create two separate models, one including factors related to labor market experience, and one related to family and life-course events.
Model 1 shows the association of labor market factors with the hazard of college enrollment in various life stages. Men are more likely to obtain higher education in China. Consistent with Figure 6, however, the effect of male sex in Model 1 seems to be significant and larger for on-time enrollment, so that males are 1.65 times as likely as females to enroll by 25. Those who were party members by age 34 had 3.45 times the yearly odds of enrollment at ages 25 to 34, and membership by 43 increases the yearly odds of enrollment in the late age group by .926. Occupation in the first reported job is measured on the Treiman ISEI scale, which ranges from 0 to 90. Respondents who report never having had a job (n=35) were dropped, and the scale was recoded into 4 categories according to quartiles. An increase in one quartile of occupational status of the first job held was highly significant for early period college entrance. Respondents in the highest quartile had 5.61 times the odds of enrollment than those in the lowest occupational quartile.
Changing from the public to the private sector is well-known to be related to increased returns to education, especially for those who changed sectors later in the reform period (Wu and Xie 2003). Job sector here is measured differently, dividing jobs into 11 sectors. Intuitively, a desire to change sector might be a reason to acquire higher education, or higher education might expose a person to the knowledge and contacts necessary to make a switch. Changing job sectors was not significantly associated with college enrollment in the middle period and highly significant in the later period. Logically, changing sectors would have no effect for those who obtained their degree before they had a chance to work in two different sectors.
Respondents were asked a 14 question word-similarity test, including "How are a king and a president similar?" Responses were graded by the interviewer on variable scales, leading to a 31 point scale which is significantly correlated with a wide range of variables, but not with age. A ten-point increase in this verbal intelligence measure increases the yearly odds of enrollment by .73. Interestingly, the significance of the association between verbal acuity and enrollment is not strong for the latest enrollment group, and seems to have a large negative effect on the chances of enrolling even though the quiz score has only a .15 correlation with age.
Model 2 estimates the hazards of enrollment in higher education by 25, by 34, and by 43 when controlling for family and background-related variables. Birth year is highly significant for the youngest enrollees and significant for the oldest enrollees, likely due to the rapidly expanding education system for the youngest cohorts and the send-down experience for the older students. Male gender is included as a control and is not significant for this model.
As expected, ever having been sent-down to a rural area has a non-significant positive effect on the earliest students (increasing the odds by 1.56 times), who either were sent down after graduation or returned to the city by enrolling in school, and a negative effect on the second and third age groups. The yearly odds of enrolling decline with age for those sent-down, decreasing them by 2.13 and 3.31 times in the middle and late terms. These mixed and sometimes positive effects of an otherwise traumatic life experience are discussed in other research (Xie, Jiang, and Greenwell 2006, Zhou and Hou 1999, Deng and Treiman 1997).
Father's occupation at respondents' age 16 was based on the ISCO occupational scale. The data is missing for 47.5% of respondents, whose fathers were unemployed or deceased when the respondent was 16, who did not know, or were not asked their fathers' occupations because they were coded as elders in the matched-pair design of the SURFL data. Missing data is dropped, values 1-200 are coded as 5, 201-400 as 4, 401-600 as 3, 601-800 as 2, and 801-998 as 1. Lower occupation codes represent higher-status occupations, so a positive coefficient of fathers' occupation indicates that respondents whose fathers had higher-status jobs were more likely to enroll in higher education. As expected, the effect of fathers' occupation becomes less significant with age while the size of effects remains similar: the odds of enrollment increase by 1.23 times in the early and late periods and 1.37 in the middle period. The relationships of fathers' education and mothers' education or occupation with respondents' education were not strong, likely a result of the rapidly expanding education system (not shown).
Marriage also shows a differing effect across the life-course. Of the ever-married male and female respondents who met the educational requirements to be included in our sample, 61% and 32% were married by age 25, and 98% and 92% by age 34. Given that 75% of the sample who had one or more children had had a birth within 3 years of marrying, it seems that respondents in the 25-34 age group often have a small child at home. It is not surprising, then, that marriage by age 34 has a highly significant, negative effect on enrollment between ages 25-34, decreasing the yearly log-odds by 5.14 times. For respondents who are unlikely to have married yet or are less likely to have small children, marriage has no statistically significant effect. The somewhat negative coefficient of marriage in the first period, and positive effect of marriage in the older category are intuitive, as lower age at marriage likely indicates that the respondent did not expect to enroll and non-marriage in a society with nearly universal marriage likely is correlated with some negative unobserved features also associated with enrollment. An interaction term of female gender and marriage was not significant (not shown), however, which could suggest that the effect of marriage is the same for men and women, or could be an area for more careful future investigation.
As many as 40% of university and 50% of junior college graduates in China graduated after age 25, yet stratification researchers have rarely observed this striking fact. Indeed, education in China cannot be assumed to be continuous, as variation at every level is substantial. While delayed and adult education are often associated with disruptions due to the Cultural Revolution, cohorts too young or old to have missed pre-college education have also experienced substantial levels of higher education at non-normatively late ages. Indeed, the widespread irregularities in educational trajectories among the urban Chinese labor force provide an excellent opportunity for study of the causes and effects of educational timing.
Timing of college enrollment is associated with a number of variables in ways that vary across individuals' employment life-courses, including occupation of first job, party membership, verbal intelligence at time of test, birth cohort, sent-down experience, father's occupation, and marriage. Income returns to various types of higher education and according to timing are likely to differ, yet returns to education have not been estimated separately for early and late graduates. Results of the hazard analysis show that marriage deters yearly odds of enrollment between ages 25 and 34, while party membership by 34 is associated with higher enrollment odds in that period and after.
Unfortunately, little data has been collected to shed light on issues related to higher education in China, even though provision of higher education has been increasing at an almost exponential rate in the last few years. Survey researchers should pay closer attention to the timing and diverse types of higher educational experiences available in China today. Survey instruments could be designed to inquire about the start and stop dates at each education level, the name and type of school, the amount of time spent on coursework vs. employment, tuition and other financing, as well as party and work unit sponsorship of individuals. More research is needed on the variety of schooling available, as well as entrance and evaluation criteria of schools present and past. Little is known about how employers evaluate job applicants' educational credentials, or how income returns to education are related to quality and timing of education. Problems of small sample size for well-educated individuals in nationally or locally representative surveys could be ameliorated by over-sampling professionals or those with degrees.

Scholars share a common argument that engineering is the most male dominated of all professions (McIlwee and Robinson 1992). Although it grew from 46 in 1973 to 696 in 1995 (Long 2001), the number of women with Ph.D. is smallest in engineering compared to law, medicine and most of the natural sciences. This has been explained in many ways surrounding one central argument: the equation between masculinity and the whole engineering knowledge and practical structure and process. The analyses of the "masculinist bases of engineering" by Hacker (1979) shifted the study focus to the interlocks between patriarchy and technology resulting in creating and maintaining inequality and oppression. This makes the graduate engineering profession a symbol of masculinity (Cockburn 1984). Many scholars argued that the masculine values currently synonymous with technology could be replaced in favor of a new value system whose application would be controlled by women through women-centered leaning strategies (Rothschild 1983; Zimmerman 1983). The optimistic position focusing on the potential of women to take hold of engineering have not come true.
Whether and how the gendering of the engineering profession is being perpetuated? Much research emphasized that the biggest obstacles facing women are not about their knowledge and skills but of their adjustment to the masculinity of engineering culture (Hacker 1990; McIlwee and Robinson 1992, Dryburgh 1999; Seymour 1999). They concluded that women should exhibit characteristics of their chosen profession and be more masculine than their counterparts in sex-typical occupations as the ways of accommodation. However, the evidences of much study on other professions, such as law and medicine, directly counter this prediction: professional women are unreservedly feminine.
Secondly, there have been very few studies on women' lived experiences and their strategies to manage femininity and masculinity in engineering. Despite feminist criticism of engineering, many empirical studies indicated that most women were not bitter about their experiences in engineering (Trescott 1987). So what draws these women to and keeps them in engineering?
Meanwhile, the ethnical issues have not been noticed. The increases of the percent of engineering Ph.D. among non-white groups are almost entirely for Asian Americans (Long 2000). Xie (2003) argued that immigrant women increase women's representation in many areas. In a sense, Asian women are overrepresented among minorities in engineering, which makes great effects on their professionalization experiences.
Therefore, I chose to center the analysis on the professionalization of women coming from Mainland China in engineering school and intend to provide insight into their early socialization into the engineering culture.
From the interviews and observations, it is revealed that Chinese women are struggling to be professional engineering as well as professional feminine. So the questions are: How do they define and construct professional engineering and femininity? What motivate them to make these efforts? How far our female engineering graduates share feminist analyses? Do they feel any need to transform their profession and its practices? In stead of regarding femininity as a homogeneous category and exploring how women diminish or prevent their femininity to fit in engineering, I examine what room there is for variations in the construction of femininity in engineering and for negotiating the meaning of femininity. I argue that particular ethnic context and engineering graduate culture shape how women "engage, respond to, and resist dominant cultural images" (Martin 2006: 4) of femininity and how women do another femininity.
The professionalization process entails learning the appropriate theory and code of ethics, associating with the professional regulating body, and adjusting to or internalizing the values, norms and symbols of the professional culture (Greenwood 1966). Graduate school is regarded as crucial for starting engineering career because the failure at this stage effectively closes the door to professional engineering careers and later career trajectory changes is more difficult the longer it is delayed (Evette 1996). By this way, the engineering students leant to take on the identity of the professional, thus gaining the authority and legitimacy granted symbolically by the engineering regulating body and granted generally by the public. In the process, they have to learn what engineers actually do.
Hacker (1981) described the culture of engineering as a professional ideology which is based on a mind/body dualism which identifies rationality with men and emotion with women. It stresses masculine over feminine traits. There is little feeling of overt personal discrimination at an individual level (Carter & Kirkup 1990). McIlwee and Robinson (1992) pointed out the existence of a particular culture of engineering in the university. Compared to other professions, the differences between the university and the workplace are more extreme in engineering. For engineering graduates, the standards applied to success are mainly academic (McIlwee and Robinson 1992) and less open to bias than the standards applied to the workplace where discriminations are reported much more. The culture is least dominated by the interests of organization and is quite compatible with the math and science capacities of women. However, both of them are characteristic of valorizes individualistic, competitive and specialized approaches (Selby 1999).
Empirical studies identified engineering culture as hardworking profession and tightly knit community. So the challenges to women are to learn to project confidence in their abilities to adapt to high working pressure and show solidarity with others in their profession.
Presuming the engineering as possessing an unique masculine group culture (Greed 1991), the professionalziation is argued to be a process of women's accommodation to masculinity (Meadow 1989) and the difficulties and pains women have sustained became the focus in this area (McIlwee & Robinson 1992; Dryburgh 1999). A review of even the most recent literature suggests that knowing how to conform to the masculine professional culture and learn to perform and enact masculine norms of attitude and interaction is critical issue to women (...). This literature explored the masculine characteristics of engineering (...). It also examined the effects of gendered socialization on women's general orientations and attitudes toward professional success (...). The important factors involving in this process include family, especially mother's occupational status, school, career mentoring and particular events. Next, it noted the production of cultural messages which define engineering as a male field and contribute to the sense of discomfort engineering women feel (...). The literature emphasizing on basic and stable gender traits cannot explain sufficiently the situations of women in engineering nor why the numbers of engineering women had increased dramatically before 1990 and then remained on a stable level (...).
The different-sameness dilemma is also at the center of studies on law and medicine profession and organization (...). The former placed strong emphasis on how women are forced to be masculine in order to survive uncivil masculine culture of legal systems (...). The latter focused on the stratification and isolation of professional practices between men and women (...).
The discussions on whether professional women are or should be masculine or feminine understood gender as an attribute and have "led to the construction of unilateral categories which suppress differences between men and between women" (Kvande 1999:306). Secondly, they returned to gender socialization and psychoanalytical development as the primary determinant of women's professional status even though there is no links between childhood experiences and engineering students' current values, such as gender-assigned tasks and behaviors in childhood and attitudes toward characteristics considered womanly. Thirdly, they conflated "femininity" with "women" and take the conception of femininity for granted which only confirms the stereotypes of femininity. Those analyses did not address the size or prevalence of gender differences; this information was essential but well covered in the literature. In a word, the criteria of masculinity and femininity themselves have not be clarified.
Instead, I follow West and Zimmerman's (1987) conception of "doing gender" or Fenstermaker's (1985) application of gender as situated accomplishments. Here, gender is done in interactions which "is not always to live up to normative conceptions of femininity or masculinity; it is to engage in behavior at the risk of assessment" (Moloney & Fenstermaker 2002). For example, Kvande's (1999) research applied a relational understand of gender to illustrated the diversity in female engineers' construction of femininities.
Kvande's (1999) approach is promising but limited. Firstly, she ascribed the variation among femininities to the range of women's personality and individual choices. Her analyses is fixated on the reifications of femininities as discrete variables rather than concerning them as active social processes in the reciprocal relations between social structure and individuals. Secondly, she weighed different femininities equally and did not notice the power relations between them. In other words, she did not explain why majority of professional women did the same choice despite other possibilities. Thirdly, she emphasized gender as adaptable and mutable at the cost of affirming the natural class differences.
On the other hand, the analyses of femininity are grounded in Butler's notion of perfomativity. Butler (1993) distinguished perfomativity from performance in theatrical or dramaturgical terms. She argued that performativity "consists of a reiteration of norms which precede, constrain and exceed the performer an in that sense cannot be taken as the fabrication of the performer's 'will' or 'choice' "
This article is a study of variations in the construction of femininity. The focus is on the heterogeneity of "femininity" category instead of on differences between women and men or on the relations between the assumed different qualities and values of women and men. Then I turn to the area of masculinity and femininity studies.
This area is based on the work of masculinity scholars such as Connell (1987, 1995), especially the concept of hegemonic power which emphasizes the historical situations in which gender relations are established. Further, Chen (1999) claims that "In discussions of gender, hegemony is associated with the taken-for-granted conceptions about the nature of men and women, of masculinity and femininity" (586) and it is about one type of masculinity establishing and preserving ascendancy over others. Meanwhile Connell argues there is no hegemonic femininity, only an emphasized one.
Yet, just such an application is useful for understanding the production of a variety of feminine appearance routines. Scholars on profession shared the common argument that "femininity has more to do with a particular state of being than with actually doing anything" (Williams 1978: 78). Women do not face the pressure to prove their feminine identity. As a result, the "difference-sameness" debate is about whether and how women struggle to protect their femininity from masculinity.
Martin (2006) provided a useful framework of "hegemonic femininity" as a resolution. She criticized that Connell ignored the power relationships between women where race and class are unequal. She argued that power is just as frequently institutionalized in these relationships among women as they are among men and certain forms of femininity have become institutionalized as more worthy, complete, and superior forms. Those who take up a position of "hegemonic femininity" are not just coerced into this conception of femininity, but they also consent to it. It is actively taken up from within (Chen 1999). From within, it is also shaped and molded. It therefore has a hegemonic power over other femininities and has ascendancy over them in the social order.
Martin (2006) claimed it's useful to claim the existence of a hegemonic femininity and perhaps of a plurality of subordinate, marginalized, complicit femininities. The literature on femininity and engineering has no recognition of these complexities. It did not acknowledge the many ways women engage hegemonic femininity, nor the variety of femininities that are produced by such an engagement.
My interest is not just in knowing how femininity is done in variety, but how "it is done systematically and with social consequence" (Fenstermaker & West 2002). In other words, how gendered practices and interpretations contribute to the reproduction of the particular femininity complying to and accommodating women's subordination to masculinity of engineering structure. On the other hand, class, ethnicity and gender are connected with one another as activities and accomplishments; they derive their particular meanings through everyday social interactions located in particular engineering culture. In other words, I try to understand accomplishment of femininities combined with ethnicity and class as "outcomes of and rationales for rendering social inequality legitimate"(...).
I have been conducting 14 in-depth interviews at one American university engineering school3. The interviewees were female graduates students who graduated from universities in People's Republic of China. Given the focus of the research, most of the interview participants were women; however, many other men were involved in the informal activities under observation. All of them were under 30-years-old and most were single. All female interviewees but one came from middle-class families. The class backgrounds of male interviewees were more multiple. In addition, participant observations took place in the engineering offices, labs and informal parties.
An effort will be made to include students from all levels of the graduate program and main ethnic groups. The observation will continue in every very public settings where a variety of students could be observed in both social (cafeteria) and academic (classes, lectures) contexts.
The engineering school I studied is characteristic of the status between manufacturing-related orientation and fundamental academic orientation. Time and structure of engineering work varies according to specific areas and how the advisors organize it. For medical engineering, the comparatively long-term nature of product development means that daily deadlines are the exception, not the norm. In many processes, success does not depend primarily on the basis of hours put in, but on the care and creativity with which the work is done and the results recorded. This is in sharp contrast to other work, such as computer engineering especially software design. For the latter, more hours are translated to more code and directly equated hours with productivity. Also, product development cycles in computer engineering tend to be much shorter, which changes the day-to-day pressures on female students as well as professors.
The professionalization process can be understood as a career, marked by rites of passage, symbols, and rituals indicating progression and culminating in a transformed identity (Trice 1993). Haas and Shaffir (1991) describe this progression as one that includes
This career model, based on Goffman's (1959) analysis of self-presentation, works well to explain the "work hard" culture of engineering. It is a ritual ordeal that requires the student to demonstrate confidence in the face of strenuous challenge, anxiety, and self-doubt. Through this process, the engineering student eventually internalizes the professional persona of confidence. Many studies pointed out, the two crucial factors of engineering culture-work hard and confidence-construct the basic contradiction for engineering women (...).
Previous studies noted that the self-confidence of the engineering women is strongly tied to their success outside of work (...). By placing emphasis on their hard working, engineering women may be undermining a deeply-held cultural premise; this could easily have negative consequences for their sense of self -- it is especially true for single female graduate who place more emphasis on work than on other aspects of their lives but are not trying to combine work with family in reality because working hard is connected with masculinity opposed to femininity (...). My study reveals the opposite results because of the intersection between ethnicity and gender as well as the women's various concern with femininity.
For all female my interviewees, the most salient thing they have to manage is not their gender but ethnicity. When I asked them whether they felt comfortable to study and work in engineering, almost all of them offered answer similar with this:
For Chinese women, the challenge is not working with men, but working with men from different countries and areas. The proportions of different ethnical groups in engineering school are different depending on the specific areas. Asian graduates make the main graduate body. Mainland Chinese graduates, especially female graduates, concentrate in doctoral program. For example, in computer science, Chinese women constitute the main body of female doctoral students and Indian women constitute the main body of female master students. The only exception is environment engineering where the proportion of female graduates is the highest and that of Chinese graduates is the lowest. Interviewees always mentioned ethnicity automatically when they talked about their working environment and their own position. To be contrary, they hardly notice the existence of "gender":
Contrary to previous studies, Chinese female graduates' ambivalence to developing technical expertise is lower. Meanwhile, they are not learning to adapt to the work hard culture of engineering and to the predominance of males in their classes and offices. Rather, they integrate work into their ethnical and feminine identities to increase their confidence.
In observations, Chinese women kept on referring to males in other ethnical groups. For example, women talked about how long their Indian male officemates stay in the lab which constitutes their motivation to work harder. Or, a girl said that she was very proud that she as the only Chinese student in her area stayed in the lab alone on weekend. Or, they often laughed at American white men's laziness and arrogance. However, they never compared themselves with other Chinese guys while they also noticed how hard their Chinese male colleagues worked.
On the other hand, female interviewees confirmed the relationship between the work and "performative" of femininity. All of them said they do not have time to pay much attention to their appearance which they believe causes persons' stereotype images about engineering women.
However, instead of acknowledging that they were not feminine, women emphasized that they were morally "pure" -- which is more important feminine criteria compared to attractive appearance. They differentiate themselves from women outside engineering. In interviews, most female interviewees automatically referred to other Chinese women in social science and humanities when. They mentioned that "those humanities women" always stayed in the dormitory, got up very late and did not need to work. They also talked that "those women" often hung around and resorted to places for relaxation or recreation and they were lack of those experience so they were behind the times but traditional and good. They said that -- compared to those women -- they had too much work to pay attention to appearance. However, they insist that they have the capacity to dress well.
All female graduates emphasize the significance of work. Firstly, they distance themselves from other ethnical groups, mark their intelligence privilege as a "Chinese" and eliminate the contradiction between femininity and work. Secondly, they admitted that engineering work, because of its time and organization, limits their time to display their femininity. But through separating hard working from the feminine stereotypes represented by women in humanities and social science, they reject the significance of appearance for doing femininity. On the other hand, they emphasize hard working and staying at labs all the time protected their femininity from contaminated.
In other words, they defined engineering femininity as "simple, clean and traditional but intelligent" the core of which is "purity" (DanChun). Therefore, "working hard" is constructed as a part of femininity for Chinese engineering graduates here.
The past three decades watched that the engineering work has been increasingly desk-based or visual-display-unit-based though the stereotypes of engineering involving a close relationship with heavy machine are partly true.
Some of my interviewees do have troubles with machines and hand tools depending on their particular areas. They manage the manual difficulties in two ways. Part of my female interviewees distinguished themselves from "women" as a general and collective category. They said they themselves as individuals had no problem with these manual issues. Meanwhile, they said engineering was not fit for "women" and they would not encourage other women to pursue engineering career.
Meanwhile, part of my interviewees linked the unpleasant manual part of engineering work with working-class while they also said they got along with manual labor well. They joked with machines and hand tools. However, they described that their lab environment are similar with factories and one of them talked about her problems in working with workers during intern practice.
On the other hand, another change in the engineering career paths is engineers' moving into management. Different from previous studies (Carter & Kirkup 1990; Mattis & Allyn 1999), only one of my female interviewees ever considered going into a managerial position though all of them expressed a desire of supporting or pushing engineering men into it. To be contrary, mechanism for male interviewee's finding new development opportunities varies from formal to informal and from systematic to random. Women's low engagement in the queue for management training makes their routes for progression into management positions very hard.
Chinese women link management with "complex and dirty" personnel management which is not "pure" feminine at all. All claimed that they were not concerned with money so much. All but one said they prefer to deal with machine. They ranked technical expertise as more valuable and morally "pure" than knowledge of society. They described social sciences in womanly terms: soft, inaccurate, lacking in rigor, unpredictable, and amorphous. However, very few felt that their male peers were inadequate because they lacked knowledge of social systems. And they looked proud of their own inadequacy and interpreted it as traditional and conservative -- in other words, pure -- when they talked about femininities of other women outside engineering. For example, one interviewee focused on social abilities and linked them with "open party girls" when she talked about why humanities women were more popular on campus.
One of the direct results for the indifferent towards management is Chinese women do not inclined to set up their network outside the Chinese community as their Chinese male colleagues. Many of them identified it as the main difference between Chinese engineering men and women. In a sense, these engineering girls are working under a comparatively isolating condition.
Dryburg (1999) talked about that engineering culture is defined by contrasting engineers with the "Artsy" which is a derogatory term referring to male students from the Arts and Social Sciences departments. The "Artsy" is impotent, cannot get a woman or job and cannot even do what he purports to do best.
Chinese engineering women shared this notion of "Artsy" though they did not use this word. They insisted that engineering guys are the best candidates for husbands. In this sense, Chinese men became the sexual objects of women. Meanwhile, they accepted themselves to be regarded as sexual objects for men. It is also linked with their commitment to the Chinese groups.
Further, women extend the notion of "Artsy" and applied it to women outside engineering. They acknowledged that those women are feminine but diminished those femininities' values: Those women are sexually attractive but they cannot get job so as to be dependent on men; those women are good in managing appearance which are trivial.
The other problem is about women's interaction with men at labs. Basically, women intended to perform "tender, quiet, patient" and sometimes "dependent" even though they themselves were strong and decisive. Different women perform differently. On the other hand, Chinese men evaluated these women's "accomplishment of femininity" in different ways. For example, they were much more friendly to those "feminine" women and offer more help.
Almost all interviewees said that the most important thing about femininity is how to perform femininity in relation to Chinese men. They refused the feminine criteria scholars used in judgment, such as personality and appearance.
All interviewees refused to take the American femininity or masculinity into account. On the one hand, they thought their American female colleagues and faculties were too masculine. In a sense, they equated "feminism" with "western". Many of them claimed that they did not like feminism and thought Americans were too sensitive to gender issues. On the other hand, they thought American women outside engineering and science were too open to be morally pure.
The same characteristics, which are treated as masculine in previous literature, can be interpreted as feminine by Chinese women. Chinese ethnicity and engineering factors are the core of femininity for Chinese engineering female graduates. On the other hand, women choose different reference groups to construct their femininity though it is always built in relations. This femininity is characteristic of morally "purity". It is contrary to Karin's study, appearance is not an important feminine criteria for Chinese engineering women.
Femininity is something need to be done. Women have different criteria to construct it. The problem is not about the content of femininity or masculinity but about their significance and positions in the identity construction.
I have tried to explore femininity in relationship to other axes of identity by exploring what utility come from thinking through gender as a performance and as performative and by exploring how the various gestures of femininity never exist outside of ethnicity and moral meanings. Gender is largely missing as a category of identity in the study, making it difficult to have a cultural or political identity.

