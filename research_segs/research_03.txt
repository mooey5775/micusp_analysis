Light incident on a metal releases electrons, called a photocurrent. This well-known phenomena is called the photoelectric effect. In 1887, Heinrich Hertz accidentally discovered the photoelectric effect during his famous experiment in which he produced and detected electromagnetic waves (Tipler & Llewellyn 1999). Hertz was using a spark gap in a tuned circuit, and he noticed that the spark length was shorter when he enclosed the apparatus in a dark case. Though Hertz was annoyed by the effect at first, he later studied it and published his results. Other physicists confirmed and extended Hertz's research. In 1900, P. Lenard determined that the particles were electrons (Tipler & Llewellyn 1999). Classical electrodynamics suggests that the maximum photocurrent is proportional to the intensity of the incident light. Lenard observed this as he expected. However, Lenard also observed that there is no minimum intensity necessary to induce a photocurrent, which contradicts classical expectations. More generally, the energy of the emitted electrons does not depend on the intensity of the incident light.
In 1905, Albert Einstein explained these confusing observations by assuming the energy quantization hypothesis used by Max Planck in his solution to the blackbody problem (Tipler & Llewellyn 1999). If it is supposed that light of frequency ν is composed of discrete packets (photons) with energy hν, where h is a constant, the above results are expected: an electron is ejected only when a photon of sufficient energy hits the metal cathode. If the frequency is too low, no photons will be emitted. The intensity does not affect the energy of the emitted electrons, but instead determines the emission rate.
According to Einstein's predictions, the energy of an ejected electron is
where W is the amount of energy necessary to eject an electron from the metal.
Robert Millikan conducted careful experiments that showed the electron energy is linearly proportional to frequency. His results, published in 1914 and 1916, confirmed Einstein's prediction and gave a value of h that agreed with Planck's value. Einstein won the Nobel Prize in 1921, and Millikan won it in 1923. One of the main reasons that both Einstein and Millikan won the Prize was their work on the photoelectric effect.
We sought to confirm Einstein's predictions and measure the value of h through the photoelectric effect. We used an apparatus made by the Daedalon Corporation that consists of a photocell (large photocathode and thin wire anode) housed inside a metal box. There is a retarding potential applied across the photocell, i.e., a power source with positive terminal attached to the photocathode and negative terminal attached to the anode. The retarding potential can be adjusted with a knob on the box. Also, there is a current amplifier that converts the pA anode current to a measurable voltage. The zero point of the current amplifier can be adjusted with a knob on the box labeled "Zero Adjust". See Amidei (2004) for schematic diagrams of the apparatus.
The apparatus has output jacks that can be connected to a digital voltmeter (DVM) to measure the retarding voltage. There is an analog ammeter on the front of the box that measures the anode current, but we ran leads out of the box so we could read the anode current off of a DVM.
We used a mercury arc lamp and a helium-neon laser as our light sources. In order to isolate single lines of the mercury spectrum, we clipped filters onto the front of the box. We used high pass filters with wavelengths of 546 nm and 405 nm and interference filters with wavelengths of 577.7 nm, 435.8 nm, and 546.1 nm. (High pass filters pass all wavelengths above the specified value. This does not matter, however, because the stopping potential is eqal to the energy of the highest frequency, i.e., shortest wavelength, electrons.) We also tried to use the mercury UV line by not putting a filter on the box, but this did not work because the maximum retarding voltage is less than the stopping potential for the photoelectrons ejected by the UV light. The wavelength of the laser is 632.8 nm.
In order to minimize the effect of ambient light, we placed the lamp and laser as close to the window of the box as possible, covered the apparatus with a black cloth, and turned the room lights out.
For each wavelength of light, we measured the photocurrent as a function of retarding voltage. Before we took measurements, we determined a rough estimate of the stopping potential and set the zero point for the anode current. Starting with retarding voltage VR = 0.00 V, we measure the photocurrent I (output as a voltage) every 0.10 V until we neared the neighborhood of the stopping potential VS. When close to VS, we decreased our step size. By observing the range of values of the anode current for each retarding voltage, we determined uncertainties on the current values. The uncertainty decreased as we neared the stopping voltage; in this regime, the uncertainty was on the order of ±0.1 mV.
It was expected that the uncertainty in our final values would be dominated by the uncertainty in I and by systematic error introduced through our method for determination of VS. The uncertainty in wavelength is minimal, because Professor Amidei showed that the FWHM of the mercury lines is on the order of angstroms. The uncertainty in VR is also negligible because the DVMs have sensitivity of ±0.1 mV.
Plots of photocurrent I versus retarding voltage VR for each filter are given in Figure 1. The colors of the data points correspond to the filter used, as is explained in the key. (The filters denoted "cheap" are the high pass filters.) It is easily seen that lower wavelength (higher frequency) light required a greater stopping voltage. This is in qualitative agreement with Einstein's predictions. Error bars are not plotted to prevent the figure from being cluttered. As mentioned above, uncertainties near the stopping voltages are on the order of ±0.1 mV.
We extracted VS from the curves of Figure 1 through a simple procedure: we found the two smallest values of VR that had the same value for I and defined VS to be the retarding voltage halfway between the two points. We defined the symmetric uncertainty to be half of the difference between the points.
The above procedure is the biggest source of possible systematic error in our measurement. The stopping voltage is not well-defined for various reasons. Most importantly, the photocathode has a nontrivial depth, so the emitted electrons will have a range of energies. It is useful to think of the cathode as a diode. The natural direction of current flow for the photocell is from cathode to anode, and the retarding voltage resists this. The plots of I vs. VR resemble the same plots for a diode, as neither are sharp delta functions as they would be ideally (Melissinos & Napolitano 2003). Also, in both situations, if a large enough voltage is applied, current can be made to flow in the opposite direction.
The stopping potential vs. frequency is shown in Figure 2. We did linear regression on the data and found that the best fit is given by
The standard deviation of the fit is 1.61, which is of order 1, so the fit is good and uncertainties are reasonable, i.e., the fit residuals essentially obey a Gaussian distribution.
Figure 2: Stopping voltage (V) vs. frequency (Hz), with uncertainties. The six data points are fitted with a linear regression line. The slope is the measured value of Planck's constant and the y-intercept is the work function of the photocathode.
The measured value
The work function of the photocathode is
We have verified Einstein's model of the photoelectric effect and measured Planck's constant. It was observed that photoelectron energy is linearly proportional to frequency with proportionality constant equal to Planck's constant. Our value

Chemistry is intrinsically concerned with the structure of molecules and the reactivity of molecular systems. One of the principal concerns of chemistry is to find the energetically accessible confirmations and/or equilibrium structure of a given chemical system.
In general, the potential energy surface (PES) of a molecule is a 3N-6 dimensional surface, where the coordinates are the 3N-6 internal coordinates obtained when translational and rotational degrees of freedom are annihilated. In theory, one can calculate the PES for any system by solving the time-independent electronic Schrodinger equation for every possible position of every atom, however, such ab initio calculations are too expensive to be considered.
Many force fields used in practice today are empirical force fields (AMBER, CHARMM, etc.) that can model the PES of a large variety of chemical systems. Such empirical force fields are based on theory, experiment, and also intuition. Therefore, in practice, the PES is almost always defined through the choice of an empirical force field that describes the inter-atomic interactions in a molecule. This is basically choosing the "correct" functional form and the parameters which combined, will describe all inter-atomic interactions.
In considering chemical systems, the PES is a 3N-6 multidimensional function. From basic multivariable calculus, one knows that at a minimum, the first derivative of the potential with respect to each individual variable vanishes (that is) and that the second derivative of the potential with respect to each variable must be greater than zero. For a chemical system, finding the minima on the PES corresponds to finding the stable points of the system and thus, finding the geometry of the molecular system. Finding the global minimum corresponds to finding the conformation of a system with the least energy, the equilibrium conformation. Transition states that connect minima on the PES are defined as saddle points, that is a point on the PES where the PES is at a maximum with respect to one coordinate and a minimum with respect to all other coordinates. The transition mode, which will describe the motion in going from one minimum to the next, is described by the single coordinate that is at a maximum at the saddle point.
In this laboratory exercise, we will focus on the Newton-Raphson method and steepest descent methods to find minima of a potential energy surface. Both of these methods are based on the Taylor expansion of the PES:
Note that the Newton-Raphson method, which uses up to second derivative information, assumes that the PES is a harmonic potential. For a simple quadratic function, the Newton-Raphson method can find the minimum in one step, but for more complex systems, it will take several iterations. For the Taylor expansion above, the first derivative of V(x) is:
If the function is purely quadratic, which is assumed in a harmonic potential, the second derivative is the same everywhere, that is:
At the minimum point x=x*:
Therfore the minimum point is:
This can be extended to a chemical system if one makes each variable x and V a matrix of 3N-dimensions, where N is the number of atoms in the system and V is defined by some empirical or ab initio force field.
Note that this is computationally expensive since it requires inversion of the Hessian matrix. Therefore, in most quantum chemistry programs (Gaussian, Q-Chem, Jaguar), quasi-Newton-Raphson steps are used to find a minimum.
In exploration 1, we will be using the steepest descents method which uses only first derivative information to find a minimum of a PES. In this method, the "ball" on the potential energy surface will move in the direction exactly parallel to the net force. For 3N coordinates, this direction is conveniently represented by the negative of the unit vector of the gradient of the PES, that is:
The step size that one takes in moving along this PES is defined in the programs that we use in this exploration.
The function given to our group was:
A contour plot and a surface plot of this function are shown below:
Note that there are two local minima on this surface.
For this exploration, we tried using several step sizes, initial positions and tolerances. Below are the results of these explorations. First consider changing only the initial guess:
Note: For these trials, the tolerance was held constant at 0.0001 and step size was held constant at 0.001.
When the initial guess was changed and all other parameters such as step size and tolerance were held constant, we see that the minimum that is found in a given optimization is dependent on the initial guess. This is because the steepest descent will always tend to push the "ball" on the PES in the direction of the steepest negative slope. Therefore, sometimes it will push it all the way to the global minimum and sometimes it will push it to the local minimum. Once the "ball" reaches any minimum point on the surface, it is no longer subject to a force since the gradient of the potential is zero at any minimum.
Below are plots of minimization tracks for various initial guesses:
Consider changing the tolerance:
When the tolerance was changed and all other parameters such as initial guess and step size were held constant, we tended to arrive near the same minimum point, but not exactly. The tolerance indicates the maximum value for which a gradient is considered zero by the algorithm. Thus, for a larger tolerance, there is a larger space around the exact minimum point which will be considered to have a gradient of zero. We see that at low tolerances such as 0.0001, we reach a fairly accurate point but as we increase the tolerance by orders of magnitude, the minimum tends to shift away from this point. Once the "ball" reaches the cusp of the space where the gradient is considered zero, it will stop because there is no force acting on the ball. Thus, at higher tolerances, we will actually reach a less accurate minimum point. In order to have acceptable values, the tolerance should be less than or equal to 0.0001, however, one should always try to use the lowest tolerance possible to attain accurate results.
Consider changing the step size:
When the step size was changed, we tended to find drastically different minima. When the step size was increased by one order of magnitude from 0.001 to 0.01, the minimum point found differs slightly. When the step size is too large, such that moving along the steepest gradient with a given step size will move to a point that is not defined by the function (f(x,y)=0), the steepest descent method essentially stops. This is because moving to a point that is not defined by the surface then has a value of f(x,y)=0, which has a derivative with respect to all coordinates which is also zero. Thus, no further movements will be made toward a minimum. When the step size is increased by two orders of magnitude from 0.001 to 0.1, we find that the large step size causes the steepest descent vector to move to an "undefined" point very quickly in the minimization routine. This is why the minimum point found is so drastically different compared to the results for the other two step sizes. In order to be accurate, one should use a step size which is less than or equal to 0.01 for qualitative results which are accurate to the tenths place. However, to get quantitative results, the step size should be less than or equal to 0.001.
Note that in the above exploration, we have located all minima of the surface that is described by the function
For exploration 2, we will be using the Lennard-Jones 12-6 potential to model the van der Waals interactions which hold the two and three atom clusters together which we will be studying. The Lennard-Jones 12-6 potential is of the form:
This potential is empirical and has been derived from experiment, but it has firm theortical grounding. It is important to note that the Lennard-Jones 12-6 potential only models pairwise interactions, however, for many systems, this proves to be a fairly good approximation. The first term,
For explorations two through four, we will be using the following parameters given to our group:
For exploration 2a, we produced a contour plot and a surface plot of the constant angle potential for the three atom van der Waals molecule, Ar-Kr-Ar. This constant angle potential corresponds to the angle indicated being held constant and the other two internal coordinates, r1 and r2, are free to vary.
Below is the equation which is used to calculate the LJ-potential in the polar coordinates which will make it easy to develop a constant angle potential.
Below is the surface plot and contour plot of the constant angle Lennard-Jones 12-6 potential when the angle θ is held constant at 70.777° as calculated by Mathcad after geometry optimization in Cartesian coordinates. Below is the calculated cluster geometry after energy has been minimized.
Note that we reach a minimum at a certain distance, but that at inter-atomic distances closer than the equilibrium distance, the potential goes to infinity, representing the repulsion of the atoms at close distances. This is modeled by the (1/r)12 part of the potential. Also note that the potential is symmetrical since the pairwise interactions between krypton and each of the two argons are exactly the same. Also, at distances which are greater than the equilibrium distance, the potential rises up out of the minimum (modeled by (1/r)6 part of LJ potential), but at a slower rate than when the inter-atomic distances are less than the equilibrium distance.
For exploration 2b, we set out to construct a Lennard-Jones 12-6 potential for the van der Waals cluster Ar-Kr-Ar where the bond length between the two argon atoms remains constant while the other two parameters vary. For this case, it will be easier to use Cartesian coordinates to construct the potential.
Note that when the interactions are between Kr and Ar, we take the averages of the ε and σ parameters of the two atoms to describe the interactions. When it is the Ar-Ar interaction, we only use the ε and σ values for argon to describe the pairwise interaction. Using this information, we calculate the Lennard-Jones potential for the Ar-Kr-Ar van der Waals cluster when the Ar-Ar bond length is held constant:
Where x1 is the distance from the argon to argon along y=0, y2 is the y-axis projection of the vector connecting the Ar atom to the Kr atom, and x2 is the x-axis projection of the vector connecting the Ar atom to the Kr atom. Also note that as stated above, ε12 and σ12 are defined as:
Mathcad minimizes this Lennard-Jones potential to give an equilibrium geometry. The equilibrium geometry is as pictured below with the coordinates indicated:
Below is a surface plot of the Lennard-Jones potential where the Ar-Ar bond length is held constant at 406.672pm as determined by the Mathcad minimize function. Note that the minimum energy state has energy of-3.148*103 J/mol.
Note that in this plot, there are two global minima, but several local minima which kind of surround the global minima. There are two global minima due to the inherent symmetry of the system. That is to say that the values of r2 and r3 in figure 9 may exchange such that r2=______and r3=_________. Therefore, we reach a symmetrically equivalent molecule with the same energy. Also note that at distance closer than the minima (local or global), the potential tends to infinity as expected. This of course is dictated by the repulsive nature of atoms at close distances which is governed by the (1/r)12 term in the LJ potential. Also note that the global minima has energy which correspond to the calculated minimum energy above. As the atoms move farther away, that is, when interatomic distances become large in either direction, the energy increases with respect to the minima, but not nearly as fast as when the interatomic distances become very small. This is because as the atoms move far apart, they are largely governed by the (1/r)6 term rather than the (1/r)12 term.
In exploration 3, we constructed a program to perform a steepest descent minimization of the van der Waals cluster consisting of Ar-Kr-Ar. The potential is still described by the Lennard-Jones 12-6 potential, but this time, no parameters are held fixed as in exploration 2. We have calculated the equilibrium geometry and energy of the van der Waals cluster for five different initial starting geometries to test the robustness of this steepest descent minimization. Below is a table summarizing our results. The program which was used to run the steepst descent algorithm is attached in the appendix.
From these results, it appears that the steepest descent minimization routine is not robust as it tends to lead us to different equilibrium geometries based on the initial guess. For the fourth trial where the initial guess has all the atoms aligned in the x1-x2 plane, it converges to another geometry which is also planar. Since y2=0, the first derivative of V(x1,x2,y2) with respect to y2 is always zero. This implies that the force in the y2-direction is always zero, therefore, the atoms will never be pushed out of the x1-x2 plane. For the last guess, which involves an initial guess where the atoms are absurdly far apart, it appears that the geometry only shifts in the x1 direction. This indicates that at very far distances for x2 and y2, the potential has the following characeteristics:
However, it appears that the steepest descents method takes us to the correct place for x1, which indicates that there is a significant gradient with respect to x1 at x1=1000 to push us down into the minimum.
In exploration 4, we will be looking at a van der Waals cluster of Ar5Kr whose minimization is governed by the minimization routine in Mathcad. According to valence shell electron pair repulsion theory (VSEPR), the equilibrium geometry of such a 6 atom cluster should be a trigonal bipyramid. For the first few starting geometries, we chose the trigonal bipyramid geometry to be sure that nothing was incredible wrong with the minimization routine. Below is a summary of our results, which includes starting geometries, bond lengths between all Ar-Kr atoms and energy at equilibrium geometry.
Starting geometry given in Cartesian coordinates (x,y,z) for each atom. Note the the last row is the krypton atom, while the other rows represent the argon atoms. The diagram to the right indicates graphically what the q matrix looks like in space.
Starting initial geometry is given in the same matrix as in trial 1 and a diagram is included once again to indicate graphically what the q matrix looks like in space.
Starting initial geometry is given in the same matrix as in trial 1 and a diagram is included once again to indicate graphically what the q matrix looks like in space.
This time we start with all the atoms in a plane. Starting initial geometry is given in the same matrix as in trial 1 and a diagram is included once again to indicate graphically what the q matrix looks like in space.
According to our results, we have found four different energy minima which indicates that this energy minimization routine is not robust. Furthermore, each initial guess appears to converge to a different geometry, most notably the case where the atoms all start in a plane (trial 4). It appears that this minimization routine must somehow use a steepest descent method because the optimized geometry also has a planar geometry. As discussed before, if the z-component of every atom is always zero, then the partial derivative of the potential with respect to z will always be zero, which indicates that there is no force to push the atoms out of the z=0 plane.
Below is a figure comparing the energies of the various confirmations of the local and global minima that were located. It appears that the first equilibrium confirmation is the global minimum and according to chemical intuition, this should be the global minimum since six atom clusters should tend to a trigonal bipyramidal structure.
In this exploration, we will be using the Lennard-Jones potential to calculate vibrational frequencies of a Ar-Kr-Ar cluster (part a) and a Kr-Ar-Kr cluster (part b). To calculate the vibrational frequencies, we expand the potential in a Taylor series and eliminate all anharmonic terms so that we work in the harmonic approximation:
We choose V(0)=0 out of convenience and note that at local minima, the second term will vanish. Thus, we are left with a potential of the form:
If we use Hooke's law to describe the restoring force, then this potential becomes:
Now we can describe this motion by Newton's second law:
This is a second order linear differential equation which may be easily solved, however, it is not done here. For more complex systems involving more degrees of freedom, the challenge of calculating vibrational modes is rather complicated because the relation between the PES and the vibrations is inherently more complex. Note that for a system of N particles, there are 3N-6 vibrational degrees of freedom since there will be 3 rotational degrees of freedom and 3 translational degrees of freedom.
We may derive the equation of motion for this system which involves taking the 3N coupled differential equations in Cartesian coordinates and doing a unity transform to the 3N-uncoupled differential equations of the normal modes. By following this procedure, each vibrational spatial coordinate, qi is described by the following differential equation:
The solution to this differential equation yields:
Therefore, one may reconstruct the differential equation above as:
And this may be written in terms of matrices and vectors which will give us a "prescription" to find its result:
The non-trivial solution of this equation occurs when the coordinates qi are not zero. Thus, the normal mode motions are described by the eigenvectors of B, also known as the mass-weighted Hessian or mass-weighted "force constant" matrix. Its eigenvalues will give us the corresponding vibrational frequencies for each vibrational motion.
The vibrational coordinates of the Ar-Kr-Ar cluster and the Kr-Ar-Kr cluster differ slightly. This is due to the increased weight of the Kr atoms with respect to the Ar atoms. By having a heavier system, this will slow down the vibrations, as seen by the reduced frequencies in part b of this exploration. Thus, one witnesses that the eigenvalues of the Hessian matrix are larger in part b where the mass weighting slows down the vibrations since having two Kr's versus two Ar's will slow down the vibration.
In exploration 6, we use Spartan '04 (Windows edition) to explore the robustness of various molecular mechanics force fields, especially the Merck Molecular Force Field (MMFF). In this exploration, we will be running experiments on hydroxyamphetamine (a derivative of methamphetamine, a popular street drug):
In this exploration, we will try different starting geometries to see if using the molecular mechanics method with the MMFF parameters finds the same minimum. The starting geometries were varied by rotating around the single bonds in the floppy alkyl group of the molecule. Below is a summary of results and diagrams of the optimized geometries for trials 1 and 3:
It should be noted that the optimized molecular geometry of trial 3 differed greatly from the optimized molecular geometries of trials 1 and 2, whose optimized geometries looked relatively similar.
The calculated minimum energy confirmation is dependent upon the input of the geometry. This is because once the minimization routine finds any minimum, local or global, the gradient of the potential with respect to all coordinates is zero which implies that there is no force to push the structure toward a lower energy. Thus, for trials 1-2, the minimization routine found a different minimum point on the PES than for trial 3.
For exploration 6b, we used a Monte Carlo search method to find the conformational distribution of our molecule in a particular starting geometry. We used the input geometry for trial 3 as our initial guess.
This Monte Carlo algorithm employed here does a Monte Carlo search with a path that biases low energy conformers. Although we might not find the global minimum, Spartan finds the conformers that one keeps constitutes a Boltzmann distribution.1
For the given starting geometry, the Monte Carlo conformational search found 18 different minima. The lowest energy confirmation has an energy of 17.3791 Hartrees and its geometry is plotted below.
In exploration 6c, we rotated around one of the flexible bonds in our molecule to develop an energy profile for rotation about this bond. Below is an image of the molecule which indicates the bond about which the molecule rotated and the confirmation that corresponds to the zero degree mark.
For this exploration, we have used two different force fields, MMFF and SYBYL, to calculate an energy profile. Below are plots of the results:
The results of this exploration indicate that using different force fields will give different results. This is because different force fields correspond to different functionals which may have been subject to different target sets. MMFF has been "specifically parameterized to reproduce geometries and conformations of organic molecules and biomolecules" while SYBYL is parameterized for the entire periodic table.
Below are hand-drawn sketches of three vibrational modes of hydroxyamphetamine in its minimum energy state found in part c. All of these vibrations will be seen in the IR spectrum (which is plotted below). This is because the overall dipole moment of the molecule changes upon excitations at these specific frequencies.

Properties of electrons can be studied in order to determine the e/m ratio of electrons. The e/m ratio is important because it absolutely predicts a particles direction of travel in a vacuum when subject to magnetic or electric fields. In this lab a deflection tube is used to measure the effects of an electric field on a beam of accelerated electrons from a cathode ray tube. Helmholtz coils are used in conjunction with the cathode ray tube to measure the effects of a magnetic field on a beam of accelerated electrons. Measurements of the deflection distance of the beam at various electric and magnetic field strengths can be used to determine the e/m value for an electron.
To discover the deflecting force a magnetic field causes on an electron accelerated perpendicularly through its field and to use these data to find the e/m ration of an electron.
The magnetic field produced from the helmholtz coils creates a force that causes the originally straight electron beam to be deflected, either positively or negatively, depending on the direction of the current. The measurements recorded are then used to calculate the e/m of an electron. The above calculations were completed using the following equations:
The results of the individual calculations for the e/m are fairly far from the accepted value of e/m = 1.7589 x 1011 C/kg. However, the average value of 1.369 C/kg is closer, less than 2.5 standard deviations from the accepted value.
Choosing a point with a large x-value value will increase the accuracy of the measurements because it will yield larger y-values, which will decrease the relative error. However, a source of error in the data may be because the beam was aimed to intersect (0.10, 0.01) instead of trying to keep IB as close to 0.150 A. This technique may have caused increased error in the measurements, such as that found in the positive deflections where the experimental e/m is calculated to be less than the accepted value for e/m. Due to the force of the earth's magnetic field higher values than the accepted e/m would be expected.
Charge-to-mass ratio calculation accuracy increases when balancing both the positive and negative values which serves to negate the effects of the earth's magnetic field. Additionally, the electron beam may not have been perfectly at zero y and finding the average value would eliminate this error in measurement.
The objective of this experiment was accomplished -- the data revealed that the electron beam will move in a circular path in accordance with the magnetic field force equations (F = iL x B which causes an inward acceleration, leading to a circular path). Also, the e/m ratio for the electron was successfully calculated using the data collected. The experiment was important because a method for calculating e/m was learned.
To observe the deflection of an electron beam when accelerated through an electric field and to report the experimental slope of the electric field using these measurements.
The electrostatic field causes the beam of electrons accelerated perpendicularly to be deflected along a parabolic path. This is different from the magnetic field which causes electrons to travel in a circular pattern because the magnetic force causes an inward acceleration. In the above graph x2 is used instead of x because of the results found when the following equations are solved for y. The explanation of this parabolic path and the expected slope of this graph can be derived from:
The calculated experimental slope (Δ y/Δx2) of our graph is 2.44.
The electric field strength can be calculated two ways. Easily by using the equation:
A more precise calculation can be derived from combining the following two equations and replacing ve2 in the first by solving for it in the second.
Now the e/m ratio can be eliminated from the equation and E can be solved for using the slope.
This second value for the electric field is more accurate because the experimental electric field is not constant across the capacitor plates. The size of the plates is of the same order of magnitude as the distance between them; therefore, the electric field near the ends of the plates varies from that of the center. The equation Vd/d assumes a constant electric field, while the other equations allow the experimental slope to be used in order to more accurately represent the non-constant electric field strength.
The experiment's objective was accomplished. The experiment was important because the slope of the electric field was calculated which can now be used in the last experiment to more accurately find the e/m value. The experimental slope yields more accurate results than using the equation Vd/d when capacitor plates are not infinite.
To balance the forces from both an electric and magnetic field on a beam of accelerated electrons and use the experimental deflection values to find the e/m ration of an electron.
The above calculations for IB, e/m, and e/m* were completed using the following equations:
Charge-to-mass ratio values calculated using E calculated from the slope instead of Vd/d serve to increase accuracy by accounting for the non-infinite plates of the capacitor. The mean values for the e/m in both cases are very far from the accepted value of 1.759 x 1011 C/kg. Subtracting the average values shows that the value corrected by using slope is closer to the accepted value. The corrected value does have a smaller standard deviation which may have resulted from increased precision with this method. However, both e/m's are very far away from the accepted value.
There are many compounding sources of error in this experiment. Again, there is the problem that the electric field is not constant across the plates because they are not infinite. This causes systematic error in the experiment such that the trajectory of the electron beam is bowed in the middle with the B field unable to completely cancel out the E field. Also, the effects of the earth's magnetic field have been disregarded and may have caused inaccurate measurements.
The goal of the experiment was poorly accomplished with the measured e/m being very far from the accepted value. This experiment was valuable because it facilitated the investigation of various sources of systematic error.
Theoretically, there would be no change in the trajectory of an electron doubled in mass and charge in a magnetic field because it is dependent on the ratio of charge to mass which would be unchanged.
The charge-to-mass ratio of an electron was calculated using two separate methods. First, the deflection data of an electron beam accelerated between two capacitor plates were used to find the e/m. The second method involved balancing the forces on an electron beam from an electric field and a magnetic field. More accurate results were found using the first method which may be because the second method had more sources of error due to error from both the E field (capacitor plates) and B field (earth's own magnetic field). Additionally, it was learned that the slope method for calculating E yielded more accurate results than using the equation Vd/d when finding the e/m of an electron.

Gamma rays are highly penetrating photons produced by positron-electron annihilation or by the decay of a radioactive nucleus. Gamma decay tends to occur following an alpha or beta decay to bring the nucleus down to ground state. These photons were first detected by Antoine Henri Becquerel in the late 1800's. Becquerel deduced that gamma rays interact with matter like light using photographic film to detect the radiation. The detection of gamma rays using scintillation spectroscopy rests on this discovery that gamma rays are photons. Gamma rays interact with matter by three processes -- the photoelectric effect, Compton scattering, and pair production.
Each of these processes transfers energy to electrons, which then lose energy by ionizing atoms as they travel through matter. In the case of the photoelectric effect, all of the energy of the gamma photon is transferred to an electron. Since the ionization energy of the atoms in the scintillator is small compared to that of a gamma ray, the electron is considered to have an energy equal to that of the incident photon. This process is dominant at gamma energies lower than 511 KeV. For a nuclear charge Z, the photoelectric cross section for a K shell electron is:
(1)
Compton scattering is a demonstration of the particle-like behavior of light. During this process, a photon and an electron collide elastically, producing a scattered photon of lower energy and an electron with the energy lost by the photon. The energy of the new photon is dependent on the angle of deflection. The energies of the scattered photon (Eγ') and electron (Ee) for an incident photon of energy Eγ are:
The Klein-Nishina formula gives the differential cross section for Compton scattering:
where P(Eγ,θ) is the ratio of the final photon energy to the initial photon energy, re is the classical electron radius and dΩ is the solid angle. Compton scattering is dominant for middle-energy gamma rays.
At energies higher than the rest mass of two electrons (
All three of these processes result in the transfer of energy from a photon to an electron. As the electron travels through matter, it ionizes atoms through a series of inelastic collisions. This ionization is measurable by a number of techniques making it useful for detecting particles.
As described above, gamma rays deposit their energy in matter such that a number of ionized atoms are created. In a scintillator, this ionization energy is converted into visible light. This property was first employed in 1909 by Ernest Rutherford who used a ZnS screen to detect scattered alpha particles. The visible light produced in the scintillating crystal is linearly proportional to the energy input of the crystal. In this experiment, we used sodium-iodide, an inorganic crystal, doped with thallium, which has an efficiency of 12% and produces 4×104 photons per incident MeV and has a time constant of ~200 ns. The NaI(Tl) crystal is enclosed in a sealed can and optically connected to a photomultiplier.
We used a standard photomultiplier tube (PMT) from RCA. The PMT proportionally converts the visible light from the scintillation crystal into a small electrical pulse. The first part of a PMT is a photocathode, which converts light into electrons by the photoelectric effect. Next comes a series of dynodes made of materials with good secondary electron emission, between which a high voltage is applied. This creates a potential difference of 50 to 100V between dynode pairs. The HV source used in this experiment is an Ortec 478 HV supply at positive 1.1 kV. The electrons emitted from the photocathode are accelerated to the first dynode, which then emits a proportional number of electrons to the next dynode. This signal continues to amplify along the chain of dynodes, adding up to a total amplification of ~106. This pulse is collected at the anode, which is connected to a preamplifier. The photomultiplier sends out a negative pulse of charge with a width on the order of 6μs and amplitude of about 200 mV.
The preamplifier converts the charge on the anode to a positive voltage signal with width of about 140μs and sends it to a pulse shaping amplifier. An ORTEC 485 amplifier was used in this experiment. The pulse shaping amplifier removes the effect of the scintillator time constant on the width of the pulse. The pulse width is now controlled by the shaping time of the amplifier. Typical pulse widths were on the order of 7μs with a height of around 25 V. The now amplified voltage signal is digitized using a Pocket Multi Channel Analyzer analog-to-digital converter. The MCA is connected to a computer in which a histogram of voltage pulse heights is plotted. This plot gives the energy spectrum of a gamma ray source, once channels are calibrated to energies. Cesium-137 was used to calibrate pulse height to photon energy (see appendix for details).
The cobalt-60 gamma spectrum shows two strong gamma lines. This indicates a cascade of two gamma emissions following a beta decay. The first gamma has an energy of 1173.7 keV and the second has an energy of 1332.5 keV. As seen in the decay scheme to the right, cobalt-60 occasionally decays by a high energy beta emission followed by a single gamma decay to ground state.
The presence of two different gamma energies means that there should be two different backscatter peaks and Compton edges. However, these features were predicted to be very close to each other and are not distinguishable on the spectrum. The backscatter peak is at approximately 251 + 20 keV, which is ~40 keV higher than expected. The Compton edge is located at about 980 + 20 keV and is in between the two expected values.
In 89.8% of cases, 22Na decays by the emission of a positron followed by the emission of a 1277 keV gamma photon to the ground state of 22Ne. The positron annihilates with an electron, producing two gamma rays of 511 keV, the rest mass of an electron. This reflects the conversion of the masses of the positron and electron into energy by E=mc2. The other 10.2% of the time, the decay begins with electron capture in the K shell, followed by the 1277 keV gamma decay. The energy resolution of the positron-electron annihilation peak is 7.6%, compared to 4.9% for the 1277 keV gamma peak.
The sodium-22 spectrum appeared as expected except for a high energy gamma peak of unknown origin. This peak occurred at an energy of 1820 keV, which is reasonably close to the combined energy of a sodium gamma and an annihilation gamma (1788 keV). We suspect that this high energy peak is the result of a 1277 keV gamma and 511 keV gamma entering the scintillation at the same time. This is possible given our experimental setup in which the source was place directly on the scintillation counter.
Gamma spectroscopy can be employed to determine the constituents of an unknown radioactive sample. In this experiment, we were given two unknown sources, which we attempted to identify. We did this by comparing the unknown spectrum to the gamma energies of known sources.
This unknown was determined to be a combination of 137Cs and some kind of zinc compound. The cesium contributed Peak A, which is interpreted to be the barium x-ray peak of the spectrum, and Peak B, which is interpreted as the full energy peak. Zinc has a number of isotopes that decay with gamma energies between 1366.4-1560.4 keV which may be the source of Peak C. The values are not exact because adding the two spectra together cause the peaks to shift relative to one another. Also, as seen in the above experiments, the cesium calibration does not work perfectly for other spectra, which may have affected the accuracy of the calibration of Peak C.
We determined the composition of unknown 2 by adding together the spectra of various sources available in the lab until we found a reasonable match.
We concluded that unknown 2 is composed of 137Cs, 60Co, 133Ba, and 109Cd. Peak A is the barium x-ray peak from cesium. Peaks B, C, and D are the result of combining the spectra of the four sources. They do not exactly line up with any single line of the sources since the Compton continuum from each source is in this energy region. Cobalt-60 is the source of the dual peaks E and F.
In this part of the experiment we tried to determine if gamma spectroscopy could be useful in detecting trace amounts of copper in a material of unknown composition. We exposed 16.037g of Copper to an active neutron source to create 64Cu. This isotope has a gamma line at 1345.84 keV and a half life of 12.7 hr. The parent isotope, 63Cu, has a relative abundance of 69.17%, making Cu a good candidate for this experiment.
We observed the expected peak and used the data below to calculate how many 64Cu were produced. These "back of the envelope" calculations were done under the assumption that 20% of the emitted gammas were absorbed in the detector, which has a 12% efficiency.
The signal can be approximated by a Gaussian distribution using the method of least squares.
The above equation describes a Gaussian distribution centered at Eo with variance σ2. The least squares method is used to determine the constant A, as well as the error in A. We found A to be 3427
We can approximate the number of 64Cu nuclei produced while the copper was exposed to the neutron source using the data gathered with the scintillation counter and the properties of the isotope and its parent isotope, 63Cu. We also assumed that 20% of the emitted gammas were absorbed in the detector, which has a 12% efficiency.
According to this calculation, there were 4.8×108 Copper-64 isotopes made while the sample was exposed to the neutron source. Even though this is a very small number compared to the total number of nuclei in the sample (
Gamma ray spectroscopy is an excellent method of investigating the radioactive decay of certain isotopes. Our scintillation spectroscopy setup detected the two gamma decay cascade of cobalt-60, as well as the positron-electron annihilation that characterizes the sodium-22 spectrum. Scintillation spectroscopy has greatly increased our understanding of radioactive decay since spectra have been recorded for many isotopes.
Since there is such a large base of knowledge, gamma spectroscopy is also useful in identifying unknown samples. As shown with our copper experiment, it may possibly be used to detect trace amounts of certain elements. This application should be further explored through more research.
137Cs has a single gamma line at 662 keV and a barium x-ray peak at 32 keV. The decay of 137Cs starts with the emission of an electron to an excited state of 137Ba, followed by a gamma emission in 92% of cases. The other 8% of beta emissions bring the nucleus directly to ground state. The x-ray results from electron capture in the K shell of barium followed by the emission of an x-ray, which occurs 10% of the time.
The 137Cs spectrum shows the characteristic features of a gamma spectrum. The Compton continuum represents all the energies of 662 keV gammas that scatter in the crystal with the scattered photon escaping without depositing its full energy. The backscatter peak shows the 662 keV gammas that scatter off something outside the detector and get sent into the crystal. When lead was placed nearby the source and detector, the backscatter rate increased. The Compton continuum is a continuous range of energies because gammas can scatter at any angle from 0 to π. The highest energy of this range corresponds to the energy of the recoiling electron for a scattering angle θ=π in equation (3). This energy is the location of the Compton edge and the energy of the scattered photon at this scattering angle is the location of the backscattering peak.
Calibration of the channels of the MCA was done for several amplifier settings and is summarized in Table 1. At every amplifier setting used, the features described above appeared at approximately the same energy once the channels were calibrated.
Table 2 shows a statistical analysis of the barium x-ray and full energy peaks for an amplifier setting of CG 2 FG 3. The uncertainties were determined using the peak finding software in the MCA program and converted into keV using the calibrations calculated above.
We calculated the energy resolution to be 8.2% using the full width half maximum of the full energy peak divided by the centroid of the peak.
Using equations (2) and (3) the expected energies of the Compton edge and backscatter peak were calculated. Our measured energies for these features were 7.1% and 12% different from the expected values, as shown in Table 3. However, due to the dispersed nature of these features in the spectrum, as well as calibration error, the uncertainty in the measurement was approximately 20 keV.

Atomic and molecular spectroscopy has provided experimental verification for many of the most profound predictions in physics. There has been excellent agreement between theory and experiment in the study of visible light emissions.
In this experiment, we used spectroscopy to test theoretical predictions on three levels of complexity. We start with hydrogen, the simplest atom, to test Bohr's model of quantized atomic energy levels. We then increase the complexity by studying sodium, an alkali atom. Sodium displays nuclear screening, electron spin, and selection rules, all predicted by quantum mechanics. Finally, we looked at the vibrational and rotational excitations of diatomic nitrogen to infer the structure of the chemical bond between the two atoms.
We used an Ocean Optics SD2000 fiber spectrometer calibrated with mercury and helium. The spectrometer consists of two gratings: the master, which has a spectral rand of 632 nm to 880 nm, and the slave, which has a spectral range of 371 nm to 677 nm. Both gratings are mounted in a crossed Czerny-Turner configuration. The device has a resolution of 3.57×10-7 pixels and is therefore able to resolve features such as the sodium doublet, which has a separation of 0.121 pixels. However, the spectrometer is not fit to study the hydrogen Lyman series, which does not lie in the spectral range of the instrument.
The Balmer formula describes the visible spectrum produced by excited hydrogen atoms. In 1889, Rydberg recognized the Balmer formula to be a particular case of a more general expression in which n1 = 2 and RH is the Rydberg constant.
(G is an empirical constant)
In 1913, Bohr revolutionized our view of the atom with his model of atomic quantization. Bohr suggested that orbital energy is fixed at discrete values, as a function of a quantum number n. Changes in energy states are accounted for by the emission or absorption of a photon, which balances the energy difference.
This relation is described by the Planck formula:
Our measurement of the visible part of the hydrogen spectrum is evidence of Bohr's quantized energy levels.
Figure 1 shows a plot of:
By the Rydberg formula, the slope of the line gives the Rydberg constant. Our estimation of this constant is 109683 + 1000 cm-1 (accepted value RH=109737.318 + 0.012 cm-1).
The straightness of the line is a test of Balmer's hypothesis regarding the quantized energy levels of the hydrogen spectrum. We used Excel to plot a linear trend, which gave an R2 value of 1. This indicates that a straight line is a good fit for the data, thus verifying the Balmer hypothesis.
Sodium is an alkali atom, meaning it has one l = 0 electron in its valence shell. As the nucleus is shielded by the charge distribution of the inner shells, his lone electron "sees" a net potential similar to that of hydrogen. However, for smaller n, the behavior of electrons of small l deviates from hydrogenic behavior. The small l states do not have enough angular momentum to maintain a large radius, causing their orbits to cross inside the closed shells (Fig. 2). When this occurs, there is less of a screening effect and a lowering of the Coulomb energy.
Evidence for nuclear screening is found by fitting our measurement of the sodium spectrum to a Balmer-like relation. We did this using a quantum defect, which mathematically summarizes the effect of nuclear screening. The quantum defect (δ) is a correction to the quantized energy predicted by Bohr.
Following the selection rule,
, the wavelength of a photon emitted to bring an electron from energy state nd to 3p is determined by:
Figure 3 is a plot of
The sodium spectrum also shows evidence of electron spin. In the rest frame of the electron, the proton creates a circular positive current and therefore produces a magnetic field. The spin of an electron can point either up or down with respect to the internal magnetic field. The interaction energy between the magnetic field and the internal magnetic moment of the electron is described as the spin-orbit interaction.
Since spin can only have two values, the spin-orbit interaction energy can also only have two values:
The interaction energy is either added or subtracted from the total energy of each state. This leads to a small energy split for every state with
The spin-orbit interaction manifests itself as a ~0.1% split of spectral lines. Therefore, the presence of doublets in the sodium spectrum verifies the prediction of spin-orbit coupling. These doublets can also be used to infer the magnitude of the internal magnetic field of the atom. The classic example of a doublet is the sodium D line, shown in Figure 4.
We calculated the expected internal atomic magnetic field to be 1.332×1021 T for an expected energy split of 0.00617 eV. Our measured energy split was 0.00104 + 0.001 eV. This gives 2.2432620×1020 T for the internal magnetic field, which is about an order of magnitude lower than expected.
The spectrum of diatomic nitrogen differs from those of hydrogen and sodium greatly. The hydrogen and sodium spectra consist of several sharp peaks whereas the N2 spectrum shows a broad "comb-like" band of emission wavelengths. Diatomic nitrogen is more complex and has both vibrational and rotational degrees of freedom.
According to the Born-Oppenheimer approximation, vibrational and rotational excitations can be treated separately. The vibrational mode is described by the quantum harmonic oscillator model. In the following equation, ω0 represents the fundamental frequency oscillation.
A transition in which n is decreased by 1 forms a spectrum of equally spaced lines. The separation of these lines can be measured and then used to calculate ω0. In figure 5, the coarser set of lines represents the vibrational excitations. These lines were not exactly equally spaced, which we expect to be the result of coupling with rotational motion. We found the fundamental vibrational frequency of nitrogen to be 3.11×1013 Hz.
The bond holding the two nitrogen atoms together can be viewed as a spring. The
strength of the bond can therefore be approximated using
, where m is the reduced mass (7.0015 amu). We found k=11.237 N/m.
The rotational excitations are seen in the fine structure of each of the coarse peaks in the N2 spectrum. These emissions are described by a quantum rigid rotator.
In the above equation J=0,1,2... h is Planck's constant and I is the moment of inertia of the molecule. For two particles separated a distance r with a reduced mass m, the moment of inertia is I=mr2.
We again extracted a frequency for rotational motion, which was found to be 2.52×1012 Hz. This is an order of magnitude smaller than the vibrational frequency. We also calculated the distance between the atoms as 0.19 nm.
It is curious that the same comb-like structure was not seen in the diatomic hydrogen spectrum. This is because the rotational energy is a function of 1/m and the mass of a hydrogen molecule is very small compared to nitrogen. This would result in emissions with wavelengths much larger than our apparatus can detect.
An application of our knowledge of molecular excitations is found in a common kitchen appliance. A molecule will absorb a photon that matches its fundamental frequency.
This causes the energy of the molecule to increase, which increases the temperature of the material. In a microwave oven, microwaves are used to excite water molecules in order to heat food.
The atomic and molecular spectra studied in this experiment verify several famous theoretical predictions in physics. The hydrogen spectrum verifies Bohr's model of quantized energy levels. On a more complex level, quantum mechanical predictions of electron spin, nuclear screening and selection rules are manifested in the sodium spectrum. Finally, the spectrum of diatomic nitrogen shows the validity of the Born-Oppenheimer approximation regarding vibrational and rotational energies.

The voter model is one of the most elementary interacting particle systems. It can be used to describe simple opinion dynamics, as the name implies, as well as certain kinds of dimer-dimer kinetics [1]. However, the voter model is often studied primarily because the relative simplicity of its rules frequently allows it to yield exact solutions. Each lattice point k can be in one of two states. For convenience, I shall denote these by + and −. A lattice point and one of its nearest neighbors are selected at random. The selected opinion then takes on the value of the chosen nearest neighbor.
A number of important aspects of the model jump out of this dynamic immediately. First and foremost is the exitance of two absorbing states, one comprised of all + opinions, the other of all -- opinions. For a finite system, one of these solutions will be the necessary outcome. Furthermore, the system will behave identically under the transformation ± → ∓, implying a Z2 symmetry. Thirdly, despite the stochasticity of neighbor selection, the system will never change a value randomly. Equivalently, the system is at zero temperature. This means that all of the dynamics takes place at the edges of domains of similar opinion. A correspondence thus exists between the behavior of domain edges and random walks that helps the solubility of many VM problems.
Although the voter model is at zero temperature, the critical temperature is also zero, so the standard array of critical exponents may be found [6]. For the remainder of this paper, however, I will focus on the dynamics of opinion switching and persistence in the manner of Ben-Naim et al. [2]
In order to get an initial idea of what solutions to the voter model will be like, I will start with a mean field approach to a large lattice. Being a mean field solution, the exact nature of the lattice does not matter and a complete graph is used instead. Also, set the density of + opinions, c+ is equal to that of -- opinions, c−. Define Pn(t) to be the fraction of sites at time t that have made n flips. Since the system is large, we can approximate the change of Pn(t) with a set of differential equations:
where λ sets the characteristic time of flipping. Note that we implicitly invoked the requirement that c+ = c− in treating the system as one population. Relaxing this will lead to interesting behavior later. Redefining
This is the large N limit of a Poisson distribution, thus
Therefore after a very short amount of time, the initial opinions are effectively lost to the system. The mean-field topology makes this unsurprising.
The case of uneven distributions of opinions can be derived in a similar manner, but the system must be separated into two species on the basis of the initial state of each lattice point. Thus P+n (t) is the probability of a voter with an initial + opinion to flip states n times by time t. Note that this also makes even numbers of changes distinct from odd numbers, since each will come from different pools of initial voters. The set of equations analogous to (1) is then
Before solving the system, note that we can trivially solve
This is again a simple exponential equation to solve, taking the initial condition that
This is the first indication of substantively different behavior of the even and odd changes. For
That A(t) does not tend toward zero may seem strange given the lack of conservation of net opinion during an interaction, but in the limit of large system size that is being considered, the average concentration of opinions is in fact fixed. A single voter may not have a memory of its initial opinion, but the statistical prevalence of the majority opinion initially skews the future toward the majority opinion for all time, keeping the autocorrelation positive.
The mean field techniques provided some insight into the time scales of the voter model and the effect of concentration on the long term behavior. However, large amounts of information were obviously lost as the mean field solution exhibits none of the expected coarsening. Continuing in the manner and notation of Ben-Naim et al. I will now investigate the exact solution of Pn(t) on an arbitrary lattice. This process will show that dc = 2 is the critical dimension above which coarsening does not occur. For the remainder of this paper the initial opinions will be set equal, thus
The state of the system S as a whole must be considered now. Let Sk be the opinion at site k and Sk be the lattice S with
The rules of the voter model make Wk(S) easy to write down, as well. When all nearest neighbors are different, Wk = 1 and when all are the same, Wl = 0. Between these two values, the rate should scale linearly with the number of different nearest neighbors. Thus for z denoting the coordination number,
With these definitions in hand, it is now easy to determine the evolution of the average opinion at site
Remembering that Sk differs from S exactly at k,
Putting equations (10) and (11) into (9) gives us the final relation:
A similar derivation gives the equation for the two point correlation function:
Reflecting briefly on the form of these equations, it is apparent that
with
Since
It is also possible to calculate the average number of flips in opinion,
However, the right hand side has a physical meaning. Since
Much of the physics of what goes on lies in these equations. The convergence to zero of the density of unlike nodes for d ≤ 2 means that the system is taking on more and more order. Furthermore, d = 2 is a critical dimension, above which coarsening is not able to occur. After a simple integration, the long time behavior of the average number of flips is also known.
The mean field time dependence occurs for d > 2, but fails for lower dimensions. This implies that, while the mean field distribution of Pn(t) may be acceptable for higher dimensions, the previous discussion does not have any input on d ≤ 2 distributions. However, Derrida et al. solved for one particular value, the persistence of unchanged opinions, P0(t), for the q state Potts model [5] for d = 1. The details of this calculation are beyond the scope of this discussion, but the idea was to again employ the duality with random walks. When a site is changed for the first time, it must be at the edge of an ordered domain. This relates the problem to that of finding a first passage time of certain random walkers. This can in turn be described by a reaction-diffusion equation which they then solve. After much calculation, they find that the exact power law describing persistence is given by
For the equal initial concentration voter model, q = 2 and θ(2) = 3/8. Uneven concentrations can be handled by defining larger q Potts models and equating certain states. For instance, if c+ = 3c−, a q = 4 state model would describe the dynamics and three states would together form those with initial state + [2].
The dynamics of coarsening of the voter model are a rich subject, and the above treatment only scratched the surface. A mean field solution qualitatively reproduces many of the dynamics of opinion changing for d > 2, but fails for smaller dimensions. This is expected, as the voter model is sensitive to dimension and converges relatively weakly even for d = 2. Neglecting a discussion of more exotic topologies, the universality class, other critical exponents, and a field theoretic treatment [7] [6], there are other questions open based on the work described here. Understanding the two dimensional coarsening process further would be important, and seems likely solvable for small initial concentration. Also, an investigation of the behavior of smaller systems and the impact of fluctuations could be very interesting for its use as a caricature of opinion propagation in the social sciences.

In this project, I determined the site percolation threshold of the Kleinberg small-world model (Kleinberg 2000). Site percolation on a lattice consists of labeling each site on the lattice "active" or "inactive." Percolation can be used to model the spread of epidemics and catalytic surfaces. Applying site percolation to small-world networks could be used to better understand the spread of diseases and innovation. The Kleinberg small-world network (2000) is similar to a two-dimensional version of the Watts-Strogatz small-world model. The main difference between the two models, besides the dimensionality, is that the long-range connections in the Kleinberg model are directed. The main feature that Kleinberg wanted to show is that there is an optimal value to the clustering coefficient, r, for greedy search with local information. The clustering coefficient determines the probability that a long-range bond, say from a to b, is chosen,
Percolation on small-world networks can answer questions about the spread of disease through networks that resemble real human contact networks. In this context the "active" nodes are people who are susceptible to the disease; "inactive" nodes are people who are immune to disease. By determining the effect of different long-distance contact distributions and number of long-distance contacts on the percolation threshold, we can start to answer epidemic prevention questions. One possible question is, "is it more important to reduce the total number of long-range contacts or changing the distribution of long range contacts to make the contacts more 'clustered'?"
Another motivation is my initial motivation: modeling the diffusion of information through high-loss small-world networks. One known issue with Milgram's experiment is that a number of the messages were dropped by intermediate people. In terms of percolation these people would be "inactive" nodes. With both stochastic and deterministic message passing schemes, a message that goes through more steps is more likely to be dropped. Therefore, the average path length of a message that makes it to its destination is shorter than the average shortest path if there were no people who dropped messages. I planned to look into the probability of messages reaching a target at the percolation threshold. As it turned out, just finding the percolation threshold as a function of r and the total number of long connections (N) was difficult enough.
The subject of percolation is very old and the percolation threshold has been determined for practically all 2D (Suding 1999) and 3D (Lorenz 2000) lattices. Lattices are said to percolate if there exists a path from one side of the lattice to the other through only active sites. This group of active sites is called a percolation cluster. Networks, on the other hand, require a different definition of percolation. Percolation on networks requires determining the size of the giant component of the network, after the sites have been labeled active or inactive. Network percolation has been performed on random graphs (Callaway 2000), 1D Watts-Strogatz small-world networks (Newman 1999), 2D Watts-Strogatz small-world networks (Newman 2002), and on Heterogeneous networks (Sander 2002). Due to the similarity between the Kleinberg and Watts-Strogatz models, one limit of my analysis should yield similar results to Newman 2002.
My goal is to determine the percolation threshold as function of the two parameters of the Kleinberg model, the power-law distribution coefficient (or "clustering coefficient") r, and the number of long range connections N. The percolation threshold is defined as the percentage of active sites required to always percolate for infinite size systems.
To find the percolation threshold I used simulations written in C++. The details of my simulations are as follows. I create a square lattice of equal height and width. Each site gets N additional directed long range links. If N is not an integer, then some sites get
I measured the profiles of I(p) for several size systems, Figure 1. The most common method of determining the percolation threshold is to measure pc, say where
My main result is shown in Figure 3. This figure shows how pc varies as a function of r and N. There is a lot to digest in the figure so I will discuss some of the interesting behavior. The first limit to note is the limit of N = 0; this corresponds to a normal square lattice with no long-range connections. The percolation threshold for the square lattice is known to be 0.59274621(13). My simulation calculated pc to be 0.5927. This limit shows, at least partial, functionality of my programs.
This limit is shown in Figure 4. This limit corresponds to long bonds of any length being allowed. This result can be compared with the results of Newman 2002. For both results we see the percolation threshold start at that of the square lattice, 0.5 and 0.5927 for bond and site percolation, respectively. For my results, pc monotonically decreases until N=2, after which pc is a constant value, 0.1897. Newman's results don't show this behavior and if I understand their paper correctly, they don't predict any point where pc should become constant. This discrepancy is because, in that paper, he did not measure pc for N>1 and he used the giant component to determine percolation, whereas I used crossing probability.
My approximation of this limit is shown in Figure 5. The slice of the contour plot is at r=15, a good approximation of the infinite behavior. From the plot we can see that there is a linear decrease in pc until N=2 where pc becomes constant. One interesting thing about this limit is that it can be compared against other lattices because it corresponds to adding some connections to next-nearest neighbors. In terms of lattices, a square lattice has a coordination number (z) of 4, i.e. four neighbors. N=1 and N=2 correspond to z=5 and z=6, respectively. I found that pc for z=5 and z=6 are 0.528 and 0.470. These results can be compared against the percolation threshold of normal lattices. My results for z=5 are smaller than known lattices (ranging from 0.550 to 0.579). The common z=6 lattice is the triangular lattice which has a site percolation threshold of 0.5. In all of these cases, the r= limit have a lower percolation threshold than normal lattices. This is an interesting result, without a clear explanation.
A plot of pc versus r for N=1 and N=2 is shown in Figure 6. For large r, pc approaches some asymptotic value as the "long" links become next-nearest neighbor links, as seen in Figure 5. When r decreases, pc also decreases until it reaches a constant value at r
I studied site percolation on the Kleinberg small-world model. Using numerical simulations I determined the percolation threshold for a wide range of values of the system's two parameters: the "clustering coefficient" r and number of long range links N. The percolation threshold values were corrected for finite-size effects. The percolation threshold contour showed several interesting features. First, I found that for large N, N>2, pc does not change anymore. This result is surprising and warrants further study. I plan to simulate similar systems to see if the result is occurs commonly. Another interesting feature is the limit of small r for N=1 and N=2. In these cases, pc takes on values of
There are a number of "special" values of pc, N, and r. Unfortunately, I have so far been unable to come up with any theories or compelling conjectures for why these values should be important.
Apart from theory, there are some "practical" things that can be gleaned from the main contour plot. If this model was an accurate description of real human networks, then my results could help make several policy positions in regard to epidemics. First, if people on average tend to have more than two long-distance friends, then it is always better to try to reduce pc by increasing r. This means convincing people not to contact very long distance friends. If the average number of long-distance friends is less than one, then it's always better to try to reduce the average number of long-distance friends. If on the other-hand, the goal is to spread innovation, then the exact opposite set of recommendations would be best.

Following President Suharto's resignation in 1998, the electoral reforms passed into law on January 28, 1999 shaped Indonesia's first democratic election in almost fifty years. Under Suharto's repressive rule, five general elections were held between 1977 and 1997, with only three political parties allowed to participate: Golkar, PPP, and PDI (King, "The 1999 Electoral Reforms" 90). These elections were designed to present a façade of legitimacy for Suharto's government; they were neither free nor fair. Under Suharto's corrupt "New Order" government, Indonesians were deprived of their rights and effectively excluded from the political process. Although Suharto held regular elections in order to portray a false image of democracy, he refused to submit to direct election and forbid the organization of opposition parties in rural areas, where most voters were located. Instead, Suharto's term was extended by an electoral college whose members he had individually appointed to ensure his continued power (Case 9). In view of Indonesia's long history of authoritarian control, the 1999 elections signified its tentative transition to democracy. The legitimacy and quality of Indonesia's emerging democracy heavily depended upon the electoral system, rules, and institutions developed by these electoral reforms. To break away from its corrupt and tyrannical past, the first free and fair elections to occur in over 50 years carried significant consequences for Indonesia, ultimately determining which leader would be chosen to guide its course towards individual liberties and freedom of democratic rule.
In contrast to Suharto's corrupt government-controlled electoral college, the existence of an independent body to preside over elections suggested greater legitimacy for the results. How did the establishment of an Independent Election Commission during Indonesia's 1999 electoral reform influence the number of political parties within Indonesia? This is an important question to examine because the number of political parties can affect levels of vote segmentation, ability to produce an effective leader, and political fragmentation within the country. Since Indonesia's Independent Electoral Commission carried the responsibility of administering elections, which was previously a task controlled by the government, this body held the ability to greatly shape the breadth of political parties competing. In transition governments, such as Indonesia's, the quantity of political parties is of great importance for the emerging government's stability and quality, as well as the degree of representation for the median voter's preferences.
According to Paige Johnson in "Anti-party Reaction in Indonesia: Causes and Implications", Indonesia's Independent Election Commission, now known as the KPU, was formed as a key electoral reform in 1999 as a specific reaction to the government's manipulation of elections during Suharto's ruling years (485). The Independent Election Commission was conceived in order to plan and oversee the execution of the elections, collect data on results of the election, and determine the number of seats won by each party (King, "Half-Hearted Reform" 54). It was composed of five government officials and one individual from each of the parties qualifying to participate in the election. In order to qualify for participation, parties had to have an organization established in one-third of the provinces and half of the districts in each of those provinces (King, "Half-Hearted Reform" 51). The presence of party representatives on the commission was designed to prevent the former government-backed party, Golkar, from using the election administration to declare its own party as the winner.
While more than 200 parties requested to compete in the 1999 elections, the Independent Election Commission determined that only 48 met the qualifications for participation (Evans 136). The Commission's resulting 48 parties were significantly greater in number than the previous three designated eligible to compete, supporting my initial hypothesis that the number of participating parties would drastically increase. However, with such a large number of parties competing for seats, the Independent Electoral Commission reached an impractical size of 53 members. An additional component of the electoral reforms required political parties to obtain two percent of the seats in the national legislature in order to participate in subsequent elections. As a result, when 42 of the 200 parties did not meet this threshold, they were plunged into "lame-duck status" according to Dwight King in "Half-Hearted Reform: Electoral Institutions and the Struggle for Democracy in Indonesia" (210). This caused the commission to break into factions and led to a deadlock over the certification of the election results. According to Hermawan Sulistyo in "Electoral Politics in Indonesia: A Hard Way to Democracy", the election results were required by the Law on Elections to be endorsed and signed by at least 75 percent of the competing parties, or 36 of the 48 parties competing in the 1999 elections (82). When the parties that failed to meet the two percent threshold refused to endorse the final results, acting President B.J. Habibie signed a presidential decree recognizing the outcome.
Despite the domineering influence of the Independent Electoral Commission on the number of political parties in Indonesia, there are theories presenting other institutional variables as the cause for such a large number of parties. According to William Clark and Matt Golder, Duverger's theory states that more permissive electoral systems, such as proportional representation, produce more political parties (681). However, Clark and Golder argue that Duverger's views are not solely focused on institutional factors, but take into account societal influences as well. Duverger believes political parties are a "reflection of social forces," with greater social forces leading to the multiplication of parties, depending upon the degree of constraint applied by electoral institutions (Clark and Golder 681). The degree to which party systems reflect existing social cleavages depends upon the type of electoral institution. Permissive electoral systems, such as proportional representation, producing a large number of parties when levels of social heterogeneity are high (Clark and Golder 683). Given that Indonesia is an ethnically and regionally diverse archipelago, this theory may appear attractive for explaining the large number of political parties. However, Indonesia has a mixed electoral system, dubbed "PR Plus," allocating 76% of the seats in the legislature to single-seat districts according to the plurality principle and distributing the remaining 24% of the seats according to proportional representation (King 56). Therefore, while Indonesia is a very heterogeneous country, its electoral system is only partially designed as proportional representation, and therefore this theory cannot fully be applied to its specific case.
In addition to competing theories attributing the quantity of political parties to other institutional factors, the influence of further electoral reforms must also be taken into consideration. Other aspects of the electoral reform package, such as the requirements that parties must have an organization in one-third of the provinces and half of the districts in each of those provinces, certification of non-involvement in leftist organizations, and an ideology that did not conflict with the national philosophy of Pancasila influenced the number of political parties participating in the 1999 elections. These reforms eliminated narrow ethnic or religious parties, promoted parties with mass-based organizational structures, prevented leftist orientation from gaining any representation, and ensured that the national ideology was upheld. These reforms not only limited the number of political parties eligible to compete in the 1999 election, but also ensured that Indonesia's democratic transition did not cause the country to stray from its past.
In order to analyze the influence of Indonesia's Independent Electoral Commission on the number of political parties within the country, I will examine the quantity of political parties participating in elections over the course of the past four decades, which will reveal any change following the 1997 Electoral Reforms. In addition, I will assess the change in inter-party competition following the establishment of the Independent Electoral Commission by studying the number of political parties participating during the elections of 1971, 1977, 1982, 1987, 1992, 1997, 1999, and 2004. To determine the degree to which smaller minority parties gained representation following the 1997 reforms, I will consider the participation of such parties in the subsequent election as compared to their inclusion in previous elections. Inter-party competition will also be explored in an effort to better understand the degree to which the 1997 Electoral Reforms increased competitiveness within the government in comparison to the authoritarian-style pseudo-democracy and its lack of true opposition.
To address the effects of the electoral reform, political parties will be defined as those competing in elections. I hypothesize that the establishment of an Independent Electoral Commission will result in a dramatic expansion of the number of political parties competing for seats within Indonesia's government. Due to decades of profound censorship by the military-controlled government, a vast array of groups will strive for representation of their interests. Previously, political parties served the sole purpose of establishing legitimacy for Suharto's power. By maintaining frequent elections, the government provided a limited sense of public access and participation in the political process. However, the establishment of an independent body, rather than a government-run electoral college, would guarantee the inclusion of parties beyond the previous government-sanctioned three. With the world watching to see how Indonesia handles this opportunity for transition, the Independent Election Commission holds the responsibility of establishing free and fair elections and proving that Indonesia deserves to be regarded as a legitimate democracy rather than a falsely depicted one. The establishment of an independent body to oversee the 1999 election addresses the greatest flaw of Suharto's government: the lack of "an arena of contestation sufficiently fair that the ruling party can be turned out of power" (King, "Half-Hearted Reform" 5). Instead of simply allowing a select few parties to compete under a fixed election, the Commission provides a neutral electoral institution to prevent such government manipulation.
I hypothesize that the creation of an Independent Election Commission will increase inter-party competition. Given that electoral contest will be opened to all parties who wish to request to compete, it would follow that the level of competition between parties would increase as well. With more parties eligible for inclusion in the election, the competition to gain a seat will become tighter. The great importance of this election also influences the political aspirations of parties who wish to gain power within the government during such a significant period of Indonesia's history. An independent body such as the Commission will no longer prevent competition within political parties, as had occurred under Suharto's regime, but will instead encourage a healthy level of contest indicative of a democratic electoral system. Furthermore, I hypothesize that the Independent Election Commission will result in greater representation for smaller, minority parties. Such parties were previously forbidden from competing under Suharto's rule, essentially neglecting the interests of ethnic minorities within the country. With the expansion of electoral participation under the new rules of inclusion, smaller political parties will have a newfound opportunity to compete for the representation of minority interests. The Commission's impartiality will apply rules equally to all parties wishing to compete, allowing those who meet the qualifications to participate, regardless of size or ethnic membership.
As evidenced by Table 1.0, the number of political parties participating in elections from 1971-1997 remained stagnant at three parties, while the number expanded to eight parties receiving at least one percent of the vote in the 1999 elections and thirteen parties gaining one percent or more of the vote during the 2004 elections (Suryadinata 32). This dramatic increase in political parties competing in the elections following the electoral reforms of 1997 affirms the hypothesis that the establishment of an Independent Election Commission would result in a greater number of parties. Despite the fact that the three parties permitted to participate in elections from 1955 to 1997 remain active during the 1999 and 2004 elections, they receive competition in the form of additional parties. The Independent Election Commission took control of party inclusion and participation out of the government's hands, enabling more parties to compete in subsequent elections.
The Independent Election Commission may have successfully enabled more parties to compete in the election, but it failed to account for the high levels of inter-party competition that would unavoidably result. According to Shaheen Mozaffar and Andreas Schedler in "The Comparative Study of Electoral Governance", high levels of distrust between parties in democratizing countries motivates them to create an independent election-management body (17). During a country's transitional regime, opposition parties are particularly suspicious of the electoral process due to the government's former manipulation of electoral structures and processes (Mozaffar and Schedler 9). Therefore, election authorities must ensure that their political neutrality is widely accepted in order to ensure an effective transition. However, the structure of the party system also affects the decision to establish an electoral commission. Within a two-party system, political parties are more likely to choose to relinquish their powers of electoral governance to an independent commission, which allows them to share power while retaining the right to veto. In a more fragmented multiparty system, political parties tend to choose a multiparty commission instead in order to retain a level of control (Mozaffar and Schedler 17).
When governments choose to arrange a multiparty commission to manage their elections, some parties may form exclusionary alliances against others (Mozaffar and Schedler 17). This occurred under the Independent Election Commission in Indonesia when it became apparent that most seats would be concentrated among the top six or seven parties. Thirty-one of the 48 total parties refused to sign off on the election results, preventing the commission from receiving the two-thirds majority needed to certify the election (King, "Half-Hearted Reform" 87). According to King, the party representatives were effectively overturning the decisions of their colleagues to verify the results ("Half-Hearted Reform" 87). In addition, several representatives for the small parties also demanded legislative seats for all parties that had participated in the elections, regardless of how many votes they had acquired, citing the defense that they had helped successfully administer the elections and did not have sufficient time to prepare for their campaigns following Suharto's resignation (Johnson 488). This request was not granted, however, and these small parties abused their positions in the Independent Election Commission by uniting in an effort to block the Commission's decision and buy themselves time to obtain seats through corrupt measures (King, "Half-Hearted Reform" 77). This evidence supports my second hypothesis that the establishment of an Independent Electoral Commission would result in increased inter-party competition. Thirty-one of the smaller political parties that failed to obtain a sufficient number of votes manipulated their positions on the Commission in an effort to secure seats in the legislature from parties that had received more votes. Competition for seats was extremely high, with only six parties receiving a sufficient amount of votes to meet the 2% threshold required to compete in the next election and only 21 parties receiving a seat (King, "Half-Hearted Reform" 77). Since this election occurred during Indonesia's influential transition from an authoritarian regime to a democratic government, the level of distrust between parties was particularly high. In addition, parties were accustomed to governmental manipulation and were willing to resort to corrupt measures in order to secure a position for their party in the legislature.
In order to test my third hypothesis, which speculates that the establishment of an Independent Election Commission will result in greater representation for smaller, minority political parties, I must first examine the preceding election of 1997. Under Suharto's regime, the predetermined three parties were given permission to participate: the United Development Party (PPP), the Indonesian Democratic Party (PDI), and the government-backed Golkar. Since the two opposing parties were participants merely to present an image of electoral competition and legitimacy for Suharto's regime following Golkar's fixed victory, PPP received 22.4% of the total votes and PDI only received 3%, leaving Golkar the majority of votes with 74.5% in the 1997 election, as evidenced in Table 1.0 (Suryadinata 32). Smaller parties had no opportunity to compete in this election, let alone obtain representation. During the 1999 election, 21 political parties may have won seats, but the top five parties accounted for 90% of the votes and therefore almost all of the 462 seats in the House of Representatives (King, "Half-Hearted Reform" 207). PDI-P received 33.7% of the votes, Golkar obtained 22.4%, PKB acquired 12.6%, PPP gained 10.7%, and PAN won 7.1% (King, "Half-Hearted Reform" 78). In total, these five parties obtained 416 of the 462 seats in the House, leaving parties that each acquired less than 1% of the vote with less than 10 seats each and 10 parties with only one seat. Despite the electoral reform, the three parties which were formerly the only electoral participants still received 66.8% of the votes and 331 of the seats, leaving the lesser parties susceptible to their majority control in the legislature.
The lack of representation for smaller parties can be traced to the electoral restrictions enforced by the Independent Election Commission. In order to qualify to compete, parties had to have an organization established in one-third or nine of the provinces and half of the districts in each of those provinces in an effort to exclude ethnically and regionally based parties (King, "Half-Hearted Reform" 51). Suharto's government had restricted participants to three political parties, but these reforms now limited on the basis of "insufficient geographical coverage and depth or penetration of their organizations" (King, "Half-Hearted Reform" 51). This data disproves my third hypothesis, revealing that minority interests continued to be overlooked and smaller parties were unable to gain enough votes to contend with the top five winning parties. For instance, the ethnic Chinese party PBI was eligible to compete in the 1999 election, although it only captured 0.34 percent of the Indonesian vote, securing a single seat (Suryadinata 132). This may be due to confounding variables, however, such as fear of voting for an indigenous political party. Within Indonesian society, ethnic Chinese were often victims of ethnic violence, causing them to stray from political participation as a defense strategy. According to a survey conducted by the Indonesian weekly news magazine Tempo, many Chinese were still afraid to express their political views in 1999 (Suryadinata 127). Nevertheless, smaller parties fortunate enough to gain a single seat in the legislature, such as the PBI, would be incapable of advancing any of their interests when met with the larger parties that dominated the House of Representatives.
However, while each of these requirements did limit and curtail the number of parties participating in Indonesia's elections, the institution of the Independent Election Commission carried out these electoral rules and limitations. Initially designed to prevent the government from intervening in the election's outcome, the Independent Election Commission ultimately failed to perform its obligations to ensure a truly free and fair election as necessary for qualification as a truly democratic nation. Its expansive, largely undefined and misused powers allowed each party to overstep and manipulate its boundaries. As evidenced by the formation of a coalition of 38 parties refusing to recognize the election results, this institutional body was corrupt and far from independent. In its final report, the central supervisory committee of the Independent Election Commission charged the body with overreach and abuse of its authority, stating that the commission allowed "parties greater authority than the law allowed in determining their elected candidates" (King, "Half-Hearted Reform" 87).
According to Larry Diamond in "The Global State of Democracy", for a country to be considered a liberal democracy, a truly independent electoral commission must first exist to ensure the possible removal of corrupt elected officials from office through frequent, free and fair elections (418). Truly free and fair elections are one of the greatest indicators of a democratic government, revealing the power of the public to hold its representatives accountable and restrain their authority. In emerging democracies, the role of an independent electoral commission is particularly vital to the legitimacy of the subsequent government. Electoral governance in such transitional regimes must balance administrative efficiency, political neutrality, and public accountability (Mozaffar and Schedler 8). Credibility of electoral results can be attained when electoral rules and institutions meet each of these challenges. Within electoral governance, the level of rule application is most susceptible to errors due to the magnitude and complexity of tasks and the authorized discretion of officials involved while accomplishing such tasks (Mozaffar and Schedler 9). During Indonesia's 1999 election, whenever new electoral procedures were ambiguous or poorly understood, members of the Independent Election Commission tended to resort to old operating procedures, contributing to the relatively high incidence of procedural violations (King, "Half-Hearted Reform" 86). Although thousands of violations of election laws and regulations were documented, international and domestic election monitors agreed that the 1999 election was the freest and fairest election since 1955 (King, "Half-Hearted Reform" 77).
The results of this research indicate that the establishment of an Independent Election Commission helped to bring a higher degree of legitimacy to the election's results than had been achieved under Suharto's regime. Although the Commission was not without flaws, it is expected that the first elections to occur following the fall of an authoritarian government will contain a degree of attempted manipulation on the part of participants. The Commission did allow the expansion of electoral participation to encompass a greater number of political parties, although competition among participating parties increased as well and minority interests were not represented to a greater degree than before. Continued research is necessary to further assess the role of electoral governance in relation to political parties within emerging democracies. According to a growing compilation of evidence, ineffective electoral governance is an important cause of many flawed elections that have occurred in transitional regimes within the last three decades (Mozaffar and Schedler 6). However, electoral governance has been largely overlooked as an influential variable of democratization in political research (Mozaffar and Schedler 6). The importance of electoral institutions that enforce the rules, carry out the elections, and determine the allocation of seats among parties should not be discredited as inconsequential. Electoral results are due to party systems, political participants, individual campaigns, and voter turnout, but electoral governance plays a greater role than is typically acknowledged.
In addition to indicating the need for additional research regarding the role of electoral commissions, this research reveals the influence of social, economic and political variables that affect the process, outcome, and legitimacy of elections. The institutional changes and electoral rules established in Indonesia by the 1999 electoral reforms contain restrictions; they cannot reverse the damage that decades of authoritarian rule have inflicted upon the integrity and strength of elected representatives. Factors outside the limitations of an electoral institution such as the Independent Election Commission also affect the number of political parties, levels of inter-party competition, and degree of minority representation. Establishing an Independent Election Commission in an effort to ensure the neutrality and legitimacy of electoral results will not yield the preferred outcome of increased legitimacy for Indonesia's government without taking into consideration the factor of human error. Individuals working as members of the Commission do so as agents of its authority, but institutional regulations cannot compensate for their corrupt ambitions of personal advancement. This research indicates the large degree of uncertainty, even within electoral rules and institutions, which cannot be controlled in an effort to democratize a nation. Rather than seeking to contain unexpected circumstances, electoral bodies and the laws they enforce must instead account for such factors in advance in order to avoid the breakdown of well-intended establishments, as occurred in the case of Indonesia's Independent Election Commission.

Over the past years, the media has been accused of having a liberal bias. The Daily Show faces the brunt of the criticism with their "fake news" program that highlights liberal ideas and openly criticizes policies of the Bush administration. In 2004, the Daily Show created its own series to cover the presidential race entitled Indecision 2004. The Democrat and Republican Conventions were each covered in four consecutive episodes in July and September. Assuming that the "liberal bias" of the Daily Show helped filter the content shown during convention coverage, it could be inferred that the Democrats were covered more favorably than the Republicans. However, when isolating and analyzing the convention coverage from the DNC and RNC, the Daily Show's coverage better highlights Republican ideals and message.
The DNC was covered from July 27 to the 30th with the series title featuring the Democrat's "Race to the White House." The RNC was taped later from August 31st to September 3rd, and was entitled "Target New York." Each series contained four episodes covering convention highlights and in-house interviews with John Stewart. By analyzing the sound and image bites of the convention coverage, the noted strength of the Republican party to speak in "one voice" was clearly shown. This study provides an analysis of the speakers shown, topics covered, and subsequent commentary. While the clips of Republicans focus on topics related to the war in Iraq, homeland security, and Bush's promise to keep America safer, the clips of the Democrats fail to produce a unifying theme and message.
The potential effect of the Daily Show's coverage to influence viewers has been studied in the past years through the presentation of sound and image bites. Scheuer and others agree that "television is the dominant medium of a media-dominated age" and effective because it "manipulates our emotions, numbs us with stereotypes, saturates us with the trivial and the superficial" (1; 6). Scheuer also notes how simplicity drives the television message and "because of this language (sound bites), television favors certain political ideas and disfavors others. The electronic culture fragments information into isolated, dramatic particles and resists longer, more complex messages" (10). Liberal ideas are then naturally challenged, as they appear more complex and progressive. "Simplicity, I will suggest, is epitomically conservative, whereas complexity is quintessentially progressive" (Scheuer, 10).
The existence of televised political coverage then enables journalists and commentators to transmit messages to the common public. They are able to take events and sound bites and treat them "as raw material to be taken apart, combined with other sounds and images, and reintegrated into a new narrative" (Hallin, 9-10). The Daily Show adds a humorous component to its reporting of political news, which diminishes the distance between the viewer and the situations being reported on. "Humor can enable people to confront authority, to diminish it, to reduce its distance and majesty, thereby revealing authority holders as imperfect mortals, error-prone humans, ordinary people unworthy of special respect, deference and continuation in office" (Paletz, 8). Hallin went farther to note the feeling of public efficacy derived from political commentary. "Often it was extremely interesting...to hear a politician, or even once in awhile a community leader or ordinary voter, speak an entire paragraph. We had a feeling of understanding something of the person's character and the logic of his or her argument that a 10 second sound bite can never provide. One also had a feeling of being able to judge for oneself" (Hallin, 19). Bucy and Grabe also emphasized the importance of image bites because "nonverbal emotional displays of leaders can serve as a potent vehicle for expression" (659). The study of these sound and image bites in political news coverage, even the "fake news" of the Daily Show enables valid conclusions to be drawn about the effect of potential communications on the electorate.
The goal of the analysis was to find a way to distinguish the content of the Democratic and Republican coverage in a way that was measurable. The best method to do this was to pick out direct points of reference that would occur frequently in coverage of both conventions. The compiled coding sheets are available for reference in the appendix of this paper.
The first thing coded for was the ways to define Democrats and Republicans in the episodes and who made those statements. Examples of this included the Daily Show's opening segment, which defined the Democratic Convention as featuring "black people, trial lawyers...celebrities who believe in their hearts that they're helping but really aren't." The coded definitions were only those that meant to apply to the party as a whole and not individual people or the nominees. By noting only the generalizations for the party, conclusions can be drawn about a party as a whole, rather than one representative of it.
There was also a distinction made between references to the party rather than the convention. The coding sheet marked references to the Democratic National Convention and Republican National convention. The messages coded in this section noted the themes presented at the convention and general remarks made by commentators about the event. The intent of marking down the ways in which the parties and convention were defined was to note the differences not only between the parties themselves, but the ways in which the opposition party was referenced by the other.
The third point of analysis for the convention coverage was the references to the nominated candidates, John Kerry and George W. Bush. This type of analysis provides insight to how the opposition party defines the other candidate while also presenting their own. The main point of a convention is to introduce a Presidential candidate to the nation and so the ways they are defined are crucial.
In addition to noting references made to specific candidates and parties, the coding sheet accounted for blatant attacks made by one party on the other, and in some cases, attacks made by one party on itself. Attacks were only marked when one person made a direct reference to a party with a subsequent statement or negative tone. The attacks were labeled as issue or character based. The reason for making this distinction is to note whether there was a significant difference in the type of partisan attacks.
The speaker clips shown in the convention coverage represent the party's message. The coding sheet included the clip of every speaker who was not a correspondent for the Daily Show at each respective convention. The coding for each speaker included their title, talking point, length of time shown in the clip and the context in which they were shown. The context varied in three mediums: either a collection of video clips chosen by the Daily Show, a speech on the convention floor, or an interview in the Daily Show's studio. All of the time measurements were taken in seconds to be able to make comparisons more accurate.
The last two things coded for were the actual length of sound and image bite coverage of the two conventions. Sound bites were only measured when someone who represented the convention, not the Daily Show's representative, was heard. Image bites were taken when a person was shown, but not heard. The measurements do not overlap and adding them would denote the entire length of time the convention was covered for both parties. Sound and image bites of the convention do not include the studio interviews by Stewart or commentary from the Daily Show correspondents. The intent of this analysis is to show that the message from the clips of actual convention coverage and subsequent commentary from direct party and Daily Show representatives themselves produce a message that can then be interpreted by a viewer.
The results of the content analysis show that there is a more unified message delivered by the Republicans through the Daily Show's convention coverage. The first two references coded for were ways to define the parties and the conventions generally. This analysis was more focused on the rhetoric used by both the parties. In both series of convention coverage, the terms to define Republicans and Democrats came from the Daily Show correspondents and other media figureheads. Every episode in the "Race to the White House" series opened with an announcer proclaiming the convention featured "black people, trial lawyers, unifying anger, organized labor, godless sodomites, and abortion for everybody,..." A serious tone was not associated with the message, but the viewer is introduced to Democratic coverage by their stereotypes. Furthermore, when Stewart is talking about the theme delivered by the Democrats, he mockingly insinuates that they are "laying it on too thick." The viewers are encouraged to view the clips as very rhetorical, so they would possibly then analyze the speeches from a more critical perspective. The statements made generalizing Republicans were much more directed and offensive. Stewart himself made these comments when generalizing the GOP convention to be a lot of "white men." Stephen Colbert also criticized the Republican's management of convention coverage to falsely highlight their small number of minority members, "if you're a non-Caucasian Republican, you got yourself a heaping helping of face time."
The commentary on the Republican's relationship with minorities continued when coding the ways in which the two conventions were described. The opponent's convention was mentioned only once during the Republican convention and not at all during the Democrat's. The attack on the Democratic Convention came from Arnold Schwarzenegger when he described it to be as the same as his movie, "True Lies." All of the other references to the Democratic convention, with the exception of one made by Gov. Bill Richardson were statements made by Daily Show correspondents. John Stewart called the convention a "Dance Party" three times and immediately switched to image bites of delegates dancing in their seats. There was no parallel mention of this jovial spirit during the Republican convention. The constant theme of the message describing the Democratic convention was insisting that the event was a "farce, a scripted stage managed event, it's not news, it's not even fake news." Stewart went further than Colbert to say that both conventions merely resembled "infomercials."
While the commentary for the Democratic convention targeted their energized spirit and mocked the event itself, the Daily Show's commentary for the Republican convention was much more directed towards the Republican character. Steward described the convention atmosphere as a time when "Disneyworld closes for gay couples" and Colbert reminded the viewers that "we're doing the same show tonight -- homosexual white men." Five out of the nine mentions by the Daily Show about the Republican convention mentioned homeland security and the Republican theme of a "safer and stronger America." Despite what could be viewed as negative commentary, "is it tonight that they exploit 9/11 or is tonight empty promises for the future?" the constant associations of the Republicans to homeland security issues could also serve to reinforce their talking points.
The graph further emphasizes how the Democrats did not make any attacks on the Republican's convention, while the Republicans not only attacked them, but also kept their message about themselves positive.
The following table and charts show the differences in commentary related to the Republican and Democrat nominees.
The charts clearly show that the Republicans did not make any attacks on their own candidate, yet the Democrats made three direct attacks on Kerry. Those attacks came from Democratic congressman Zell Miller who spoke at the Republican convention to express his disdain for John Kerry. Miller did not hesitate when describing John Kerry as "more wrong, more weak, and more wobbly than any other national figure" and proclaiming that "Kerry would let Paris decide when America needs defending." The Daily Show included the clips of Miller's speech and spent several minutes discussing why a Democrat would attack his own party. Miller further emphasized the Republicans' talking points against the Democrats as John McCain questioned whether John Kerry had shot his dog. While the table shows both nominees received an equal number of attacks, none of George W. Bush's attacks came from his own party. The nature of their attacks on John Kerry were about his war record and security issues, while the comments for Bush targeted on his character and intelligence. Joe Biden described how he knows the President "doesn't read anything, but I think he watches television." The content analysis showed that while both nominees were described in relation to a war context, the Republican Convention contained more pointed and direct attacks towards John Kerry's record.
The results issues coded for related to the concept of issue ownership amongst the Democrats and Republicans. The coding sheet accounted for the number of positive and negative mentions between certain issues and the different parties.
The most notable result is that the Republicans had 15 positive mentions associated with them and homeland security and only 6 negative comments. The Democrats, in relation, had only 3 positive mentions associated with security issues and 8 negative. The issues of health care and religion were not mentioned negatively for either the Democrats or Republicans. Religion was mentioned positively once for both parties and health care once for the Republicans. Social issues covered topics ranging from abortion to gay marriage. The Republicans and Democrats balanced both their negative and positive associations with social issues so that no party clearly had more significant coverage.
Another main focus of the content analysis was to evaluate the number of speakers clips shown from the convention coverage and evaluate the topics and length of airtime. The graph clearly shows a difference between the sheer volumes of Democratic speaker clips as compared to Republicans.
The Democrats' coverage had 25 speakers, while the Republicans had only 14. The following pie charts highlight greater discrepancies between the types of speakers shown for both types of convention coverage.
Over half of the Democratic speakers shown held some form of public office, while only six of the Republicans did. Seven of the Democrats were related to the President and Vice Presidential nominee and the remaining were media figureheads, such as celebrities, who spoke in support of the Democrats. In contrast, the Republicans had six of their speakers related to their nominee and one from the President's staff. While the number of speakers for those categories may be roughly the same, the Republicans had a smaller number of speakers overall, thus their percentage of speakers directly associated to the President and Vice President was 50%. It is also interesting to note that roughly one-third of the Democrats shown were candidates for the nomination themselves and five of them including Howard Dean, Joe Biden, Joe Lieberman, Wesley Clark, and Al Gore, openly talked about their attempts to run for President. Governor Dean shared stories from the campaign trail for 20 seconds and Former General Wesley Clark shared his war credentials and history for 14 seconds. The stories shared by elected public officials from the Republican convention evoked memories of 9/11 and the war on Iraq. Senator McCain talked about the Iraq War for 20 seconds while Governor Pataki and Mayor Giuliani referenced September 11, 2001 with a banner with those words in the background. No matter the sound bite shown, viewers would still receive an image of 9/11.
The coding sheets for the Republican and Democratic National Convention also show a difference between the number of in-house interviews Stewart conducted during each series. Stewart interviewed four people during the Republican convention and only two during his coverage of the Democrats. However, two of the interviews during the Republican coverage were with media commentators, Ted Koppel and Chris Matthews. The two other interviews were with Senator McCain and White House Communications Director Dan Bartlett, and they focused almost exclusively on the Republican message and President Bush's handling of the war. The two interviews with Democratic Governor Bill Richardson and Senator Joe Biden covered some of the convention message, but also focused on the official's personal story and their advocacy issues. There was no significant discrepancy between the time each interviewee was allotted during coverage.
The length of time an image or sound bite is shown on television can be significant. The following graph shows that the Republicans were shown more than the Democrats, with 330 seconds devoted to image clips, while the Democrats had only 282.
The Democratic sound bites were longer totaling 408 seconds, while the Republicans had 330 seconds. The nominees themselves also received an equal amount of coverage with Kerry receiving 77 seconds of image and sound bites and Bush being heard for 61 seconds with no image clips. The majority of the Republican image bite coverage was pictures of delegates on the floor, while many of the Democratic clips were speakers walking out to the podium on stage. Looking only at the length of actual convention coverage, including video and audio footage of the DNC and RNC, the amount of time was roughly equal for both parties. The Democrats were heard and shown for 13% and the Republicans for 11.8% of the Daily Show's total series coverage for each convention.
It seems slightly farfetched to hypothesize that coverage of the Republican and Democratic National Convention on the Daily Show could provide some advantages for the Republican candidate. Especially in a media environment that is constantly accused of having a liberal bias and favoring progressive ideals. The conclusions drawn above are extrapolated evidence from a small sample size of only eight episodes. The size of the sample could have affected the results because only four episodes are being compared with their four counterparts. The Daily Show is a television show that has been established over years and it seems impractical to make general statements about its content on the basis of eight episodes.
Yet analyzing the content of the sound and image bites of the conventions and their subsequent commentaries show that the Republicans had a more focused and more disciplined message than the Democrats. The number of convention speakers shown for each the Democrats and Republicans support the image of a closed Bush administration. Auletta noted in his article in the New Yorker that "what seems new with the Bush White House is the unusual skill that it has shown in keeping much of the press at a distance while controlling the news agenda" (4). After studying the communications and public affairs office of the Bush White House, Martha Kumar explained that, "Bush has a great focus on the structure of his communications operation -- advocating for him is not the same thing as explaining his decisions or actions"(15). It is this strength, which shines through the coverage by the Daily Show on the Republican convention. Most of the Republican speakers mentioned the war in Iraq, September 11th, and the ability of George Bush to help make America safer. Most of the Republican speakers were related to or staff for the Bush administration. The administration speaks with one voice and they "never get off their talking points" (Auletta, 4-5).
The Democrats, by contrast, through this analysis, are presented by their depth. Over half of the convention clips shown by the Daily Show were of previous candidates for the nomination and most of them even talked about their own Presidential ambitions. Senator Barack Obama and Congressman Dennis Kucinich talked about uniting America and the "hope of a skinny kid with a funny name who believes America has a place for him too." But those hopes for unity fell short of the coverage offered by the Daily Show. Democrats attacked fellow Democrats and the coverage of Zell Miller's speech at the Republican convention was the highlight of one episode. Not one Republican spoke ill of President Bush, and not once did Democrat Zell Miller speak positively about his own party.
The Daily Show commentary played a strong role in influencing the perception of Democrats and Republicans by the viewers. When referencing the nominating candidate, the Daily Show was responsible for the majority of insults against President Bush. Their video montages echoed Bush's ability to control the media with the headline, "George W. Bush: Because He Says So." There is no doubt that reporters "see the White House as a fortress" (Auletta, 2). The Daily Show introduced John Kerry in their montage to be the alternative choice, "Because He's Not George Bush." Despite the clear insult towards the President's authority, the viewers associate the images of John Kerry constantly with George Bush. Kerry and the Democrats are unable to dictate the message.
One possible reason for the inability of the Democrats to control the message is the circumstances surrounding the 2004 campaign. The Republicans were facing a re-election year, so President Bush did not have to go through the vetting stage at the same time as John Kerry. The Democratic convention discussed the trials that took place through the nominating process. Joe Lieberman discussed how there was almost a "three way tie" for third place. Governor Dean shared stories about his campaign and the Daily Show highlighted his famous scream. Although the chair of the convention, Bill Richardson stated that this convention was a "build up to tell the story of Senator John Kerry," the story was lost amongst the breadth of speakers and inner party attacks.
Another subtle influence on the coverage of the two conventions was their locations. The Democratic convention was held in Boston, Senator Kerry's home state. The Daily Show had to stay in travel accommodations and did not have their familiar set available. For the Republican convention, the Daily Show was on its home turf. They had more in studio interviews during their Republican coverage, possibly because more people were available to come onto the show and they had more access to other media commentators in New York like Ted Koppel and Chris Matthews. It is also no coincidence that the Republican National Convention was hosted in New York three years after the attacks on the World Trade Center. The President and other Republicans were able to stand outside the twin towers and further emphasize September 11th as the central focus of their re-election strategy.
This militant focus on message is also shown through the analysis of positive and negative issue associations with the Democrats and Republicans. The Republicans had 15 positive messages associated with them and homeland security. Mayor Rudy Giuliani spoke in front of a backdrop of September 11, 2001 that was in bolded white against a black screen. If the Daily Show wanted to clip any of his speech, they would inevitably be fed the image bite of 9/11 regardless of Giuliani's words. In comparison, three issues, which would normally be associated positively with Democrats, social issues, health care and the economy, did not distinguish them more significantly than Republicans. In fact, the Republicans had more positive mentions in relation to social issues than the Democrats. They also had less negative associations with homeland security than the Democrats. The election of 2004 was arguably a referendum on the Bush administration's ability to "keep America safer" and by noting the message of the Democrats and Republicans in relation to that theory, the Republicans message was more emphasized and thus could be more directly extrapolated upon by the viewers.
The coverage of the Daily Show's Indecision 2004 highlighted the ability of the Democrats and Republicans to present a message through their respective conventions. Although the Daily Show is known as one of the most liberal television programs on TV, the direct convention coverage showed that the Republicans are more organized, more disciplined, and have a more coherent message. This reflects the strategy of President Bush that was also echoed by a senior official weeks after the Republican convention. "We're an empire now, and when we act, we create our own reality. And while you're studying that reality -- judiciously, as you will -- we'll act again, creating other new realities, which you can study too, and that's how things will sort out. We're history's actors . . . and you, all of you, will be left to just study what we do"(Suskind, 2004). The Democrats' attempt to create a coherent message from a nomination field of over seven candidates and the unsure war record of the nominating candidate clearly pales in comparison.
The next question then, is to ask why coverage assumed to be skewed in one direction actually promotes another as a result of its failure to diminish the effects of the Republicans' talking points. It is assumed that "journalists can create importance and certify authority as much as reflect it, in deciding who should speak on what subjects under what circumstances" (Cook, 87), but that is not reflective in John Stewart's coverage of the conventions. The entire Republican Convention reflected a well-oiled machine of speakers and unflinching support for the current President. It would have been difficult for the Daily Show to find footage of the actual convention that showed otherwise. Chris Matthews of MSNBC produced insight into this phenomenon two years later, after Stephen Colbert gave his speech at the annual White House Correspondents Dinner. He proclaimed that "the president is our head of state, not just a politician" and because of that, is he subject to different rules pertaining to the scrutiny of his campaign strategy and message (Matthews, 2006)? Analysis of the Daily Show's presentation of Indecision 2004 exposes how the Republican "voice" will effectively and inevitably break through their political coverage.

The issue of health care in America has not always been at the heart of every political debate. In a study conducted during the 2004 Presidential elections, although health care ranked higher in importance among voters than most other domestic issues, it was only fourth in importance in determining their vote for president, with affordability of health care and health care insurance being chosen as the specific health related issues of greatest concern (Blendon, et al. 2004). However, health care has, at certain times, been extremely contentious and highly politicized, bringing it to the forefront of voters' attention. A current prime example would be the bitter partisan debate between the Democrats and Republicans regarding the reauthorization of the State Children's Health Insurance Program (SCHIP), an extremely volatile issue precisely because it concerns the well being of society's most vulnerable -- children. SCHIP, which will be discussed in greater detail later, has brought forth a barrage of petty party bickering and accusations of politicization from both sides of the aisle. This has led to much media criticism about the futility of such political theatre and rhetoric in addressing the problem at hand. Thus, what sort of effects has the politicization of health care brought to the American people? Have these disagreements paradoxically brought about more thoroughly analyzed policies and led to pragmatic and optimum compromises or has this political theatre and exaggerated rhetoric only served to impede the improvement of healthcare due to both parties' inability to cooperate and break the deadlock in political ideologies? The link between the variables of politicization of healthcare and its effect on the public will thus be studied with SCHIP as its main case study.
SCHIP was enacted as Title XXI of the Social Security Act by the Balanced Budget Act of 1997 primarily to expand insurance coverage to low-income children living in families who earned too much to qualify for Medicaid but were yet unable to afford private insurance. At that time, more than 10 million children lacked health insurance, with about seven million of them living in families with incomes below twice the federal poverty level (FPL). Although 75 percent of those uninsured children lived in a family with at least a parent who had full-time employment, and 90 percent had one parent who was employed either full or part-time, their families were either not provided with job-based health insurance or lacked the finances to buy the insurance offered (Families USA). Thus, Congress appropriated approximately $40 billion to fund the program for 10 years. Although it was optional, within two years of SCHIP's inception, all fifty states had adopted the program to expand coverage for children. Based on steadily rising enrollment in SCHIP, rates of uninsured children from low-income families have declined 2.2 million, from 23 percent in 1997 to 14.4 percent in 2004, and the program is seen as highly successful (Pediatrics). However, in 2006 the U.S. Census Bureau reported that 9.4 million children in the U.S. did not have health coverage-one in eight being uninsured. What this meant was that after six years of improvement in the rate of health coverage for children, the number of uninsured children has again begun increasing. In 2006 alone, although SCHIP was found to provide health insurance coverage to 6.6 million children, there was found to be one million more uninsured children than two years ago (Pediatrics).
This set the stage for SCHIP to become a highly contentious issue and a political hotbed as children have always been perceived as political untouchables. Thus, the reauthorization of SCHIP is a very charged one because it has been a highly popular program that has drawn bipartisan support in the past. In fact, a survey by the Center for Children and Families at the Georgetown University Health Policy Institute has also found that nine in ten Americans say the program is important, with support for it crossing party lines (Lake Research Partners). Democrats are in support of the SCHIP reauthorization bill and have championed the fact that the SCHIP bill would cover 10 million children and be completely paid for by the tobacco tax, adding no strain on the federal budget. However, on the other side of the aisle, Republicans have felt that a $35 billion expansion would be too great of an increase and goes "too far in federalizing healthcare." (Pediatrics) The inclusion of a tobacco tax, purported claims of covering illegal immigrants, middle-class families and adults, and the possibility of the reauthorization adversely affecting the private insurance market are also the opposing arguments that are most often brought up by the Republican side.
The debate that has taken place in Congress over the course of the past three months is indeed a complicated affair that has seen usual legislative processes thrown out of the window. The reauthorization issue first arose in early August when the House passed the CHAMP Act in a partisan fashion 225 -- 204 with primarily Democratic support, in an attempt to renew SCHIP before it expired on September 30, 2007 (Children's Defense Fund). The CHAMP Act would provide health coverage to 4.1 million more uninsured children. Nearly $50 billion would be needed in additional funding over the next five years and will be paid for by a 41-cent increase in the federal tobacco tax and cuts in overpayments to private Medicare plans (Children's Defense Fund). However, the Senate's own Children's Health Insurance Reauthorization Act of 2007 (CHIPRA) would expand coverage to another 3.2 million uninsured children and would cost an increase in $35 billion of the federal SCHIP contribution, from $25 billion to $61.4 billion over the next five years. This package is funded by a $1.00 tobacco tax, a 61-cent increase from before. CHIPRA eventually passed 68 -- 31 (Children's Defense Fund). President Bush has expressed his opposition to both plans and promised a presidential veto if the bill passed both houses. He proposed increasing funds by a comparatively lesser $4.8 billion over the next five years. In addition, Bush also proposed the highly unpopular move of reducing funding for states that have expanded SCHIP eligibility to children in families with yearly incomes more than 200 percent of the FPL (Pediatrics).
On September 24 this year, after nearly two months of negotiations, the House and Senate finally attained a bipartisan compromise to reauthorize SCHIP for an additional five years and cover 3.1 million more uninsured children. The compromise bill is similar to the Senate's, but includes additional provisions adopted from the House bill. This legislation would cost $35 billion over five years, cover a total of 10 million children, and would be paid entirely by a 61-cent increase in the tobacco tax (Children's Defense Fund). It has also obtained the endorsement from 43 governors, and a wide range of constituencies ranging from AARP, the National Council of State Legislatures and the American Academy of Nursing. In the House, the bill passed 265-159-insufficient to override a presidential veto-with 45 Republicans voting with all but eight Democrats in support of it. In the Senate, however, the bill passed veto-proof in a bipartisan manner 67 -- 29. However, President Bush followed through with his promised veto, but signs a continuing resolution temporarily funding SCHIP at former levels till November 16, 2007. Thus, even though SCHIP coverage has not ceased, the ideological deadlock has placed the health insurance coverage of the 6.6 million children currently enrolled at risk (Children's Defense Fund).
Following that, the House then won a procedural vote that allowed it to postpone until October 18 2007 a vote to override the veto of the SCHIP legislation. In those two weeks, congressional Democrats and their allies advanced with a paid media and grassroots campaign to pressure Republicans in vulnerable districts to vote for the override. According to CQ Today, the delay was intended to give Democrats and bill supporters time to "make a 'no' vote as politically unpalatable as possible for Republicans." (Kaiser Network) Public pressure and the media spotlight continue to mount as a joint survey conducted by NPR, Harvard School of Public Health, and the Kaiser Family Foundation show that 65 percent of Americans support increased funding for SCHIP, even after hearing opponents' arguments against it. On October 18 2007, the House voted 273 -- 156 in an unsuccessful attempt to override President Bush's veto of the reauthorization of SCHIP, with 229 Democrats and 44 Republicans voting in favor of the override, just 13 votes shy of the two-third margin needed (Children's Defense Fund). On October 25 2007, the House again successfully passed a revised but weaker CHIPRA bill 265-142, again without a veto-proof majority. This bill was essentially the same as the first and would cover 10 million children, but it was altered cosmetically to pacify Republican demands. It would limit coverage to children in families with annual incomes below 300 percent of the FPL, made it less attractive for parents to switch from private to government funded insurance, phased out SCHIP coverage of childless adults within one year, and also required states to apply more thorough citizenship documentation standards to prevent undocumented immigrants from enrolling in the program (Kaiser Network). The Senate followed suit and passed the revised compromise bill as well, this time without a veto-proof majority. Bush again promised to veto the bill if it was sent to his desk and this time went further to state that he would veto any bill that consisted of a tobacco tax increase. Therefore, in order to obtain more time for bipartisan negotiations, Congress passed and President Bush signed another continuing resolution that funded SCHIP at 2007 levels of $5 billion a year through December 14, 2007. After reaching a stalemate in negotiations right before the two week Thanksgiving recess, the SCHIP reauthorization bill was picked up again when Congress resumed on 3 December 2007 (Kaiser Network).
For the purpose of this study, we will define politicization according to Dictionary.com, which describes it as the bringing of political character or flavor to, as mainly partisanship -- a person with an inclination to favor one group, view or opinion over alternatives. It is a commitment to the incumbency of the ideology of a particular political party. Thus, in this case, the politicization of healthcare would imply that the debate has become mostly a Democrat versus Republican battle, with precedence being given to securing political points and capital rather than focusing on the substantive content of the issue at hand. The specific effects of this form of politicization on the American public that will be analyzed in this study will be the variable of the availability of health care insurance because these have been selected by voters as issues of greatest concern (Klein 2006). The section of the American Public that will be focused upon specifically will be low-income children since SCHIP was created precisely with their well being in mind.
The debacle over SCHIP is not the first time that America's healthcare has been brought into the fray of partisanship. The Clinton universal health care plan of 1993 was another highly politicized health care reform package proposed by the administration of then President Bill Clinton, and created and chaired by his wife, the First Lady of the United States Hillary Clinton (Skocpol 1995). The proposal provided guaranteed insurance coverage for all employees that was to be funded through payroll taxes and provided by tightly regulated, non-profit Health Maintenance Organizations (HMOs) (Health Economics 2007). The Clinton health care plan was estimated to create 59 new federal programs or bureaucracies, enlarge 20 others, require 79 new federal authorizations, and make substantial changes in the tax code, which critics felt would have only served to further complicate the already inefficient bureaucracy (Skocpol 1995). In line with providing universal healthcare, Clinton's plan also offered the unemployed government subsidies for enrollment into the HMOs (Health Economics 2007). Thus, on Capitol Hill, as political strategizing started kicking into gear, astute right-wing Republicans appreciated the fact that their ideological fortunes within the party itself, as well as the Republican partisan interest in undermining the Democrats as a means to securing control of Congress and the presidency, could be served by first demonizing and then completely crushing the Clinton plan (Skocpol 1995). Skocpol (1995) has found that from 1993 to 1994, hundreds of special interest groups spent more than $100 million collectively to sway the outcome of this particular public policy issue, leading to the Center for Public Integrity calling it "the most heavily lobbied legislative initiative in recent US history."
The politicization of this health care initiative also went as far as to prompt Mr. William Kristol of the Project for the Republican Future to distribute a continual stream of private strategy memos urging an all-out partisan warfare in December 1993 (Skocpol 1995). He remarked that "an aggressive and uncompromising counterstrategy" by the Republicans should ultimately kill, rather than amend the plan. Abolishing the plan without offering amendments was a key priority because of the potential of a Democrat-led universal health care plan to secure crucial middle-class votes and revive the reputation of the party, putting a Republican future in jeopardy (Public Broadcasting Service 1996). The timing of the memo coincided with a mounting private consensus among Republicans that a complete resistance to the Clinton plan was in their best political interest, and they did so by labeling it as a "quintessential example of Big Government Democratic liberalism run wild." (Public Broadcasting Service).
President Clinton's had made the assertion of a "health care system that is badly broken" and revealed his proposal in a widely acclaimed speech to Congress on September 1993. Republicans were thus keenly aware that they had to convince middle-class Americans that was both untrue and inaccurate (Skocpol 1995). Therefore, under the cover of Rush Limbaugh and other conservative right-wing anchors of hundreds of news and talk radio programs that had access to tens of millions of listeners, the Republicans through skillful political strategy and a genuine belief in their ideology promoted their cause by portraying the plan as a bureaucratic triumph by welfare-state liberals (Skocpol 1995).
As increasing partisanship ensued, moderate Republicans who had originally favored reaching a compromise began to recant in the face of anti-reform demands within their own party (Skocpol 1995). Interest groups whose leaders were initially not averse to further negotiations over reforms were soon arm-twisted by their constituents and Republican leaders to draw back from cooperating with the Clinton administration and congressional Democrats (Skocpol 1995). The distinction of party lines was also further exacerbated by the media as it increasingly focused its coverage from the content of the competing health plans under review to the partisan clashes and strategy of the different Congressional groups that were vying for control of the reforms (Bok 1998). Therefore, the inevitable end result was an unbridgeable partisan based schism that played a significant role in the defeat of the Clinton proposal. Amidst the debate, there was no middle ground political compromise that addressed the inefficiencies of the health care system reached (Bok 1998). The creation of SCHIP later in 1997 was thus a milestone event in American social history. Not since Medicare and Medicaid were established in 1965 had Congress members worked together to pass a bipartisan bill that endorsed such a large subsidized health insurance program, reducing the number of low-income uninsured children (United States Department of Health and Human Services).
However, partisan politics has not always led to the complete destruction of beneficial initiatives for Americans. In fact, it can be argued that it is precisely such partisanship that promotes healthy democratic debates over issues. With myriad opinions from both sides being contributed through suggested amendments even on the basis of party lines, the final bill can be further refined and improved before it goes to the floor, increasing the probability of it being passed into law. This can be seen in the example of Medicare where in 1995, for the first time in 30 years, a highly public, partisan and ideologically divisive debate occurred (Oberlander 2003). The 1995 Republican Medicare reform bill championed achieving a balanced budget through large cuts in program spending by introducing a political cap on Medicare expenditures (Oberlander 2003). Democrats were opposed to such a change because they believed that this would have led to a shortfall in program finances and crippled the program, thus leading then President Bill Clinton to veto the bill, while congressional Democrats joined in solidarity to criticize the Republican Medicare proposal as abandoning its social contract with the people (Oberlander 2003). However, the 1995 defeat of the bill created the gateway for further political compromises, and finally in 1997, Medicare reforms were passed in a bipartisan manner with the hard cap on Medicare spending that would have activated automatic spending cuts in the program being removed (Oberlander 2003). This was a key concession by the Republicans as Democrats had identified this particular cap as threatening the ability of seniors to access quality medical care and were thus strongly opposed to it (Oberlander 2003). Spending cuts were also less harsh this time around. In hindsight, these political compromises have been recognized as essential in preventing traditional Medicare from being devastated through cuts and restructuring, allowing it to continue serving its target population (Oberlander 2003). Thus in this scenario, we are able to observe how partisan politics might have served the American people.
Various assessments of America's current health care system have led to it being called "financially inefficient, inaccessible, and administratively wasteful" (Matcha 2004). Despite spending more per capita on health care with annual costs exceeding $2 trillion in 2005-an astounding 16 percent of its GDP-America is not getting a corresponding value for its money. It continues to document higher infant mortality rates, lower life expectancy, and an actual uninsured population of 45 million as of 2005-a phenomenon virtually non existent in the rest of the industrialized world (Klein 2006; National Coalition of Healthcare). Furthermore, the problem is slated to get worse with American health care spending projected to continue increasing at similar levels for the next decade, reaching $4 trillion in 2015-a sizable 20 percent of GDP (National Coalition of Healthcare). Currently, California records the largest number of uninsured-18 percent of its residents-with 80 percent of these people being from working families with average incomes (California Teachers' Association). Without the social safety net of health insurance, their illnesses go unchecked until they become emergency concerns, further pushing up the cost of health care (California Teachers' Association). Employers have also borne the brunt of the health care crisis, seeing their insurance premiums rise 87 percent over the last seven years. General Motors now spends more on its employees' health insurance than on its purchase of steel (National Coalition of Healthcare). Clearly, the American health care insurance system is in dire need of a massive overhaul.
In addition, in a recent survey conducted by The Wall Street Journal and NBC, those who voiced pessimism about the future were asked to identify the source of their concern. Next to the Iraq war, the inadequacies of the health care system drew the most votes (Inglehart 2007). This comes as no surprise considering the fact that one in four Americans says that his or her family has had a problem paying for medical care during the past year, an increase in 7 percent over the past nine years (National Coalition of Healthcare). A new high of 30 percent has also said that someone in his or her family has deferred medical care in the past year, even when the medical condition was considerably serious (National Coalition of Healthcare). Corroborating these statistics, national surveys have also repeatedly shown that the main reason of non-insurance has been the high cost of health insurance coverage. In fact, since 2006, annual premiums for family coverage have significantly overtaken the gross income of a full-time, minimum-wage employee who takes home $10,712, illuminating the severity of the problem (National Coalition of Healthcare 2005).
Although Medicaid was established with the mission of providing health insurance coverage to the nation's poor, disabled and the destitute elderly people, it is currently facing a funding crisis that has seen several critical health programs reduced or abolished, and program eligibility adjusted (American Dental Association). Similarly, Medicare, which is a health insurance program for those 65 years and older, has also faced similar problems. Evidently, in light of the variables of health care affordability and health care insurance which polled voters valued most, the administration has failed miserably in allaying the fears of the American public.
Friedman (2000) believes that in health care, there is always the inevitability of politics. She feels that while most other countries have decided on the basic rules of whether health care should consist of universal coverage, be publicly or privately controlled, and the extent of the government's role, America has yet to decide on any of those things. Therefore, partisan politics fills the vacuum and has the potential to be destructive, as witnessed in the partisan wars over Medicare and managed care (Friedman 2000). In her opinion, excessive partisanship would thus result in the two biggest crucial health care issues -- an aging society and the rapidly increasing number of uninsured-remaining unaddressed until they reach crisis proportions. This does not bode well for the American people. However, others have maintained that it is exactly this politicization that encourages healthy debate and thus need not be perceived negatively.
Therefore, what has being embroiled in a larger struggle over ideologies meant for the general state of health care in America? Several trains of thoughts exist. From the above analyses, we can see that politicization of healthcare has its pros and cons, either a beneficial enlargement of the democratic debate or a possible stalemate in policy implementation due to excessive party bickering. The issue of SCHIP must thus be further analyzed to determine its effects on health care. My hypothesis is that SCHIP has become a politicized issue and that this has resulted in negative effects on low-income children.
The research design of content analysis was used in evaluating my hypothesis. A total of 33 newspaper articles from the weeks starting 5 September 2007 to 18 November 2007 (11 weeks) were used to determine if politicization of SCHIP was reported and if it was perceived to have been detrimental for low-income children. I had to decide to stop my collation of articles just before Congress went on its Thanksgiving recess because the SCHIP legislative process is an ongoing one. I also chose such forms of publication for content analysis because I feel that it best represents what permeates the public sphere and influences the perception of the people the most. Specifically, congressional newspapers and publications (Roll Call, The Hill, Politico, Congress Daily, Congressional Quarterly) were chosen for analysis as not only are its reporters most attuned to the workings of Congress, they also provide up-to-the-minute news of the daily legislative and political maneuvers that take place on Capitol Hill. However, editorials were left out as I felt that they represented more personal points of view and lacked the objectivity that I felt was essential in conducting a fair content analysis. Thus, out of all the relevant SCHIP related articles collated, three were randomly chosen from each of the 11 weeks that analysis was being conducted upon. A standardized coding sheet (Appendix 1) was then applied to each of them to measure the presence of politicization in the SCHIP issue, the perspective from both sides of the aisle, and also the reported effects on low-income children. The articles were coded by paragraphs with the options in each question of the coding sheet designed to be mutually exclusive so that reliable data could be obtained. Inter-coder reliability was also established in the coding of these articles.
An example of what might be a disputed coding scenario would be the classification of the term "erosion in Republican territory" in the fight to expand healthcare coverage for children in an article in The Hill on 7 September 2007 titled, "House Dems See Political Win on SCHIP." Ambiguity might result from the decision whether to classify it under politicization in the generic party context (Question 1) or in the context of the political standing of an individual Congressman (Question 2). Thus, in such a situation, I have consistently classified this as politicization in the party context. I have done so is because although the term "erosion in Republican territory" implies that individual Congress members will be adversely affected through the losing of their House seats, the usage of "Republican territory" exudes a more general collective party feel and thus should be coded as the Republican party as a whole bearing the brunt of the negative ramifications.
Several interesting findings have surfaced from the content analysis which provides us with an insight into the evidence of politicization of SCHIP and the possible impacts on children from low-income families.
First, slightly more than half of the 33 articles were found to contain terms that suggest politicization such as "political theatre,"
Sixty-four percent (21 out of 33) of the articles implicitly suggest politicization by stating how the SCHIP process has either inadvertently politically advantaged or disadvantaged the political parties or their respective Congress members (Questions 1 and 2). As seen in the above table, out of the 21 articles that coded positive for these two questions, the option that gathered the most votes at 28 paragraphs mentioned specifically "Republicans voting against SCHIP would lead to disadvantageous consequences." Trailing second with eight paragraph mentions was the option stating "Democrats voting for SCHIP would lead to advantageous consequences."Table 2: Democratic and Republican Perspective of the Optimum SCHIP Bill
The table above also illustrates the different perspectives of what each side has defined as optimum. Republicans were opposed to the SCHIP bill for myriad reasons. Some said the SCHIP expansion would cover illegal immigrants, adults and families earning too high of a percentage above the FPL and was a step towards socialized medicine. Others were opposed to the increase in the federal tobacco tax, claimed that it was too expensive of an expansion, and alleged that it would adversely affect the private insurance market. Thus, although Republicans expressed a general support for the SCHIP bill, they felt that this particular bill was flawed and needed to be renegotiated in a bipartisan manner since it would not provide the best health care for children from low-income families -- the intended beneficiaries of the program. Content analysis showed the top three reasons, ranked in order, against the SCHIP reauthorization bill as being the disputed areas of coverage for children in families earning too high of a percentage over the FPL, coverage for illegal immigrants and lastly coverage of adults. Thirty-six percent of all coded congressional publications mentioned the first two factors, with one in three of the articles that brought up illegal immigrants demanding specifically for tighter provisions verifying citizenship status. Eighteen percent also mentioned providing health insurance coverage for adults as a shortcoming in the SCHIP bill. These were frequently brought up by Congress members on the House floor during scheduled vote debates.
The Democrats, on the other hand, were mostly supportive of the passage of the SCHIP bill. They repeatedly brought up the fact that the SCHIP bill would cover 10 million children, 3.4 million more than the year before, as their main push for the program. Eighteen percent of all coded articles mentioned this factor as the Democratic rationale for supporting SCHIP. The fact that SCHIP would place no strain on the fiscal budget, since it was going to be paid for by an increase in the tobacco tax, was also another factor mentioned by the Democrats in an attempt to garner support for the program.
Lastly, in the five articles that mentioned the specific effects of the SCHIP debate on children from low-income families, two of these articles reported Republican members suggested an eventual negative effect on uninsured children. They felt that if the bill was passed in its current form without the inclusion of Republican demands such as better citizenship documentation and the immediate exclusion of health insurance coverage for adults, low-income children will not be able to reap the full benefits of SCHIP. Another Republican felt that the general politicization of the issue was detrimental to children while two articles reported on how Democratic members felt that the passage of the bill would be beneficial for low-income children. It is also interesting to note that three out of five of these articles that mentioned these effects were published during the period of 4 to 18 October, which was the time period in which the two-week delay of the override vote was scheduled and where politicization was found to be most intense. This correlation may suggest a link between politicization and its effects, whether positive or negative, on the well being of children from low-income families.
The term "politicization" has often been a loaded term with a largely negative connotation, with most assuming that its end result is usually a harmful one. Although my hypothesis assumes such, this paper thus questions if that assumption holds true or if politicization can ever lead to a betterment and improvement of society. Before we further this discussion, it is imperative to be aware that James Madison, Alexander Hamilton and John Jay-the founders of this country who crafted The Federalist Papers-had already envisioned the inevitability of conflicting factions in government. They believed that it was human nature to pursue short-term self-interest often at the cost of long-term benefits and were concerned that factions formed around these areas of immediate self-gratification might eventually demolish the moral foundations of civil government. However, Madison, often referred to as the architect of the Constitution, dismissed the quixotic notion of entirely eliminating factions since it would either destroy liberty or entail everyone having "the same opinions, the same passions, and the same interests." (Anderson 2005) He thus championed an extended republic -- a larger and more diverse society-with "each representative...chosen by a greater number of citizens," believing that while a small republic might be torn apart by factions, the larger number of representatives chosen would "guard against the cabals of a few." (Anderson 2005) He also believed that an extended republic would reduce the likelihood of one faction advancing its agenda to the omission of others. The usage of popular vote would also make it difficult for undeserving candidates to further their personal agendas at the expense of society at large. In addition, the "pluralist" reading of Madison's theory also suggests that the government would be a platform in which myriad interests of the society could be acknowledged, with public policy birthed through conflicts and compromises-the push and pull that often defines legislative politics. It is thus this ideology that frames the current American political system and process (Anderson 2005). Therefore, could politicization-defined as partisan conflict-actually be what the founding fathers of America envisioned for it and a much maligned term?
From the research findings obtained, it can thus be inferred that the politicization of the SCHIP issue did indeed take place. More than one in two of all the coded articles had terms explicitly suggesting that the issue had taken on an overtly political agenda. These were comments either made by the journalists themselves, by Congress members, their spokesperson, or lobbying groups. Rosenbaum (2007) has asserted that this effort at political framing has greatly distorted the matter at hand, leaving the public with a convoluted mass of contradictory information as both sides attempt to present their stance as being most favorable to society.
As a widely received, highly popular program, SCHIP reauthorization should have been a rapid and relatively uncluttered political process. However, the ideological vitriol that has invaded the discussion as seen in the data in the content analysis is also reflected in the politics of real life, corroborating the statistical claims of politicization (Rosenbaum 2007). An example of politicization can be defined as the act of keeping the debate alive in order to score political points rather than trying to resolve it in the fastest manner possible. This is exemplified in the Democrats' exercise of a rare procedural tool to postpone for two weeks the override vote even though an immediate override vote could be scheduled. In this period, the Democratic Congressional Campaign Committee ran 60-second radio spots and "robocalls" targeting Republican House members in vulnerable districts. This inevitably drew sharp criticism from Republicans who accused them of needlessly riding out the political debate. Don Steward, a spokesman for Senate Minority Leader Mitch McConnell (R-KY) put it most aptly when he commented, "The idea that this is about health care is gone...It's about 30-second ads in congressional districts." (Kady 2007) However, Republicans have similarly used the same tactic and have ran radio advertisements against vulnerable Democrats such as Ohio Rep. Zach Space for voting for the House bill (Kady 2007).
The focus on presenting compelling political images or rhetoric, which often skims the surface of the policy by overlooking the substantive issues, is also another form of politicization observed in the SCHIP debate. One Democratic House member has termed the White House as part of the "axis of evil" in attempt to tarnish Bush's public image while Senate Majority Leader Harry Reid (D-Nev) has stood in front of pallets of fake hundred dollar bills to symbolize the millions wasted in Iraq while America's children go without health care (Kady 2007). During a House debate on whether to override Bush's veto, Iowa Rep. Steve King also declared that SCHIP stands for "Socialized Clinton-style Hillarycare for Illegals and their Parents." (Sustar 2007) In their quest for a gripping political storyline, Democrats also recruited 12-year-old Graeme Frost to tout the reauthorization during the two-week postponement. Frost, from the city of Baltimore, Maryland, received SCHIP benefits in 2004 after a massive car accident (Kady 2007). He gave one of the Democratic radio addresses in September, offering a passionate argument to expand the program to more children. However, conservative blogs investigated the Frost family. The family home was watched and the GMC Suburban parked in their driveway was reported upon. In addition, the Majority Accountability Project, financed by a previous top aide at the National Republican Congressional Committee, dug up property records and a New York Times wedding announcement to raise questions regarding the family's net worth (Kady 2007). Democrats, however, responded to these attacks by releasing a point-by-point refutation of the criticisms of the Frost family, showing that the children were on scholarship to private school and the family of six made only $45,000 a year (Kady 2007). Although Republican congressional offices maintain that they were far removed from the investigative reporting work of the bloggers, they failed to distance themselves from the result of that work which accused a valid SCHIP recipient of being ineligible to qualify for a government program (Kady 2007). Thus, from the Republican response it is also evident that the desire to outwit the Democrats has shifted the focus from what should be the key issue of providing healthcare to uninsured children to a partisan based strategizing and manipulation.
Lastly, accusations of politicization, defined by taking an uncompromising and unreasonable partisan stance, made by both Democrats and Republicans, have also been asserted. Democrats claim that the Republicans have voted blindly in lockstep with the Republican President without considering the welfare of the American public (Kady 2007). Republicans have also similarly maintained that Democrats have voted solely along party lines, refused to come to a bipartisan compromise in an attempt to drag out the debate so as to score political points for the 2008 elections, and have thus failed to identify and correct the shortcomings in the bill. This has thus led to SCHIP being unable to obtain a veto-proof passage through the House.
This issue is precisely such a contentious one because ideological differences exist over what is to be defined as optimum for children's healthcare. The top three reasons will thus be more extensively discussed. Republicans say that the bill allows illegal immigrants to benefit from SCHIP because it considerably weakens the requirement of proof of citizenship or nationality. In addition, the inclusion of a state option which permits "express enrollment" for SCIHP benefits without proper documentation of citizenship has been denounced by Republicans who feel that this would be siphoning away resources from the intended beneficiaries, children from low-income families (House Republican Leader). However, Democrats maintain that any change to the citizenship requirement might make it too difficult for inaccessible yet eligible populations to get benefits. They say that the SCHIP reauthorization bill is sufficient as it prohibits payments to illegal immigrants and allows coverage only to citizens and legal immigrants who have been in the U.S. for at least five years. Furthermore, Democrats also oppose the inflexible citizenship requirements which would require even newborns born in the U.S., who are automatically U.S. citizens, to have their citizenship proven before being given SCHIP benefits (Johnson 2007).
Republicans also say that they oppose the fact that taxpayer dollars are being used to fund SCHIP for adults through 2012, when waivers that allow for alternative uses of SCHIP funds end. Instead, they want parents receiving SCHIP to be phased into Medicaid at an accelerated rate. Through waivers, 11 states currently use SCHIP funds to cover parents. Four states cover childless adults, and 11 states use SCHIP funds to cover pregnant women through the prerogative to define a fetus as an unborn child. Democrats are in support of this clause because they believe that allowing states to have the choice of covering low-income pregnant women and providing essential prenatal care would eventually lead to healthier babies, which will reduce the long-term cost of SCHIP (Office of Majority Leader). They also state that the states that have obtained waivers to cover adults have been allowed to do so because they had already done a good job of finding the children that needed coverage. In addition, Democrats are unwilling to attempt a one year transition of parents to Medicaid as it would mean that beneficiaries would have their insurance yanked away from them (Johnson 2007). However, the current compromise bill prevents states from covering any more pregnant adults and requires them to phase out adults who are covered.
The claim that children from middle-income families, earning too high of a percentage above the FPL, are being covered is also a disputed issue. Republicans such as Rep. Thomas Reynold (R-N.Y.) has said that the party had to "stand on conservative principles" and vote against an increased coverage that would allow middle-income families to latch onto a government entitlement meant for the poor (Kady 2007). They favor a hard-cap on income eligibility at or near the 300 percent level (Johnson 2007). Democrats, however, maintain that less than 10 percent of children currently covered by SCHIP live in a family of four earning more than $41,000 annually, and that this will be maintained under the new SCHIP bill (Office of Majority Leader). They also reasoned that different regions in the U.S. have significantly different standards and cost of living and thus there needs to be flexibility in determining the income-cap. A report by the Economic Policy Institute has showed that a family trying to make ends meet in New York in 2004 would need $58,656-three times the federal poverty amount in that year (Vermont Foodbank). However, in latest negotiations, Democrats have agreed to put the income-cap at 300 percent of the FPL.
Republicans are also against a tobacco tax as they say that it would disproportionately burden low-income Americans, be both a regressive and declining source of revenue, and have the negative condition of requiring 22.4 million new smokers by 2017 in order to fund the expansion. However, Democrats cite the Campaign for Tobacco-Free Kids which has found that a 61-cent increase in the tobacco tax would mean that 1,873,000 fewer children will take up smoking (Office of Majority Leader). They are also uncompromising with the psychologically important benchmark of 10 million children being covered under the reauthorized SCHIP.
Findings from the content analysis have shown that three out of five of the articles that mentioned specific effects on children from low-income families have either purported negative effects from the politicization of the issue or advantageous benefits on America's uninsured children if the bill is passed. However, it has to be noted that two articles presented Republican views that it was a positive move to hold off on passing a bill, which they believed was insufficient and inadequate in addressing the health care insurance problem. Thus, with such inconclusive statistical evidence, the crux of the issue, which was first brought up in the introduction, is whether these disagreements have paradoxically brought about more thoroughly analyzed policies and led to pragmatic and optimum compromises or has this political theatre and exaggerated rhetoric only served to impede the improvement of healthcare?
The second revised bill (H.R. 3963), which underwent cosmetic changes, is different from the initial one (H.R. 976) in various ways. First, it permits states to receive federal funding only for children with family incomes up to 300 percent of the FPL. Thus, New York is no longer allowed to extend its program to 400 percent FPL. However, the bill does retain the provision that allows New Jersey, which provides coverage to families up to 350 percent FPL, to continue its program (Senate Republican Policy Committee). It also phases out coverage of adults after a year instead of two and allows states to receive performance bonuses for recruiting the lowest-income uninsured children. In addition, it also clarifies the role of the Social Security Administration (SSA) in verifying citizenship for eligibility purposes. Instead of cross-checking the name and SSN provided for invalidity, applications will now be verified to determine if they are inconsistent with the records maintained by the Commissioner of SSA. This would thus allow the Commissioner to use supplementary information, such as birth place records, to determine the citizenship of the applicant (Senate Republican Policy Committee). However, both bills would provide 10 million uninsured children with healthcare insurance.
Various studies have found that the passage of the bill will be highly beneficial to children from low-income families. The Congressional Budget Office (CBO) has found that the bill will allow 1.3 million children, who would otherwise lose insurance coverage, to retain their SCHIP coverage because adequate funding over baseline levels is provided for states to maintain their current programs. An estimated 78 percent of the children who would have been uninsured in the absence of the SCHIP bill will also have incomes below 200 percent of the FPL (Kenney 2007). Furthermore, an estimated 70 percent of all children who would gain or retain SCHIP coverage, including those who move from private to public coverage, were found to have incomes below 200 percent of the FPL. A substantial number of children targeted under the bill will also have incomes below 100 percent of the FPL while very few will have incomes above 300 percent of the FPL because so few states currently have or are projected to have eligibility thresholds above 300 percent of the FPL (Kenney 2007).
Thus, the politicization of the SCHIP issue might actually have served to benefit their intended beneficiaries. President Bush had initially proposed a $5 billion increase for SCHIP for the next five years, an amount the CBO and the Center on Budget and Policy Priorities both found insufficient to maintain even current enrollment (Kenney 2007). However, because the issue has taken on such a partisan turn with Democratic ideology being fiercely pitted against Republicans', it might have led to the Democrats developing their uncompromising stance of seeing 10 million children obtain health insurance. If the bill eventually passes, their stubborn refusal to budge on this key aspect would thus have advantaged these children. Politicization, which also led to the original bill becoming revised, might have also resulted in positive effects on low-income children. With tighter regulation through the use of refined terminology, it has become more likely that illegal immigrants will not be covered under SCHIP and draw away limited funds. Nonetheless, politicization definitely also has its flip side. It is this same politicization that has caused progress on the SCHIP issue to come to a standstill as both sides endeavor to forward their ideology in an attempt to appeal to voters. Thus, until a true bipartisan compromise is reached, health insurance coverage expansion for eligible children is stalled and millions of low-income children are being negatively affected by remaining uninsured.
Legal immigrants have also borne the brunt of this politicization. Democrats are often viewed as being champions of social equality and more pro-immigration while Republicans are perceived as more nationalistic and anti-immigration. Thus, the fractious politics of immigration has also permeated the SCHIP issue. Immigrants, whether legal or illegal, have now been painted with a broad brush. The SCHIP bill specifies that legal immigrants in the country for less than five years will not be covered under Medicaid and SCHIP even if they meet income eligibility requirements (The Kaiser Commission). Thus, while a number of states with large immigrant populations have provided state-funded coverage, the lack of federal funding makes this coverage vulnerable to cuts during economic recessions. This is an especially salient issue considering that the number of immigrants living in the U.S. has continued to increase. Therefore, children from low-income legal immigrant families have been most adversely affected by the politicization of SCHIP (The Kaiser Commission).
Essentially, if SCHIP is not renewed, 6.6 million children stand to lose their health care insurance coverage instantly. From the perspective of a government's moral obligation to its people, Rosenbaum (2007) states that to reverse the government's role in creating a dependable social safety net for children and to leave financially limited families to fend for their children in the individual market would be a regrettable step in the course of America's social progress. This could lead to enrollment denials for newborns and exclusions for children with physical, mental, and developmental conditions. Thus, in her opinion, exposing children at any income level to the full force of the individual market is an unfair path for any nation to ask its families to take.
In terms of financial implications, Georgetown University's Health Policy Institute has found that the SCHIP bill, if passed, would have provided states with more than $8.9 billion in the 2008 fiscal year, compared to the previous $5 billion, as the federal contribution to defray the program's cost. This, together with the nearly $3.8 billion in unspent SCHIP funding that states can carry over, would have given states over $12.7 billion in federal money this fiscal year for the SCHIP program (Hess 2007). This is in stark contrast to the $1 billion per year increase over five years that the administration originally wanted. The CBO also told lawmakers that President Bush's offer would lead to a loss of 1.4 million eligible children from the program (Hess 2007). In fact, some of the affected parties worry that the deadlock will eventually lead to a simple continuing resolution for a year till just before the 2008 Presidential elections. Inevitably, this would bring about a setback in the number of covered enrollees as compared to what the bill originally proposes (Hess 2007). Joy Wilson, a healthcare specialist with the National Conference of State Legislatures, has said that the biggest and most immediate concern is the uncertainty that states are confronted with as the fate of SCHIP hangs in the balance while Congress battles it out (Hess 2007). If the funds that the continuing resolution allots are much lesser than the SCHIP bill, some states might be forced to drop covered children from their SCHIP roll.
The current continuing resolution in place, which extends the SCHIP program until mid-December, funds these programs at last year's levels. That is at least $1.6 billion short of what is required. A recent report from the Congressional Research Service has also indicated that more than $6.6 billion would be required to ensure no state faces a deficit in fiscal year 2008. Furthermore, if funding remains at current levels, 21 states will also not have the resources to cover their projected SCHIP spending next year, putting the health care of 1.4 million children and pregnant women at risk. In addition, at least nine states could run completely out of funds as soon as March 2008. Therefore, many states have begun taking negative and restrictive measures to prevent that from happening, which has been harmful for low-income children (First Focus). New census data has consistently showed an increase in the numbers and rate of uninsured children, which are often driven by declines in employer sponsored coverage. Thus, in the face of such a trend, SCHIP will not be able to support current program levels or expand to cover the additional six million children that are said to be eligible if enough money were available (The Kaiser Commission). On 12 December 2007, Bush again privately vetoed the second bill with an accompanying statement that said, "This bill does not put poor children first, and it moves our country's health-care system in the wrong direction." (Kady 2007) In response, congressional leaders have said that they would try to extend SCHIP well into 2008 in its current form. The House also voted 211-180 to put off until 23 January 2008 a vote on overriding Bush's veto. Republicans have again cried foul and accused the Democrats of scheduling the override vote strategically to coincide with the week that Bush comes to Congress for the State of the Union address (Kady 2007).
Therefore, much uncertainty plagues this scenario. It depends on how Congress acts over the next few weeks, especially when the continuing resolution expires on 14 December 2007, that will determine if low-income children will see a beneficial expansion of the program or a continuing resolution that will detrimentally limit enrollment. Thus, even though the jury is still out on this issue, the impact so far has been a negative one.
Through the course of my research, several key findings have emerged. First, the reauthorization of SCHIP has been shown through content analysis to have been a politicized issue. However, the term "politicization," although often viewed negatively, has been found to be beneficial for low-income children in certain aspects of the SCHIP reauthorization legislative process. For example, it has allowed for stricter citizenship documentation procedures to be implemented, ensuring that illegal immigrants do not get access to limited SCHIP funds. In addition, politicization has also allowed the Democrats to hold fast to their ideologically significant 10 million children targeted for health insurance. Thus, not only was this idea of "politicization" -- for factions to oppose each other so as to prevent a monopoly of ideas-the intention of America's founders for the democratic political process, it has indeed at times led to a more optimum outcome as seen in SCHIP. Therefore, even though politicization might seem counterproductive, we are able to appreciate the fact that allowing the political system to be inefficient does allow for the checks and balances that are imperative in every well functioning government to exist. Yet, at the same time, politicization has also been shown to have impeded the political process and been detrimental to SCHIP's intended beneficiaries -- children from low-income families. The delay in the passage of the bill has meant that while politicians wrangle over certain minor technicalities, millions of low-income children are going uninsured and lacking vital health care coverage. Therefore, through this realization of the disadvantageous effects that politicization has on SCHIP, we can become more analytical about various policy issues and start pressuring our representatives through our votes and various other feedback mechanisms to come up with effective bipartisan bills that can be most beneficial for society at large. My hypothesis, which states that the politicization of SCHIP has led to solely negative ramifications for low-income children, is thus not supported.
However, there were certain limitations to my project and I faced various difficulties through the course of my research. First, I had a rather small sample size of 33 articles which I had used to analyze the SCHIP legislative process. Thus, this might have been insufficient to draw concrete results from. It is also hard to generalize the findings from the result of my SCHIP content analysis to every legislative issue taking place in Congress. Politicization might exert different effects on different issues. Also, the fact that the SCHIP bill pertained to the well-being of children, who are often perceived as political untouchables because they represent the most vulnerable in society, needs to be taken into consideration. This fact might have made the SCHIP issue particularly highly charged and politicized, especially when framed in the context of the extremely competitive upcoming 2008 Presidential elections. Thus, all these external factors and influences might have culminated and provided the SCHIP bill with a greater incentive to be turned into a politicized issue than usual. Lastly, in the coding of the congressional publications, I found only five articles specifically mentioning the effects of the SCHIP debate on low-income children. Thus, it made it difficult for me, with a small sample size of five, to draw any reliable statistical conclusions. This might have occurred because I had intentionally excluded editorial and opinion pieces in my sample, on the assumption that these were more subjective pieces that might reflect the biases of the author. Thus, in an attempt to be objective and leave out personal predictions, the articles I coded might have refrained from casting personal opinions about the effects politicization might have on low-income children.
Since SCHIP is currently an ongoing issue and I had to stop my analysis midway, it was difficult for me to draw definitive findings about the effects of this politicization. Thus, future research projects can definitely look towards reexamining this issue once it has blown over. Also, I would be interested to know which issues are most often politicized and if my assumptions that they are issues pertaining to political untouchables such as social security, veterans and children are correct. A further area of research could also be to determine if the use of politicization did indeed result in any significant election gains or losses for any of the political parties.

The phrase "Theory of Mind" (ToM) is used in different ways to refer to distinct areas of investigation. There are the general theories that describe how people think (functionalist theories such as the Computational Theory of Mind, and brain-mind identity theories are of this kind). This paper, however, will use the term ToM
The Prisoner's Dilemma (PD) is both one of the most compelling and relevant discoveries of Game Theory4. The PD has been used to model conflicts ranging from nuclear arms races to oligopolistic competition, and a game-theoretic analysis into the PD has even been used to justify preemptive war. The PD that is relevant to our current investigation in ToM, however, is on a smaller, more individualistic scale5. Past experiments that have put two people in a PD (represented by a matrix game) has produced results that differs greatly from what traditional game theory would deem "rational". A significant amount of work into the PD has involved creating new definitions and measures of ratonality, and introducing different interpretations of the conflict. These new constructs have been developed to reconcile the central "dilemma": despite the fact that mutual defection is the most "rational" outcome by traditional measures6, mutual cooperation is preferred from both individuals, and in fact, occurs quite frequently empirically. When Nigel Howard introduced the theory behind Metagames
Howard's metagame approach is general in theory, but we will only consider it here as it is relevant to the PD. Instead of a standard PD, the metagame of the PD involves a Player 1 (P1) having the standard choice between Cooperating (C) or Defecting (D). And a Player 2 (P2), choosing a "meta-strategy", which are strategies that are contingent on P1's choice. Since P1 pickes one of two strategies (C or D) and P2 can respond to each of P1's strategy one of two ways (C or D), P2 has a total of 4 meta-strategies as follows:
Player 2's four strategies may be nicknamed I. defect regardless, II. do the opposite, III. do the same, and IV. cooperate regardless. The metagame PD thus yields the following set of outcomes:
Which in turn yields the following payoffs8:
An analysis of this metagame shows that the only Nash Equilibrium is when P1 chooses D (bottom row), and P2 chooses I (1st column); this results in the mutual defection outcome with the corresponding payoff of (2,2). So this metagame actually does not produce a new equilibrium, and the equilibrium between P1's Defect, and P2's "defect regardless" is maintained.
The metagame approach starts producing unique results once we consider the "meta-meta game". In the meta-meta game, P1 chooses from one of the four meta-strategies described earlier; P2, however, chooses from one of 16 "meta-meta" strategies. A meta-meta strategy is not contingent on P1's decision of C or D, but on one of the four (I, II, III, IV) metastrategies that P1 chooses from. P2's meta-meta strategies thus are as follows9:
The meta-meta game thus yields the following set of outcomes:
This set of outcomes, in turn yields the following payoffs10:
An analysis of this meta-meta game reveals three Nash Equilibriums. When P1 selects "I" and P2 selects "1", the familiar mutual defection occurs (as it did in the meta-game and in the standard PD). When P1 selects "III" and P2 selects "3", however, there is a Nash Equilibrium of mutual cooperation; this new equilibrium also occurs in the intersection of P1's "III" and P2's "6" strategies, respectively.
So what do these new equilibriums mean? And how is Howard's theory of metagames relevant to a Theory of Mind? Firstly, the fact that there is a new equilibrium at mutual cooperation is significant for several reasons. There has always been a lack of theoretically-convincing arguments that advocate cooperation as a "rational" strategy. Howard's theory of metagames not only makes mutual cooperation a stable outcome, it also accounts for elements of psychological game theory that traditional game theory is unable to consider. The fact that the two players involved in this conflict (the PD) are human beings, means that their theories of mind about each other will certainly play a factor in their decision-making. Although the objection that the meta-metagame PD is fundamentally different from the standard PD is valid, this fact does not imply that the findings of the meta-meta game are not relevant. Most important in introducing new frameworks to interpret the prisoner's dilemma is that the original conflict is preserved. Manipulations of the matrix that represents it or the ordinal preferences(payoffs) threaten the integrity of the conflict; Howard's metagames, however, does neither. Two individuals who have ToM's concerning each other could, presumably, interpret the game in such a way that the meta-metagame framework becomes an appropriate model for their respective ToMs for one another. If for example, P1 was employing 1st-order reasoning, he would see four possible strategies to choose from (strategies I through IV). Since P1 is aware that P2 is rational and wants to maximize his own payoffs as well P1 thus might reason "since we both want to maximize our payoffs, I should choose a strategy that would retain the possibility that of the outcome that maximizes our combined payoffs: mutual cooperation"; thus P1 would eliminate strategies I and II. P1 could then reason "I don't, however, want to leave myself open to the possiblity of being exploited. And since III can always ensure a payoff that is at least as good as what IV can ensure, I will select III11 "
Since P2 believes that P1 is employing 1st-order reasoning; P2 could thus conclude that P1 is deciding between the strategies I, II, III, or IV. The strategies 1-16 are thus appropriate representations of P2's choices. P2's thought process might be as follows "If P1 chooses I (defect regardless), I certainly will not employ a strategy where I will cooperate and be exploited", therefore P2 would eliminates strategies (5, 9, 10, 11, 13, 14, 15, 16). P2 could continue and reason "If P1 chooses II (do opposite), I again won't select a strategy where I would end up cooperating, since I would then receive the worst payoff", and so P2 then eliminates, of the strategies remaining (4, 7, 8, 12). P2 then would reason "If P1 chooses III (do same), then I would want to cooperate, since I am really choosing between either CC or DD, and mutual cooperation is better than mutual defection" and so he eliminates (1, 2). Note that P2's two remaining strategies (3 and 6) are the two (and only two) strategies that contain the mutual cooperation outcome as a Nash Equilibrium. Either one of those two choices would thus yield the mutual cooperation outcome (since P1 will, rationally, play III), but it is worth noting that P2 could further reason "If P1 chooses IV (always C), then I would prefer a strategy that defects so as to exploit him and receive the best payoff"; and thus eliminate strategy 6.12
The lines of reasoning I have presented are certainly not the only lines that can be deemed "rational". As Nigel notes, the concept of rationality "breaks down" under different conditions, and so the same definition of rationality can, employed with different logic, yield different results (in fact, this is the very nature of the paradox in the PD). The reason I have employed that specific logic, however, is to demonstrate how the Nash Equilibriums found in the meta-meta game are not only theoretically valid, but intuitively valid as well. The logical progressions of reasoning by a 1st-order and 2nd-order player not only mirrors the elimination of "unintuitive" strategies in the meta-meta game, it also concludes that mutual cooperation is a rational outcome both intuitively, and when the meta-meta game is considered, theoretically.
Using the fundamentals of the meta and meta-meta game for the 2-person PD, we can now extrapolate, and consider a 3-person meta game. A 3-person PD is defined as having the following ordinal payoffs:
Where the payoffs are ordered according to the player whose decision is the 1st of the 3 letters. For example, in outcome 1, the 1st player receives the best payoff, since both the other players cooperated and he defected. The 2nd player (corresponding to the 2nd letter), meanwhile, receives the 5th payoff (CDC) because from his perspective, he cooperated while one other player cooperated and one other player defected. The 3rd player's payoff is similarly defined.
For a 3-person meta game, let the non-meta player (0th-order) be P1, with m1 strategies (in general, m1 = 2, because that is the standard PD). The meta player (P2) then has 2^(2^( p1p3)) strategies (m2), with each strategy having 2^( p1p3) elements (where p1=p3=2, the number of "distinct resolutions", and an "element" is a single "If-then" statement). A "distinct resolution" is defined as an output (for all players, either C or D). Notice that m2 depends on p1 and p3; m2 cannot possibly depend on m1 and m3 (the number of strategies P3 has) because m3 depends on the number of strategies P2 has (m2)13. The distinction between the input pi (a strategy) and the output mi (a distinct resolution) is important not only on theoretical grounds, but on its implications for ToM.
In this 3-person meta-meta PD, the players 1 and 2 have the following strategies:
Where 1C3C → C is a single "element" that reads "If Player 1 cooperates and Player 3 cooperates, then cooperate" and each element is separated by a comma. Roman Numerals I-XVI denote P2's 16 strategies, and we will simply use "C" and "D" to denote both the strategy and distinct resolution of P1. A single strategy from P3, since it is contingent on both the strategies of P1 and P2, is as follows
Since each strategy from P3 has 32 elements, P3 has 2^(32) strategies, far more than that which can be listed here. We do not, however, need to consider all the strategies from each player in order to analyze this game.
Let us consider first how we might arrive at a Nash Equilibrium to this game without exhaustively mapping it out. If one were to take the point of view of each player, one can then "build" an optimal strategy by considering each possibility (each "If", what the other two players might do) and then deciding whether a D or C would yield a higher payoff. In doing so, it quickly becomes clear that the optimal strategy is one that corresponds to the nickname "cooperate if, in doing so, the other players will also both cooperate. Defect otherwise". This nickname makes evident why Nash Equilibriums of mutual cooperation14 exists in n-person (n-1)-meta games. By making one's strategy choice not independent of the relevant element in another person's strategy choice, meta-games possess a facet of "causality". P3, being rational, will thus select a strategy such that if P1 cooperates and P2 cooperates, he too will cooperate (if defecting meant another player would defect as well). By choosing a strategy that defects in every other situation, P3 is making sure that the other players won't have an incentive to defect; there is thus a Nash Equilibrium at the intersection of the dominant strategies that we "build"
Likewise, other (and all beyond DDD) Nash Equilibriums exist where all three players can "coordinate" a C, and where if any one person moves unilaterally away from that equilibrium to a strategy that yields a "D", the other meta-player has a strategy that would produce a "D" as well.
When trying to use our meta-game analysis to model ToMs, we do encounter a few problems. Firstly, P1 has no basis whatsoever to make his decision. By definition, a myopic player is one that is unable to consider how another player's thoughts and beliefs plays a role in his own payoff. A myopic player can therefore only consider his own desires; in order for P1 to make a choice, he must have some kind of payoff immediately available (but since the payoff is contingent on what P2 and P3 decide, which in turn is contingent on what P1 decides, P1 has no basis for making this decision without developing some kind of ToM about the other two players). Another limitation of this model is that it requires that the three players (1, 2, 3) to be 0th-order, 1st-order, and 2nd-order thinkers, respectively. This is a very specific situation, and so this model is not applicable to all n-person Prisoner's Dilemmas (or even all 3-person PDs). A final, important point to consider is that, in applying this model, we have made the assumption that a particuar player's ToM is indicative of the actual order of reasoning another player is employing. The number of strategies the meta-meta player has in this game is not derived objectively from a measure of the number of strategies the normal and meta player are actually considering. Even if a 2nd-order player is actually employing a ToM modeled by a meta-meta player, then, for our theoretical results to be valid, his subjective belief of the strategies the other players are considering must be the objective reality. Our analysis, interpretation, and application of Meta-games to ToM will be therefore be invalid if the ToMs are not themselves indicative of what other players are actually considering.
Future studies into ToM using Metagame Theory should expand beyond the Prisoner's Dilemma. Since the fundamentals of a metagame are, intuitively, very similar to the recursions of logic employed in higher order reasoning, the applicability of metagame theoretical results to ToMs during other matrix games should be investigated. One of the primary drawbacks of Metagame theory is that it is unable to account for the situation where two or more players are applying the same meta-level. If two players in a prisoner's dilemma are both 2nd-order thinkers, metagame theory currently has not prediction for what kind of reasoning would take place. Until these limitations in metagame theory are addressed, it is unlikely that the theory of Metagames will be used to model more situations where ToMs are relevant.

We've all heard of subliminal messages and their power to make you crave certain foods and have certain preferences. However, whether or not subliminal messaging is actually effective is still controversial. The research is mixed on if and when subliminal messaging works. It is important to understand if this phenomenon is really effective and if so at when and in what specific situations it works. There are many different possible uses for this information. First, and most obvious it can be used to form marketing strategeies by businesses and more importantly then used by consumers to understand what causes them to like or dislike certain products. It is also important for motivational and self esteem techniques. It may be possible to alter peoples self esteem at least temporarily or make them more motivated to achieve a certain goal through the use of subliminal messaging.
One study has looked at subliminal messaging in television commercials and its impact on preferences for certain products. Participants were shown commercials with subliminal messages and with no messages. This study showed a small but significant increase in subjects' intention to use the product with subliminal messaging compared to no messaging. (Smith, 1994) This shows that even though small, there is an effect of subliminal messaging on preferences.
Another study examined the effects of subliminal messaging on reducing anxiety. It tested participants of different levels of self-identity and the ability of subliminal messages to reduce anxiety in these different groups. It found that all groups had reduced anxiety with the presentation of a certain symbiotic subliminal messages that said "Mommy and I are one." (Orbach, 1994) This study shows that it is possible to change not only one's preferences but one's mental state as well with the use of subliminal messaging.
A third study we assessed looked at the ablity to detect subliminal messages. When one does subliminal messaging studies one tries to present the stimuli just short of them noticing it. That is you want it as close to the subjects threshold for recognition as possible. However, if the stimuli is presented too short this may decrease the power or effect of the stimuli. This study examined the possibility that participants threshold actually varies and is not stable. Therefore past experiments may have had reduced effect and not realized a subliminal effect because the stimuli was not presented close enough to the threshold. (Miller, 1991) This may be important for many studies that have had only a small or no effect for subliminal messages such as the study by Smith et. al presented above.
In this study the effects of subliminal messaging on self esteem and color preference were measured. We predict that presenting subliminal messages to enhance a color preference and others to enhance self esteem will increase the preference for the color and increase subjective ratings of self esteem. However, we hypothesize that there will be a larger effect with respect to color preference enhancement than self esteem.
Fourteen subjects participated in this experiment. The participants were all undergraduate students. They were not compensated for their participation but received credit for a class for partaking in the experiment.
A pre-survey was given to participants which contained questions from the Rosenberg Self Esteem Scale as well as irrelevant deversion questions about humor. A post-survey was also given which contained comparable questions. The video used was a five minute and twenty second clip from a Seinfeld episode.
The computer used was a Dell Opiplex GX620. A Sharp Notevision was used to project the video onto a projection screen. The video was edited to add subliminal messages by a student majoring in film.
The experiment was between subjects. Half the participants were in the subliminal condition and half were in the non-subliminal or control condition. The two conditions were exactly the same except for the presentation of the subliminal messages within the video for the subliminal group.
Particpants were first asked to take a piece of paper from a bag. The paper contained the number one or two which assigned them to one of the two groups. All surveys were placed at a seat in the room. Then participants entered the room and were asked to take a seat at one of the seats with a survey. One of the experimenters then went around the room with a bundle of pencils and participants were asked to take one to fill out the survey. Participants were told to turn over the survey when finished. Upon completion, surveys and pencils were collected by an experimenter. A short video clip was then shown. For the subliminal group a subliminal message was inserted into the video which flashed the phrase "I am wonderful" five times at intervals throughout the video. A red background was also used for the computer screen during the playing of the video. For the nonsubliminal group no message was flashed during the video and a white background was used during the playing of the video. At the conclusion of the video clip a second post-survey was passed out to participants. A bundle of pens was also handed out and participants were asked to chose a pen. There were 45 pens total, with fifteen blue, black, and red pens each. After completing the survey, the pens and surveys were collected and the participants could leave.
We predicted that the preference for red would be higher in the subliminal group versus the nonsubliminal group. This hypothesis was not supported. Participants in the subliminal group did choose a higher frequency of red pens than participants in the control group. However, these results were nonsignificant with an ANOVA test showing F(1,12) = .000, p = 1.000.
We also predicted that their would be a larger increase in self-esteem for the subliminal group than the control group. There was a difference in groups with the subliminal group having a larger increase in subjective ratings of self-esteem. (See Table 1) However an ANOVA test showed that this difference was nonsignificant, F(1, 12) = .466, p = .508.
Although are results were in the right direction and did show differences between the two groups, none of them were significant. There are many possiblities for why this was the case.
One of the major problems was sample size. We had a total of only fourteen participants, leaving seven in each condition. This could have caused a type II error. There might have been too small of a sample size to produce a significant effect even though the effect was there.
Another problem was the ceiling effect we had with self esteem ratings. Most all the participants started out a such high self esteem there was not much room for improvement. It may have been a problem with the sample population. The students at the university might have simply a higher self esteem than the general population. To correct this we might have used a different sample population that did not consist of only college students. Another solution may be to lower their self esteem in the beginning and then from there try to raise it with the subliminal message.
There were also possible reasons for the color preference portion not showing significant results. There were many factors in the room, noticed later that could have contributed to their color preference as much as the background of the computer screen. While we all tried to wear colors other than black, red, or blue to influence them the room had many of these colors in it. For example on the computer screen itself the border around the video clip was blue. The participants also had black computer screens right in front of them for the duration of the experiment. To resolve this issue participants should be placed in a plainer room with mostly white so that nothing else in the room may influence their color choice. It may have also been that participants started out with a predisposition to like blue or black pens more than red pens. And this may have had an effect on our results as well.
Another limitation we had in our study was the limited access we had to professional editing equipment. The subliminal stimuli may have been to small or to presented for too short a time to have a significant effect. Future studies may want to use more professional equipment to edit the video so that the stimuli can be presented longer and larger while still not being consciously visible by participants.

