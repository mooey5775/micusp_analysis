Quantum dots are nanometer-sized particles or islands of a semiconductor material embedded in another semiconductor material. They have electronic properties different from that of the bulk due to quantum confinement, and thus hold a promise for nanotechnology applications such as LED's, detectors, data memory devices, lasers, and single electron transistors. However, fabricating a regular, perfectly aligned dot structure is still a challenging issue. Rather than using an accurate positioning device, such as a focused ion beam, it is preferable to use the technique of self-organizing/assembling growth via a strain relaxation mechanism. This is done by depositing the vapor of a material onto a substrate made of a material with a different lattice parameter. It is found experimentally in many material systems that heteroepitaxial growth results in spontaneous self-organization and assembly of islands. [1,2]
The driving force for the quantum dot formation is the reduction of the total energy with a contribution from the elastic strain energy. The elastic strain arises by the lattice misfit between the film and the substrate material. Initially, the combination of the surface/interfacial energies and the strain energy are such that the system favors wetting. Therefore, the film material forms a flat wetting layer over the substrate. As the film material is deposited, the thickness of the film increases and the strain relaxation mechanism will result in island formation, despite an increase in the surface energy. At the early stage of surface roughening, the material is more relaxed at the crest than at the valley. In other words, the area at the crest has a lower chemical potential. Consequently, the film material diffuses from the valley to the crest, which eventually leads to island formation.[3]
In this project, we examine the formation of quantum dots by employing the finite element method formulated by Ref. 4, 5.
Based on method developed by Ref. 4 and Ref. 5 and Ref. 6, we used the model for surface diffusion and evaporation/condensation. The weak statement can be written as
where J is the mass flux, I is the mass displacement, M is the mobility of atom on the surface, j is the volume of matter added per unit area of the solid surface per unit time, i is the volume of matter added per unit area and m is the evaporation-condensation rate, and δ G is the energy change due to matter relocation and exchange on the surface. The first term in the integral associates with surface diffusion and the second term associates with the condensation/evaporation process.
The quantity δ G can be obtained from the free energy consideration. Here we consider a) the surface energy, γ s , b) the elastic energy, w, due to a misfit between the film and the substrate, and c) free energy per unit volume of atoms in a bulk solid phase, g, to model deposition from the vapor phase. The expression
where l is the length of the surface, δ rn is the normal displacement of the surface.
Fig. 1 shows one element, with two nodes at the positions (x1 target="fn1"/>, y1) and (x2, y2). The element has length 1 and slope θ , which relate to the nodal positions by the expression
with the linear interpolation coefficients being
Also for velocity,
However, for flux,
where J1, J2 and Jm, are the fluxes at the two nodes and the mid-point of the element, respectively, we used the quadratic interpolation coefficients,
(7)
Writing the position and mass displacement in a column matrix δ q and the velocity and mass flux in a column matrix q'.
We can write the integral Eq. 1 as
where
where we have used the shorthand notation
Since we need velocities for more than one element, we have to generate global H matrix, which is diagonal and includes matrix for each element. And it also has overlapped part for the same node of two adjacent elements. Here is the part of MATLAB code for global matrix.
The equation (2) can be express in term of force on the element
where
Equating Eq. 10 and Eq. 12 yields
This equation is a set of linear algebraic equation for the generalized velocities. We use Matlab to solve for the velocity matrix. Note that taking an inverse of H is not possible since H is close to being a singular matrix. One way of suppressing the singularity is to add a small mass to the diagonal term of H matrix. In our program we use the command "psudoinverse", which also suppresses the singularity.
Once solved, the nodal velocities can be used to update the nodal positions. Here we use Euler method for time evolution
where xn is the current position and xn+1 is the new position. The accuracy of this method depends on the magnitude of time step. Since Euler method is an explicit method, the magnitude of time step has to be small compared with the magnitude of l. In our simulation we have dt/l =0.1.
Calculating w can be a cumbersome task. Here, we use a finite element package, Abaqus, to automatically calculate w. We use Matlab as a main program to evolve the morphology and only call Abaqus when strain energy calculation is required.
We model the effect of the misfit as a constant force applied at the edge of the film. This is equivalent to a film on a stiff and infinitely large substrate. This assumption is reasonable when the substrate is much thicker than the film.
The workflow is as followed.
a) Matlab generates initial positions of the nodes on the surface and write the data to a position-file.
b) Matlab automatically calls a DOS batch file. In this batch file, Abaqus was called to read the position-file and a pre-programmed input file. It then generates meshes over the entire domain and, with specified boundary force, calculates the strain energy. Note that we add additional two 10 ×10 grids on both sides of the film so that the area of interest is far from the boundary. This avoids error due to edge effect.
c) Abaqus then writes the strain energy associated with the nodes in the surface to the data file.
d) Matlab reads the data file and evolves the node on the surface and write the updated nodal position to the new position-file.
e) Repeat step b)
In our simulation we didn't need to recalculate the strain energy density at every time step. If the position doesn't change too much, the strain energy from the previous step is still valid. We found that calculate strain energy at every 10 time steps is a compromise between speed and accuracy.
The command for Matlab to call batch file is:
system('callabaq.bat')
The command in the batch file to call Abaqus is:
where elasden.inp is the name of the pre-programmed input file.
From Eq. 2, we have four contributions to the reduction of the free energy, which are surface energy, evaporation/condensation, elastic strain energy, and bulk free energy. Therefore, we would like to study each effect separately and we can combine them to simulate the quantum dot.
To achieve this, we set the coefficient in front of term of interest in Eq. 13 to be non zero while multiplying other term with zero. However, to suppress the effect of evaporation-condensation, we have to further set
In this simulation, we studied the effect of the surface energy. We set the initial perturbation,
Fig. 3 shows that with only surface energy contribution, the lowest energy state is the flat film. This is expected because the flat film has the lower surface area.
In this simulation, we study the effect of stress by setting the perturbation,
From Fig. 4, the surface moves down almost uniformly. There exist a small amplitude increase but it is small and we are not sure whether this is due to the numerical error.
The face that the surface moves down is reasonable. When including stress, the energy of atoms in the solid becomes higher relative to that in the environment. Therefore, evaporation becomes more significant.
In this simulation, we study the competition between stress and surface tension contributions. We used the same initial surface profile with both misfit and surface tension term non-zero. We set g = 0 .
Fig. 5. Shows that the surface moves downward as well as reduce the amplitude, which come from stress effect and surface tension effect, respectively.
The remaining parameter is the bulk free energy, g. We use the same surface profile while setting the surface tension and stress contributions to zero. We experimented the value of g from negative to positive.
Recalling Eq (2), the free energy variation can be written as:
Because the free energy variation is associated with unit volume of solid grown on the surface, define a driving force:
Then, if
In this simulation, we increased the magnitude of stress contribution and adjust g to prevent phase change.
Considering a dimensionless parameter which characterizing the relative significance of the elastic and surface energy:
If we set s large enough, the stress effect will dominate the surface movement, the amplitude of surface will increase. In this simulation,
If s is small enough, the surface tension dominate the surface movement, eventually, the surface will become flat. In this simulation,
In experiments, quantum dots form by vapor depositing film material on the substrate. Any seemingly flat substrate is rough when looking at the atomic scale. Therefore, we would like to use simulate the quantum dot from a rough substrate. We introduce random perturbation on the surface and use the same parameter as that of the previous simulation.
From Fig. 8, the amplitude of the surface increase, resulting in formation of 6 dots. The dots still look very rough since this is still very early in the evolution. Nevertheless, the simulation shows that the simulation of quantum dots is possible.
We summarized the important contributions in each simulation in the table below.
Table 1. Summary of significant contribution in each simulation
We have employed the finite element method to study the contribution from the surface tension, the strain energy, and the bulk free energy as well as simulated quantum dot formation. With the surface tension dominating, the system will try to reduce the energy by reducing the surface area, which results in a flat surface. The role of the strain energy is to promote an increase in the amplitude of modulation. At the same time, the stress affects an evaporation rate, causing the surface to move downward. In order to simulate the quantum dot formation, the downward movement is counterbalanced by the bulk energy term which promotes the formation the solid phase from the vapor phase. We show that it is possible to simulate the quantum dot behavior although longer simulation time is needed.

The aim of this project is to evaluate the feasibility of providing UM with one hundred percent (100%) renewable energy for its electricity demand. It intends to demonstrate an instantaneous matching of the energy produced from renewable resources with the corresponding electricity demand.
There is a growing realization that renewable energy resources must be afforded a deeper penetration to meet growing global energy demand. The most important driver for this is the scepter of climate change. Increase in atmospheric CO2 and other GHG emissions are providing impetus to the climate change phenomena. However, conventional power plants such as coal and natural gas which contribute significantly to CO2 continue to be built and developed. While the arguments for continuing with the status quo are compelling, it still remains that fossil fuel sources are rapidly diminishing and their supply cannot be taken for granted anymore.
Thus, it is essential that advances be made in the development and implementation of renewable energy as a base load source. Annually, Ann Arbor's electricity consumption approximates to about 1.6 billion kWh1. UM, with its annual consumption of 550 million kWh [1], puts a staggering 450 thousand Tonnes of CO2 emissions in the atmosphere.
Any effort to offset this situation towards renewable will result in considerable reduction in CO2 emissions (fig. 2).
Source: International Energy Agency; Benign Energy, The Environmental Implications of Renewables, 1998
Inspiration for this endeavor came from a similar study conducted by the University of Kassel, Germany [2]. Their study showed that a combination of different renewable sources (wind, solar, biomass) and some pumped storage could consistently provide 100% of the City of Kassel's electrical energy needs.
The major hindrance in exploiting renewable energy resources for base load electricity has been their intermittent nature & uncertainty in resource availability. Distributed power generation, coupled with a robust energy storage system is therefore the key to harnessing renewable energy resources to their full potential. Our effort comprised of an evaluation of renewable resources in Michigan and strategic siting of energy generation systems to be able to meet the electricity demand of UM.
This section discusses in detail, materials and methods used in this study.
This study was initially intended to model allocation of renewable energy resources for the city of Ann Arbor. However, suitable data for the city's electricity demand could not be obtained from DTE, the utility provider for south-east Michigan. The control volume was therefore scaled down to encompass UM's demand. Hourly demand data for UM central and medical campus buildings monitored at the Central Power Plant was used to generate load curve for the entire university.
Distributed generation (DG) is the term used to describe a mode of power generation characterized by a decentralization of the energy systems. Small-scale units, usually in sizes up to 50 MW, are located on the distribution system close to point of consumption. It is a particularly effective way to manage different renewable energy utilities as per their availability to meet a certain demand. The underlying premise of distributed generation in this context is that when conditions are unfavorable for power generation at a particular site, at another site they would be favorable, hence allowing the system to meet given demand.
Preliminary research of the available renewable energy resources in and around Ann Arbor did not reveal an optimistic picture. Washtenaw County falls in Class 2 wind region with marginal power generation potential [3]. Michigan's annual average solar radiation is 3-4 KWh/m2/day. However, it goes down to 0-2 KWh/m2/day for an extended period from December to March [4]. Washtenaw County has the potential to provide 150-200 Tonnes of fuel to a Biomass plant annually, sufficient for only 20MW power generation [5]. Hydro and landfill account for less than 8% of the city's electricity demand, with limited potential of further development2.
This makes it evident that no one resource has the capacity to sustain UM's demand round the year. The way forward to utilize renewable energies is to find the optimum, cost-effective combination of all of these resources to power UM. With such resources being available in abundance not too far from Ann Arbor, it can also prove rewarding to consider renewable options outside the city's limits.
A brief assessment of each of the renewable energies done is provided below. The modeling approach to optimize siting and sizing follows next
A distribution generation network is most effective when power generation units are not far removed from the site of power consumption. For this reason, the NREL wind maps were explored within 100-120 mile radius of Ann Arbor. Central and southern Michigan lie in Class 2 wind regions, with scattered pockets of Class 3 winds. Five sites were geographically dispersed in these high wind density pockets to suppress the inherent variability. The transient wind speed data at four of the five chosen sites -- Carsonville, Deerfield, Twinning and Hudsonville, had been recorded at a height of 30m [6], while the data at Chrysler Proving Grounds in Chelsea was at 80m3. The One-seventh power law4 [7] was used to condition the wind speeds to a height of 50m and 100m.
Three turbines with different power ratings were chosen for analysis -- Bonus 300, Nordex S77/1500kW and Nordex N100/2500kW5. The performance of these turbines was evaluated at each of the selected sites to maximize power output with least capital investment.
Offshore wind potential along the great lakes offers an extremely potent solution to Michigan's power demand. However the limited wind speed data and complexity of transmission and technology involved in such an analysis put it outside the scope of present study.
Building large-scale solar PV farms requires buying big blocks of land -- an additional cost over and above the cost of buying and installing solar PV arrays. The roof-tops of University buildings provide a massive flat area to tap solar insolation. Roof area in Ann Arbor is estimated6 at 85,800,000 ft2. We assumed 25% of this was for UM buildings and further, a maximum of 25% of that would be available for PV installations7. The availability of vacant/waste-lands for solar farms was also considered.
Due to weather patterns, solar insolation intensity at ground level can vary substantially in a matter of just a few miles. Reliable data for solar insolation in Ann Arbor was found only for one site [8]. To increase the fidelity of the system towards small scale weather changes within Ann Arbor, data was taken from 3 more sites close to Ann Arbor-Detroit Airport in East, Jackson in West, and Howell in North. To simplify the case, it was further assumed that the UM buildings in Ann Arbor are distributed evenly in North, South, East and West regions. This approach makes it possible to maintain proximity of power generation sites, while improving system fidelity.
Most solar PV panels available in market today operate in the range of 10-14% efficiency. However, practical-use multi-crystalline silicon solar cells have recently been made available, with efficiency of 18.6% [9] and efficiency as high as 40% have been achieved in laboratories [10]. With a projected time period of transition to 100% renewable energies of 30-50 years, it can be safely assumed that solar panels deployed for electricity generation will have an average efficiency of 18.6%.
Both Wind and Solar power are beyond human control. Therefore, it is imperative that biomass power generation plants with low response time be available.
Appendix A shows the map of Michigan with a circle of 50 mile radius drawn around Ann Arbor. A comparison with the NREL Biomass availability map [11] revealed that Washtenaw, Wayne, Lenawee and Hillsdale Counties have a combined biomass potential of 900,000 Tonnes -- enough to sustain a 100MW Biomass plant. These counties do not cater to any major Biomass plant as of now [12].
Ann Arbor is located in a strategically good location to accrue biomass from all 4 counties. Moreover, a Biomass plant here can be easily coupled with UM's transmission system, thus eliminating transmission cost.
Barton and Superior Dams located on Huron River, running at 60%-80% of their maximum potential, are able to generate only 8.5 million KWh. The city's landfill power generation facility is nearing the end of its service-life, with no immediate plans to replace it3.
Modeling of renewable resources against university demand showed that 37% of the time during the year, all renewable energies put together could not match university demand. In the period from late July to early October, the electricity demand peaks (Fig 4). This coincides with the time when average wind power is at its minimum and solar power generation is falling from peak. Ramping up biomass capacity is not able to compensate for the increase in demand. Therefore, it is extremely important to develop a long term energy storage facility that can store the excess electricity generated during winter months and discharge that during the summer to meet peak demand.
A short term energy storage capacity with extremely low response time is also required to compensate for peak-shaving.
Several energy storage technologies were considered [13]. Compressed Air Energy Storage (CAES) and Pumped Hydro Electricity Storage (PHES) technologies provide the option of energy storage over required period time. However, current CAES facilities utilize combustion of natural gas to boost efficiency of the system, which sets a drawback to the initial goal of going 100% renewable. The option of utilizing Biogas instead of natural gas was considered, but found to have impractical efficiency. The Ludington PHES located on Lake Michigan has the capability of storing 1.9GWh energy [14]. A similar facility can fulfill UM's long-term storage requirement. The technology has a conversion efficiency of 65-80%.
An optimization algorithm (Appendix II) was developed, and MATLAB script was written to systematically analyze various combinations of renewable energies (APPENDIX B). Several strategies were considered in the study to arrive at an optimized allocation of resources. One strategy dealt with large Wind farms with relatively small investment in Solar, while another focused on optimizing long-term storage requirement of the system.
For simplicity of the model only one storage system was considered, which could cater to both long and short term storage requirements.
Site selection for Wind farms and Biomass plant allocation was based on detailed evaluation of renewable resources in vicinity of Ann Arbor (siting shown in Appendix A). The Twinning location was found to be unsuitable for wind-power generation in cost effective manner and was dropped from the model. The MATLAB results showed that total wind power potential dropped 60% during the summer months. Increased solar insolation in summer was able to partially compensate for this drop. The size of total solar installations was iteratively optimized to 200,000m2 -- ~11% of total University roof-tops. The remaining electricity demand was fulfilled through a 55MW biomass plant and large pumped storage.
Several iterations were done to evaluate these resources and the following combination was found to be most cost effective
Our findings reveal that a combination of renewable energy resources strategically distributed in the state of Michigan together with energy storage can provide UM's electricity demand around the clock. However, this comes with a few caveats. A considerable quantity of energy needs to be stored. During the winter months when electricity consumption is relatively low, renewables are able to meet UM's demand and store excess electricity generated. In the summer, when consumption peaks, a significant proportion is met by the energy coming from storage facility and operating the biomass power plant at maximum capacity. Thus the importance of energy conservation and efficiency in reducing peak demand (and therefore, dependence on storage) cannot be overemphasized.
For solar PV, only a fraction of the available roof areas were considered for deployment in the current model. This is primarily due to the low efficiency of PV panels presently available. However, with 40% efficiency being achieved in laboratory experiments, higher emphasis on solar power generation can be expected. Also, preliminary assessment of UM north campus parking lot area totaled to 135,000m2 -- sufficient to provide 35% of North Campus' power. Utilizing these parking lots instead of building roof-tops for solar PV can provide a good cost-benefit.
Four sites were considered for wind-farm development in this study. It was found that Washtenaw and Deerfield have good wind speeds at 80m and 100m heights. Therefore, as per availability of land, it can be beneficial to only setup wind turbines in these 2 locations and reduce transmission costs.
Even though large investments have been made in wind power development, very limited transient data is available for wind speeds across Michigan. As was found in this study, even though Washtenaw falls in class 2 wind region, the wind speeds at Chrysler Proving Grounds qualify for class 3 regime, suitable for power generation. Many such potential micro-sites can exist scattered all over Michigan. It is therefore very important that wind speeds be monitored and recorded to ease integrated planning.
Due to sparse data, offshore winds along the Great Lakes were not considered in this study. However, these offshore winds are in class 5 and 6 regimes and hold a high potential.
Since Biomass power plants can be accessed on demand, they remain a key source for reliable renewable energy. Sustainable supply of biomass feedstock had been a problem for many years and a disadvantage to biomass implementation. However, new sources, such as Switchgrass and Reed Canary Grass are much easier to grow, are found in closer proximity to Ann Arbor and have higher energy content than previous biomass constituents. Biomass in the control volume considered needs a more thorough analysis. Additional Biomass production capability can be generated by dedicating Ann Arbor Green Belt to high-yield feedstock production.
To introduce robustness in the system, it can prove useful to build two small Biomass Power plants -- one within Ann Arbor, and one in the middle of a biomass-rich zone, as against one large biomass plant. This way, even if one of the plants needs to shut-down for maintenance and repair, the other plant can come on-line to meet required demand.
Pumped hydro was found to be most suitable for UM's energy storage requirements. However, a particular potential site for such storage could not be identified. The abandoned salt-mines along Lake Huron can be developed for Compressed-air storage, to be used in tandem with biogas instead of natural gas.
Throughout this study, it has been observed time and again, that the summer peak demand is a major hindrance in eliminating fossil fuel based power; it puts excessive load on the storage system. Efforts to achieve 100% renewable can only materialize through aggressive energy efficiency programs.
Finally, resources outside the bounds of Ann Arbor were explored and tapped to a limited extent in this model. Therefore, an integrated model for the entire state of Michigan, exploiting state-wide renewable resources should prove much more effective.

Walking robots have been a popular science fiction fantasy since the mid 20th century. The control community has been actively working on this problem for several decades. Various methods are used; one of the easiest to implement in practice is a robot with 4-6 legs to solve the balancing problem. Bipedal, human-like walking is much more difficult.
Human walking is fundamentally difficult to emulate because it is a highly dynamic process with many states and little static stability. Many existing bipedal robots get around this problem by altering the gait properties to eliminate the dynamic balancing problem; Honda's Asimo is perhaps the most famous example. This method is termed the "Zero Moment Point" method and is discussed in Section 2.
The goal of this project is to study the bipedal locomotion problem using nonlinear control techniquesin a systematic way. Stable walkinggaits correspond to stableperiodic orbitsinthe state space. Byusing a variety oftechniques, controlis appliedto createthese orbits, specific orbits are identified, and their stability can be proven. These controllers are then simulated on the full-dimensional robot model and shown to create the desired walking gait.
These methods have been extensively developed and applied to complicated robots [2], including hardware testing on a 5-link robot. The work presented here applies the methods of[2] to a simple 3-link walker; All theorems and methods presented here can be found in [2]. The main techniques used are Lagrangian Dynamics, Poincare maps, hybrid systems, zero dynamics, and feedback linearization.
The outline of this report is as follows: The robot continuous dynamics and impact map are derived. A simple walking gait is chosen. A periodic orbit is found and proved stable for two different types of controllers. The zero dynamics are studied. Finally, a more complicated, energy-efficient gait is studied.
The basic idea of the Zero Moment Point Heuristic is to use actuated feet with small, flat-footed steps. In this way, the robot is statically stable at all points throughout a step and the control problem is simplified. The interaction forces between the robot and the ground are lumped into a single force with no moment acting at the "zero moment point". If one wanted to support the robot with a point force, it must act at this point.
By designing the controller to keep this "zero moment point" within the support polygon of the robot, the robot will be statically stable. With one foot on the ground, the support polygon is the outline of the foot in contact with the ground. With both feet on the ground, it is the area between the outlying ground contact points, the front toe and back heel. This method is graphically illustrated in Figure 1.
This method is based on static stability, so it works for slow, near-static gaits. For dynamic gaits, the results break down. This method does not provide a rigorous stability proof for dynamic gaits.
During a typical walking gait, humans have large phases of underactuation as shown in Figure 2. During a typical step, the period during heel strike (start of a footfall) and toe roll (end of a footfall) consists of nearly point contact between the foot and the ground. During these phases the ankle and all other joints have no authority to impart moments between the robot and the ground. The zero moment point method cannot address this problem of point contact, and typical control methods will not work due to the unactuated degree of freedom. Therefore some sense of dynamic stability is required.
To explicitly address this problem, the robot studied here has point feet. This forces the controller the continuously deal with the underactuation. Once methods are developed to deal with underactuated phases, they can be applied to more complicated robots with actuated ankles.
The robot model studied for this project is the three-link walker showninFigure3. The robot has two legs of length r and a torso of length L. Each leg has a lumped mass m at the leg midpoint. There is a lumped hip mass MH and a lumped torso mass MT. The angles used to define the robot geometry are absolute angles measured with respect to the inertial frame.
Through the course of the system analysis and design, there is significant symbolic math, even for this simple robot. While the calculations could be done by hand, most terms are evaluated using the symbolic toolbox in Matlab.
The robot is described by a set of generalized coordinates
Lagrane's equation is
where
and (2) takes the form
where
Similarly, for a torque
To begin, the kinetic and potential energies are calculated by hand based on the system geometry. The only other quantity that must be calculated manually is the vector of generalized forces
First, the equations of motion are derived for the stance phase, when one leg is always in contact with the ground. These dynamics are denoted with the subscript "s". The foot of the stance leg is motionless, so there are three degrees of freedom. The kinetic energy matrix denoted
The potential energy
the matrix
The matrix
Finally, the vector of generalized forces is
where u represents the torque applied to the two legs.
The stance phase dynamics of the robot are now completely described by
For future use in deriving the impact model, the kinetic energy of the robot is also derived for the flight phase when there is no contact with the ground. There are now two additional degrees of freedom representing the horizontal and vertical positions of the robot, making five total degrees of freedom. The two additional states are appended to the end of the state vector q. The top left corner of the flight phase kinetic energy matrix
Now that the continuous dynamics are solved for the stance phase, an impact model must be developed to describe what happens at foot impact. The development is sketched here, the reader is invited to consult [2] for technical proofs. The fundamental assumption used here is that the forces applied to the swing foot when it hits the ground are impulsive. With a relatively hard walking surface and rigid robot, these forces are very fast in comparison to the rest of the dynamics, so this assumption is reasonable. The main result of this assumption is that these instantaneous forces can produce a step change in the velocities of the robot, but not the configuration. Therefore,
The first step is to augment the stance coordinates
By "integrating" (14) over the instantaneous impact [2], the impact event must satisfy
To relate forces in the inertial frame
where
These equations are simulatenously solved as
Solving these equations yields the velocities after the impact,
Normally, the switching surface in the state space corresponds to the swing leg foot impacting the ground. However, in a robot without knees, the swing leg will always scuff the ground when it crosses the other leg.
For this case, ground impact is assumed to be initiated by the controller. The swing leg is assumed to touch the ground only when allowed by the controller. One way to do this is to have the swing leg pivot slightly outward; this puts it outside of the walking plane and it can rotate forward without ground contact. The leg is brought back inline to initiate contact. Another method is to have a small portion of the leg that can "pick up" to avoid ground contact. Both methods have been shown to work in hardware versions of robots without knees.
The control designer must somehow specify the type of motion the robot will walk. Rather than specify time trajectories to follow, this is done by assigning a relationship between the various body configuration variables called a "virtual constraint". The controller then works to enforce these virtual contstraints between various joint angles. When these constraints are correctly enforced, the controlled joint states are defined, and the dynamics that remain are the "zero dynamics".
To execute this method, first choose a generalized coordinate that is monotonically increasing during a step. For this robot, the angle of the stance leg
Now, define the output functions
When the output y is zero, dynamics have only 2 states,
In order to enforce the virtual constraints, a feedback controller is used to drive the output y to zero. Feedback linearization and coordinate transformations are used to simplify the control problem. Several of the available stability theorems require the trajectories to be exactly on the zero dynamics manifold. Therefore a controller is required that zeros the output in finite time (within one step). In practice, a sufficiently fast exponential controller also works, so this type of controller is used as well.
Assign the functions
where
and
The parameters
where
An easier controller implementation is an exponential controller for the input-output linearized system. For this method, the control input is
where K1 and K2 are turning matrices.
To simulate the robot dynamics, two distinct steps are required. Starting at some initial condition, the continuous dynamics evolve according to (12) until the trajectory intersects the switching surface S. At this point, the reset/impact map
A very simple walking gait with a stable orbit generated by assigning the virtual constraints as
and, after applying the reset map
Notice that the virtual constraint
To study how the virtual constraints are implemented, the output function
The walking dynamics of the robot can be modeled as an autonomous system with impulse effects
The following hypotheses are assumed about system
In addition, the following hypotheses are applied to an invariant submanifold of the system
Define the restricted Poincaré map
where
With impulse effects(28) satisfies the hypotheses HSH1-HSH5. Suppose furthermore that
This theorem is used to prove the stability of the orbits described in Section 11 by checking the stability of the Poincaré return map. The main benefit of this theorem is that the stability check is carried out on the zero dynamics manifold rather than the full system dynamics. For this robot, this benefit means checking a one-dimensional system rather than the five-dimensional stability map of the full system. The zero dynamics of this model are the
Next, an exponential controller is used to zero the output function h as described in Section 8.2. Using the exponential controller rather than the finite-time version means that the dynamics are no longer guaranteed to exactly converge to the zero dynamics manifold with each step. As long as the exponential controller is "fast enough", the resulting gait is very similar to that obtained with the finite-time controller. The same figures are shown here as for the finite time controller. The output h in plotted in Figure 11 for both finite-time and exponential controllers. Note that the output h does not reach zero with each step with the exponential controller, as it does with the finite time controller. Videos for this "simple gait" are available at [1].
The hypotheses HS2 and HSH4 are strengthened to make the autonomous system with impulse effects (28) continuously differentiable.
Without explicit mathematical definitions, let TI be the time until impact from any state x. Let S˜ be the points in S that map to an x that eventually intersects the switching surface again. The following Corollary can then be used to prove stability of the system with an exponential controller.
By perturbing the initial conditions around the fixed point, the jacobian of the Poincaré map is numerically evaluated. Note that this is the jacobian on the switching surface and thus has five dimensions rather than the full six. The jacobian matrix is 5 x 5, and its eigenvalues are
All the eigenvalues have magnitude less than one, so the fixed point for the exponential controller is stable. If the gains of the exponential controller are decreased, the eigenvalues become unstable, as does the orbit. This happens when the controller is no longer fast enough to bring the dynamics back to the zero dynamics manifold between steps.
The dynamics of the system can also be studied by restricting the dynamics to the zero dynamics manifold. The virtual constraints are assumed to be enforced exactly, making
This can be reduced to a reset map that determines the velocity on the zero dynamics manifold after impact
For this robot and the gaits considered here, the function
The zero dynamics are simulated by making
By studying the stable orbits of the zero dynamics, one can find orbits of the full system by searching in a lower-dimensional space. Though not repeated here, theorems are available in [2] to prove that stable orbits of the zero dynamics are stable orbits of the full system under certain conditions. Results are available both for finite-time controllers and for "fast enough" exponential controllers.
Table 2: Parameters for an optimized gait
A more complicated gait was also simulated that is more energy efficient. This gait is defined by the virtual constraints
where
The output function h in (20) is now defined by these more complicated functions. The impact/reset map does not change. The full-dimensional system is simulated with a finite-time controller, and the results are shown in Figures 13-15. Note that
Stable Periodic Orbit with exponential controller
Full Dynamics
Biped walking is a challenging problem due to complicated dynamics and large state space for all but the simplest models. A fundamental challenge is the natural underactuation that occurs with walking. A useful solution to this problem is to define walking gaits by virtual constraints rather than trajectory tracking. This yields robust, provably stable orbits of the system that correspond to walking. When these virtual constraints are satisfied, the dynamics of the system can be analyzed on the zero dynamics manifold, allowing the designer to study a lower-dimensional system.
In this project, a 3-link biped walker was studied. The continuous dynamics and impact map were derived. Finite-time and exponential controllers were used to control joint angles, and the stability of the periodic orbit was proven for both cases. Two walking gaits were studied, a simple version and one that is more energy efficient.

Driver modeling is an essential part in studies on closed-loop human/vehicle/road systems. However, it is a challenging task to model human drivers not only because there are no equations or theories that fully describe the complex human cognitive process, but also because drivers adapt themselves to different driving and traffic situations, thereby constantly changing their strategies and characteristics. Although driver behavior is in general complicated, lane keeping is a relatively straightforward driving control task. Even drivers without much training are able to maintain lane position and maneuver when following twisting roadways.
Driver directional control models have been developed based on different philosophies and approaches, and detailed reviews can be found in MacAdam [1] and Ploechl et al. [2]. As suggested by McRuer [3], the driver directional control actions could be separated into three types: compensatory, pursuit and precognitive (Figure 1). Compensatory control is carried out in a closed loop. The driver is assumed to utilize feedback loops based on the position error and the heading angle with respect to road tangent to minimize undesired deviations. Pursuit control operates by using the driver's preview of the upcoming road path and initiating feedforward control actions. Precognitive control usually plays a role only for repetitive well-learned tasks such as pull-in maneuvers in a parking lot, and thus can be ignored in daily highway driving.
Among the existing driver lateral control models, the model developed by the University of Toronto [4] was composed of three components: curvature preview, heading angle feedback, and position feedback. It was assumed that the three components were sampled, possibly at different rates, and combined to generate the desired steering angle. At least five parameters needed to be specified. This model was applied to both lane keeping and lane change maneuvers. The STI (Systems Technology Incorporation) model detailed in [5] consisted of two parts: the motion feedback from vehicle yaw rate and the visual feedback from curvature error. The elaborate curvature error term included the contributions from lateral offset, heading angle error, projected future vehicle position, and aim point position on curved road. An extra "trim" term in integral controller was added to eliminate steady-state error. In combination, they were used to generate a desired steering wheel velocity. In the "structural model" developed by Hess et al. [6], the driver was represented by a low-frequency compensator and a high-frequency compensation block, which included driver time-delay, "proprioceptive" feedback, and a second-order model of the neuromuscular system. The outer loop was closed using visual feedback based on an aim point error. Qualitatively, a reasonable match between model data and simulator test data was achieved. In the optimal preview model developed by MacAdam [7], the driver was assumed to behave like a preview optimal controller with time delay. A path error functional was constructed by previewing the road. The control objective was to minimize the weighted integral of squares of the differences between the previewed path points and the corresponding estimated lateral positions over the preview horizon.
Vehicle lateral control is a broad notion, which includes lane keeping, lane change, obstacle avoidance, stability control in critical situations, and so on. Among these maneuvers, lane keeping on normal highways belongs to low-bandwidth, low-acceleration plain tasks. The two primary sources of stimuli for lane keeping control are: (1) the desired path provided by the roadway markers, and (2) the perceived vehicle states. A skilled driver is expected to use feedforward control to respond directly to the effective future road inputs, as well as feedback control to respond to deviations from the desired states.
In both [4] and [5], separate gains or transfer functions were proposed to associate with lateral deviation, heading angle error, and the road curvature term. Their model parameters were calibrated with a limited driver subject group and reported in mean values and standard deviations. These values were direct results of model fitting and no clear internal relationship among them was revealed. The "structural model" [6] is formulated as a feedback structure, and it does not explicitly take the upcoming road curvature into account. The MacAdam model in [7] is proven versatile enough to fulfill tasks such as double lane change and slalom if tuned properly. However, as will be shown later, for a less complex task like lane keeping, by partitioning the control actions into feedforward/feedback components, a compact system structure can be formulated, and only two control parameters remain to be specified.
The system analysis starts with investigation of the plant dynamics in the next section, followed by the formulation of a control-oriented system structure. The determination of control parameters is detailed in the subsequent section. Then simulation studies are presented to validate the proposed model, and finally conclusions are drawn in the last section.
The dynamics of a passenger vehicle can be described by a detailed nonlinear model [8]. Under simplifying assumptions, the vehicle lateral motion can be decoupled from the longitudinal dynamics and characterized by a classic 2-degree of freedom (DOF) linearized model [9]. This planar "bicycle model" assumes a small road-wheel steering angle, retains only lateral and yaw dynamics, and is parameterized by a constant longitudinal velocity. Grouping the two front wheels and the two rear wheels separately, one obtains the single-track model illustrated in Figure 2. The variables and parameters are explained in the Appendix. Under normal highway driving conditions, it is justifiable to assume that the lateral force varies linearly with the tire slip angle. By applying the force and moment equilibrium conditions to the free body diagram and substituting the kinematic relationship of the tire slip angles, the equations of motion can be expressed in the state space form:
The sign convention conforms to the SAE coordinates system, and the angles are positive in the clockwise direction. This 2-DOF model is known to predict vehicle lateral behavior reasonably well when the lateral acceleration is below 0.3 g [10]. The analysis in subsequent sections is based on the parameters of a full-size sedan summarized in Table I.
Since the relative motion of the vehicle with respect to the road is of interest, additional variables need to be defined. Two coordinate systems have been introduced in Figure 2. The inertial system (X0, Y0) is fixed on the ground, which serves as a reference frame for vehicle motions. The body-fixed coordinate system is denoted by (x, y) with its origin located at the vehicle's center of gravity (CG). The CG lateral displacement YCG and the relative heading angle need to be added into the states. As in [11], the state-space equation can be written as
The front wheel steering angle Tf is the control input and the road curvature (since rd = u00) enters the system as an exogenous disturbance input. For steady-state lane following on a road with a constant curvature , one supposes the vehicle CG tracks the curve successfully. By setting and , the steady-state values of the relative heading angle and the front wheel steering angle can be derived
where Kus is the understeer coefficient. Both
Apart from the lateral position of vehicle CG, the lateral deviation Yp of a preview point (P) down the road is of interest as well (Figure 3), since the driver typically looks forward, extrapolates from current states and projects to an "aim point" a finite distance ahead [12]. Accordingly, given a preview distance Lp, the trigonometric relationship can be derived from the law of cosines.
The preview distance Lp along the vehicle's longitudinal axis can be replaced with the preview time Tp, namely
where
Evidently Yp consists of three terms, which arise from the local lateral deviation at the vehicle CG, relative heading angle, and curve bending, respectively. If the vehicle CG tracks the desired path successfully, Eq. (8) represents the previewed lateral deviation at steady state.
Hence Yp* is also proportional to road curvature, and the proportional constant is a function of vehicle parameters, longitudinal velocity, and preview distance. Consequently the complete state-space description can be expressed as a two-input-two-output system:
The control objective of lane keeping can thus be formulated as follows: to regulate lateral displacement YCG by assessing the previewed lateral deviation Yp and manipulating steering wheel angle input f to counteract road curvature disturbance . How a skilled human driver might process the perceptual cues and translate them into appropriate steering wheel angles, so that an adequate performance and stability margin can be achieved, is the topic of the following study.
A human driver's lane keeping control behavior can be divided into the open-loop pursuit and the closed-loop compensatory parts. The pursuit part previews the upcoming path and generates the primary portion of the steering action, whereas the compensatory part attenuates the remaining errors.
In the subsequent analysis, we assume that for lane keeping control, a driver only makes use of the estimated road curvature within the driver's preview distance, vehicle heading angle, and current lateral displacement from the reference position, which may be corrupted by noises or biased due to human perception limitations. The driver processes this set of information, follows the road curves, and stabilizes the vehicle. We also assume that the driver is a competent, but not necessarily perfect, lane keeping controller. Because in real life, most drivers manage to maintain lateral positions in spite of various disturbances, but few of them make every effort to stay at a single desired lateral position. Actually as long as sufficient stability margin and performance requirements are fulfilled, the resulting controller can be deemed a valid representation of drivers' control action.
Motivated by the pursuit/compensatory dichotomy, the proposed control structure is illustrated in Figure 4. The vehicle/road dynamics module is driven by the control input (u) and the disturbance input (d). The desired output (z) is the lateral displacement of the CG, and the measured output (y) is the lateral position at the preview point, which is corrupted by noise (n) then sensed by the driver as measured output (ym). The driver controller has three major elements: disturbance feedforward Gff, reference generation GR and feedback compensation Gfb. The driver controller also contains a multiplicative curvature estimation uncertainty (.) and an inherent human remnant term (nn) [13]. One additional degree of control freedom, Tp, is hidden in the vehicle/road dynamics module.
In actual driving, the driver looks ahead and perceives upcoming road geometry. The curvature disturbance can be anticipated and a feedforward compensator can be used to alleviate its effect preemptively, so that the driver turns the steering wheel based on his/her internal empirical model of the vehicle yaw dynamics [14]. The mathematical relationship for this curve negotiation behavior at steady state is expressed by Eq. (4). Essentially the driver tends to match the yaw rate of the vehicle with that of the road tangent. Therefore
For the reference generation, Eq. (8) is used to derive the desired lateral deviation of the preview point, therefore
If the road is straight, the reference lateral position stays at zero. However, if there exists a substantial road curvature, the reference position should be biased. The magnitude of the bias is dependent on vehicle parameters, the preview distance, and the curvature itself. Figure 5 shows how Gff and GR vary as a function of longitudinal velocity when other parameters and variables are fixed. For easier interpretation, the unit of the curvature is taken as 1/km, and the steering angle is computed at the steering wheel in degrees.
The feedforward loop partially inverts the vehicle dynamics so as to achieve faster response. Any remaining deviation needs to be taken care of by the compensatory loop. As an initial step, a basic feedback controller is assumed, which is composed of a pure proportional action along with human cognitive limitations:
In brief summary, a skilled driver is supposed to take advantage of knowledge about future disturbances and relies on feedforward and feedback actions to reject the disturbance, essentially reducing the norm of the closed-loop transfer function Tzd from the curvature input to the lateral displacement at CG. Given the proposed lane keeping control structure in Figure 4 and the above analysis, two control design parameters (Kp and Tp) remain to be determined to achieve satisfactory performances compatible with what drivers normally do.
To visualize the potential effects of driver's forward-looking, the Bode plots for the plant dynamics (from steering wheel angle lSW to the previewed lateral position Yp) are shown in Figure 6, with varying preview times. In general, a longer preview results in more substantial phase leads. However, a larger preview time does not necessarily lead to more substantial phase margin. The maximal phase margin occurs when Tp takes an intermediate value. Figure 7 presents the Bode plots for the open loop transfer function (GfbfGyu) with fixed preview time and varying feedback gains. Due to the human time delay, the phase plot has a much different pattern from that in Figure 6. It is evident that a low gain results in small phase margin and low cross-over frequency, whereas an improperly large gain reduces both gain margin and phase margin, thus undermining closed-loop stability. Therefore an appropriate range of feedback gain exists for achieving adequate performance and stability margin.
The selection of the control parameters Tp and Kp is conducted with an optimal search procedure. During the optimal search, it is not aimed to achieve the best feasible performance. Instead, as long as sufficient stability margin and performance requirements are met, the solution will be accepted.
Sufficient phase margin and gain margin are essential in order to avoid excessive steering oscillation and maintain stability. In an automatic steering system design [16], a 50-degree PM was obtained by optimization. In an earlier study [3] by McRuer et al., a PM of approximate 40 degrees was determined for compensatory driver steering control tasks. In the present optimal search procedure, constraints of a 40-degree PM and 3.2 dB GM will be imposed. In order to avoid lane straddling, maximal CG lateral displacement has to be lower than 0.9 m in response to a 0.25g lateral acceleration disturbance. According to the AASHTO Green Book [17], the minimum radius for freeway horizontal alignment generally results in lateral acceleration lower than 0.25g. The 0.9 m constraint is based on the 1.8 m nominal vehicle width and the prevailing 12 ft (3.66 m) highway lane width [18].
The objective of the optimal search is to obtain the minimal preview time and the associated gain that satisfy the above constraints. The assumption regarding the preview time is: the farther the driver has to look ahead, the noisier the measurement becomes and the more stringent the visibility condition is. So the driver settles for short previews as long as system performance is ensured.
The search procedure is implemented as follows. For every vehicle speed and for any given preview time, the optimal feedback gain is sought to minimize the infinity norm of the closed-loop transfer function Tzd, while satisfying all the constraints on PM, GM, and maximal CG lateral displacement. Then among the feasible solution pairs Tp and Kp, the smallest Tp and its associated Kp will be chosen. This process is repeated for a range of reasonable highway driving velocities and the resulting optimal parameter pairs are illustrated in Figure 8. Also shown is the equivalent optimal preview distance (further divided by 10).
An inspection of Figure 8 reveals the following observations. The optimal preview distance increases with the vehicle velocity. The optimal preview time has a slight downward trend, but overall falls into a narrow range (1.4 ~ 1.7 s) over normal highway travel speeds, which is consistent with the values reported in [19]. A large feedback gain is required at low vehicle speeds. Beyond 20 m/s (45 mph) the optimal control gain varies quite gently; its smoothness is expected to lead to consistent driver behavior as velocity changes.
Figure 9 shows the Bode plots for the open loop and closed-loop transfer functions after the optimal control parameters are specified (at u0 = 25 m/s). The open loop frequency response is in agreement with the Cross-over model [20]: around the gain cross-over region, the magnitude response can be approximated with a-20 dB/decade line. The right panels show that under ideal circumstances (accurate knowledge of vehicle parameters, no measurement noise, no curvature estimation uncertainty), the closed-loop transfer function Tzd has an infinitely small DC gain; effectively the road disturbance can be completely rejected. In practice, despite parameter uncertainty and estimation error, adequate performance can still be maintained, which will be explored in the next section.
A simulation of lane keeping despite curvature disturbance was implemented first in Simulink. The linear model of combined vehicle and road dynamics in Eq. (9) was employed and the driver controller was parameterized as discussed in previous sections, with 0.2s time delay and 0.15s neuro-muscular lag. To test the robustness, it was assumed that the curvature perceived by the driver was only 80% of the true value. No driver remnant or measurement noise was included. The simulated road track consisted of a circular arc with two straight segments appended on both ends. The curvature was chosen to induce a 0.25 g lateral acceleration at steady-state curve negotiation.
Figure 10 presents the closed-loop responses and driver inputs at u0 = 25 m/s. The abrupt transitions between straight and circular segments result in step changes in road curvature at simulation moments 2s and 15s. In the subpanels, the dashed lines denote the theoretical values of steering wheel angle, yaw rate, lateral acceleration, heading angle, and preview point displacement respectively, if the vehicle is to track the circular curve perfectly.
Figure 10 demonstrates that despite curvature estimation inaccuracy, the lane-keeping task can be accomplished successfully despite estimation errors. The steady-state values of the solid lines converge to the theoretical levels marked by dashed lines. The transient phase is brief and without much oscillation. In the topmost subpanel, the total steering wheel angle is consistent with the theoretical computation. Although the feedforward part does not perform well due to the estimation inaccuracy imposed by us, the remaining deviation is nulled by the feedback part. Eventually YCG at steady state is nonzero, but almost negligible. During the transience YCG is within the limits of 0.9 m, and no lane exceedance occurs.
Then simulations based on a nonlinear vehicle model were conducted by using the CarSim software from the Mechanical Simulation Corporation [21]. CarSim simulates and analyzes the dynamic behavior of light vehicles on 3-D road surfaces. It is capable of predicting 3D forces and vehicle motions in response to driver inputs such as steering, throttling, and braking. CarSim also generates a great number of output variables for visualization and analysis, and allows an interactive animation of simulated tests.
Figure 11. A serpentine roadway geometry, also used in [6] and [22].
Vehicle parameters were specified in CarSim, especially tire characteristics. Comparison tests were run to verify responses from the 2-DOF model and CarSimr model against field experiments [23]. The driver steering module, implemented in Simulink blocks, was interfaced with CarSim to provide lateral directional control. A serpentine roadway (Figure 11) with an approximate total length of 2000 m was constructed in CarSim (also used in [6] and [22]). Within the CarSim environment, the upcoming road curvature will not be provided directly to the driver module. Instead, it is assumed that the driver retrieves the coordinates of five points on the road centerline within preview distance, performs a circle fitting by least squares [24], and derives the corresponding road curvature for feedforward control. Figure 12 shows time histories of estimated curvature, CG lateral position, steering wheel angle, and lateral acceleration at a cruising speed of 50 km/h (13.9 m/s). The control parameters are determined for this velocity as discussed before and adequate performance is attained. Except at locations with large and fast-changing curvatures, the lateral displacement can be well maintained close to the lane center. No lane excursion occurs. Qualitatively the pattern of the steering wheel angle bears a close resemblance with the simulator results in [6].
Although human characteristics are clearly nonlinear, linear analysis can still provide significant insights into human behavior. Motivated by human perception of upcoming road geometry and vehicle states, an effective albeit simplistic approach is proposed to analyze driver lane keeping control. The driver is assumed to look ahead and make use of future road curvature and lateral deviation of an "aim point" to adjust the steering wheel angle. The control system structure is established on linearized curve negotiation dynamics, and only two control parameters remain to be tuned. The resulting driver controller reflects the characteristics of human operators.
Satisfactory system performance is validated in realistic nonlinear simulation environment. Moreover, the proposed feedforward/feedback control structure can potentially be implemented for automatic lane-tracking if road preview information is made available by machine vision [23] or magnetic markers [25]. However, practical constraints, such as sensor noises, measurement or estimation accuracy, and bandwidth limitations of the steering actuator dynamics, need to be carefully addressed to maintain consistent and robust performance. Naturalistic driver lane keeping models can also be developed from this template if the necessary measurements are available.

Zooplankton are an integral part of freshwater lake ecosystems. As primary consumers, zooplankton can control the abundance and composition of phytoplankton and can affect water quality, trophic state, nutrient cycling, and food web resilience to perturbations (Dini et. al., 1987, Carpenter et. al., 1992, Cottingham et. al., 1997, Taylor & Carter, 1997, Stephen et. al., 1998). Zooplankton are also an important food source for planktivorous fishes and larval fish of many species (Wetzel, 1975), and can affect growth rates and survival and recruitment of planktivores (Cryer et. al., 1986, Bremigan & Stein, 1997, Dettmers et. al., 2003). The relative influence of zooplankton on lower and higher trophic levels varies as a function of community composition and size structure (Brooks & Dodson, 1965, Galbraith, 1975, Dini et. al., 1987, Bremigan & Stein, 1997, Dettmers et. al., 2003). Therefore, understanding the factors that regulate community composition and size structure is of great interest.
Biotic interactions such as predation and competition are important in shaping zooplankton community composition and size structure (Brooks & Dodson, 1965, Gliwicz & Lampert, 1993). These processes do not act independently and Brooks and Dodson's (1965) "size-efficiency hypothesis" describes how zooplankton community composition, and thus size structure, change via different competitive scenarios under varying predation pressure. Fish tend to be size-selective in their feeding and prefer larger-bodied individuals (Brooks & Dodson, 1965), especially large cladocerans such as Daphnia (Brooks, 1968, Vinyard, 1980, Turner & Mittelbach, 1990). This size-selectiveness means that fish predation serves to shape zooplankton community composition and size structure (Mills and Schiavone, 1982, Hobæk et. al., 2002).
The stability of predator-prey dynamics often depends upon the presence of a prey refuge from predation (Sih, 1987). In a broad sense, a refuge is a strategy that decreases the risk of predation. One common strategy is the use of a spatial refuge (Sih, 1987). Seven possible refuges from predation have been identified for lacustrine zooplankton: gradients of light, temperature, and dissolved oxygen (DO), macrophytes or other physical refuges, open water interference refuges, behavioral refuges, and predator inefficiency refuges (Shapiro, 1990). Perhaps the most important physical-chemical refuge for Daphnia is a region with DO concentrations too low for fish survival (Shapiro, 1990). Avoidance of predation has been accepted as the ultimate reason for diel vertical migration (DVM) by many zooplankton species between these oxygen-poor deeper waters and feeding areas (Zaret & Suffern, 1976, Stich & Lampert, 1981, Gliwicz, 1986, Dodson, 1990, DeStasio, 1993). Tessier and Welser (1991) found that the presence of a refuge from predation was important in determining zooplankton species abundance and diversity. Refuge availability also plays a significant role in Daphnia population dynamics (Wright & Shapiro, 1990).
There are many possible ways to define the extent of hypolimnetic refuges available to zooplankton. Tessier & Welser (1991) defined a refuge as the thickness of the water column between the bottom of the epilimnion and the depth at which DO is less than 0.5 ppm (hereafter referred to as critical depth) . The controls on these two parameters will thus affect the presence and size of refuges for zooplankton in lakes. Thermocline depth is a function of lake morphometry (particularly fetch), wind strength, and turbidity (Patalas, 1984, Mazumder & Taylor, 1994, Kalff, 2002). Hypoxia is a natural occurrence in the hypolimnion of stratified lakes (Charlton, 1980) because respiration rates are higher than photosynthesis rates in deep, unmixed waters. Oxygen depletion can be greater in eutrophic lakes due to large amounts of organic matter sinking to the lake bottom and being decomposed, and although eutrophication can be a natural process, it is often accelerated by human activities (Wetzel, 1975). Deforestation increases runoff and results in more minerals being leached from soil, providing ample nutrients for algal blooms (Hargrave, 1991). The dumping of wastewater into aquatic systems and runoff from agricultural fields can also contribute to nutrient enrichment (Kalff, 2002).
It is clear that anthropogenic forces can cause or exacerbate eutrophication that results in hypolimnetic anoxia – one of the parameters determining refuge size. Human perturbations that increase the turbidity of water also can affect thermocline depth (Mazumder & Taylor, 1994), the second control on refuge thickness. Therefore, humans could have a significant impact on the size of the refuge available to crustacean zooplankton. Since refuge size has been shown to influence zooplankton community structure (Tessier & Welser, 1991, Bertolo et. al., 1999), human activities could potentially affect community structure as well. My goal in this study was to determine what factors influence zooplankton community and size structure in lakes in southern Michigan. In particular, I was interested in whether or not residential lakeshore development has an affect on zooplankton community metrics such as taxa richness and mean size.
All lakes used in this study were warmwater and located in southern Michigan. Study lakes were selected by first identifying all southern Michigan lakes that had a surface area between 4.0 and 81.0 ha and a maximum depth of at least 6.1 m. These selection criteria were used to reduce the influence of lake size (Dodson et. al., 2000, Kalff, 2002) and stratification pattern on zooplankton community composition and size structure. To ensure that the study lakes spanned a gradient of residential lakeshore development, lakes on the initial list were plotted on a map of public land ownership using a Geographic Information System (GIS). Lakes were grouped into three categories based on the amount of state- or federally-owned land surrounding their shores: Group 1 lakes were surrounded by two-thirds or more state land and were defined as low impact, Group 2 lakes were surrounded by between one-third and two-thirds state land and were defined as medium impact, and Group 3 lakes were surrounded by less than one-third state land and were defined as high impact. Three lakes from each impact group were selected for sampling. Data from an additional 11 lakes fitting the selection criteria and representing a range of human development were obtained from the Michigan Department of Natural Resources (MDNR). All data were collected using similar methods and were pooled for analysis.
All lakes were sampled between early August and early September from 2003 to 2006. Temperature and dissolved oxygen profiles were measured in the deepest basin of each lake using a YSI 600 QS-650 MDS water quality monitor. Readings were taken every 0.91 m until the thermocline was reached or the lake bottom was approached, in which case readings were taken every 0.30 m. Water clarity was assessed by measuring Secchi depth. In lakes with more than one distinct basin, profiles and Secchi depths were measured for each basin. A plankton net with a mesh size of 153 μm and a diameter of 0.13 m was used to make vertical hauls from each of four quadrants (approximately aligned with the cardinal directions) in each basin (Galbraith & Schneider, 2000). The mouth of the net was lowered to the critical depth. In lakes having no critical depth, the mouth of the net was lowered to approximately 0.91 m above the lake bottom in order to avoid stirring up sediments and sampling benthic organisms. Zooplankton were anesthetized in carbonated water and immediately preserved in a 70% ethanol solution.
Temperature and oxygen profiles were used to estimate a hypolimnetic refuge following the methods of Tessier and Welser (1991). They defined a refuge as the thickness of the water column between the thermocline and the critical depth. This definition was appropriate for this study because southern Michigan lakes are dominated by Centrarchids (particularly bluegill sunfish) that are unable to feed below the thermocline (Werner & Hall, 1977, Tessier & Welser, 1991) and because zooplankton typically are not found in water with a DO concentration of less than 0.5 ppm (Tessier & Welser, 1991). In lakes with more than one basin, thermocline depths and critical depths were averaged across all basins to obtain an average refuge thickness.
Human development was quantified by counting the number of houses located directly on the shoreline of each lake. Houses on artificial channels or across roads were not included in these counts. House counts were divided by shore perimeter to obtain a dwelling density for each lake.
Each zooplankton sample from each lake was subsampled and placed in a counting wheel to estimate taxa richness, relative abundance, and size structure. The first 50 zooplankton encountered in each subsample were identified and their lengths were measured using Image-Pro Plus imaging software. The remaining individuals in the counting wheel were counted. Most zooplankton were identified to genus using keys in Edmondson et. al. (1959), Balcer et. al. (1984), and Aliberti et. al. (2007). A few genera that were difficult to identify were grouped together. Skistodiaptomus and Leptodiaptomus were considered Diaptomus and Acanthocyclops, Diacyclops, and Tropocyclops were all considered Cyclops. Zooplankton lengths were measured from the top of the head to the end of the caudal rami for copepods and to the base of the tail spine for cladocerans.
Twelve different crustacean zooplankton taxonomic groups were identified across the study lakes (Table 2). Cladocerans, and calanoid and cyclopoid copepods were found in all lakes. Among cladocerans, Daphnia were most ubiquitous followed by Diaphanosoma, Bosmina, and Ceriodaphnia. Eubosmina, Chydorus, and the predacious Leptodora were relatively less common, occurring in only 3 of the study lakes. Among calanoid copepods, Diaptomus were most ubiquitous, being found in every lake, while Epischura were less common, occurring in only six of the study lakes. Among cyclopoid copepods, taxa in the groups Cyclops and Mesocyclops were very common, occurring in all lakes. Ergasilus were found in only five of the study lakes. A higher number of cladoceran genera appear in Table 2 because members of this group were more easily identified and were not lumped together as was done for copepods.
Measures of size structure appeared to be influenced by zooplankton community composition. Proportion of the community comprised of Daphnia had the strongest influence of any variable on mean length of all zooplankton (Figure 4) and mean length of cladocerans (Figure 5). Zooplankton mean length and cladoceran mean length tended to increase with refuge thickness, although these trends were not significant. Mean length of Daphnia was the only measure of size structure that was correlated with dwelling density (Figure 6).
Secchi depth appeared to be the most important abiotic factor controlling community composition and indirectly controlling size structure in the study lakes. This finding contrasts with Tessier and Welser's (1991) finding that seasonal zooplankton community change within a similar set of southern Michigan lakes was best predicted by the decrease in refuge size over the summer. The difference between these studies may be due to the time scale over which response variables were measured. Since the present study focused on refuge and zooplankton community characteristics for a single point in time, a hypolimnetic refuge may still be important in the study lakes if viewed across the growing season.
Across the study lakes, as Secchi depth increased the zooplankton community shifted to a cladoceran-dominated, and more specifically, a Daphnia-dominated assemblage. Since Daphnia-dominated zooplankton communities tend to have larger mean lengths (Taylor & Carter, 1997), this result is consistent with Stemberger and Miller's (2003) finding that mean cladoceran body length was positively correlated with Secchi depth. A possible explanation for this result is Daphnia's superior ability to reduce phytoplankton biomass through grazing (Dini et. al., 1987, Carpenter et. al. 1992, Cottingham et. al., 1997). Another possible explanation for the increase in percentage cladoceran with increasing water clarity involves the differences in feeding strategies between cladocerans such as Daphnia and copepods. The foraging efficiency of Daphnia decreases at very high food densities due to their complicated filtering mechanism (Wetzel, 1975, Starkweather, 1978, Rigler, 1961). In the study lakes with shallower Secchi depths, phytoplankton concentrations may have been too high for Daphnia to forage efficiently, allowing copepods to exploit the food base in lakes where Daphnia handling costs are high.
The results of this study also suggested that biotic interactions may be important in structuring zooplankton community composition. Percent of the community that was comprised of calanoid copepods was inversely related to percent cladocerans, Daphnia and cyclopoid copepods. These relationships could be a result of competitive effects between superiorly competitive cladocerans such as Daphnia and copepods (Brooks and Dodson, 1965, Gliwicz and Lampert, 1993). It could also be due to the complicated interactions described in Brooks and Dodson's (1965) size-efficiency hypothesis. Indirect effects of predation and trophic cascades could also be shaping the community composition in these lakes. For example, in the presence of bass, cladocerans tend to be more abundant, while copepods are more abundant when bass are absent (Turner and Mittelbach, 1990). Additional information on fish assemblages in the study lakes would be valuable in understanding the factors influencing zooplankton community structure.
Variation in zooplankton size structure across the study lakes was strongly influenced by community composition, in particular, the relative abundance of Daphnia. This finding supports previous conclusions that zooplankton assemblages dominated by Daphnia tend to have larger mean lengths due to the large body size of members of this genus (Taylor & Carter, 1997).
Dwelling density, the variable being tested for its effect on zooplankton community structure, was not significantly related to many parameters in this study. It was, however, negatively correlated with mean Daphnia length (i.e. lakes with a larger number of houses per km shoreline had smaller Daphnia). The reasons underlying this relationship are not clear. It is possible that people prefer to live on lakes with good recreational fishing quality and thus lakes with smaller Daphnia were lakes with high numbers of fish whose preferred food is large cladocerans like Daphnia (Brooks & Dodson, 1965, Brooks, 1968, Vinyard, 1980, Turner & Mittelbach, 1990). The observed decline in Daphnia mean length with increasing residential shoreline development also could be due to a refuge effect. If it is hypothesized that residential shoreline development changes limnological conditions in a way that would decrease the size or availability of a refuge from predation, since large-bodied Daphnia are preferred by fish (Brooks & Dodson, 1965, Brooks, 1968, Vinyard, 1980, Turner & Mittelbach, 1990) they should disappear first in zooplankton communities in lakes with more development. A positive trend was seen between refuge thickness and cladoceran mean length, but this relationship was not significant.
For the purposes of this study, it was assumed that the relative effectiveness of a refuge was positively related to its thickness (Wright & Shapiro, 1990). However, it should not be assumed that the presence of these refuges is an indication that DVM is taking place or that the refuges represent an area of decreased predation pressure (Wright & Shapiro, 1990); if this were the case, refuge availability and size would have no impact on zooplankton community structure. There are at least four plausible explanations for why refuges may not actually be regions of decreased predation: firstly, zooplankton may not have been present in the refuges at all. Horppila et. al. (2000) found that most cladocerans in a stratified lake inhabited the epiliminion due to several factors, including a metalimnetic oxygen minimum and predation by the midge larva Chaoborus in the hypolimnion. While no metalimnetic oxygen minima were observed on the sampling days, Chaoborus was found in several lakes. Deep water refuges are less important in lakes with effective deep water predators (DeMott & Edington, 2004), so DVM may not have occurred in lakes where Chaoborus exerted predation pressure in the hypolimnion. Secondly, even if zooplankton were distributed throughout the water column, including in the refuge, this does not necessarily imply predator avoidance behavior – they could simply be distributed such that the most vulnerable individuals occupy the deepest layers. This phenomenon is related to food distribution; with homogenously distributed food it is not necessary for zooplankton to pay the energetic costs associated with DVM (Pijanowska & Dawidowicz, 1987). Thirdly, in conditions of extremely scarce food, zooplankton will expose themselves to greater predation risk by remaining near the lake surface to feed during the day and night (Johnsen & Jakobsen, 1987). Lastly, fish may feed below the bottom of the epilimnion and the refuge definition used in this study, therefore, may be inappropriate. In order to be confident in the definition used, planktivore distribution in the water column would need to be determined. Information on the abundance and vertical distribution of zooplankton, their food, and their predators within the entire water column would need to be collected in order to be certain that refuges are affecting zooplankton community structure.
The temporal component of refuges also must be considered. Thermocline depth and hypolimnetic anoxia change both seasonally and from year to year (Shapiro, 1990, Tessier & Welser, 1991). In lakes in southwestern Michigan, refuge size was reduced by 50% between June and August (Tessier & Welser, 1991). Zooplankton community composition and size structure may change as a result of changes in refuge thickness (Wright & Shapiro, 1990). Since my study lakes were sampled once during the entire season, only a snapshot of refuge availability and thickness was obtained.
The results of this study indicate that zooplankton community structure is affected by numerous, interrelated factors. Information on the distribution and community structure of both fish and phytoplankton populations would lend much more insight into the factors shaping the zooplankton populations in these lakes. It is possible that human shoreline development could have an important influence on any number of things that would in turn affect zooplankton, and future research should take a holistic ecosystem approach in investigating these connections.

Tsihrintzis (1997) found that in 1972 the Federal Water Pollution Control Act set standards to drastically reduce point source water pollution. Amendments to the Clean Water Act in 1977 and 1983 caused even further reductions in point source water pollution. As a result of these reductions in point source pollution, non-point source water pollution quickly became the major source of water pollution in the United States and continues to be the major contributor even today.
Due to this, in 1990 the EPA mandated that municipalities must develop monitoring programs for their storm water discharges (Tsihrintzis 1997). According to Bobrin (2000) the city of Ann Arbor, Michigan has developed such monitoring programs for its effluents into the Huron River Watershed. From 1995 to 2020, Washtenaw County is expected to see an increase in population by 28% and 26,000 more acres will become developed. This is likely to have a large impact on storm water pollutants if no action is taken. In previous investigations of the Huron River's quality by Wiley and Martin (1999) they have suggested that it may be in the early stages of ecological degradation due to an observed decreasing trend in invertebrate species diversity.
However, according to Bobrin (2000), the Ann Arbor-Ypsilanti Watershed Management Programs focuses on monitoring the river itself and its major tributaries without an emphasis on studying the physical storm effluents into the watershed. Studies assessing the storm drain effluents could help to better understand specific pollutants entering the watershed, as well as help to locate probable sources of the pollution.
The purpose of this study is to better understand the effects of pollutants directly entering the watershed via storm drains into the Huron River. Broccoli was chosen as a plant of study due to its high rate of germination and quick growth rate. Its function in this study is to represent a plant in the watershed. Damselflies were chosen as the animal of study due to their high sensitivity to pollutants and the fact they are naturally present in many aquatic environments, including the Huron River.
I hypothesized that, first of all, broccoli plants grown in differing concentrations of urban storm water will grow at different rates due to pollutants and toxins present in the runoff. I predicted that in this experiment, a greater overall biomass will be generated by the broccoli seedlings grown in the pure water, with a decreasing amount of biomass created by those plants grown in greater concentrations of storm water due to the growth inhibiting action of such pollutants as motor oil and metals. In addition, I also hypothesized that damselflies placed in varying concentrations of storm water will exhibit different lengths of survivorship also due differing levels of pollutants present in the storm water. I predict that damselflies grown in increasingly greater concentrations of storm water will die quicker due to the toxic effects of motor oil and metals.
I collected storm water in a 5 gallon jug from a storm drain effluent emptying into the Huron River near Depot St. and Broadway Rd. I collected the water during a rain event on October 4, 2007. I next diluted the storm water in one gallon jugs with tap water to 25% and 50%. In addition, 100% storm water was put into a jug and pure tap water was poured into another jug (later referred to as 0% storm water). The 0% storm water acts as the control in this experiment to contrast the effects of storm water against clean water on the growth of plants and animals.
I chose broccoli seeds as a plant of study to determine storm water's effects on plant growth. I chose them due to their high germination rate and fast growth. Ten broccoli seeds were placed into four separate 25 cm x 25 cm plastic planters with about 2.5 cm of potting soil and the seeds sowed 1 cm below the surface. They were watered exclusively with one of the four separate concentrations of storm water every other day for about one month. Eaching watering was about 200 mL. Due to logistical problems, I grew the plants in a 22 C apartment near a glass doorwall. I rotated the planters daily in order to help ensure equal amounts of sunlight to each planter.
After one month, to determine the effect on growth of varying storm water concentrations, I cut the seedlings at the surface and weighed them in order to find the total biomass generated in each of the four planters. Since not all 10 of the seeds germinated in each plot, I calculated the average single plant biomass within each of the four plots in order to standardize the data. Using SPSS stastical software, the average plant mass in each of the four concentrations was ploted against the concentration and a regresion analysis was conducting to determine correlation between plant growth and storm water concentration.
In addition, I harvested 20 damselfly larvae using a net and were taken from the pond at the University of Michigan Matthaei Botanical Gardens in Ann Arbor, Michigan. I filled four 6 oz glasses with each of the concentrations of storm water and five damselflies were placed in each of the glasses. I recorded their daily survivorship. I kept the damselflies in a 22 C environment, away from direct sunlight. I put a 1 g piece of bread in each glass at day 1 as a food source for the damselflies.
After all of the damselflies had perished, I analyzed the length of lifespan data. In order to avoid pseudo-replication, the average damselfly lifespan for each concentration was calculated rather than treating each damselfly as an individual data point. To determine the effects of storm water concentration on damselfly survivorship, the average damselfly lifespan for each concentration was plotted against the concentration level and a regression analysis was conducted using SPSS statistical software.
Finally, I used test strip indicators to identify the presence of oil, lead and phosphates. I dipped the strips in the 100% storm water concentration mixture and the strips were formulated to change colors upon the presence of each of the various pollutants.
A strong positive correlation is observed between increasing storm water concentrations and average broccoli plant biomass. The r-sqaured value is 0.942. In addition, this correlation is determined to be statistically significant as a 0.05 significance level since the p-value was found to be 0.029.
A moderatley strong negative correlation is observed between incrasing storm water concentrations and length of survival of damselflies. The r-square value is 0.813. This correclation is not statisticaly significant at a 0.05 significance level since the p-value was found to be 0.098.
The test strip indicators determined oil and lead to not be present in the storm water, though a moderately high level of phosphates were found to be presenet.
The first hypothesis that broccoli plants grown in differing concentrations of urban storm water will grow at different rates due to pollutants and toxins present in the runoff is supported by this study since each planter produced a different amount of biomass. However, the prediction that a greater overall biomass will be generated by the broccoli seedlings grown in the pure water with a decreasing amount of biomass created by those plants grown in greater concentrations is not supported by this study. The broccoli plants grown in higher concentrations of storm water produced a greater biomass. Since an R2-value of near 1 and a p-value of less than 0.05 was determined for this set of data, this provides strong evidence that this sample of storm water contained pollutants capable of increasing plant growth rates.
Since phosphates, possibly from lawn fertilizers, were observed in the storm water, it may be that lawn fertilizers present in the storm water caused the broccoli plants to grow at a higher rate at increasingly higher storm water concentrations. Previous studies of the Huron River by Bobrin (2000) have indicated that the biggest pollutants include suspended solids, phosphorus, bacteria, and metals. In addition, Wiley and Martin (1999) found that large algal blooms in Ford Lake, downstream from Ann Arbor, are frequently observed during heavy fertilizing periods throughout the year.
The second hypothesis that damselflies placed in varying concentrations of storm water will exhibit different lengths of survivorship due to differing levels of pollutants present in the storm water is supported by the general trend illustrated in this study. The data reflects a negative trend in which damselflies generally died quicker when placed in higher concentrations of storm water. However, the damselfly data are not statistically significant and thus further studies should be conducted to verify. Originally I had predicted the negative trend in lifespan would be due to increasing levels of metals and oil, though this study found no oil or lead present. Even though lead and oil were not detected, it is quite possible that other harmful metals and pollutants are present in the storm water since heavy metals can be correlated in watershed systems with the decline in invertebrate species diversity. Gray (2004) found that heavy metals, once inside the body of aquatic invertebrates, can accumulate and interfere with crucial biological processes such as metabolism.
One major error of this study is the lack of replicates. Ideally, multiple broccoli planters and damselfly sets should have been used for each storm water concentration rather than only one. This would have created a larger sample size and in the case of the damselflies may have provided statistically significant data. In addition, broccoli is not naturally occurring in the Huron River Watershed. Ideally, further follow-up studies should utilize native plants of the watershed. It may be that native plants react differently to some pollutants compared to broccoli plants.
Overall, this study illustrates that storm water does contain pollutants that are capable of effecting both plant growth and aquatic invertebrate survival. Even though this study was conducted at relatively high storm water concentrations that likely are not representative of average conditions in the watershed, it is possible that such concentrations are reached during large rain events and in areas of the watershed in close proximity to sewer effluents. As the watershed region becomes more urbanized, non-point source storm water pollution is likely to become more of a problem. Construction sites, fertilizers, automobiles, and pet wastes are some of the major contributors to decreased watershed health and all of these factors will likely increase as the region grows (Carpenter et al 1998). If polluted storm water can affect plants and animals, human exposure to polluted storm water in the watershed may be a danger as well, posing threats to the safety of those participating in recreation activities in the region.

The demands of large-scale human settlement, such as agricultural production and the building of urban environments, often come into conflict with the welfare of non-human natural systems. Human society requires a constant stream of resource inputs from the natural environment and also creates a constant stream of waste outputs that must be dumped into one environmental "sink" or another. One of the most problematic of these waste streams is sewage effluent, since it can cause significant damage to the environment into which it is dumped if left untreated, and it is inevitable. Thus, modern industrial societies since the nineteenth century have developed increasingly sophisticated treatment systems in order to at least partially mitigate the environmental damage resulting from sewage.
Of course, the impetus for sewage treatment has largely been government regulation, since the benefits of treatment (cleaner water, healthier natural systems and more robust fish populations) are widely distributed within a watershed and the costs are highly concentrated in the organization responsible for the treatment. These costs are significant, and no private entity on its own has an incentive to guarantee clean water for everyone else. In other words, the benefits of sewage treatment are public goods, and economic theory predicts that public goods will be inadequately supplied unless the government intervenes and requires a particular entity to provide them. Also, there is a question about exactly what minimum level of cleanliness is necessary to maintain a healthy environment, and government must set a common standard throughout a watershed.
The assumption in the case of sewage is that it must be treated in order to ensure the health of the local and downstream environment, and that some form of regulation is required in order to guarantee that a certain minimum amount of sewage treatment takes place. This lab, then, is designed to model the costs and benefits of selecting various sewage treatment options and the variables which affect that selection. By exploring the effect of these variables on the total costs to society from sewage treatment using a mathematical model, one can make better-informed decisions about which sewage treatment options are most cost-effective.
The system modeled in the lab is the case of a sewage treatment plant releasing effluent into a river. There are several variables considered in the lab: the design of the treatment plant, the strictness of regulation, the rate of river discharge, and the channel flow. The effects of manipulating each variable, independently of the others, are modeled in MathCAD.
There are four possible treatment plant designs to be modeled: secondary treatment (about 90% of [BOD] removed, with about 50 mg/L left); two different levels of tertiary treatment (which reduce [BOD] to either 30 mg/L or 25 mg/L); and a newer technology that reduces [BOD] to about 15 mg/L. The more that [BOD] is reduced, the more expensive the treatment plant is. All other things being equal, rational choice theory predicts that society would prefer the plant with the lowest total cost.
Regulation in the case of sewage treatment plants is assumed to take the form of a fine for non-compliance with water quality standards. These water quality standards require that the waste load in sewage effluent not cause dissolved oxygen [DO] in the river to drop below a certain threshold at which the ecosystem would be unacceptably damaged. There are two different fine levels, $5,000 per violation and $5,000,000 per violation; and also two different [DO] standards, 6 mg/L and 4 mg/L. The higher standard would be necessary to preserve all fish species in the river, whereas the lower standard would be required to maintain only smallmouth bass and carp.
The last two variables are river discharge rate and channel flow. These both affect the river's flow and therefore the [DO] available in the water. River discharge rate can either be set at twice the normal flow rate in order to model a wet year, or at half the normal flow rate to model a dry year. Channel flow, a measure of the slope of a river and therefore of water speed, can be set at 0.00001 to model the presence of a dam, or 0.0001 to model the removal of the dam.
The lab includes four different experiments, each modeling the effect of a different variable. In each experiment, one variable is tested at two different values, and the performance of each plant design is predicted at each of the two values. Plant performance is measured by the failure rate, i.e. the frequency with which the plant fails to meet the water quality standard. An annualized plant cost is then calculated, as a sum of capital and operating and maintenance costs plus the cost of any fines incurred as a result of failure to meet water quality standards. This total cost is used to rank the desirability of each design in each experimental situation. In Experiment 1, the effect of different fine levels is tested; in Experiment 2, the effect of different [DO] standards is tested; in Experiment 3, the effect of different river discharge rates is tested; and in Experiment 4, the effect of different channel flows are tested.
Since it would be extremely costly and time-consuming to go out into the field and perform all of these experiments in the real world, mathematical modeling with MathCAD of these different sewage treatment options is perhaps the most direct way to explore the question of sewage treatment. So long as the assumptions, variables, and model equations themselves are sufficiently sophisticated to produce realistic results, this approach is an excellent way to enable students to make conclusions about sewage treatment options using hard data. The only problem encountered during the lab was the minor hindrance of having difficulty with inputting values in MathCAD.
The lab itself yielded a number of important results. From the first experiment, it is clear that the level of the fine imposed on the sewage treatment plant has a large impact on the total cost of treatment. In this case, comparing fines of $5,000 and $5,000,000, the higher fine actually reverses the order of preference for the plant designs. Whereas with the low fine Design 1 yielded lowest total costs, with the much higher fine the lowest total cost is generated by Design 4 (Figure 1). This is because the cost of the fines in this case greatly outweighs the capital and O&M costs. Such a result suggests that, if it is concluded that it is critically important to maintain water quality at a certain level and keep violations below a certain level, a high fine will provide a powerful incentive to install the best technology for [BOD] reduction.
From the second experiment, one sees that even at a lower [DO] standard, the fine level of $5,000,000 still generates total costs such that Design 4 is preferable (Figure 2). One may have predicted that a lower [DO] standard would give results that make a less expensive plant design seem preferable, but that is not the case here. This may not be true at all [DO] standards -- for example, at a [DO] standard of 1 mg/L Design 1 will most likely be preferred -- but at least for the two values tested it appears that the effect of the fine level outweighs the effect of the desired [DO] value. Because of limited time in the lab, there is not very comprehensive data, so it would be hard to predict in general which parameter, fine level or [DO] standard, would have the greater effect on total costs for each increment of change.
The third and fourth experiments provide a different sort of information than the first two experiments. Rather than modeling the effects of parameters imposed solely by humans, they model the effects of natural parameters (i.e. local conditions) over which humans have little or no control. The third experiment shows that, though the flow rate does not affect the order of preference of the plant designs, it does affect the total costs (Figure 3). Thus, the total cost to society will change based not on difference in human choices but on uncontrollable natural conditions -- a drought year will make it more difficult for any plant design to maintain water quality. If there is still a preference for the best technology (Design 4) under this scenario, it may make sense to lower the fine in drier years, because the purpose of incentivizing the optimal technology has been achieved and keeping the fine high will raise costs unnecessarily.
The fourth experiment demonstrates that the channel flow and water speed have a significant impact on the water quality. With a shallow slope and slower water speed, the most expensive plant design is still preferable (Figure 4). A steeper slope and faster speed in this experiment meant that no violations occurred with any of the plant designs -- in other words, the choice of plant design would have no impact on water quality and therefore one could choose the lowest-cost design (Design 1) without any negative environmental consequences.
This lab vividly demonstrates the impacts of various variables on the desirability of different sewage treatment plant designs. Before gathering data, one may have drawn widely divergent conclusions based on certain assumptions. For instance, one might have assumed that the more expensive plant design (Design 4) was always best, because cleaner water is always worth the cost. The results of the lab show that this is not always the case. Assuming that the goal of sewage treatment is to optimize cost/benefit ratios for both human and natural systems (i.e. optimize the cost/benefit ratio for the whole human/non-human system), it is not always necessary to choose the most expensive treatment option, because this may impose unnecessary costs on the human system which provide little benefit to non-human systems, creating a sort of "deadweight loss." In certain cases, one can maintain a high level of water quality even with the cheapest treatment plant. Experiment 4 suggests that one must first examine local conditions in order to determine what water quality standards are necessary to maintain environmental health, because a stream with a steeper slope and faster water speed will require less stringent regulations on water quality.
The thesis that a regulatory system with substantive, punitive fines is essential to the protection of ecological values in our rivers is not always true. Fines are one way to create a disincentive for disposing of untreated human sewage, but another method may be for downstream users to pay the upstream sewage treatment operator to maintain clean river water. Besides financial incentives like fines or ecosystem services payments, the government may choose to set water quality standards and institute a criminal penalty (e.g. prison sentences) for failing to achieve them. Though it may not be very practical, one could also institute a cap-and-trade program for sewage dumping within a particular watershed. Thus, there are other methods besides punitive fines for maintaining the ecological value of rivers.
If one does choose to use fines, Experiment 1 shows that they must be set high enough to overcome other costs. Low fines are not any more effective than no fines at all. Experiment 3 demonstrates that it would be wise to allow for an adjustment of fines based on environmental conditions like flow rate. As mentioned above, in a drought year even the most advanced treatment plant design will have a higher violation rate and this will create unnecessarily high costs for treatment through more frequent fines. The goal is to create an incentive for the most cost-effective way to maintain environmental quality, and so once the fine level has led to the adoption of the best technology, it is no longer cost-effective to allow the cumulative amount of fines to increase further. Flexible fine levels, which would decrease in dry years and increase in wet years, would maintain water quality more cost-effectively.
Through all of these experiments, though, it is clear that water quality will not be maintained unless local sewage treatment operations are subject to some form of external regulation, either through government-imposed penalties or private ecosystem services payments. Regulation, then, in one shape or another is essential to the protection of ecological values in our rivers.

To use arcGIS to create a future land use map that appropriately allocates development, agriculture, and nature preserves, protecting biodiversity in northeastern Washtenaw County.
Habitat loss has been described as the primary threat to the loss of biodiversity at the landscape, species, and genetic levels (Wilson, 2002). In northeastern Washtenaw County, biodiversity is becoming more threatened as the area continues to face rapid economic growth. Land use for development and agriculture has led to the fragmentation of nature reserves throughout the area. Such fragmentation is seen by many landscape architects as a major issue for the protection of habitats; small habitats are found to be more homogenous than larger habitat in terms of soil content, species populations, and landscape character (Collinge, 1996).
Upon the observance of a 1995 land use map, I decided that an allotment of land conserving natural areas with fair distance from the urban growth of Ann Arbor should be a priority in land allocation. Since the three land use types influence each other through numerous and often unquantifiable interactions, each land use type was carefully considered. A target for allocation of developed, agricultural, and natural area was provided, with each land use type to equal 10%, 36%, and 54% of available land, respectively. This means that future land allocation would equate to 31.5% developed, 48.1 % natural, and 20.4% agriculture. With these goals in mind, I was able to produce allocations of these three land uses, while maintaining a commitment to the minimization of habitat fragmentation.
This study followed the directions provided in the lab handout, which can be summarized as a six step process. My first task was to use the vast compilation of maps and raster grids provided for this experiment to create a series of sub models for each land use type. In arcGIS, ModelBuilder and the Spatial Analyst tool box were used to quantify the criteria for each of the three land use suitability maps; all recommended criteria were used. Each criterion involved a model that produced a score (1-100) to be used for suitability mapping. The second step utilized the criterion to create the suitability maps in ModelBuilder. Each criterion was assigned a weight through the website provided in the lab handout; weights can be viewed in Tables 1, 2, and 3, below.
Third, weights were distributed with environmental protection as the primary objective. Water sources were secluded from agriculture and development. Since wetlands are a unique habitat that has been diminished in the state of Michigan, it was important to allocate natural areas around wetland habitats. In Table 1, it is clear that clay content was a low priority, since the data source was deemed unreliable (improper scale). In both agriculture and development, slope was a priority because of the prospect of erosion, which directly ties into the aim of environmental protection. Upon the creation of the weighted suitability maps, I discovered that each land use type had different ranges of scores (all within 1-100). To eliminate the prospect of preferences in the proceeding steps, each range was sliced into values from 1-100, through the equal area setting, giving each land use type equal value. I considered giving a preference to the natural land use type since it had the largest target area, but I was most concerned with grouping natural areas together to minimize fragmentation. If natural area was allowed to have a higher score range, the resulting allocation map might prescribe natural preservation to areas that are better suited today for agriculture or development
Once the three final suitability maps were created, I used the Band Collection Statistics tool to generate Table 4, which displays the correlation between each land use type. I proceeded to perform the land allocation through arcGIS's Highest Position tool, as directed in the lab handout. The final step involved the displaying of results through a comparison to the 1995 land use map, provided. This comparison was also carried out though the use of the various recommended spatial analyst tools.
Table 4 illustrates the negative correlation between each land use type. Figure 1 shows two land use maps: "1995 Land Use" is a reclassified version of the provided raster dataset, while "Allocation for Future Land Use" illustrates the results of this lab's land use redistribution. Between the past and future maps, land allocated as nature preserve decreased by 4.6% of total land, agriculture decreased by 7.8%, and development increased by 15.4%.
The land allocation model produced results that are surprising on several accounts. First though, the negative correlations illustrated in Table 4 communicate the fact that none of the land uses were conflicting. An analysis of Figure 1 proves that the modeling technique was relatively successful at minimizing habitat fragmentation, while completely missing the targets of future land use allocation. Many nature areas were clumped into larger habitats. As expected, most of these areas were distant from development, since agriculture was weighted to be closer to urban areas. Coincidentally, habitat corridors were common, strengthening an attempt to maintain biodiversity. Corridors are beneficial, providing access for mobile terrestrial species to large habitats relatively far away. However, they bring completely unintended benefits to the allocation strategy, likely attributed to the value of streams and tributaries in the suitability weighting. Unfortunately, this model did not erase habitat fragmentation, as there were many small areas placed as natural preserves that would not be able to support healthy population sizes of organisms in need of a large habitat area.
Land for natural preservation was short by 13.5% of overall land, while development was 4.6% steep and agriculture was 8.8% steep. Land allocation would have been closer to the target were more of the agricultural areas allotted for natural preservation. I stumbled across a more appropriate allocation when I neglected to rescale the agricultural score, which ranged from "0-59". Since the highest areas were still much lower than the "1-100" scales of the other two land uses, much of the land that I present as agriculture was natural or development. To maintain consistency I sliced agriculture to a scale of "1-100".
With the information that this model provides, it is easy to see that many patterns are at work, with and against each other, in ways that cannot be observed until the model is ran. Development virtually took over the southwest side of the study area since this is the closest area to Ann Arbor, while insulating the major roads crossing through the study area. The small natural areas may be indicating the presence of wetlands and tributaries. These patterns, and many more that go unnoticed after a quick glance, could not be easily predicted without running the model itself. If this weren't a fictional situation it would be necessary, and relatively painless, to edit the criteria and tweak the weights. To ensure distance between natural preserved areas and development it would only seem wise to use this as a major criterion (however this would inhibit the planning of city parks). Since the recommended agricultural area is so small, it would be necessary to reduce the "1-100" scoring scale to "1-80" or "1-75".
Given the stated objectives, it would be important to make these changes, as well as limit the amount of development by 6-9% overall land allocation. Perhaps another model could be run based on this model that works to allocate land in the developed area for parks and recreation. The 1995 land use map indicates a number of areas for outdoor recreation, so the model could use the Euclidean Distance tool to determine where these areas could be appropriately expanded, so that not to infringe upon other existing development. Adding or subtracting criteria, establishing more model parameters, and shifting weights are all ways that arcGIS makes the editing process of modeling very quick and simple.
The model building function of arcGIS is invaluable to the formulation of land use allocation maps, because of its data processing capacity and flexibility. With the ability to run a model, display the results, and enable the user to edit and run the model again, this is a vital tool for land use planning, especially with specific goals, such as the protection of biodiversity.

The Mount Graham red squirrel (Tamiasciurus hudsonicus grahamensis) is one of twenty-five subspecies of North American red squirrels. The Mount Graham red squirrel separated from other subspecies about 10,000 years ago. Studies have shown that it is genetically different from other red squirrel subspecies in North America. The Mount Graham red squirrel is only found on the Piñaleno Mountains in southeastern Arizona. As of present, the squirrels are known to occupy mixed conifer and spruce-fir habitat zones on the mountains. More specifically, they only inhabit mountain zones from 8,700 feet and up (Mount 2006). Because of this limited habitat range, the Mount Graham red squirrel has a relatively long history of susceptibility to environmental variability.
The Mount Graham red squirrel has been a conservation topic for several decades. Believed to be extinct in the 1950s, it was again sighted in the 1970s and added to the Federal Endangered Species list in 1987. There is no known data regarding the population of the squirrels prior to the mid 1950s. In the late 1980s, biologists believed that the squirrel could only occupy spruce-fir habitats in high elevations on the Piñaleno Mountains (see Figure 1). The U.S. Fish and Wildlife service assigned these higher areas as Mount Graham red squirrel refugium in 1988. The refugium requires a permit for people to enter the area. More recent research has shown that the Mount Graham red squirrels also occupy slightly low elevations. This raises questions as to the effectiveness of the refugium and the need for possibly expanding its boundaries (Mount 2006).
Currently, the Mount Graham red squirrel habitat covers approximately 6,460 hectares of Piñaleno Mountain upper elevations. Some biologists believe that competition with the introduced Abert Squirrel has resulted in some of the Mount Graham red squirrel's population decline. In addition, the loss of habitat due to the development of a university observatory, roads and other facilities are believed to have impacted the species population as well (Arizona Game 2003). Previous logging throughout the 1800s and 1900s also contributed to substantial habitat loss (USDA Forest 2001). The 2007 Arizona Game and Fish Department survey data estimates their population at approximately 299 individuals as seen in Figure 2. Data is collected by studying the activity rate at known areas where the squirrels store their cones (also called middens). The population data collected from 1991 show a sharp rise and decline in population between 1998 and 2000. This is believed to be caused by a range of factors threatening their habitat such as drought, poor cone crops, fire, and insects. After this point, their population appears to oscillate around 250 individuals (Arizona Game 2007).
Since the population size and habitat range of the Mount Graham red squirrel is so restricted, research is somewhat limited on its life history traits. Despite this, several physical and reproductive characteristics are known (Table 1). The squirrel's average length is 13.3 in and its average weight is 8.3 oz. Studies estimate that their breeding season lasts from January to April. Mount Graham red squirrels use leaves and twigs to build nests in snags, hollows of living trees, logs, underground, or in the branches of trees. Sometimes they may even use pre-existing tree holes built by other animals like woodpeckers. Thus, squirrel reproduction is very dependent on the presence of trees or appropriate nest habitat (USDA Forest 2001).
A female's first reproduction occurs after her first winter. The proportion of yearling and adult squirrels that breed is not consistent on a year by year basis, but the adult females always reproduce at higher rates than the yearlings. The squirrel's gestation lasts from 35-40 days. It is believed that the Mount Graham red squirrels have two litters per year. Each litter usually consists of three young. Once born, the squirrels are nursed in the nest for 6-8 weeks. Only the mother gives parental care and she begins to wean the young at 7-11 weeks (USDA Forest 2001).
The Mount Graham red squirrels also exemplify territorial behavior. They establish territory by using piles of their meal "leftovers" (cone debris) to build middens around their nest. These middens are used for storing food. Generally, middens are located on or around trees or logs that also function as the nest and protection from predators. Middens are almost always occupied by only one squirrel. Even after breeding, the females force the males out of their midden/nest area (USDA Forest 2001).
The juvenile mortality for Mount Graham red squirrels is about 67%. This generally occurs due to the harsh effects of winter between weaning and first reproduction. On average, Mount Graham red squirrels only live to age two to three. Survival rates are believed to vary according to the availability of closed cones which are their winter food source (US Fish 1993).
Examining life history is an important part of establishing good management plans for threatened or endangered species. In order to examine the life history of the Mount Graham red squirrel, a Leslie Matrix was assembled to study age specific survival rate and fertility. Data was used from previous research determining the red squirrel's demographic parameters (Buenau and Gerber 2003). The average age-specific survival rates and fertility for juveniles, squirrels in year two, and squirrels in year three or higher were used to construct the Leslie Matrix (Figure 3). This information was then used to develop a life-cycle diagram (Figure 4).
The Leslie Matrix was analyzed in Microsoft Excel through the PopTools Matrix Tools Basic Analysis. This software allowed for the calculation of valuable life history information. The largest Eigenvalue, 0.61, provided an estimation of , or the per capita geometric rate of increase. In addition, the per capita exponential growth rate, r, was calculated to equal-0.48. Thus the growth rate was negative for the red squirrels. The net reproductive rate, , was 0.18. The Eigenvector demonstrated that a stable age structure for red squirrels would consist of 38% year zero, 21% juveniles, 19% year two, and 22.4% year three and up.
The analysis of the Mount Graham red squirrel life history reveals information that could help shape effective conservation management. As Figures 3 and 4 indicate, the Mount Graham red squirrels exhibit higher fertility later in life. Thus it is important for them to be able to survive past juvenile stages in order to maximize their reproduction. Unfortunately, as the figures indicate, juvenile squirrels have the lowest age-specific survival rates at 0.33 while older squirrels have a much higher survival rate, 0.73. The primary reason for this low juvenile survival rate is the impact of wintering (US Fish 1993). If juvenile squirrels have not acquired sufficient winter food storage or physical strength to withstand harsh winter weather, they may not survive the severity of the season. Thus they will not survive to reproduce later in life. Management can play a role in helping juvenile survivorship by preventing the deforestation of trees or the destruction of habitat where Mount Graham red squirrels build middens to store food for the winter. Since biologists now know that the squirrels occupy both mixed conifer and spruce fir zones on the Piñaleno Mountains, managers could enforce specific limitations in these areas. Although there are already permit restrictions in the spruce fir zones, it could be very important to expand those restrictions to mixed conifer zones within the elevation where the Mount Graham red squirrel is known to reside.
The basic matrix analysis also provides information relevant to management. Since the per capita geometric rate of increase is less than one, it the model indicates that population is decreasing. The negative per capita exponential growth rate also suggests this. While the fact that the population is decreasing may not provide much guidance in itself to managers, it does offer evidence that protecting this subspecies should be a priority since the population is clearly decreasing. The matrix analysis also provides the net reproductive rate. This rate indicates that an estimated 0.18 daughters are born to each female. This is a very low number considering that females have an average three young per litter and also have a relatively short parental care period. This can again be attributed to habitat loss. In order for Mount Graham red squirrels to mate, they need a nesting site. If their habitat is disrupted or destroyed for development, the squirrels cannot easily create nesting sites.
Finally, the analysis offers a look the age distribution within a stable age structure for the Mount Graham red squirrel. The structure suggests that the highest percentage of squirrels should be from zero years to juvenile. Since a large number of these young squirrels do not survive the winter, more young squirrels are needed so enough survive to be able to reproduce in the spring. Although research sources with information regarding the Mount Graham red squirrel's current age distribution could not be located, the stable age structure information could still be valuable to conservation managers. Knowing that a stable age structure would have more young squirrels than other ages informs managers that they need to find ways to protect young squirrels. Since the most important threat to the young is wintering, managers could ensure that plenty of closed cones (the squirrel's winter food source) are available for young squirrels to collect and store. They could also ensure that middens, where the squirrels store their winter food source, are not disturbed by humans. In addition, managers could purchase property that has already been developed and replant the native trees that red squirrels rely on for their habitat.
While it may still be possible to successfully carry out a recovery plan for the Mount Graham red squirrels, I would think the impact of such a population decline would not leave them unaffected. More specifically, it could be helpful to conduct research regarding the remaining genetic diversity within the Mount Graham red squirrel gene pool. I would imagine that much of it has been lost since their population is now down to only a few hundred squirrels. This research could also be helpful in determining if there are any genetic traits that make the Mount Graham red squirrel more vulnerable to habitat destruction than other squirrels. It might then be possible to genetically engineer these traits so as to enhance the survival of the Mount Graham red squirrel. Without accounting for genetic diversity, a recovered population of squirrels could be even more susceptible to extinction due to their genetic homogeneity.
Habitat loss results in detrimental biodiversity threats around the world. This is no different in the case of the Mount Graham red squirrel. Because of its limited habitat range, it is very dependent on a safe and undisturbed habitat. Logging, road construction, recreation, and other human activities have stressed the squirrel's already limited habitat. By cutting down trees and building roads, humans are removing the nesting sites, food sources, and habitat corridors that the Mount Graham red squirrel relies upon for survival. The life history of the red squirrel indicates that a suitable habitat is especially important for juvenile squirrels. They are the age group most vulnerable to wintering, and need to be able to find food and storage areas for surviving the winter. If low proportions of juveniles survive, then there will be less left to reproduce, and the population decreases. This seems to be the current situation as the population is reduced to no more than several hundred individuals. In order to prevent their ultimate extinction, strict habitat protection policies need to be enacted and enforced. In addition, research regarding the genetic diversity left in the Mount Graham red squirrel population would help develop an even better understanding of the severity of their situation. Although past habitat destruction has caused tremendous problems for the Mount Graham red squirrel, there is still hope in the species recovery. Previously thought to be extinct, the Mount Graham red squirrel has proved it has some resilience in surviving such extensive habitat damage. If managers work to protect the habitat and encourage the growth of the Mount Graham red squirrel, it may be possible for the species to recover.

The level of dissolved oxygen (D.O.) in rivers is a main variable in assigning the streams health. D.O. is used by all aquatic organisms, most as the only means of available oxygen. The amount of dissolved oxygen, usually measured in parts per million (ppm) is one of the main determining factors of what type of biota will live in the rivers and streams. Many big game fish, such as trout, are very sensitive to D.O levels, and require high levels to survive. The pollution put into river systems consists of excess nutrients, and raw sewage full of bacteria.
The current pollution problem in many streams and rivers are caused by sewage plants dumping too many nutrients into the water. Because water treatment plants need someplace to put the treated water, most industries are found near natural stream and rivers. However, the efficiency of these sewage treatment plants varies greatly. How efficient a treatment plant is, is determined by the amount of bacteria and nutrients that is put into the river as "treated" water. The nutrient rich byproduct from sewage plants entering the river increase the bacterial levels in the river, and thus increase the biological oxygen demand (B.O.D.) The B.O.D. is the amount of oxygen needed for bacteria to decompose waste. Increasing the concentration and overall amount of bacteria will raise the B.O.D. If the demand for oxygen is increased, all the oxygen in a reach of a stream, near the pollution site, will be used up by the bacteria, leaving nothing for the other organisms, mainly fish, and microorganisms to use. Healthy rivers will be able to absorb a fair amount of nutrients and bacteria before the oxygen levels drop to dangerous levels. However, sewage treatment plants tend to dump massive concentrations into the river, which it cannot process in a short period of time.
The problem addressed in this essay is to see how different factors will lead to building treatment plants of different efficiency. Also, looking at the system between which design of treatment plants will be build, the cost to the city members, and the cost to the environment. In this experiment four general designs of plants are available to build. The designs are numbered one through four, and vary in efficiency, one being the least efficient and four being the most efficient at removing bacteria and nutrients from the treatment water. Another key factor when looking at the efficiency of the different plant designs in that the more efficient a plant design is the more expensive to build it is for the people of the city. This brings up the common problem of saving money or doing what is right for the environment. In writing this essay, the topic of saving money going against caring for the environment will be a central discussion point. In discussing this topic, a few variables such as the amount of a fine, as well as setting the minimum ppm standard will be included. I predict that is using a ranking system that places priority on the cost per person and not the cost to the environment, high fines will be needed to preserve the river ecosystem.
Using the computer program Mathcad, our group set up, using mathematical equation, a chart where we could find the cost of building different designs of sewage plants, relative to different variables.
Failure rate indicates the number of days, per year that the D.O. level of a stream, that the sewage treatment plant is on, drops below the standard 6ppm. Dropping below this standard rate will result in a fine for that day. The higher the percentage of the failure rate, the more fines that specific design will cost the people of the city. Cost per person is used as a price set, due to the setting of the people of the city having the decision of which plant to build. For this study, we will assume that the people of the city will always choose the cheapest design, even if it is not the best environmental choice. The final column is for ranking the sewage treatment plants in order of cost, 1 being the least expensive, and the choice picked by the city, and 4 being the most expensive.
Four main types of experiments were run in lab. One experiment used the effects of fines to rank which sewage plant design would be cheapest per person. Two limits of fines were used, one setting the fine for going under the standard D.O. at five thousand dollars everrry day of failure, and another test run setting the fine at five million dollars. Another variable looked at in our system was one of setting the standard of B.O.D. at 4 ppm, rather than 6 ppm. This lowered the amount of pollution the sewage plants would have to correct for, and thus had an effect on the price and ranking of each sewage design. Other variables included the effects of a wet or dry year on design cost. Also how a dam, and dam removal would affect the cost ranking of the different sewage designs.
For experiment number one, two different fine levels were set and then run through our mathcad system in order to rank the sewage plant designs in order from cheapest to most expensive per person to build and maintain.
In the above table the failure rate is highest for design one of the sewage treatment plants. This is a way of indicating that design one is least efficient in removing bacteria from sewage waste water. The higher the bacteria levels in the incoming waste water the more it will drive the D.O. level down, increasing the number of days of failure. When comparing the cost/ person and rank the table must be broken down further into A and B sections. In section A, where the fine for failure to meet the standard 6ppm D.O. level is five thousand dollars, we find design one is least expensive, and design four most expensive. This is due to design four being more expensive to build then the less efficient design one. The cost of the fines for the increased number of days for design one, is not enough to outweigh the cost of the more expensive plant design. In section B, of Table 2, the cost / person is more for design one, then design four, opposite from section A. In this case with the fine being extremely high at five million dollars, the cost of the more expensive plant, design four, is much less than the cost of paying the fine more often, in the case of the high failure rate of designs one.
Another variable viewed in the lab experiment was the effect of lowering the minimum D.O. level from 6ppm to 4ppm. Table 3, shows the results.
In table 3, lowering the minimum standard ppm from 6 to 4 ppm has a reversal effect on the ranking of the sewage treatment designs. Lowering the minimum standard ppm lowers the impact that the sewage treatment plants have to counteract. With lower standards less must be put into the efficiency of the plants to meet the limit. This lower limit is most noticeable when comparing the failure rates of similar designs in section A, and section B. The lower standard limit lowers the number of failure days of each design. Special note is added to design four in the 4ppm standard section where the efficiency of the sewage treatment plant, allows the plant to have a failure rate of zero.
The other two experiments; wet and dry season, as well as a dam, and dam removal, are not vital to my discussion so the results of those experiments will not be explained at this time.
The overall focus of this experiment is to discuss whether or not substantial fines are necessary to protect the ecological values of our rivers and our systems. In my opinion substantial fines are necessary to protect the ecological value of our river systems. Substantial fines are needed to force sewage treatment plants to take accountability for their waste deposits, and also for the members of the city to provide funding for the best water treatment possible.
When looking at the results from the fines experiment, when fines are low (five thousand dollars) the cost per person is lowest if the least effective treatment plant is built. When determining the cost per person, two values must be assessed. The first value is the basic cost of the treatment power plant, which is dependent on how efficient it is. The second cost is the cost of the fines given to the treatment plant for failure to meet the set standards of nutrient levels, which effect B.O.D. In the case where the fine amount is low, the less efficient plant is still cheaper to build then the more efficient plant, even though it is fined nearly four times as much. The price of the more efficient treatment plant (design four), even though it has a failure rate of .258, compared to the .992 of design one, is more expensive to build. In ranking the designs by cost per person, we are saying the determining factor for how well we care for the river system, is the cost per person of the treatment plant. This is not necessarily the best way to rank treatment plants, if caring for the river ecology is one of our main goals. Luckily, we can change the severity of the fine, and force the city to choose a more ecological friendly choice. As seen in the results, when the fine amount for minimum dissolved oxygen failure is increased to five million dollars the ranking of sewage treatment plant designs is reversed. In this case the high percentage of fines of design one, outweighs the high initial cost of the more effective design four. Although the cost per person is higher in every category then when the fine was five thousand dollars, when ranked by cost per person, the most efficient treatment plant is chosen, and our rivers will be cared for much better. This portrays when ranking treatment designs strictly by cost per person when considering installation, the environment takes a back seat to money in the pockets of the city members. But in making the most environmentally sound choice the most affordable, through heavy fines, doing what is right for the environment can be chosen through this ranking system.
It should be noted that smaller fines are not always bad. In this system of the city, treatment plants, and the river ecosystem, lower fines have benefits for the city. A lower fine means less cost per person in the city, which is always a good thing. In this particular example, however, we are pinning what is good for the pocket books of the people directly against what is good for the environment.
The other experiment that shows how environmental factors take a back seat to money in the city members pockets is the experiment done adjusting the minimum dissolved oxygen ppm level. Although the ranking of the different treatment design systems stay the same for both experimental findings, interest should be taken in the costs. When the minimum ppm level is dropped to 4ppm, the costs to meet this standard are dramatically lowered, when compared to the 6ppm minimum standard costs. In the real world, industries will always do just enough to stay above the failure rate. Even if dropping the minimum standard to 4ppm, instead of 6ppm, would have drastic effects on the river ecosystem, the treatment industry would not care, because it is cheaper to meet the minimum standards for 4ppm, then 6ppm levels.
The ranking system in this group of experiments always placed more emphasis on the price per person, then the consequences to the environment. Sadly, this is the way that the real world works as well. If tables were made ranking different situations by their total negative effect on the environment, many findings from this lab would be turned completely around. However, in what may seem like a system setting up an environmental crisis, there are ways to set standards for environmental care. If we set fines or consequences, on environmental harm, higher than the amount saved by doing the cheapest design, then the cost per person ranking system will pick the most environmentally friendly choice. In this aspect high fines are a necessity for preserving our river ecosystems, and our environment in general. Finding the right amount to fine is another story. In the above experiment, it is obvious that a five million dollar fine would never work in the real world. As long as we keep the combined fine amount above the overall cost difference between environmentally friendly choices and those that may harm the environment, we will be able to preserve our fragile ecosystems, in a world where everyone is looking for the cheapest alternative.

The Florida Panther, Puma puma concolor , is a large predatory cat which has an average length of around 7 ft. including the tail. Females weigh in general 35-45 kgs, while males are larger by about 15-20 kgs. The coat is generally dark brown, with light spots of darker brown, almost black markings. Small patches of white hair can be seen on front section of the body (head, and shoulders). Florida panthers used to have a range that included all of the Southeastern United States. Due to human interaction and development, range has been reduced to small isolated areas in southern Florida. The type of habitat that Florida panthers usually seek is a combination of forests and swamps, with heavy vegetation. A low reproduction rate is one of the reasons that Florida Panther is in need of conservation. With adults not becoming reproductively active until the second or third year of life, and parental care lasting one and a half years, reproduction rates are not fast enough to keep the present populations stable. With the long parental care time period, this forces female to breed only every other year, further decreasing the reproductive rate. The Florida Panthers have been on the endanger species list since 1973. Along with habitat reduction and fragmentation, other human impacts include hunting, and killing the panthers as to protect livestock. (1)
Florida panthers genarlly live to be 3-5 years old, in the wild. For our simulation we only accounted for age classes of newborns, first year, second year, third year, and four year individuals. There was a record for five year old individuals, however in our simulation, no individuals ever survived to an age of five.
With these numerous problems facing the declining populations of the Florida panther, action must be taken if we don't want to lose this vital species. Many different conservation techniques could be used in order to try and strengthen the populations of the Florida panther. One method would be to try and connected the isolated populations of the Florida panther populations, in order to gain more cross breeding between sub populations, as well as give the panther more room to hunt, and gain less interaction with humans. This method would work, however given the tight hold that development has in Southern Florida, converting land back to panther habitat is not very likely.
Another method would be to bring in panthers from other regions to increase the number of panthers in Southern Florida. This would also increase the genetic diversity of the populations of the Florida panther, and strengthen the populations. As with the first method, importing panthers also has a downfall. Bringing in other non-Florida panthers would remove the pure Florida panther sub species, once outside genes were brought into the population through breeding. Losing the pure Florida panther species is a concern to many.
In order to have these methods work more effectively in helping the Florida panther, we must understand it's life history and how factors affect the population structure and stability. The life history of the Florida panther, for this particular lab, can be summarized in figure 1 below.
In figure 1, the values along the bottom are the survival rates of one year of age from the previous year (Px). For example the 0.8 between 1 and 2, show that the probability that a one year old panther will survive to be two years old is 80%. The curved lives near the top of the figure are the fecundity of each age class within the panther population (Fx). These numbers show how many offspring will be produced from that specific age class, per individual of that age class. For example the curved line from circle 3 to circle 0, shows that for every panther that is 3 years of age, 0.38 newborns will be produced.
We can combine this information with the Florida panther's life history traits, to better understand how to help increase the population. The Florida panther is a k-selected species, in that it is slow to evolve to changes in its environment, due to slow reproduction rates. As mentioned above, the slow breeding rates, and long term parental care, are two key examples of a k-selected species.
In the lab we used a Leslie Matrix to see how exactly changes in life history traits of the Florida panther will affect the populations stability. Using the numbers presented in figure one, we constructed the Leslie Matrix seen in table 1.
The Leslie matrix was generated from the age, lx, mx, Px, and Fx values given to us at the beginning of the lab. The vector for this lab was set at each age group beginning with 25 indivduals.
Using the present matrix as basis analysis was run and found values for r (rate of population increase) Ro (expected number of replacements) and T (generation time.)
Along with a basic analysis, a projection was also run. The projection was used to see how the population as a whole, and individual age classes would fluctuate during the next ten years. Each age class began with 25 individuals, and any numeral value under one, was considered to be extinct / removed from the population.
Elasticity was also calculated using the values generated by the Leslie matrix. Elasticity, which is a values relative impact on the overall system, was calculated for each age class survivability (Px) and fecundity (Fx). The higher the elasticity of a value the more impact it will have on the entire system.
The original Leslie matrix was used to generate basic analysis, projection, and elasticity values. Next the matrix was changed in one area, in this case P1, and the analysis was run again. For a third and final time, more numerical values were changed in the matrix, and the outcomes were viewed and compared to the first two trials.
In using the original Leslie matrix (Table 1) we calculated the r value to be -0.37012, a Ro value of 0.353, and a T value of 2.8133. The projection for the population, starting with 25 individuals of each age group, found the population unstable. After ten years, no single age group had a concentration over 1, which indicated that all age groups, and therefore the population was extinct. The elasticity of the first analysis found the P1 and P2 values most elastic at values of .34 and .23.
For our second analysis, the P1 value was changed from 0.5, to 0.8. For a real life example, this would mean that the survivability of newborns to first years had a slight increase. This small alteration of the Leslie matrix had an effect on the analysis, projection and elasticity, but was not significant. The r value was still negative at -0.206, the Ro value was 0.56, and the T value was decrease to 2.77. The elasticity was still greatest for the P1 and P2 values, at .35 and .22. The projection analysis was found to have higher numbers after the ten year period; however, the population was still unstable. As seen in table 2.
After ten years the values are all above one, which indicates that individuals in all of the age classes are still alive. However, the values are not the same as the starting values, so the population is not stable.
In the final analysis, a number of changes were made to the Leslie matrix. The F1 and F2 values were increased to 1.2, and the F3 value was increased to 0.8. The P2 value was increased to 1, the P3 value was increased to 1.2, and finally, the P4 value was increased to 0.8. These changes had a dramatic effect on the outcomes of the basic analysis, projection and the elasticity of the model.
In the basic analysis the r value had increased to a positive value of 0.18, the Ro value had increased to a value of 1.68, and the T value had increased to 2.87. In the projection the final population dynamics after ten years were found to stable, and growing at an accelerated rate. Beginning with 25 individuals in each age class, this number was increased to 308 individuals for newborns, 130 individuals for one year olds, 107 individuals for two year olds, 109 individuals for three year olds, and over 70 four year old panthers were present in the population. The elasticity had minimal change. The elasticity of P1 was 0.35, P2 was found to be 0.21, and the elasticity for the fecundity values were all between 0.14 and 0.82.
In the first two trials the population was found to be unstable, simply because the starting age structure was not represented after the ten year projection. After ten years, the entire population in the first trial was extinct. In the second trial, with the increased P1 value, after ten years, individuals of each age class were still present, but in lower numbers. The drop in the numbers in individuals in every age class suggests that the population is unstable, and will become extinct eventually; it will just take longer than the ten year projection. It was interesting to see how such a small adjustment in the life history make-up of the Florida panther could have such an effect on the population dynamics.
In the third trial, the small increases in the Leslie matrix, had a huge affect on the population dynamics. The small changes made to the Px and Fx values, allowed the population to increase to nearly exponential levels. With the adjustments made to the matrix we overshot the goal of attaining a stable population.
With one of our trials falling short of a stable population, and the final trial overshooting a stable population, we have a general idea of where the life history trait values need to be to achive a stable population. It would be only a matter of time, and trials, before a stable population could be simulated, using the Leslie matrix. Once survival and fecundity values of a stable population were found, then this would give us values to achieve in the real population. We can influence the fecundity and survivorship values of a population by using conservation techniques, to help raise the values, to support a stable population. This is how we can use the Leslie matrix, and other tools, to help us conserve populations of endangered species to help preserve the fragile populations.
If I had sole control over how to save the Florida panther, I would try and connect the isolated populations, in order to achieve one, larger, more diverse, population. I realize that this would take a huge cooperation with land owners in Southern Florida, and would definitely decrease profits for industries, and reduce the amount of land available for development. As well as turn massive amounts of developed land, back into useable panther habitat. In increasing the habitat available, and increasing the gene exchange between populations, a stronger single population will be built, and will have a much better chance of being stable. This tactic would also keep the Florida panther sub species a pure species, by not bringing in individuals from other sub species of panther to aid the failing populations of the Florida panther.

Carbon dioxide is a green house gas produced by the burning of fossil fuels. Since the industrial revolution the amount of CO2 produced by human action has increased the amount of atmospheric carbon to levels never seen before. With the increase of atmospheric carbon global warming has occurred due to thicker atmosphere not allowing radiation to escape as easily as when less carbon was present. The carbon cycle allows carbon to be transported between the atmosphere, Land and shallow oceans, and the deep oceans. In a simplified carbon model, used for this experiment, several inputs and outputs were used. Industrial emissions are the main current source for placing carbon in the atmosphere, mainly in the form of CO2. The atmosphere on average holds about 735 Gt (giga tons) of carbon. Through the processes of fixation and respiration carbon can be exchanged between the atmosphere and the earth's surface (land and shallow oceans). Fixation is the removal of carbon from the atmosphere to the earth's surface usually stored in organic forms, trees, plants, animals etc... Respiration is the release of carbon from the earth's surface to the atmosphere through the biologic processes of burning of organic matter and breathing. Another exchange occurs between the shallow oceans (which include the carbon found on land in this simplified model) and the deep ocean. Around 781 Gt of carbon can be stored in the shallow oceans, compared to the massive 19,230 Gt's that can be stored in the deep ocean. Downwelling and upwelling are the two major exchange processes that occur between the shallow and deep ocean. A minor amount of carbon is deposited in the deep ocean floor, but the small amount is not a primary focus of this experiment. This simple model is the bases for the MathCAD program used.
Carbon sequestration is the process by which carbon is moved by the cycle and stored within the deep ocean. The deep ocean has a large capacity to hold more carbon than any other part of the carbon cycle. With the increase of carbon in the atmosphere leading to global warming, it is in our best interests to store carbon in a place where it will not have such a strong effect on our earth's climate. Carbon sequestration appears to be a reasonable way to remove the carbon from the atmosphere and aid in lessening the effects of global climate change the earth will be facing in the future. In looking at the variables we can use to move the bulk of the atmospheric carbon to the deep oceans we have fixation, downwelling and deposition. Going against the movement of carbon from the atmosphere to the deep oceans are respiration, upwelling and emissions. Controlling emissions has been a main concern ever since we have realized the problems that are caused by the increased CO2 in the atmosphere. Special organizations and regulations such as the Kyoto protocol (Kyoto) have been initiated in trying to manage the amount of CO2 emissions produced by industry. To return the global carbon balance back to how it was prior to the increase in CO2 emissions, humans would have to cut carbon emissions by 80% (Agrawal). A stabilization point where we are producing the maximum amount of carbon the natural cycle can handle was also found. This value would be around a concentration of 500 ppm of carbon in the atmosphere. Due to the difficulty in the market and political world of cutting emissions, carbon sequestration is viewed as a way to fight global warming without reducing emissions to their lowest levels. MathCad was used to project what the levels of carbon would be in the atmosphere when different degrees of emission controls were used. Also the sensitivity of the atmospheric CO2 levels to the different carbon transporting variables was viewed as to how they function in the global carbon cycle.
In the first section of the lab experiment, emission levels were changed and CO2 levels were graphed and recorded for each emission change. The time scale was also varied throughout the experiment. Time ranged from a span of 340 years to up to 1000 years time, to view the reaction of atmospheric CO2 levels to the change in the level of emissions. The second section on the experiment focused on the sensitivity of atmospheric CO2 to different processes. Five variables were altered in this experiment, fixation, upwelling, respiration, downwelling and deposition. The five variables were altered either by decreasing by 50% or by doubling the rate at which they are able to move carbon through the global carbon cycle. The response of the concentration of atmospheric CO2 was recorded after each individual variable was changed. Each variable was changed independently of all other variables, in order to record the direct effect each variable had on the atmospheric concentration of CO2. The mathCAD program was based on a simple equation, that resembled the simple global carbon cycle mentioned in the introduction. MathCAD took the five variables, as well as emissions and combined them in groups as they effect the three main areas of carbon storage (atmosphere (At), land / shallow oceans (St), and deep oceans (Dt).) The equation can be seen as Figure 1.
When changes were made to the amount of emissions produced overtime the level of atmospheric CO2 in Giga tons of carbon were graphed overtime. In the most complex example, emissions prior to the year 2100 were set at 20 GtC (giga ton of carbon) and emissions after 2100 were set at 10 GtC. The results showed a steady increase in giga tons of carbon until the year 2100, then after the emissions were reduced to 10 ppm??? the amount of carbon in the atmosphere was maintained at around 1560 Gts. The concentration of atmospheric CO2 followed a similar trend. This trend occurred over a 340 year span. The CO2 concentration increased from 280ppm to about 840ppm prior to the year 2100. After the year 2100, the CO2 concentration held constant at about 820ppm (Figure 2).
Figure 2 can be compared to figure 3 which shows the same data when emissions were set to zero. In figure 3, the tons of carbon in the atmosphere fall steadily to zero, and the CO2 concentration holds constant at about 280ppm during the entire time span.
For the second section of the lab experiment the sensitivity of atmospheric CO2 concentration were measured as a result of independently changing each of the five emission variables. The CO2 concentrations were measured for two specific time periods. The first set of sensitivity was measured at the year 2200, the second set was measured to find the maximum concentration the CO2 would reach over the 1995 – 2335 time period. The results followed general increasing or decreasing trends, when considering which variable was manipulated. When variables that work to remove carbon from the atmosphere such as; fixation, downwelling, and deposition, were decreased by 50% the concentrations of CO2 were found to increase in the atmosphere. Counter to that point when the variables that increase the amount of carbon in the atmosphere such as; respiration, upwelling and emissions, were decreased by 50%, the CO2 concentration in the atmosphere declined. All results can be seen in Table 1.
It was also found that if emissions were cut to 25% of original levels then the atmospheric CO2 concentrations were found to be 498ppm for year 2200, and 631ppm for the maximum over the 340 year period. When fixation was increased 3 fold, the 2200 CO2 concentration was found to be 460ppm, and the maximum for the 340 year period was 660. Also if respiration was reduced to 25% of original level the CO2 concentration was found to be 615ppm for year 2200, and a CO2 concentration of 895 was found as the maximum over the 340 year span.
In terms of setting a optimal CO2 emission limit, I feel that finding the balance point at which the natural carbon cycle would be able to cycle all our emissions would be the ideal amount. This would be more profitable then working to reduce CO2 emissions to zero, and would also allow the natural process of carbon cycling to not be overwhelmed with the amount of carbon released by our emissions. To reduce emission rates our way of life must change. Our current emission rates if not controlled and reduced will have us facing devastating effects of global climate change. Ways to reduce our overall emissions include standards and regulations for industrial companies and engineering more fuel efficient vehicles. Simple life changes such as using a programmable thermostat or turning off unused lights would also greatly reduce human carbon emissions.
When looking at using carbon sequestration to reduce the amount of carbon in the atmosphere, the data suggests that it could be done by adjusting the rates of the variables that effect how the carbon moves through the cycle. The costs and energy needed to for example increase the fixation rate, should then be compared to the costs and energy needed to reduce emissions to reach the same CO2 concentration in the atmosphere. The overall amounts of carbon that is exchanged between the atmosphere, shallow oceans, and deep ocean is very small when compared to the amounts of carbon that is held in each area, especially when considering the deep ocean. This suggests that the large amount of CO2 we would want to remove from the atmosphere by sequestration would take an extremely long time. It would be much more effective to focus our attention and energy on reducing carbon emissions. This would have a more direct effect on the amount of CO2 in the atmosphere. Controlling CO2 emissions would also allow us to engineer greater technologies to help us continue to reduce emissions for the future, rather than just shifting the bulk of the carbon to the oceans from the atmosphere.
The effects of carbon sequestration have not been thoroughly studied. How the oceans will react to the increased amount of carbon are greatly unknown. From a human standpoint it seems that we want to use carbon sequestration as a way to put the CO2 and excessive carbon out of sight, and out of mind. With CO2 emissions we didn't worry about the affects of putting excess carbon in to the atmosphere until we started to see complications arise as in global warming. Moving excessive carbon from the atmosphere ultimately to the deep oceans will be difficult and the results are not certain. Focus should be placed on holding more carbon in fixation in forests and plant life on land and coastal areas, as well as reducing overall emissions. Carbon sequestration though it would reduce the amount of CO2 in the atmosphere seems like just sweeping dirt under the rug. We know the carbon is still there, we are not worried about it currently, but at some point we will have to deal with it.

Atmospheric concentrations of greenhouse gasses (GHGs) have been steadily rising since the start of the Industrial Revolution due to anthropogenic emissions. This has caused global mean temperature to rise by an estimated .5 degrees Celsius, with some regions experiencing much higher increases than the average (Agarwal, 2008). Higher temperatures have ushered in rising sea levels, more erratic weather patterns, and adverse human health impacts. This trend is difficult to reverse because human emissions are a net addition to the carbon cycle; they represent an additional input to the global carbon balance which has led to ever-increasing net storage. Furthermore, GHGs persist in the atmosphere for long periods of time. In order to reduce atmospheric concentrations of GHGs, either the inputs (primarily emissions) must be lowered or the outputs (primarily carbon sequestration in forests) must be raised. This paper looks at the relative efficacy of different approaches to reducing the concentration of carbon dioxide, and concludes that a combination of measures, including both emissions reduction and sequestration, is needed to achieve stability.
I first performed background research using lecture notes and existing literature to determine how climate change works, what the impacts of climate change are, how carbon dioxide is cycled through the oceans and atmosphere, and how carbon dioxide concentrations have changed over time. Next, I modeled these flows using the upwelling-diffusion climate model from the Intergovernmental Panel on Climate Change (Harvey et. al., 1997). This model is shown in Figure 1. There are three "storage" compartments for carbon dioxide: the atmosphere, land and shallow oceans, and deep oceans. The IPCC model identifies the input rates and output rates from each of these storage boxes. For example, the outputs from land and shallow oceans are downwelling into deep oceans, at a rate of .11S gigatons per year, and respiration up into the atmosphere, at .27S per year.
Using this model, I simulated future scenarios using IS92 data as my baseline. In these scenarios, I demonstrated the impact of halving and doubling each of the six flows (emissions, fixation, respiration, downwelling, upwelling, and deposition), while holding the other flows constant. The model produced future projections for atmospheric carbon dioxide concentrations based on these parameters for the year 2200 and the maximum level. The output from this scenario modeling allowed me to determine the impact of each "lever" on achieving carbon reductions. In my analysis, I assumed that sequestration is equivalent to fixation (as it represents the earth's ability to sequester carbon in vegetation). However, one could also consider man-made carbon sequestration and storage (CCS) techniques in devising a practical, comprehensive strategy to reduce GHG concentrations (for instance, the possibility of trapping emissions from coal-fired power plants and burying them underground). I also applied learnings from my literature review to assess which levers could in fact be altered and by how much.
The results of the scenario modeling show that the most effective lever in altering atmospheric carbon dioxide levels is emissions: the range in carbon dioxide concentrations that result from halving or doubling emissions is the highest, at 1,229.9 ppm in year 2200 or 2,072.5 ppm maximum. Fixation is a close second, with a range of 1,028.2 ppm in year 2200 or 1,684.6 ppm maximum. Respiration is still somewhat effective, while downwelling, upwelling, and deposition yield smaller and smaller changes in storage. However, note than even a halving of current emissions still leads to atmospheric concentrations of 702.9 ppm CO2 by the year 2200 – this is twice the level deemed "safe" by Hansen and his team of scientists! Doubling fixation also does not achieve our goal, reaching only 653.2 ppm CO2.
Furthermore, this simple model omits several complicating variables that would have to be considered in crafting a realistic response to climate change. Scientists still don't fully understand the whole suite of "interactions between the terrestrial biosphere and climate"( Harvey et. al., 1997). For instance, what will the impact of melting permafrost be on methane emissions? Additionally, cooling mechanisms such as aerosols and cloud cover are poorly understood and not reflected in the model above. Indeed, earth scientists are still struggling to create and run models which incorporate the true complexities of climate change.
Carbon sequestration and reduced emissions are the most viable strategies for reducing atmospheric concentrations of GHGs. But while changes in emissions have a slightly greater effect on carbon dioxide storage than fixation, neither one alone can reduce atmospheric concentrations to a level low enough to avoid runaway temperature increases resulting from the initiation of positive feedback loops. Earth scientists have yet to derive completely accurate models, but it is clear that a multi-pronged strategy is needed to curb – and hopefully reverse – the accumulation of these gasses in the atmosphere. Intensive reforestation measures must be coupled with rigorous efforts to reduce anthropogenic emissions of GHGs.

Estuaries and other coastal ecosystems represent interfaces between terrestrial and marine worlds, and an important role they play is in the assimilation of nutrients such as nitrogen (N) and phosphorus (P) that enter the water from the land. Growth of plankton for example, preyed upon by benthic filter feeders like oysters, or the growth of sea grasses grazed upon by larger aquatic animals are portions of some of the chains by which nutrients are taken into estuarine ecosystems (Jackson et al., 2001). These ecosystems typically are comprised of multiple trophic levels, any or all of which may be affected by environmental changes. The goal of the current study is to look at the relative importance these disturbances, grouped very broadly into bottom-up (meaning changes to nutrient inputs) and top-down (meaning changes to the relative presence of upper trophic levels due, for example, to changes in harvesting rates) perturbations may have across different aquatic ecosystems.
Urban development in areas surrounding coastal ecosystems has been accompanied by changes in ecosystem structure. A commonly reported estuarine malady is an increase in the frequency and severity of seasonal hypoxic and anoxic conditions, associated with increased levels of algae and phytoplankton in the system (Jackson et al., 2001; Brawley et al., 2000). These events are often explained as being caused proximately by increased anthropogenic contributions to estuarine nutrient loads and have triggered significant effort into the modeling of nutrient inputs to estuaries from groundwater, point and non-point source runoff into estuarine feeders. The goal of such work is to determine estimates of critical nutrient load, or loading targets to guide agricultural and land-use policy (Brawley et al., 2000; Cerco et al., 1995). However, it is important to recognize that increased runoff and nutrient input is not the only anthropogenic influence on coastal ecosystems. Habitat destruction and fishing practices have affected the ability of macrofauna, like the oysters, to exert control over the structure of their ecosystems. The oyster reefs of the Chesapeake Bay were once capable of filtering the entire water column in only a few days, and it is only since exhaustive dredging of the bay led to the collapse of the oyster fishery that hypoxia and anoxia begin to be observed (Jackson et al., 2001). Jackson et al. discuss this and other correlations of ecosystem decline with destructive fishing practices, making it clear that comprehensive attempts to curb hypoxic and anoxic events via policy should consider the potential for both bottom-up and top-down effects to be significant in a target system.
Whether looking at top-down or bottom-up control, or both, a difficulty in modeling all estuarine systems is that the term "estuary" in itself is a fairly loose categorization of bodies where freshwater and marine systems meet. Estuaries may differ greatly in, among other factors, the structure of their biological communities, the distribution of nutrient and chemical inputs, and their basic physical structure (NRC), which in turn affects another important factor in estuarine heterogeneity-hydrodynamics. Both the volumes of river and tidal flow into the estuary, and the way that they flow into and out of the estuary, will affect the degree of mixing and the salinity gradient, an important characteristic of a freshwater-marine interface (NRC). Mixing characteristics of flows entering an estuary determine the dilution of nutrients entering with the flow, and flow levels in and out of the estuary, in concert with mixing, control the residence time.
In short, estuaries can be nearly as different as they are similar, presenting a difficult problem in ecosystem management. Management tools must be somewhat general in order to be useful over a wide enough range, but must also describe systems well enough to give meaningful results. In the case of estuaries, this has proven to be a difficult balance to achieve. Sophisticated three-dimensional hydrodynamic models have captured many of the flow and salinity characteristics of specific estuaries (Chau et al., 2001; Cugier et al., 2002), but the calibration and use of these models is resource intensive. In contrast, simple mechanistic box models that explain well some important processes in estuarine ecosystems have been around for decades (Kremer et al., 1982), but lack the spatially explicit resolution needed to make judgments. An open question in estuarine ecosystem modeling then, is "how general can the model be before it is no longer meaningful?" This question frames the second objective of the current investigation.
Estuarine modeling is at this time a rich and well-developed field but there does not yet exist significant work analyzing the relative importance of both bottom-up and top-down anthropogenic influences on ecosystem conditions. The long-term goal of the current study is to contribute to this work by comparing these effects across several ecosystems.
The immediate deliverable within the time constraints of the course project is a modeling framework linking nutrient input, anthropogenic harvesting and habitat-destructive effects to planktonic and macrobiotic growth in the ecosystem. This simple module will form the ecosystem component of larger, spatially explicit simulations that examine entire ecosystems. Within the reduced scope of the course project, the questions framing the overall study collapse then, to "can top-down and bottom-up effects be observed in physical subcomponents of an estuarine ecosystem?" The first target system for this study, and the system that is the scope of the course project, will be the Chesapeake Bay.
Already an extensively studied ecosystem, model results and historical data looking at estuary nutrient loading (Brawley et al., 2000; Cerco et al., 1995) and macrofauna (Miller, 2003) in the Chesapeake Bay exist as a backdrop against which to compare the results of this integrated study. In particular, Ulanowicz et al. have assembled box models of the mesohaline reach of the Chesapeake Bay that provide an excellent initial comparison for model results.
The Chesapeake Bay is a deep estuary, dominated by plankton growth and possessing a long residence time (NRC). A large share of phytoplankton production fuels zooplankton growth, which in turn are preyed upon by ctenophores and sea nettles, a seasonally-variant process peaking in summer, and by fishes such as the striped bass (Baird et al., 1989). Remaining phytoplankton may sink through the water column where it feeds several commercially important benthic macrofauna, namely oysters and blue crabs, which are thought to have significant roles in benthic community structure.
Seasonal variations in light and nutrient availability result in annual cycles of the ecosystem (Baird et al., 1989). While these should be incorporated into the simulation within the larger context of this study, for the purposes of this project only a single season (summer) is used for data input.
GeiLoVe utilizes simple Lotka-Volterra dynamics, which allow for four different types of fluxes into, out of, or between model pools (Ulanowicz et al. 1992):
While simple in their treatment of food web interactions, Lotka-Volterra dynamics are simple to solve, as there exists only a single unknown coefficient for each new flux to the system. A single snapshot of the food web fluxes and average pool values over some interval is all that is required for input data.
GeiLoVe reads all pool and flux values from an input file (Appendix A) and calculates the model coefficients by simple algebra. A mass balance is then calculated for each pool to capture any mass imbalance in the input data. GeiLoVe then performs a simple forward time step of the form:
where M is the vector of pool biomasses, dM/dt is the accumulation of biomass over the interval dt, and E is the vector of mass imbalances arising from the model input. Summing the mass contributions to each pool in the current interval based on the calculated Lotka-Volterra coefficients and the values Mt generates the vector dM/dt. The calculations required for this time-step are in no way computationally limiting, and there is no need to sacrifice ease-of-coding for more computational efficiency.
GeiLoVe was applied to a set of season-average summer food web data for the mesohaline reach of the Chesapeake Bay (Appendix A). Dr. Robert Ulanowicz of the Chesapeake Biological Laboratory provided the raw data, as well as the software AGG.EXE used to aggregate the large number of pools into the 14 boxes used in this study. The boxes in the aggregated food web model are labeled as follows:
These 14 boxes form a moderately complex food web from which a variety of aquatic species are harvested (Figure 1).
As a first check of the model, a non-perturbed system should remain at steady state for any length of time t. The constant mass profiles over the period of about 5000 days, generated in MATLAB from the model outputs, show this to be true (Figure 2).
Stepping through the model in this way serves to check that mass is being conserved in the model, beyond the error allowed in the input data.
The input data for the summer season has a single fixed input to box 13 (Suspended Nitrogen), and as a first test of GeiLoVe's performance this input was manipulated to 110% of the original input value (Figure 3).
The most striking result of this manipulation is that a number of food web members, such as the zooplankton and all fish species, have gone extinct within 1000 days in the run. Without needing to compare explicitly to experimental data, it is fairly clear that this is a specious result, an artifact of the simplicity with which the food web is modeled and the lack of the myriad negative feedback processes that in a real food web might serve to dampen the effect of perturbances on the system.
A rudimentary attempt at recreating some of these stabilizing bounds might be to assume that a given perturbation in any parameter should not move any state variable beyond a certain fraction F of its initial value. Such an approach compromises conservation of mass, but at least allows stocks whose prey pools would otherwise have speciously dropped to 0, a chance to respond to these dynamic changes. As a test, the same run was repeated with F = 0.2; that is, no state variable may drop below 80% of its initial value given the perturbation of 10% in the input nutrient flow (Figure 3).
The effect of these bounds is immediately obvious. While some species, such as the attached bacteria and suspension-feeding fish, still tend as much toward extinction as possible, others have rebounded (Figure 4). At least two cyclic patterns can be seen. One includes the zooplankton, the sediment feeders, deposit feeders, and others, and exhibits three peaks between day 1000 and the end of the model run. The second pattern is the predator-prey lag relationship seen between the carnivorous fish and the benthic fish. This cycle exhibits a distinctly different period than that observed in the first pattern.
Looking now to effects from the top down, the parameter for harvest (export) of carnivorous fish was dropped to 99% of its original value with all other parameters retaining their original values (Figure 5).
Since intuitively, a decrease in fish harvests (i.e., a decrease in human impact) should restore stability to the food web, the mass extinctions that follow may seem surprising. Again, the limitations of the simple dynamics used in the model are exposed. A check was performed using the same bounding assumption as before with F = 0.2 to see the effect on the food web (Figure 6). However, in this case the application of bounds did little to preserve food web dynamics, and the system appears to be headed toward an invariant steady state beyond 2600 days of the model run. Noting that the carnivorous fish pool is small (on the order of 4mg/m2) compared with lower trophic levels such as phytoplankton (on the order of 400mg/m2), it is perhaps not appropriate to use it as a perturbation variable in the noisy context of the Lotka-Volterra dynamics.
There are other commercially important species in the Chesapeake Bay, such as crabs and oysters. These are included in the input to GeiLoVe as the deposit feeder and suspension feeder boxes, respectively. A manipulation of the deposit feeder harvest coefficient to 90% of its original value (with a bounding factor F = 0.2) yields an interesting dynamic solution (Figure 7), and so the deposit feeder pool is the first target for a response surface analysis.
The harvest coefficient for deposit feeders was varied from 85% to 115% of its original value, and that for the nutrient load was varied from 85% to 115% of its original value to generate a multivariate response surface (Figure 8 A and B). Since in many cases the food web settled into stable periodic dynamics rather than a steady state, both the minimum and maximum observed in the last 25% of the run were plotted in order to more fully represent the state of the system. In general, the periodic dynamics occurred when nutrient loading was high, whereas the system reached a stable steady state when nutrient loading was kept low (Figure 8 B). Since the results in 5.1 to 5.4 indicated that a stable steady state was linked to an extinction of many of the food webs, it should be noted that this may be true for many (or all) of the steady state cases shown in the response surface.
The response surface reveals that when nutrient loading is high and harvests are low, the deposit feeder stock is quite high (Figure 8 A). In contrast, when harvests are high and nutrient loading is low, the stock is low. These results have intuitive appeal. The shape of the response surface is also important, in that it has a significant slope along both axes of perturbation. Taken together, these results suggest that in the case of deposit feeders, perturbations to nutrient load as well as harvests can act as independent control knobs on the simulation, with their effects being discernable within the simple model framework.
The response of the suspended nitrogen pool to the same set of perturbations was also generated (Figure 9 A and B). It is clear that when harvests of deposit feeders are high, the level of nutrient in the system is also high (Figure 9 B), supporting the intuitive notion that upper trophic levels have a strong influence on the ability of the food web to take up nutrients.
In the same way as for the deposit feeders, a set of response curves showing the responses of suspension feeders (Figure 10 A and B) and suspended nitrogen (Figure 11 A and B) to perturbations in suspension feeder harvest and nitrogen loading were developed.
The suspension feeder stock responded to perturbations in nitrogen load in a very similar way to that of the deposit feeders, and the suspended nitrogen response was similar to that in the previous case. However, both pools appeared relatively insensitive to changes in suspension feeder harvest over the range of analysis.
Since it is expected that when the relative perturbations for both variables are 1, the stock response is also 1, it may seem surprising that this point does not appear on any of the response surfaces. The reason for this is that the due to grid spacing (a total of 50 grid points over the range 0.85 to 1.15), the point shown as 1.00 is actually 0.997. Noting that the system is further constrained by the bounding factor F = 0.2, which in many cases pulls the pool values away from their unconstrained state, it is not surprising that pool values for systems even close to the unperturbed state will be very different.
Ulanowicz et al. (1992) used a set of analogously derived data using a carbon basis to look at the effect of oyster harvest on the same mesohaline reach of the Chesapeake Bay. Their study also made use of Lotka-Volterra dynamics; in fact, much of the method applied in the current study used this Ulanowicz study as a basis.
The Ulanowicz study used a 13-box model of the system, one of which modeled oyster biomass. They derived the Lotka-Volterra coefficients defining the food web model in the same way as defined in this paper, and then proceeded to decrease the coefficient defining oyster export from the system in increments of 1%, allowing the system to solve to a steady state at each step.
One key difference in methodology is that they used an iterative Newton zero-finding approach to find the time-invariant solution (i.e., dM/dt = 0), and used that solution as the steady state value. It is clear that such an approach would not have solved satisfactorily in most of the cases in the current study – periodic dynamics are not time-invariant and thus there is no zero to be found. The forward time-step method applied in the current study is more appropriate to this type of behavior.
The Ulanowicz study also observed what was regarded as specious extinction of a pool, though not to the extent observed in the current work. The major finding of the modeling study was that a XX% reduction in oyster harvest (as a proportion of the standing stock) led to an overall higher catch volume. This is consistent with the findings of this study that low harvests of deposit feeders, and to a lesser extent harvests of suspension feeders (oysters) led to overall higher stocks (Figures 7 and 9).
Lotka-Volterra dynamics alone are not sufficient to reproduce the food web dynamics of the Chesapeake Bay, although it seems that an augmentation of these simple dynamics may help. One possibility would be to allow the Lotka-Volterra flux coefficients to vary as functions of the biomass pools they connect to. As prey grow scarce, they become harder for predators to find, which in the Lotka-Volterra framework can be interpreted as a decrease in the value of the coefficient for a feeding flux. This semi-mechanistic approach may help to dampen the wild oscillations in the current system that contribute to extinctions of upper trophic levels, and should be a target of future work.
While the current model was unable to adequately simulate effects of perturbations to exports of the top trophic levels in the food web, a two-factor response surface showed clear and distinct responses to perturbations in nitrogen input and deposit feeder export. Specifically, deposit feeder stocks were high under conditions of high nutrient load and low harvest, and low under conditions of low loading and high harvest. Additionally, high harvest of the deposit feeders led to high levels of suspended nutrients.
Without reproducing similar response curves for other pairs of perturbed variables in the system, these results are at best anecdotal. However, they satisfy intuitive expectations for the system and suggest the potential for application of this method to other systems.
In its current form, the food web model generated with Lotka-Volterra dynamics in GeiLoVe is not stable enough to reproduce the dynamics resulting from perturbations to flux parameters. Even small perturbations can cause some pools, particularly in the upper trophic levels, to go to extinction. However, artificial bounding of pool values in some cases allowed the system to rebound and produce interesting dynamics. If this effect could be reproduced in a more mechanistic way by augmenting the basic Lotka-Volterra dynamics, the resultant model might be sufficient to study top-down and bottom-up effects on the Chesapeake Bay and other relevant coastal ecosystems.

Healthy People 2010 defines several health risks for adolescents. Diabetes, depression, inactivity, sexually transmitted infections (Health & Human Services, 2000), and HIV are just a few of these health risks affecting adolescents. Providing the education for adolescents to protect themselves from STIs and HIV can be easy and effective and has the chance to make a major difference in the health of this population. One of the goals for Healthy People 2010 is to prevent HIV infection and its related illness and death. Increasing the proportion of sexually active persons who use condoms from 23 % to 50% is one objective for decreasing HIV infection (Health & Human Services, 2000). Education on consistent condom use as a method to prevent HIV infection as well as other methods of risk reduction and harm reduction will contribute to the 2010 goal of preventing HIV infection.
In 2004 the Centers for Disease Control and Prevention (CDC) reported an estimated 4,883 young people as receiving a diagnosis of HIV infection or AIDS, representing 13% of people diagnosed that year. Young people are particularly at risk for HIV infection because of a lack of knowledge of HIV transmission or prevention. In conservative communities such as Monroe County, HIV education may focus on abstinence and adolescents may not be taught about harm-reduction methods for HIV prevention. Reports from the National Youth Risk Behavior Survey by the CDC (2006) show 46.8% of students grades 9-12 had sexual intercourse during their lifetime. 14.3% of students had sexual intercourse with four or more persons during their life. Of students engaging in sex, only half report using condoms consistently. Among rural high school students, a study of STD-/HIV-related sexual risk behaviors and substance use revealed 37.9% of the survey population was sexually active. Of these sexually active rural teens, 57.4% reported using a condom the last time they had sex (Yan, Chiu, Stoesen, & Wang, 2007). White rural adolescents in this study were most likely to report not using a condom the last time they had sex. Female students were also more likely to report not using a condom (Yan, Chiu, Stoesen, & Wang, 2007).
The Health and Consumer Protection Directorate of the European Union (2007) published a press release saying 24% of EU citizens are wrongly convinced you can be infected with HIV/AIDS by kissing on the mouth and 30% are unsure on this, meaning half of all EU citizens do not understand how HIV is transmitted. Although no such statistics exist for the United States, we can assume a percentage of this population is also unsure how HIV is transmitted.
A study by UNAIDS reported less than 50% of young people surveyed in 18 countries had comprehensive knowledge about HIV. In an overwhelming majority of these countries, young women knew significantly less about HIV than young men (UNAIDS, n.d.). Because of lack of knowledge and inconsistent use of protective measures for HIV, these studies show young people, particularly women and those in rural areas, are at a high risk for HIV infection.
When considering the epidemiologic triangle (agent, host, and environment), the best way to affect HIV transmission, with our current technology, is by breaking the chain between agent and host. There are two ways to stop transmission of the virus: eliminating activities that have been proven to transmit HIV (risk reduction) and using measures that have been proven to reduce the chance of transmission during such activities (harm reduction). Education is the cornerstone for HIV prevention because it gives people the options to protect themselves from infection.
Education related to prevention is necessary for decreasing HIV incidence because the natural history of HIV does not allow for recovery, only death. Early diagnosis is also important for HIV because it is now possible to maintain a healthy life for a longer period of time than ever before. The CDC estimates, of the 1-1.2 million persons in the U.S. that are infected with HIV, one-quarter are unaware of their infection. 54%-70% of new sexually transmitted HIV infections can be attributed to these 25% unaware of their infection (Branson, 2007). By preventing new infections of HIV with prevention education and providing early diagnosis to prevent spread by unknown infected persons, HIV incidence could decrease dramatically.
Ida Township is a rural area of Monroe County, Michigan. Monroe County's HIV infection rate is 32 per 100,000 (Michigan Department of Community Health, 2008), much higher than the national rate of 18.5 per 100,000 (Centers for Disease Control and Prevention, June 9, 2006). Ida High School has no limits on what can be mentioned during HIV education, unlike some other schools in Monroe County. Ida Township, however, has very few resources for HIV testing. The four closest testing sites to Ida are all about a 40-minute drive away. STI testing is available at the Monroe County Health Department, about a ten-minute drive from Ida. Some public transportation exists between Ida and Monroe (where the health department is located) but none exists between Ida and any of the HIV testing areas. Without personal transportation, Ida community members cannot receive HIV testing services.
A class of 12 Ida High School students was selected to receive an HIV education program intervention. The class included 12 white seniors, 4 of which were male. The intervention was a comprehensive HIV education program designed to not only provide risk reduction options for HIV prevention (i.e. abstinence), but also provide harm reduction options for prevention (i.e. condoms). The rationale for adding harm reduction options to the standard risk reduction options comes from the National Youth Risk Behavior Survey by the CDC, which shows 46.8% of students grades 9-12 had sexual intercourse during their lifetime (Centers for Disease Control and Prevention, 2006). As almost half of American adolescents have had sex, an intervention using only risk reduction is no longer an option.
The intervention aims to increase knowledge of HIV on three domains: cognitive, affective, and psychomotor. The cognitive domain is affected by memory, recognition, understanding, reasoning, application and problem solving. Recognition was started with a pre-intervention questionnaire where concepts were introduced, memory and understanding of the information was tested in a post-intervention questionnaire, and reasoning, application, and problem solving were assessed during student participation throughout the intervention. By informing students about HIV, the intervention hopes to change the affective domain and increase sensitivity to the subject. The psychomotor domain was affected while students actively participated in the intervention by standing in the front of the room and holding signs of different activities or body fluids and their likeliness to transmit HIV (Stanhope & Lancaster, 2004).
Objectives for this intervention include the ability of every student, post-intervention, to list at least one way they could protect themselves from HIV. All objectives are listed in table 1.
After designing an educational program on HIV it was presented to seven senior nursing students and one public health nursing instructor. The students and instructor critiqued the program and appropriate changes were made. Arrangements were made to implement the program to a group of 12 students at Ida High School in Monroe County.
The intervention acts on the level of primary prevention. Primary prevention efforts in public health aim to inhibit development of disease before it starts. The design of the intervention is to take HIV-negative youth and provide them with the knowledge to protect themselves from infection and remain negative for their lives. The intervention also works on a secondary prevention level. Secondary prevention in public health aims for early detection and treatment. Students participating in the intervention are given information on HIV testing for early detection. The intervention also provides a small amount of information on strategies to stay healthy for those who are HIV positive.
The implementation of this program could, at a minimum, take only one person. A contact at the school is important to set up an aggregate for the presentation. The program was designed with a PowerPoint presentation so a computer and projector are required. The time limit, enforced by class length, is a major limitation to a truly complete education program.
Prior to the intervention the 12 students were given a questionnaire on HIV. Following the intervention the same questionnaire was given to determine the change in knowledge from the intervention. Ten students returned the pre-intervention questionnaire and 11 students turned in the post-intervention questionnaire. Results of the questionnaires are below in table 2. The first two sections of the pre-intervention questionnaire are labeled as having inaccurate data. The students were still in possession of the questionnaire during the beginning of the intervention and they used this learned information on the questionnaire, making it an inaccurate estimate of their original knowledge pertaining to HIV.
The most important goal of the intervention was to provide the students with the knowledge needed to protect themselves from HIV infection with risk-reduction methods or harm-reduction methods. Ten of the eleven students to return a post-intervention questionnaire reported abstinence as a method they could use to protect themselves from HIV, seven students listed safe sex or condoms, five listed not sharing needles, two students listed one partner or limiting partners. Most students, 90.9%, listed risk-reduction methods, abstinence, as a method to protect themselves from HIV. This shows that despite the inclusion of harm-reduction information, students still understand and may use risk-reduction methods. More students identified abstinence as a way to protect themselves from HIV over other harm-reduction methods.
Objective one, correct identification of the acronym HIV, was met, 90.9% of students correctly identified this. Only 45.45% of students were able to identify correctly the acronym for AIDS, most students were able to identify most of the acronym, just not all four words. 90.9% of students could identify the difference between HIV and AIDS, meeting objective three. 90.9% of students could list four ways to transmit HIV, not quite the goal of 100% but an improvement from zero students who were able to list four ways to transmit HIV before the intervention.
The intervention was effective as no student could list four ways to transmit HIV before the intervention and after the intervention 90.9% of students could list four ways to transmit HIV. The students all, after the intervention, could list at least one way they could protect themselves from HIV, the main goal of the intervention. Providing the students with the knowledge to protect themselves from HIV infection was a major strength of the intervention. The active audience participation is also a strength of the intervention. The weaknesses of the intervention include the inability of the program to clearly define the acronym AIDS and its meaning. The time limitations of the program is also a weakness of the program because an hour is not enough to implement a complete HIV education program. A limitation of the study was the small study size, not being able to provide accurate information on the intervention's effectiveness.
If the intervention was to be re-implemented several improvements could be made. More audience participation could improve knowledge retention after the intervention. For long-term knowledge retention, students could be given an HIV fact sheet to refer to at any time after the intervention. It would be ideal if the intervention provider could, before the intervention, evaluate the HIV knowledge of the students with time to tailor the intervention to the specific knowledge needs of the population. Condom use was mentioned several times as a harm-reduction method of HIV prevention. With the current intervention, it is impossible to know if the students understand proper condom use. The effectiveness of the HIV education program could be increased if it included a condom demonstration. The intervention would also be stronger if it detailed protective measures for each method of transmission. For example, the current intervention mentions not sharing needles as a way to prevent HIV transmission. If the intervention went into more detail, about bleach methods for cleaning needles and needle exchange programs, it would be more effective.
Overall the program was very effective in providing the small study group with knowledge relating to the definition of HIV, the difference between HIV and AIDS, and methods of protection from HIV infection. The intervention was mostly effective in providing students with the knowledge of ways to transmit HIV and it was not effective in explaining the definition of AIDS. The most important part of the program was imparting the knowledge of prevention methods so the students possessed the knowledge to protect themselves from this preventable infection.

School age children (ages 6-11) used to have worries of homework, tests, and picking out a best friend. Currently, they are at high risk for obesity, type II diabetes, severe food allergies, poor self-esteem, bone and joint problems, and high cholesterol/blood pressure (Oakland county health profile, 2002). All of these health problems have a direct correlation with physical activity and obesity (Oakland et al., 2002).
The surgeon general has recognized childhood obesity to be an epidemic. An epidemic is a sudden increase of a condition or illness within a population (Stanhope & Lancaster, 2000). Contributing factors for obesity include genetics, lifestyle, diet, and medical conditions (Oakland et al., 2002). Risks determined by race are more difficult to determine. African-Americans, Hispanic, and American Indian's have a slightly higher prevalence of obesity (Institute of Medicine, 2004). It has been determined that having a low socio-economical status and living in the south is directly correlated with childhood obesity (Institute et al., 2004). Obesity can affect any child, no matter what age, gender, or ethnicity. Many studies have concluded that health problems caused form obesity could be prevented and/or controlled through physical activity.
A study from the University of Minnesota found that time spent performing physical activity significantly decreased from early adolescents to late adolescents (University of Minnesota, 2007). Decreasing physical activity becomes a habit, and a person will develop a sedentary lifestyle. This lifestyle leads to increase risk for obesity and other related illnesses (University et al., 2007). In today's world it is often harder to be active then it is to be not active. The priority of convenience and time saving habits leads to less leisure time, and less activity. People depend on cars more for transportation to decrease effort and save time, work leads to a decrease time to participate in healthy activities.
A study published by The Journal of Adolescent Health verifies the high prevalence of minimal activity in adolescent males and females. Sixty percent of females, and forty-three percent of males did not meet the national guidelines for activity. Compliance was defined as being physically active for 60 minutes five times a week. This study also concluded that race or region of residency did not have an impact of amount of activity. Girls were more likely than boys to decrease activity as age increased (Butcher, Sallis, Mayer, Woodruff, 2008). This article supports efforts to increase the amount of physical activity beginning at or before adolescents. This will prevent the onset of a sedentary lifestyle as well as the physical and emotional complications form inactivity. (Butcher et al., 2008).
Drextel University studied the contributing factors of childhood obesity by observing the physical and social environment changes. The study recognized the physical environment decreasing opportunity for activity, and the social environment promoting food high in fat content and calories. The study concluded the need for a shift of assuming individual responsibility for obesity, and recognition of the environment as the primary determinant of obesity (Budd, Haymann, 2008). While there may need to be a larger focus on the contributing factors to limited activity among the population, an individual must do what s/he is in control of. Drextel recommended the use of nurses to promote and support individual change while setting an example for the community (Budd et al., 2008). Until America can accept responsibility and admit its faults, individuals must do what is in their power to decrease their risk of the complications of inadequate activity.
Using descriptive epidemiology the problem can be summarized using person place and time. While all persons are at risk for health complications related to inadequate activity, the focus is on adolescents and pre-adolescents. Studies show that there is a significant increase in illnesses and diseases which could be prevented or managed using physical activity. Targeting the age-group where the activity ceases will prevent the onset of a sedentary lifestyle while prolonging good health habits. Inadequate activity affects females more so than males, perhaps this is related to the stigma of participating in sports and being athletic for males. Inadequate activity affects all races and all income levels. Studies show all places are affected by inadequate activity. While there are indications that the south may be effected more than the north, all geographical regions have an increase in childhood obesity, therefore, no region should be overlooked while educating about the need for activity. There is less physical activity during winter months than during warmer climate (Yasunaga, Togo, Watanabe, Park, Park, Shephard, & Aoyagi, 2008). Suggestions of activities that can be performed during winter months should be made, as well as summer months.
Becoming less active is similar to many disease processes: there is an onset of symptoms, and if not treated promptly, it worsens and leads to many other health problems. Acquiring a sedentary lifestyle does not have one path of transmission like other health issues. Typically, a young child will be very active. Once s/he begins school the time for "play" is limited to gym, recesses, and after homework is completed. In high school, gym and recess are eliminated, but sports are available. The focus becomes more academic and work related, and eating becomes rushed and usually less healthy. A significant amount of teens report inadequate activity levels. With college, it is harder to become involved in sports, and there is little encouragement to become active. There seems to be a stigma that sports are for the younger population, which may discourage adults from participating in recreational activities. This, along with self-consciousness and decrease accesses to activities, eliminates many adults from participating in an active program. The problem resolves with the increase in opportunity to perform activities, and a increase in self-responsibility to take action. The aggregate population studied is the school age population. At this age, activity usually begins to decrease will continue to decrease throughout their lifespan. Using this population, the inadequate activity will be prevented, rather than treated. In order to prevent the decrease in activity, enabling factors must be assessed.
Decreased activity is determined mainly by lifestyle and social surroundings. While it is an individual choice to participate in activities, access to the opportunity is a strong determinant of commitment to physical activity. As age increases and the opportunity for activity decreases, many people shift their focus of activity and playtime to more sedentary behaviors (Butcher et al., 2008). Additionally, surrounding environment has a direct correlation on activity status of the population. One study concluded that winter months had a significant decrease in activity level than other seasons (Yasunaga et al., 2008). Similarly, a parent's goals and priorities are often transferred to their children. If parents do not recognize physical activity as an important part of everyday life, their children may be less likely to engage in regular activity. Making physical activity a priority for families will have a positive impact on the family as a whole (Villaire, 2008). A decrease in physical activity will decrease self-esteem and motivation, which in turn will decrease the likelihood of physical activity. The cycle continues and poor habits are developed, and a sedentary lifestyle begins.
The web of causation can be used to describe the multiple factors that influence the sedentary lifestyle, and the consequences that occur with failure to attain adequate activity (See appendix). Attaining adequate activity is determined by many factors, additionally, many consequences of inadequate activity present additional barriers. This begins a cycle which is difficult to overcome.
Healthy People 2010 recognize obesity as a problem that needs to be addressed. Objective number 19-3 is to "reduce the proportion of children and adolescents who are overweight or obese". Objective 7-11 supports prevention by having a goal to increase culturally appropriate health promotion and disease prevention programs in health departments (United States department of health and human services, 2001). However, the statements of the need for change are only the beginning to a revelation. The trends of America are showing that even with the knowledge of the problems that occur with a sedentary lifestyle, change is not guaranteed.
In 2004, nineteen percent of Americans ages 6-11 were overweight (Morbidity and mortality weekly report, 2007). This is an increase from eleven percent in 1994, and is continuing to rise (Morbidity et al., 2007). While there are no conclusive statistics for the city of Walled Lake, or Oakland County, obesity has been recognized as an epidemic the county is working to prevent and treat cases of obesity. Of those surveyed, over 21% had a body mass index greater than 30, and 45% reported that they were trying to lose weight (Oakland et al., 2002). The amount of physical activity can contribute to a person's body mass index. In 1996, 26.9% in the United States report getting no leisure time or physical activity in the past month, compared with the 22.9% of Michigan. There were no statistics for Oakland County. In 2002, 33.8% of Oakland County reported getting no leisure time in the past month. (Oakland et al., 2002). There are no comparisons to Michigan or the United States for the year 2002, the significance of the data suggests the need to address this issue.
Oakland County Health Department has made resources available for their population. The website includes information on the topic, preventions, and treatments. There is a list of activities to suggest to children, activities to do with children, and activities that can be done in the cold weather. Additionally the "Count Your Steps" program will restart in the community. People who participate will wear pedometers and be rewarded for taking the most steps in one month. In 2005, 4.4 billion steps were taken by third and fourth graders (Patterson, 2006). With the success of community involvement, the decision was made to continue this program to support the goals of increased health in childhood.
An intervention was developed to be used as a prevention tool for diseases that are caused from inadequate activity. Because of the age group of the intervention group, the focus was to increase activity, not to prevent obesity, being overweight, or diabetes. Exercise was always referred to as "activity" to decrease association with sounding mandatory and helping to imply opportunity. The children were encouraged to be active for fun and to feel good. To demonstrate how fun activity can be, a competition was organized. An assessment determined the knowledge of the importance of activity, the importance of a warm up, and cool down, and how to tell if a person is playing to hard/not hard enough. Finally, an explanation was delivered about the competition. Since there are four 5th grade classes, each competed against each other. For every 15 minutes of activity (plus the warm up and cool down) the children recorded a tally on their own daily log. At the beginning of each school day, they tally's were added up. At the end of 6 days (from Friday to Thursday), the class that was the most active received a prize. The prize rewarded were free activities around the community. The prizes were donated by family owned bowling alley's and mini-golf facilities that were willing to support the good cause. Additionally, this prize helped the business, promoted their facility, and helped the community from an economical standpoint. This intervention was appropriate for this age group; it motivated individuals to become active, and allowed for self-achievement, which also increased self-esteem. It did not mandate anything to be done, but rather encouraged proper behavior. Additionally, it allowed children to work at their pace and engage in activities they enjoy.
Our short term goals would be to see adequate activity levels among these 5th grade classes. With thirty students in the class, exercising for 60 minutes a day for six days, we would expect to see at least 720 tally marks by the end of the competition. With this achievement, our goals for adequate activity will be attained. Additionally, during the knowledge assessment, children should be able to answer all questions regarding activity at the end of the discussion.
Long terms goals include seeing an overall increase of physical activity, better attention in school, and an increase in mood. While these will not be able to be assessed in this study, compliance with the activity recommendations would result in all long term goals being met. If we were to measure these long-term objectives, we would assess the grades overtime and expect to see an increase. Additionally, there would be less need to report people to the office, and less need to remind children to be quite during class time.
The amount of activity was measured based on tally marks. Each tally mark represents fifteen minutes of activity. On Thursday, the amount of activity of all four classes were evaluated and analyzed. There were expectations of many tally marks, which indicated a lot of activity. With the recommendations of sixty minutes per day of activity, tally marks will determine if these objectives were met. With thirty students in the class, exercising for 60 minutes a day for six days, we would expect to see at least 720 tallies. With this achievement, our goals for adequate activity will be attained. Although the activity will likely decrease after the competition is done, there will be ongoing benefits from the competitions. After a week of activity, the children will notice some of the benefits. These include sleeping better at night, able to pay attention in school, and feeling better (Patterson, 2006). There was an expectation for an increase in general mood during the post-assessment with the children. The children were encouraged to perform activities with friends, which will start trends for the children, allow them to make it a regular activity, and provide support and encouragement for their activities.
This implementation is a primary intervention because it is working on prevention and education of health problems such as obesity, diabetes, and poor emotional health (Stanhope et al., 2000). For those children who are already faced with the struggles of these health problems, it is a tertiary intervention because it focused on delaying the progression of the disease (Stanhope et al., 2008). To implement this program, there is little to no monetary cost. The program will take 6 days to complete. Children are able to put as much or as little time into the program, depending on their motivation and time capabilities.
No money is required for this program. Approvals were needed by the organizations tat donated prizes. Without their approval, the main motivation (the prize) would not exist. At this age, having a materialistic prize to work for is beneficial, therefore the approval was a vital part of the program. The personnel involved are the teachers. The teacher's cooperation to keep tallies on the board allowed for record keeping for the class during the week. The original plan was not accepted due to policies from the health department. To support the goal of making things as convenient as possible for students a checklist was designed. On the days activities were performed, a checkmark would be placed next to the listed activity. Since this checklist list did not have the heath department logo on it, it could not be handed out. This presented a problem because not only would it make our project less convenient for students, but it also meant there were not any handouts for the class. As a substitute, hand outs were printed and used as an example for children to see. Examples of activities and guidelines for the activities were organized and placed on the chalkboard. Information presented included warming up, beginning the activity, and cooling down. However, by allowing the children do document their own activities, they will gain more independence. Additionally, giving a list of the possibilities would have made them feel limited on the activities they could do if many of their everyday activities were not listed.
During the presentation a verbal evaluation was conducted. Students were asked the importance of activity, and the benefits of warming up/cooling down. The students were engaged in all aspects of the presentation and were very knowledgeable about the subject. An open discussion occurred about the importance of activity, and the key points were reiterated by the leaders of the study. After the explanations, children were assessed by asking important points of activity. All questions were answered correctly.
Many of the objectives were met: students were able to list activities, the time that should be spent on the activity, safety precautions during activity, and warning signs of when to slow down. By the end they were motivated to engage in the competition and daily activity.
During the follow up visit, the winning team was awarded their prize. The short term goal of achieving 720 tallies was not attained. Although the willing team did not reach the expected goal, there are many factors which could have contributed to that. The students were instructed not to tally points for gym class or recess. This was to motivate activity outside of the time provided for specific activity. Adults are less likely to engage in regular physical activity when it is removed from their mandatory schedule. By not allowing children to count time where physical activity is required, it will encourage self-discipline and responsibility. Allowing the school activities to be tallied would make this goal more achievable.
This intervention had many positive contributions to its effectiveness; it provided motivation for activity, while explaining the benefits of activity. Additionally, it did not force students to do anything, but gave them the choice and inspiration. The information being delivered by nursing students also gave the information more accreditation because of the direct connection with health care.
The weakness of this activity is the possibility that the activity level will return to its baseline after the competition is over, making this a temporary solution. However, it is likely after the week the students will feel the benefits of activity, and enjoy engaging in activities with their friends, which will motivate them to continue with activity. Additionally, the education presented about activity is not temporary; therefore they are able to make an informed decision about activity. If they chose not to have regular activity, they will know the risks that are involved with their decision.
If this intervention were to be redone, there would be significant benefits to having a handout that could be distributed to each individual student. Having a personal reference as a reminder would increase the likelihood of engaging in activities. However, since this was not possible, a fact sheet was given to each individual class as a reference. Having more attainable short-term goals or allowing for tallies to be added from school would be beneficial as well. Additionally, providing more time for the intervention would allow for a thorough pre-assessment, post-assessment, and research to be done on factors impeding activity in our aggregate population.
While studying this aggregate, the importance of early intervention became very apparent. It is important to assess factors contributing to the specific population, rather than the population as a whole. Providing this individualized care for the aggregate population will allow for attainable results. The study performed, along with the implementation of an intervention, demonstrated the benefits of an individualized plan for the population.

Asthma is a chronic respiratory disease that is characterized by three equally causative components (Berger, 2004). The first component is chronic inflammation of the lining of the airways (Bernstein, 2000). This inflammation can develop over a period of time due to the presence of chronic stimulation by asthma triggers and untreated or under-treated bronchospasm (Bernstein). Constriction or spasm of the muscles that surround the breathing tubes, commonly called bronchospasm or bronchoconstriction, is the second component of asthma (Berger). This constriction usually occurs due to acute exposure to triggers such as allergens or cold viruses and may also occur in response to exercise (Bernstein). The third component of asthma is the tendency of the lungs of an asthmatic to produce thick, tenacious mucus when over-stimulated (Berger).
Asthma, as briefly described above, is a significant cause of morbidity and mortality among school aged children in the United States (Doull, Williams, Freezer & Holgate, 1996). Childhood asthma is a disease entity that has a significant impact on pediatric patients as young as kindergarteners in terms of limitation on activity, missed days of school and emergency medical treatment (Grant et al, 1999). Pediatric asthma also has a significant impact on society due to the demand for healthcare resources, and the impaired quality of life of children with poorly managed asthma and their families (Clayton, 2005).
Unlike some other childhood health conditions, asthma is highly treatable and its long-term sequelae highly preventable (Creer, 2001). Children with previously undiagnosed or currently under-treated asthma are being identified in vast numbers due to a plethora of in-school asthma screening programs springing up as of late. In spite of these programs, asthma remains the leading cause of pediatric hospitalization and school absenteeism in this country (Clayton, 2005). In addition to the discussion of programs for school-based screening of children for asthma that may have been previously undetected or under-treated, recent literature has also discussed many asthma education and self management programs, frequently incorporated into the regular school curriculum of asthmatic children (Velsor-Friedrich & Srof, 2000). Most pediatric asthma assessment tools published to date focus on quality of life issues or rely on partial parental input for completion.
This pilot study will test a pre and post educational intervention assessment tool designed to aid the asthma educator in gauging the effectiveness of a given intervention program. It may also be useful in an office setting to gauge the effectiveness of educational information provided during an evaluation. This will involve revision of an existing assessment tool for adults with asthma, the KASE-AQ, to be appropriate to a pediatric population. The proposed tool is intended to be completed solely by the pediatric patient with asthma. Continued efforts in the detection and management of asthma with school based programs nationwide, may significantly decrease school absentee rates, activity limitations, hospitalizations and emergency room treatment required by children as a result of asthma (Christiansen & Zuraw, 2002). These improvements in pediatric management would contribute to meeting the objectives of Healthy People 2010 that relate to asthma and decrease the financial burden on the healthcare system that is caused by asthma related expenditures; however, a tool to assess the effectiveness of such programs is indicated. Such a tool must be developmentally appropriate in content, pertinent to the everyday life experiences of children with asthma and broad enough in scope of material covered and questions asked in order to be applicable and useful with various asthma education programs.
A review of recent literature reveals varied questionnaires and surveys designed to assess assorted aspects of asthma in children. Many published pediatric asthma assessment tools to date examine only one aspect of this chronic condition, typically quality of life (QOL). While quality of life is a very important aspect of any chronic condition, assessing the knowledge of a child regarding his or her asthma is not addressed. Many asthma assessment tools rely on input from a parent or completion of the survey entirely by the parent. One might argue that a survey tool completed by the child would better determine his or her own attitude regarding his or her asthma and the level of self-efficacy of symptom management.
The Paediatric Asthma Quality of Life Questionnaire, the Feeling Thermometer (Juniper, Guyatt, Feeny, Ferrie, Griffith & Townsend, 1996) was one of the first questionnaires published in recent literature that deals primarily with evaluating asthma in children. This 23 item tool uses a Likert-type scale to assess different aspects of everyday life that a child with asthma might find troublesome. To use this questionnaire, a trained interviewer would administer it one on one with a child at a clinic visit. The interviewer asks a question and the child responds by choosing an answer from color-coded flash cards. By incorporating changes in the questionnaire over time, coupled with objective data such as pulmonary function test results and physical assessment, the authors suggest that a more complete picture of a child's health might be evaluated. Clinical testing of this tool noted that it had valid measurement properties in children from age 7 to age 17. The limitation of the questionnaire identified by the authors at the time of original publication is the small sample size, 52 children, used to test it. Strengths, as discussed by the authors are that it "is simple and easy to use and is applicable to children as young as seven" (Juniper et al).
Bursch, Schwankovsky, Gilbert, and Zeiger (1999) reviewed four health belief measures constructed for children with asthma and their parents. The Parent Barriers to Managing Asthma scale measures perceived barriers such as lack of access to medical care, transportation difficulties, side effects of prescribed medications, cost, time constraints, child care burdens, difficulty understanding medical devices and problems getting the child to take the asthma medication. The Parent Asthma Self-Efficacy scale measures self efficacy of the parent with regard to prevention and management of asthma attacks in the child. The Parent Treatment Scale measures the parents' beliefs about whether various asthma treatments will be effective in preventing or managing asthma symptoms. The Child Asthma Self-Efficacy scale measures the self-efficacy of the child with regard to prevention and management of asthma attacks. One limitation of these assessment tools explained by the authors is that they were tested only among families enrolled in the Kaiser Permanente Health System. Because this is a pre-paid health plan, member's perceived barriers may differ from those of families with an asthmatic child who is enrolled in other health care plans. They suggested future research with a broader population to examine the relationships between these scales.
Bukstein, McGrath, Buchner, Landgraf and Goss (2000) described a parent-completed asthma quality of life questionnaire. For evaluation of this tool, a parent of an asthmatic child completed a general questionnaire and an asthma specific questionnaire during an office visit. Parent responses to the survey questions were compared to pulmonary function test results and physical assessment finding in order to gauge changes over time. The final result was an eight item, asthma specific questionnaire, the Integrated Therapeutics Group Child Asthma Short Form. A limitation of this tool is that the parent assesses the quality of life of the child without input from the child. Though this may be applicable and likely necessary with younger children, one might surmise that health-related quality of life of adolescents and teens might be more accurately assessed by patient completed questionnaires. One strength of the tool is that its brevity makes it practical for use in clinical and office settings.
Santanello (2001) contends that asthma medications may not be adequately tested in children because of the unique challenges inherent in pediatric clinical trials. In attempting to address these challenges, she discusses the validation of two symptom diaries, conducted by researcher in the pharmaceutical industry and used for pediatric asthma assessment specifically in clinical trials. The Pediatric Asthma Diary is a patient completed tool designed to evaluate daytime and nighttime asthma symptoms in children ages 6 to 14. The Pediatric Asthma Caregiver Diary is a parent completed survey to assess asthma symptoms in two to five year old children. Responses to the diary questions by either the parent or child were compared with objective assessments of health and asthma symptoms made by health care professionals. Results of both diaries correlated accurately with physical findings of a health care provider over the course of the pilot project. Santanello concluded that diaries such as these can provide a reasonable assessment of pediatric participants in asthma trials but that because varied aspects of the asthma disease state are measured, accuracy increases when diaries are used in conjunction with physical examination and pulmonary function testing.
Hall, Wakefield, Rowe, Carlisle and Cloutier (2001) validated a questionnaire designed to aid primary care providers in diagnosing pediatric asthma. The Easy Breathing Survey, as it is called, was administered to all new pediatric patients seen at six primary care clinics in Hartford, Connecticut. The finished product is a four question survey that can be used in children ages 6 months to 18 years. It can be completed by parents of children who are too young to read or by the children themselves. A combination of the responses on the survey, additional verbal questioning and history from a child's medical record was used to determine if the child had asthma. Major strengths of this survey are that it can be completed rapidly and that it demonstrates high sensitivity for picking up severe levels of asthma. One weakness is that it is not as accurate in diagnosing milder asthma in patients being seen for the first time.
Redline, Larkin, Kercsmar, Berger, and Siminoff (2003) conducted a school-based asthma and allergy screening project utilizing survey instruments completed by parents and children. The Parent Symptom Questionnaire included five questions regarding asthma symptoms, four questions that deal with allergic rhinitis symptoms and one question that pertains to symptoms of atopic dermatitis. The Student Symptom Questionnaire contained 25 questions assessing asthma, allergic rhinitis and atopic dermatitis symptoms. The authors commented that their validation sample was rather small but that the surveys were accurate in recognizing previously undiagnosed asthma and allergy type conditions in the participating children. The major drawbacks of any school based program, such as this one, are the expense involved and the need for repeated attempts to collect data due to initial low response rates.
Gorelick, Scribano, Stevens and Schultz (2003) discussed a tool designed to predict response to intervention in children treated for acute asthma. The Child Health Questionnaire was administered at the time of treatment for an acute exacerbation of asthma and then at a 14 day interval following emergency room treatment. The majority of the patients enrolled did not have regular access to health care and tended to use emergency services as primary care. While this is not an ideal situation for long term asthma management, it worked well for this project as the majority of the patients returned to the emergency department for the recommended 14 day follow up. Overall, changes in the perceptions of the children with regard to asthma symptom status correlated with objective findings. A weakness of this study is that physiologic measures, such as peak flow values or pulmonary function testing were not used as comparative measures at the follow up visit.
Varni, Burwinkle, Paroff, Kamps and Olson (2004) designed and tested a modular instrument to measure quality of life in asthmatic children ages 2 to 18. They explained the benefits of incorporating disease specific language and assessment parameters into a generic quality of life survey. They noted that the cross information obtained from surveying both the pediatric participants and their children supported the need to measure both perspectives in order to fully understand the impact of asthma symptoms on quality of life. They also explained that the correlation between the responses of the younger children and the responses of their parents was important in that the parental perspective can be utilized at times when children are unable or unwilling to answer questions.
Chan, Mangione-Smith, Burwinkle, Rosen, and Varni (2005) used a shortened version of the same pediatric quality of life tool in a later project. For the purpose of this project, children without asthma, termed "healthy" by the authors, and asthmatic children were surveyed with the same instrument in order to determine if the instrument could distinguish between the different clinical statuses of the groups. They determined that the shorter version of the survey should be useful in assessing treatment effectiveness and quality of life changes in clinical research studies of children with asthma. One module of this survey deals with activity limitations in the form of missed school days. A weakness of this tool is that the missed school days are not qualified as pertaining to asthma symptoms or to other, un-related illnesses or reasons.
While all of the reviewed pediatric asthma assessment tools report accurate measurement of quality of life and related changes, whether completed by the pediatric asthma patient or by the parent of the patient, none assess concrete knowledge about asthma symptoms or how the child feels about his or her asthma. The KASE-AQ measures three aspects of chronic disease management that contribute to quality of life. Attitude, the first variable of the KASE-AQ, is an important variable in the success or failure of treatment. Self-efficacy, or the confidence that a patient has in successfully implementing his or her treatment plan is a second variable measured by the KASE-AQ and not specifically addressed in a quality of life tool. Knowledge of asthma physiology and symptom recognition is the third variable specifically addressed by the KASE-AQ. If a patient exhibits a positive attitude, he or she will be more likely to be compliant with medication and to devote time and effort toward symptom management. A high level of self-efficacy, coupled with a positive attitude and a working knowledge of asthma symptoms, triggers and self-management techniques may not only increase treatment compliance and allow for recognition of early warning signs of asthma destabilization, but also increase the persistence of a patient in attaining control of symptoms during periods of symptom exacerbation.
The Knowledge, Attitude and Self Efficacy Asthma Questionnaire (see Appendix A) was originally designed as a pre and post adult asthma intervention tool to aid in determining the effectiveness of a program about asthma management (Wigal, Stout, Brandon, Winder, McConnaughy, Creer & Kotses, 1993). The authors created a questionnaire designed to assess three domains: knowledge of asthma, attitude toward one's asthma and self-efficacy of asthma management. Each domain has 20 questions for a total of 60 questions that comprise the entire questionnaire. Each question is presented in a multiple choice format with a total of five possible answers supplied. Only one answer for each of the knowledge questions is correct. Rather than a correct answer, the attitude and self-efficacy scales are used to assess the effectiveness of an intervention by demonstrating a positive shift in answers between the pre test and the post test.
The knowledge domain considers factual information that an adult may have regarding asthma. A patient, in conjunction with his or her health care provider, may work better to manage the symptoms of a chronic condition if he or she posses a working knowledge of the disorder. In the case of asthma, a patient who recognizes early warning signs of an asthma exacerbation may better manage the increased symptoms by knowing the appropriate steps to take. An example of a question from the knowledge domain would be one asking what part of the body is not a component of the respiratory system.
Which one of the following is not a component of the respiratory system?
The attitude domain is the second component of the KASE-AQ. This domain is used to evaluate a patient's outlook on his or her illness or condition. If a patient exhibits a positive attitude with regard to his or her asthma, he or she will probably be more compliant with prescribed medication and other interventions suggested by the given health care provider. In the pre and post intervention test scenario, a change in attitude, ideally a shift toward the positive, would be exhibited as a result of a successful intervention program. An example of a question dealing with a patient's attitude toward asthma would be one that uses a five point Likert-type scale, from "True" to "False" to assess the patient's attitude at that time. Since these questions deal with the patient's attitude toward his or her own asthma, there are no correct or incorrect responses. A higher score suggests a patient has a more positive attitude toward his or her asthma and would likely be more willing and eager to work in cooperation with the physician to manage asthma symptoms. A lower score is suggestive of a more pessimistic and uncooperative attitude.
My physician can handle my asthma without my having to become involved.
The self-efficacy component of the KASE-AQ aids in assessing an individual's confidence in his or her ability to contribute to the management of his or her asthma. It has been hypothesized by asthma researchers that when a patient is non-compliant with a prescribed medication or treatment plan, health care providers frequently do not know if the subject is simply not cooperative or if he or she may lack the confidence in his or her skills with regard to asthma management (Creer & Levstek, 1996). An example of a question from the self efficacy domain would be one that is answered on a five-point Likert-type scale, from "True" to "False" and asks what a patient thinks about self-management techniques. Since these questions deal with the patient's self-efficacy regarding his or her asthma, there are no correct or incorrect responses. The higher the score, the more confident the individual would be in his or her ability to manage and control the asthma. The lower the score, the less confident the individual would be in his or her ability to manage and control any asthma symptoms.
I can take the necessary steps to avoid or to manage an asthma attack effectively.
In order to determine initial usefulness of the proposed tool in the pediatric population, approximately ten subjects, ages 9 to 12 inclusive, were recruited by means of flyers posted in the office of an asthma specialist. Criteria for inclusion into the pilot test group were boys and girls, ages 9 to 12 inclusive at the time of the project, who carried a diagnosis of asthma. The subjects had to be taking at least one daily controller medication to manage asthma symptoms. Subjects also needed to read and understand English. No specific exclusion criteria, other than not satisfying any of the inclusion criteria were employed. Subjects were recruited from a single asthma practice so as to produce a more homogenous knowledge base regarding asthma terminology and asthma symptom management, prior to the pilot testing project.
The instrument tested was based upon the Knowledge, Attitude and Self-Efficacy Asthma Questionnaire (KASE-AQ). This tool was designed in the early 1990's for use as a pre-post asthma intervention assessment in adults (Wigal et al). The KASE-AQ looks at three variables, as described previously, which the authors of the adult questionnaire explain successful medical regimens, for any disease process, are dependent upon (Wigal et al).
For the purpose of this project, the KASE-AQ was re-written in a language that might be more conducive to use in an early adolescent population (see Appendix B). All of the proposed revisions were reviewed by one of the primary authors (see Appendix C). The nature and order of the original questions was not altered in order to preserve the integrity of the scoring system. Because asthma treatment has undergone major changes since the publication of the KASE-AQ (NAEPP Expert Panel Report Guidelines for The Diagnosis and Management of Asthma-Update on Selected Topics 2002), some questions were no longer appropriate. For example, question 58 in the adult version refers for possible side effects of theophylline, a medication commonly used to treat asthma at the time the original tool was developed. Since theophylline is rarely used by asthma specialists at the present time, the population being tested has no knowledge of the side effects. This question was changed to refer to side effects of bronchodilators, the rescue medication prescribed for each of the ten participants selected and commonly used for asthma treatment today. Other changes included terminology substitutions as children in this age group think in concrete terms rather than abstractly. Question number 23 in the adult version assesses the person's confidence in managing exercise induced symptoms. For the purpose of the pediatric questionnaire, the question was altered to ask about gym class rather than exercise, thereby making an abstract idea, namely exercise into a concrete condition, that of gym class, so as to be more easily understood by children. Permission of the authors was sought prior to revision and use of the tool. The proposed questionnaire, an assent form and a consent form, and a recruitment flyer were approved by the University of Michigan Institutional Review Board (see Appendix D).
Flyers were posted in the office of an asthma specialist in Northwestern Ohio who is also one of the authors of the adult version of this assessment tool. Children and parents who were interested in participating in this project filled out a form and were contacted, via telephone, by the primary investigator to determine eligibility. The first ten children who satisfied the aforementioned inclusion criteria were scheduled to come to the physician's office for one appointment.
At the time of the appointment, the research project was explained to the child and his or her accompanying parent and any questions they had were answered. Each was given time to read the consent and/or assent forms and again asked if they had questions or did not understand any of the material provided. Each was asked questions, by the primary investigator, in order to determine that all of the material was understood. Informed consent was sought from each parent (see Appendix E). Assent was acquired from each child (see Appendix F).
Verbal directions consisting of "please read each question thoroughly and circle the best answer" were provided to each child by the primary investigator. The children were further instructed that in addition to assessing their ability to choose the correct answer, the project would also attempt to assess their ability to understand the language of the questions. Hence, they would not be allowed to ask questions of or request assistance from the parent who accompanied them to the appointment. The children were told that if they had a question, they would be able to ask the primary investigator only. All of the children verbalized understanding of the directions as presented. The only questions asked were regarding clarifying a word that likely would have directed the child to the correct response. In each instance, three in total, the child was reminded that part of the project was to assess their understanding of the tool. Each was encouraged to choose the best answer as well as they could and save the question until they were finished with the project. All verbalized agreement.
The only potential risk to any of the subjects that was identified by the investigator or the asthma specialist was that reading the questions of the tool might trigger some negative or undesirable feelings about asthma exacerbations or related instances that may have occurred in the past. The children and their parents were told that if any concerns or ill feeling arose as a result of reading the questions, they could feel free to speak with the primary investigator, the physician or any of the office staff regarding these matters. None of the children verbalized concern or ill feelings during the completion of the questionnaire or upon the conclusion. When each child finished the questionnaire, they were verbally asked to write those thoughts about it on the last page of the questionnaire. No mediations were administered and no changes were made to any of the participating subjects' current asthma therapy as a part of this project. Upon completion of the questionnaire, each child was given twenty dollars as a thank you for their time and participation.
The first ten potential subjects who responded to the posted flyer each met the aforementioned inclusion criteria and completed the project. Of these ten, six respondents were boys and four were girls. Four of the subjects were age nine, three boys and one girl. Three of the subjects were ten years old, two boys and one girl. Three of the subjects were 11 years of age, one boy and two girls.
As this project was conducted during the summer, the children were asked to provide the grade of school that they would be entering for the coming school year. Five subjects, four boys and one girl reported they would be entering the fourth grade. Two subjects, one boy and one girl, indicated they would be entering the fifth grade. Two subjects, one boy and one girl documented they would be entering the sixth grade. One subject, a girl, denoted she would be entering the seventh grade. The sample breakdown is provided in the following table. For the purposes of this pilot project, no special attention was paid to creating an even ratio among the variables of gender, age and grade in school. The following table denotes the sample breakdown.
Table 1 Sample Descriptors
There were no missing items on the ten questionnaires completed. It took the participants between 20 and 40 minutes each to read and respond to the entire survey. All were told prior to beginning the exercise that they could take breaks if they got tired. None of the respondent requested a break. Questions asked of the primary investigator revealed a rather large span of vocabulary knowledge, or lack thereof, among the children who participated. For example, several of the younger children were unfamiliar with the organs of the respiratory system, the words "resent" and "manage" and the meaning of the phrase "get the upper hand." The older children did not ask questions about these items.
Internal consistency coefficients were determined for the attitude and self-efficacy domains individually and in combination. A Cronbach's alpha value of .800 was demonstrated for the attitude and self-efficacy scales in combination, 40 items in the entirety. Seven of the items of these domains utilize reverse scoring, such that a response of "false" might indicate the most desired answer and "true" the least desired. These items are numbers 22, 28, 37, 40 and 50 of the attitude scale and 52 and 55 of the self-efficacy scale. Removal of the seven items of the attitude and self-efficacy scales from the analysis demonstrates a Cronbach's alpha of .892. Cronbach's alpha values for the individual scales were: attitude .843 and self-efficacy .767.
The results of the knowledge domain responses from the ten participants demonstrate a ceiling effect on one question, meaning that all of the children answered the question correctly. Ninety-percent of the children gave correct responses for one additional question. Of the remaining items pertaining to the knowledge domain, three items had eight correct responses and four had seven correct responses. The responses to the remaining ten questions were varied with no predictable pattern. There were no statistically significant correlations between correct responses and age of the child or grade in school. No statistically significant correlations were found between self-efficacy or attitude scores and age of the child and/or grade in school.
Each item of the knowledge domain was scored based on one point for the correct answer and zero points for any incorrect answer. There were 20 questions in the knowledge section for a total of 20 point s if all items were answered correctly. The mean response of the ten children who participated in this pilot project was 10.4 with a range of 7 to 15 points. A ceiling effect, meaning every child gave the correct response, was noted on question number 30, a question asking about things that could make an asthma attack worse. None of the children correctly answered question number 33, which dealt with side effects of rescue medication.
The attitude domain was made up of 20 questions, each with five responses. The responses were scored on a scale of one to five with a "perfect" score being 100. The mean score of the ten participants was 80.2 with a range of 55 to 94 points noted.
The self-efficacy domain of the tool was also comprised of 20 questions using the same five point scale for scoring. The range of scores for the pilot testing was 59 to 97. The mean of the ten children's scores was 80.5.
Additionally, a final page was included with each child's questionnaire asking for feedback about the instrument. The participants were given a list of descriptors that might be pertinent and asked to circle all that they thought applied. From the list provided, three children thought the exercise was fun while four felt it was not fun. One thought it was short, versus six responding that it was long. Six children chose interesting from the list while two chose boring. Seven indicated that it was easy to read. None of the children chose hard to read as a descriptive term. Lastly, six circled easy to understand and two circled hard to understand from the list of terms provided. Comments are displayed in the following table.
Table 2 Children's Feedback on the questionnaire
In addition to the list, the children were also afforded space in which to write any thoughts or comments they had about the tool. Of the ten participants, one chose not to write any thoughts about the survey in the space that was provided. The responses collected ranged from "some of the questions were strange" to different variations of it was fun and would help people. Two of the children commented that it didn't take as long as they had thought it would to complete the questionnaire. One said it seemed long because he "mostly knew most of the stuff." Actual comments given are listed in the following table.
Table 3 Subject Comments
Absence of missing items suggests that children of the age group employed were both willing and able to provide quality data of this nature. Most of the children indicated that the instrument was easy to read, though most said it was long. On average, it took approximately 30 minutes to complete, perhaps making it a bit lengthy to be used in its entirety during a clinic visit. In general, the instrument was understood by these children. The questions asked by the children were few and were mainly related to a wide variation in vocabulary knowledge among this age range.
The children who participated in this project came to the situation with much the same knowledge base regarding asthma and symptom recognition or management, having all been treated by the same physician and educated by nurses with similar training. The question with the ceiling effect dealt with situations that might make an asthma attack get worse. It stands to reason that the population examined would know the correct response since they had specific and repeated education about this during clinic visits. As expected, nine of the children responded correctly to a question about peak flow meters. This can be attributed to the fact that most of them have used a peak flow meter at some time. This information is also regularly reviewed during clinic appointments. None of the children knew the answer to a question regarding exercise-induced asthma. It is likely that these children were educated about how to prevent such symptoms rather than the physiologic processes that caused it making this finding understandable. It may also indicate an area where clinicians need to put a greater emphasis.
The attitude and self-efficacy scores showed a positive skew for most of the children. Even the lowest score among the ten was above the 50th percentile. One would expect scores of this nature, given the strong educational input and clinical support these children have received as part of their care at this specialty practice. It would be useful to compare these scores to those of children who are followed for asthma in a primary care practice.
The reliabilities for the attitude and self-efficacy scales combined and as individual scales were good. According to Nunnally (1978), in early stages of development, reliabilities of .70 or higher are sufficient. If significant correlations are found, further refinement of the instrument is warranted. A Cronbach's alpha of .800 for the attitude and self-efficacy scales combined indicate high reliability. Removal of the reversely scored items strengthens the reliability further to .892. Separate reliabilities of .843 for attitude and .767 for self-efficacy were also good. This suggests that the scales could be used separately or combined depending on the purpose of the research or clinical encounter.
In examining the items utilizing reverse scoring, the responses provided by many of the children are not consistent with their responses to other items of the same nature and topic. The fact that the combined reliability improved with removal of these items calls into question the effectiveness of such questions in this population. It is possible that asking a question in negative terms that expects a positive answer is more abstract than these children were able to comprehend. Further examination is needed to determine if some items could be deleted or modified. The tool is long and the possibility of reduction of some items warrants further consideration. This may increase its practicality and usefulness.
There are several weaknesses of the KASE-QA with pediatric revisions as demonstrated by this pilot testing. One weakness is the lack of clarity regarding the use of negatively worded questions or questions that seem to require a negative response in the early adolescent population. Further exploration of this area may reveal that more concrete questioning would better serve the need this tool aims to meet.
Another weakness is the length of the tool if used in its entirety. Though none of the children declined to complete the questionnaire, a majority did comment that it was long. This could be a limiting factor in usefulness in a clinic setting unless the time needed to complete the survey was calculated into the appointment. The reliabilities for the individual scales suggest that it would be possible to use the individual scales separately.
Another weakness to be discussed is the small sample size used for this project. The instrument needs to be administered to a much larger sample in order to do more advance statistical testing of its psychometric properties.
A final weakness of note is the relative homogeneity of the pilot sample. The ten children who participated in this project are all treated for asthma by the same specialist hence their knowledge of asthma and attitude toward asthma is consistent with the knowledge and attitude demonstrated by the staff of this office. Testing this instrument with children who are newly diagnosed with asthma or who are followed for asthma in a primary care setting is also needed.
More diversity in the sample demographics would also be useful. A majority of the ten participants come from families that enjoy higher socioeconomic status than that of average Americans. Many of these ten children come from homes that enjoy a two-parent household with a stay-at-home mother. The majority of the children have other family members, either a parent or sibling who are also patients of he same asthma specialist. A tool such as this might not reflect the same findings if used in a general population of asthmatic children.
Future work with this questionnaire should encompass a much larger sample size with special attention to an even ratio of boys and girls, ages nine, ten, eleven and twelve, and the grades in school representative of this span of ages. Testing in a population of asthmatic children from a more diverse background, both socioeconomic in nature and in the access they have had to health care professionals is also indicated to determine if the aforementioned findings hold true. Some items in the knowledge domain could be simplified to better represent the vocabulary range of the intended population. A comparison between the attitude and self-efficacy domains with and without items that utilize reverse scoring is also indicated. Consideration of this tool for use as two or three separate measures may also be of value.
Overall, the findings suggest that further testing and evaluation of the KASE-AQ with pediatric revisions is warranted and would be valuable. An instrument of this nature could be helpful to both researchers seeking to understand the mechanisms children use to cope with asthma and to clinicians working with these children in office or school based settings.

In Naming & Necessity [1980] Kripke claims that there are truths that are both contingent and known a priori, and others that are both necessary and a posteriori. In this paper I aim at two goals: (1) to study the cases of necessary/a posteriori and contingent/a priori truths using Stalnaker's two dimensional apparatus; and, given the results of such evaluation, (2) defend that there is a puzzle here, a genuine philosophical befuddlement that should be properly solved.
Stalnaker's two dimensional apparatus is a technical device that is meant to be useful for understanding a particular kind of speech act, that of assertion. It works mainly by accounting for the different things (e.g. propositions) that might be said by (i.e. expressed as the content of) different assertive utterances of any particular sentence.
Assertions are the speech acts by which we communicate truths. It is my goal to find out whether certain particular statements express truths that are both necessary and a posteriori. Thus if there are such truths we could, at least in principle, assert them. I will focus on the content determination feature of Stalnaker's apparatus in order to explore whether there are any assertions that may communicate something that is both necessary and a posteriori.1 That is why the two dimensional apparatus is relevant for the discussion. So, let us see how the apparatus works.
Some definitions are important to bear in mind:
(W) A possible world (w from now on) is a complete way in which the world might be. (P): A proposition (P from now on) is a representation of the world as being in a certain way (more formally, it is a function from w's to truth values). (PC) A propositional concept (PC from now on) is a function from ordered pairs of w's into truth values. (SP) A speaker presupposition SP is a proposition the truth of which is taken as a background assumption for the conversation. (CS) A context set CS is the set of all w's which are compatible with all SP's.
Stalnaker's apparatus has the following tenets:
 (i) An utterance U of a sentence S expresses a proposition P. (ii) A proposition P is determined by the set of w's which it represents. (iii) A proposition P is determined by the set of w's in which S is uttered. (iv) The main goal of assertion is to reduce CS as to eliminate all w's incompatible with what is said.
(i) is an assumption that a good number of philosophers share nowadays. I will not comment on that in this paper, although I think it might be a problematic tenet. What about (ii) and (iii)? According to (P), for every P there is a set of w's which are represented by P and which share that particular way of being. But not only, it is also true that for every set of w's that share a particular way of being there is a P which represents that. Thus, there is, as Stalnaker puts it, a one on one correspondence between P's and sets of w's. The proposition will be true in all the w's of that set and false in all others. That is why it makes sense to understand propositions as functions from w's to truth values; and also why (ii) is part of the tenets of the apparatus.
But the content of S is not only determined by the set of w's which it represents but also by the set of w's in which S is asserted. The content of an utterance (e.g. the assertion of P) is determined by the facts of the world in which it is asserted. The world plays a determining role as the context of utterance. This is another sense in which a set of w's determines a proposition, and which is implicit in (iii).
As for (iv), suffice it to say that when a speaker asserts something it is her intention to get the hearer to believe in what is asserted (e.g. a proposition) and thus, to take the actual world as being in that way. In order to do this it is necessary to reduce the set of possible situations compatible with what is said so that, at the end of the day, there will be one P taken to be said and one w taken to be the actual. If this were not true, then asserting would stop being useful, and perhaps end up being among the endeavors that humans take just for fun. No wonder why Stalnaker takes CS and SP to be central notions for the characterization of speech contexts.
To be fair to the ways in which the content of an assertion is determined one must take an ordered pair of w's and P's, such that for every w we get a P. Suppose that I assert (1),
and let us take it as an example of a stipulation. Now, evaluate it according to the following different worlds: where the standard meter is one meter long (w1), where it is two meters long (w2), and where it is three meters long (w3). This is supposed to be a case of a statement that expresses a contingent a priori truth; let's assume, if only for a brief moment, that by "truths" we mean "what is said" by the utterance of the sentence (i.e. in this case a proposition). A will be the propositional concept of (1), where the horizontal lines represent what is said by utterances of (1) in different w's taken as contexts.
We can see that there are only contingent propositions represented in A. This seems a little bit awkward. At a first glance, if I stipulate the length of a meter by asserting that The standard meter is one meter long, what I say is something that is true in every world where I make the assertion (i.e. in every world where I make the stipulation), and, thus, should somehow have true evaluations in every world. Put in other words, there is certainly a sense in which I cannot be mistaken about the truth of my assertion, a sense in which it is necessarily true. This being such because in every world where I assert (1) I know a priori (i.e. thanks to my stipulation) what the length of the standard meter is.2
This has to be among the things that I could say by an assertion of (1); i.e. it is at least among the information that could be imparted by it. However, there is no horizontal representation of such a proposition in A. Nonetheless, such necessary proposition is in fact represented diagonally from left to right and from top to bottom in A. Since there is only one such proposition, we can call this the diagonal proposition of A. We can make use of a two dimensional operator (i.e. one that a PC and makes it into a PC) called dagger operator, such that it takes the so called diagonal proposition of a PC and projects it into a PC. dagA is the PC which results from applying the dagger operator into A.
dagA accounts for the possibility of my expressing a truth in every w as a context by asserting (1). Let us then take A and dagA as offering the set of propositions that a speaker may convey by asserting (1); which, as (iv) says, will be ideally reduced to up to one line, such that both speaker and hearers will know what is asserted.
This is enough about the two dimensional apparatus for our purposes. With it we are able to see which propositions can be expressed by the assertion of a particular statement. These propositions are the contents (truths and/or falsehoods, according to our assumption) that a speaker gets to assert. Now we are up to see whether this standpoint helps us finding out if there are contingent a priori truths and/or necessary a posteriori.
At this point you might wonder whether I still need to say something in order to show that there is a truth that is contingent a priori. You might even think that I am too stubborn in aiming at this same spot once again. And you might be right, but not for the proper reasons. Yes, A clearly has propositions that are contingent. And, yes, thanks to the device of the diagonal proposition, it is also clear that there is a proposition that is a priori. But, no, up to now we have no clue as to which one is the truth that is both contingent and a priori. I am sad to say that (given our assumptions) no such clue will be offered, or so I will argue.
We can see here that a particular relation holds between the diagonal proposition and the (a priori/a posteriori) type of knowledge that the speaker has of the proposition asserted according to the context. Which relation this might be is not clear at all. Stalnaker puts it this way
If we take Stalnaker's claim to be merely about sentences -- i.e. about things like "The standard meter is one meter long" -- then it is trivially true that there is a truth which is both contingent and a priori. But this just means that the sentence can express contingent and a priori contents. Whatever is contingent and whatever is a priori would not be the truth (i.e. the sentence) but the content of the truth (whatever that means). You might then argue that it is not the sentence but the whole of A which is contingent and a priori. This cannot be the truth in question, for several reasons. First, according to tenet (i) when uttering (1) I expressed some content or other among the ones that A has, but not all of them. What I expressed is a proposition, not five, or ten, or a thousand of them. Second, according to tenet (iv), if I were to do this (i.e. assert A and not just one proposition among the set) then my assertion would be useless. I would not be able to reduce CS and, hence, to communicate something at all. My assertion would not help to carve logical space in a way that it helps us locating the actual world. Propositions seem to be a good candidate for whatever is expressed in our utterances.3 Why not take propositions to be the truth in question?
This option, however, is also problematic. We should be careful not to mistake Stalnaker's claim above. It might be taken to say that the diagonal proposition is that which constitutes the a priori truth. But this is a mistake, since the diagonal proposition is necessary and the a priori truth -- at least in our case -- is contingent. It seems that we should take claim that the proposition that constitutes an a priori truth is not a necessary proposition, but that in order for it to be a priori, the diagonal proposition of the PC in which such proposition is represented must be a necessary one although not the one which constitutes the a priori truth. This seems like a nice interpretation, but it is not.
We have to ask now which proposition is the a priori truth. As we have seen, it cannot be the diagonal one, but it cannot be either any of the horizontal propositions for one main reason: none of these propositions expresses a truth in every context, as Stalnaker thinks the a priori truth does. Of course, if we take all the horizontal propositions together we could say that there is at least one truth expressed in every context. But then we would, again, be conflicting with (iv). You can see why I think there is a problem here.
You might wonder whether my argument is flawed. I did myself, but I don't now. I have a way to solve this particular disagreement about my argument. Why not take a look at the other interesting case, that of necessary and a posteriori truths? According to Kripke, statement (2) can express such a truth.
If, say, a famous chemist in the 1950's, asserted (2) while talking to one of her colleagues -- she just found out about the chemical structure of gold -- she would have uttered (if a truth) a necessary truth for two reasons. One, she would have used a rigid designator, a natural kind term such as "gold" that would refer to all and only the instances of gold in every possible world where such natural kind has instances. Two, she would have ascribed an essential property: having atomic number 79. Hence, a property that (if true) is true of an object in every possible world where such object exists. So, our chemist would have uttered something true of gold here and true of gold in every possible world. She would have uttered a necessary truth. But, of course, as I said above, she found that out (presumably after many years of tough empirical research). So, what she claims to know she knows it a posteriori.
Now, let's run the two dimensional apparatus for these w's: w1 (the actual world where the natural kind Gold is the kind of elements that have atomic number 79), w2 (a counterfactual world where the tokens of Gold have atomic number 15 and where the element with the atomic number 79 does exist), and w3 (a counterfactual world where Gold is not a basic element and where there is no element having atomic number 79). Accordingly, the PC of (2) is given by B
In B we find either necessary truths or necessary falsehoods (or impossibilities if you want), but nothing that might look as a contingency. However, there is a sense in which what our famous chemist uttered in 1950 could have been false. After all, that's what she found out after some research, perhaps she messed up some stuff and came out with a mistaken result. Who knows? The point is that it seems to be true that it is not necessary that she came out with those results and knowing what she knows. So what she knows -- which allegedly is what she asserted -- is something contingent. This, as we have seen in the former case, is not a problem. There is, at least Stalnaker thinks so, some contingent proposition represented in B. That is the so called diagonal proposition.
We said that the truth expressed was a priori if and only if the diagonal proposition was a necessary truth. Mutatis mutandis we can say that the truth expressed is a posteriori if and only if the diagonal proposition is contingent. We may ask now, which one is the necessary and a posteriori truth-taking truths as propositions, not as sentences. This should be easy for there is only one necessary proposition in B. Is that our proposition? Well, it depends. If our proposition could be false, as it seems it could, given the fact that it is a posteriori, then this is not our proposition. Well, then, the proposition must be the diagonal one. That one certainly could be false for different w's, it is actually false for different w's. Is it? Well, its not. Our proposition is not only a posteriori, it is also necessary and the diagonal proposition is not.
There seems to be some incompatibility between a proposition being necessary and its being known a posteriori, mutatis mutandis for the other case. We come to a point where one must stop looking for such a proposition, at least within our two dimensional apparatus. This however is no satisfactory conclusion, for one main reason: the knowledge of the famous chemist who asserted (2) was in fact false for different w's as context, as much as my knowledge of what I assert with (1) is in fact true in every w as context. Presumably what she and I know is just what she and I respectively asserted. But what she asserted is just necessary (i.e. true in every w) and what I asserted is just contingent (i.e. true in some w's and false in others). If you feel like there is some sort of problem here you have the proper feelings (if not, you should read back again).
You might still think that this does not prove Kripke to be mistaken. I agree. We must try to make sense of this somehow, but we should be careful not to do it in the wrong way. I think there is a bit of a problem here for philosophers to solve. This is because, as I shall argue, Kripke is not only correct in claiming that there are necessary a posteriori truths. Even more, Kripke must be correct if our commonsensical view that empirical scientific knowledge is possible is to be correct.
Our problem goes like this: there is this peculiar set of truths which we can know a priori regardless of their being contingent (mutatis mutandis for the necessary a posteriori); however, there is no such proposition which is at the same time contingent and a priori (...). Nevertheless, it seems clear that what is expressed by a sentence, understood and known by the speaker who understands the content of a utterance, is a proposition.
One way to eliminate this problem would be to claim that nothing can be, at the same time, contingently true and known a priori (the same for the necessary a posteriori). Knowledge and truth go together in only one way, if it is necessary it must be a priori, if it is a posteriori it must be contingent. So, actually, there is no problem here. There was just confusion to begin with.
There are some reasons why we should avoid this strategy. First of all, it will force us to accept either one of these two really bad claims. If we accept that a posteriori truths are only contingent -- assuming the necessary ones are a priori -- and claim that we can know what the world is
Perhaps we don't want this conclusion, but still claim that only a priori truths are necessary and the rest are contingent, so we choose the second bad option. If we accept the former, and defend that, nevertheless, we can know what the world is about by a priori means, then: (b) we can know about the nature of the empirical world without even looking at it, we just need to engage in some a priori reflections and we are set, we will know everything there is to be known about the nature of things.
These two claims are equally bad and for the same sin: they are both too polarized. The first one seems to be some extreme realism of the form you can't -- where "can" has a metaphysical strength -- really know what things are made of. The second one seems to be some extreme idealism of the form it's all in your mind, you can know everything by just deploying the proper concepts. The first one demands too much about our epistemic apparatus; the second one demands too little about it. We should avoid both as much as possible.
Another strategy is perhaps to avoid the problem . This would be possible if we just decide not to talk about a priori and a posteriori truths, for instance. We might be content if we just talk about the necessity or contingency of what is asserted or known. But this does not seem to solve the problem We can still raise the problem by claiming that there are necessary truths that are known by doing empirical research; and we still have to explain how this is possible. I think this is just a cheap way out and, as many have said, there are no free lunches in philosophy.
So, if we don't want to end up getting nothing (i.e. no solution) for too little (e.g. avoid talking about a posteriori/a priori), or asking too much (i.e. extreme realism) or too little (i.e. extreme idealism), we should look for a solution. Something, that is, which might explain how a truth can be contingent and known a priori, and even more importantly, how scientific truths are a posteriori and nevertheless necessary. I guess there are many different strategies to follow from this point. I think, however, that the most promising one is to revise our assumptions, particularly tenet (i) according to which what is said, expressed and known is identified with one and the same single proposition. We might need to make some complex distinctions within the content in order to solve this, perhaps distinguishing -- as Evans [1979] suggests -- between that part of the content which has the modal properties and that which is known, believed, desired, etc.
