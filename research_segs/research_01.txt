GG Brown and Associates have performed compaction and Atterburg limit tests on the Gold Art Clay as requested. Tests were performed in general accordance with ASTM D-698-07e1. The Proctor test was used to determine the compaction curve. It was determined that the maximum dry density for Gold Art Clay is 110.6 pcf. The Atterburg Limits of liquid limit is 39% and plastic limit is 25.3%. The optimal water content is 15%. The clay was classified as CL in the group of leen clay.
A letter dated October 16, 2007 from William Piper Associates, requested GG Brown and Associates determine group symbol and name for Gold Art Clay as well as the Atterburg Limits, the optimum water content and the maximum dry density. We have completed testing on November 6, 2007 using the Protector Test to determine the compaction curve of the soil of interest and therefore to determine the maximum dry density and the optimal water content as well as Atterburg limit tests to determine the Atterburg limits of the soil.
The soil was previously sent to G.G. Brown Associates. The soil is the Gold Art Clay.
The Compaction Test (proctor test) was used to determine the maximum dry unit weight and the optimal weight and was tested according to the following;
ASTM D698-07e1: Standard Test Methods for Laboratory Compaction Characteristics of soil using Standard Effort
The Atterburg Limits were found and tested according to the following:
ASTM D 4318-05: Standard Test Methods for Liquid Limit. Plastic Limit and Plasticity Index of Soils.
The Proctor test was performed to determine the compaction curve for a soil of interest, to include the determination of the maximum dry unit weight and the optimal water content for the Gold Art Clay. Mechanical compaction is the most common and effective way of stabilizing soil and prior to compaction in the field; the compaction characteristics should be tested and determined.
The Atterburg Limits Tests were performed to determine the liquid limit and the plastic limits of the soil. The water contents separating the transition from a semi solid state to a plastic and a plastic to a semi liquid state are called the plastic limit and the liquid limit respectively. The water content in soil significantly influences its behavior. And the difference between the liquid limit and the plastic limit is an indicator of potential problems
The detailed procedures for both tests can be found in Appendix A of this report.
The group symbol and group name were determined to be CL and leen clay respectively. This is because the PI is greater than 7.
The Atterburg limits were determined to be the following:
These values calculated from our test results, a full table of data results is included in Appendix B.
The Plastic Limit corresponds to the water content at which the soil will begin to crack when rolled out into a diameter of 3mm.
The liquid limit was calculated by comparing the number of blows to the water content. Table 1 summarizes the data. The liquid limit corresponds to the water content at 25 blows.
From this table, the data can be plotted and seen in Graph 1. From this graph, a trend can be seen. As the water content increases, the number of blows to bridge the gap in the test decreases.
The maximum dry density and optimal water content were to be the following:
When the data from Table 2 is plotted against each other along with the Zero Air Voids line, the compaction curve can be found as in Graph 2.
From this graph it is clear where the maximum water content and dry unit weights are obtained.
These values calculated from our test results, a full table of data results is included in Appendix C. From the results found in the laboratory, the optimum moisture content is lower in comparison to that of the empirical correlation ( about 18%) provided in Figure 1 of Appendix E. This could be because of the fact that some of the moisture was lost while conducting the experiment. However the values are very similar as they were only about 3% away from each other. See Appendix E.
Sample calculations form both tests are included in Appendix D.
Using the Proctor Test, the Max Dry Density was determined to be 110.6 pcf and the Optimal Water Content was determined to be 15%. The ASTM soil symbol and group name was determined to be CL and leen clay, respectively.
From the laboratory determined optimal water content that was found, it is determined that it is only slightly lower in comparison to the empirical correlation provided in Figure 1 of Appendix E. The empirical relation estimates that the optimal water content (based on the values of the liquid limit and the plastic limit) is about 18%, where as that found in the laboratory was 15%.
From the Atterburg Limit test the liquid limit was determined to be 39% and the plastic limit was determined to be 25.3%.

The purpose of this experiment is to determine both the specific gravity and absorption capacity of coarse aggregate. This information will be used in the proportioning of concrete mixtures. Specific gravity is the characteristic generally used to calculate the volume occupied by aggregate. Absorption capacity describes the change in mass of an aggregate due to water absorbed in the pores.
This test is performed in accordance with ASTM C127: Standard Test Method for Density, Specific Gravity, and Absorption of Coarse Aggregate. The following materials are required:
Table 1 gives the data from the lab and the appropriate calculations. From this data the Bulk Specific Gravity at the SSD condition (BSGSSD) is 2.57 and the Bulk Specific gravity at the OD condition (BSGOD) is 2.49. The absorption capacity is 3.23%.
The values of absorption and specific gravity for aggregate that is not oven dried before it is soaked in water can be significantly higher that aggregate that is oven dried. (1) Larger particles, especially those over 75 mm, may be too thick to allow water to penetrate the pores to the center of the aggregate particle. Therefore it is critical that the procedure state if the aggregate was oven dried and, if so, how long it was submerged in the water. In this case, since the data is being found for a concrete mixture (where the aggregate will be in its natural, moist state) the aggregate was continuously submerged.
Another important procedural point that must be followed to assure reproducible results: The experimenter should assure the aggregate is fully submerged when finding the weight in water.
The purpose of this experiment is to find both the specific gravity and absorption capacity of the fine aggregate. This information will be used in the proportioning of concrete mixtures. Specific gravity is the characteristic generally used to calculate the volume occupied by aggregate. Absorption capacity describes the change in mass of an aggregate due to water absorbed in the pores.
This test is performed in accordance with ASTM C128: Standard Test Method for Density, Specific Gravity, and Absorption of Coarse Aggregate. The following materials are required:
Table 2 gives the data from the lab and the appropriate calculations. From this data the Bulk Specific Gravity at the SSD condition (BSGSSD) is 2.684 and the Bulk Specific gravity at the OD condition (BSGOD) is 2.669. The absorption capacity is 0.553%.
(a) Bulk dry specific gravity, bulk SSD specific gravity, and apparent specific gravity are all calculated in experiments 1 and 2. The key concept to understanding these terms is understanding the concept of pores in aggregate particles. A single particle of aggregate has a total volume that includes solids and pores. At the SSD condition the pores are filled with water and at the OD condition the pores are filled with air.
From these equations, these bulk and apparent specific gravities can be compared. In general
(b) Reproducible Results: In order to produce reproducible results, an accurate SSD condition needs to be obtained. It is critical that this is performed correctly, because the basis of all calculations relies on the appropriate SSD weight.
The purpose of this experiment is to determine the particle size distribution for fine and coarse aggregates using the sieve method. This data will be used to select proportions for concrete mixtures.
This test is performed in accordance with ASTM C136: Standard Test Method for Sieve Analysis of Fine and Coarse Aggregates. The following materials are required:
Tables 3 and 4 give data from experiment 3. Table 3 gives the sieve gradation analysis for coarse aggregate. Table 4 gives the sieve gradation analysis for fine aggregate. Figures 1 and 2, following the data charts, show the particle distribution graphically. As both the tables and figures demonstrate, both the fine and coarse aggregates do not meet ASTM specifications at some point in the distribution.
(a) In figures 1 and 2 the upper and lower limits for the ASTM standards are plotted in the lighter color. As demonstrated, both the coarse and fine aggregates do not fully meet ASTM specifications. The coarse aggregate does not fall in the range specified by ASTM standards for 3 of 4 given standards. The fine aggregate meets specifications for all sieves besides the #50 and #100 sieves. This tells us that both specimens are not appropriate for use in construction.
(b) In order to obtain reproducible results it is crucial that the aggregate is agitated for the appropriate amount of time. It is also necessary to weigh all aggregate sample retained in a sieve. Loss of any particles causes error in the calculations.
The purpose of this laboratory is to determine the minimum and maximum unit weights of coarse aggregate. This data will be used to select proportions for concrete mixtures.
This experiment is performed in accordance with ASTM C 29: Standard Test Method for Bulk Density (Unit Weight) and Voids in Aggregate. The following materials are required:
Table 5 demonstrates the data obtained in the lab, as well as the resulting calculations of specific gravity. From this data the rodded unit weight () is 1424 kg/m3 and the loose unit weight ( is 1328 kg/m3.

The development of necking and localization in specimens subjected to a uniaxial tensile load are triggered by a bifurcation. This bifurcation occurs when a critical load is reached where the displacement path becomes unstable. Necking and its subsequent phenomena localization show the mechanics behind material and geometric non-linearities. These non-linearities which can make a specimen decrease in cross sectional area (necking), can induce strain localization at later stages. Specifically, necking occurs due to geometric non-linearities and localization occurs due to material non-linearities.
This report contains implicit finite element analysis of models with plane stress and plane strain elements, and of different materials. Two materials were used, an elasto-plastic and hyper-elastic material, to analyze their effect on necking and localization. The first part of this report contains some explanations about what bifurcation, necking, and localization is and the second part discusses the results of the analysis.
Bifurcation is defined as the loss of uniqueness of solution in a non linear problem. It corresponds to a sudden change in behavior when a critical load parameter (λc) is reached. In buckling or necking the bifurcation point is defined as the point when a compressive or tensile load reaches a maximum triggering a sudden change in displacement. Figure 1 shows the bifurcation paths for buckling (red) and necking (green). As the load increases from zero both paths are stable with displacements close to zero. The mathematical theoretical solution states that the load will increase with zero displacement until λc is reached (yellow solid line). In real life because of imperfections and in finite element because of approximations there will always be some displacement before the critical load is reached. This can be observed in the initial solid red and green lines. Once λc is attained, the paths will bifurcate into non unique solutions. The buckling path will become stable while that of necking will become unstable. Ahead the finite element results are presented which excellently capture this behavior.
This reports aims to analyze two different bifurcation phenomena's. The first one is necking which depending on the material used might lead to the second bifurcation phenomena, strain localization.
Necking occurs due to geometric non-linearities and can be observed as a decrease in cross-sectional area of a specimen under tensile load. If a specimen is loaded in tension as seen in Figure 2 it will undergo three stages that will culminate with necking. First, the behavior will be stable with a constant decrease in cross sectional area throughout the length of the specimen. When the critical load is reached bifurcation occurs and the center of the specimen will neck. In order to see this complete behavior the sides of the specimen must be under shear free boundary conditions (i.e. rollers on both edges). In lab experiments it is very hard to give shear free boundary conditions so fixed boundary conditions (clamped) are given at the edges which then force the specimen to start at the second step and go directly into necking behavior. The same is true for finite element analysis where boundary conditions play a crucial role. These boundary condition requirements will be discussed in more detail in the boundary condition section. The analysis performed in this report starts the model when the bifurcation that triggers necking occurs. Capturing the pre-necking behavior is much more involved and is not of much use since most real-life elements undergoing necking will be fixed at both ends (does not allow for pre-neck behavior).
Localization which occurs after necking is defined as a bifurcation phenomenon which creates diagonal shear bands in the necked area which eventually cause the specimen to fracture along these bands. These bands are formed because of plastic strain localization. Strain localization is due to the materials non-linear behavior. Not all materials that undergo necking will experience localization. A more detailed discussion on why certain materials undergo localization is presented in the next section. Figure 3 is an example of strain localization for a steel specimen.
An elastoplastic J2 flow theory behavior was used to model the behavior of steel. An elastoplastic material experiences linear behavior until yield stress, where the material enters into the plastic range. The non-linear plastic behavior is defined by the following work hardening uniaxial equation:
Where the stress σ is a function of strain ε. The following values were assumed for steel: yield stress σy of 50 ksi, modulus of elasticity E of 29,000 ksi, poisons ratio of 0.3, and hardening parameter n as 10. Different n factors were analyzed but n=10 gave a smoother looking curve. The G is the elastic shear modulus which is a function of modulus of elasticity and poisons ratio. As mentioned before this material allowed the model to undergo necking and strain localization. Figure 4 shows the behavior of this elastoplastic:
The hyperelastic incompressible Arruda Boyce model was used as an example of a material that will cause necking in the specimen but will not allow for strain localization. The Arruda-Boyce model stress-strain relationship has the following form:
Where U is the strain energy potential, λU is the stretch in the uniaxial direction, and Ii are the deviatoric strain invariants. This model assumes full incompressibility (J= λ1 λ2 λ3=1). The coefficient λm is referred to as the locking stretch. Approximately at this stretch the slope of the stress-strain curve will rise significantly. This model is also known as the eight-chain model, since it was developed starting out from a representative volume element where eight springs emanate from the center of a cube to its corners. The values of the coefficients C1...C5 arise from a series expansion of the inverse Langevin function. The series expansion is truncated after the fifth term. The coefficient λm is referred to as the locking stretch. Approximately at this point the slope of the stress-strain curve will rise significantly. Figure 5 shows the stress-strain curve for the uniaxial form of the Arruda-Boyce model.
Both plane stress and plane strain elements were used to analyze the model. They both provided similar results. The mesh had to be refined as much as possible in order to capture localization. A denser mesh was used in the middle section were localization occurs. Reduced integration was used to avoid volumetric locking.
Defining the correct boundary conditions in the finite element model is crucial to capture the necking and localization phenomena's. As mentioned before to undergo through all the stages that lead to necking, shear free supports must be given at the edges of the specimen. Given that this is almost impossible, the analysis can be started right before necking occurs by either providing fixed boundary conditions or a geometric imperfection at the center of the model. If the geometric imperfection is used then rollers can be assigned to the edges providing a shear free edge support. Usually this geometric imperfection will be a small dip at each side of the plate on the center for a shell element analysis or a change in thickness at the center for a solid element analysis. This report was completed using fixed boundary conditions, not geometric imperfections. The results should be very similar because they both have the same effect which is to trigger a concentration of stresses in the center of the specimen.
A trial run was performed to validate the results using neither a clamped support nor a geometric imperfection. The results were as expected where the plate experienced a constant decrease in thickness throughout the length without any necking or localization.
This problem incurs both geometric and material non-linearities. The material non-linearities as discussed above arise from the non-linear behavior of stress as a function of strain. For the elasto-plastic material the analysis does not include material non-linearities until it reaches the yield stress were it changes to non-linear behavior. The hyper-elastic material is always non-linear. The geometric non-linearties arise from non-linear strains. The Abaqus step module has an option "Nlgeom" which allows the user to include the non-linear effects of large displacements.
For the stated conditions the plate was analyzed and the results are shown in Figure 6 (contours of plastic strain). Figure 6.a shows the preliminary plate before any displacements (dense mesh in middle section). Figure 6.b shows the initial strain formation at the upper and lower edges of the plate, and some other symmetrical accumulations of strains in the middle portion. Figure 6.c shows how the strain in the corners radiated inward and met with the ones previously found in the center. At this point the non-uniforms strains can be observed specially in the edge support (dark blue-smaller strain). Figure 6.d shows the circular formation of strains in the middle section right before localization occurs. This figure also shows how the whole plate is loaded (plastic strain) but the boundary edges have zero strains. This loading and unloading process is very important for localization. At Figure 6.e bifurcation occurs and there are early signs of shear band formations. This figure shows how the top and bottom edges of the center of the plate have lower plastic strains than the middle section from where the shear bands radiate diagonally. Figure 6.f is a close up of the shear bands already formed. Figure 6.g shows the whole specimen at the end of the analysis where in a real test it would have likely already fractured.
To better understand when the bifurcation occurs the following load curves were plotted. This curves shows the bifurcation that starts the strain localization. The following load curves are normalized both in the x and y axis by length and yield force, respectively. Figure 7 shows the normalized load curve for longitudinal displacement. The maximum load occurs at a factor u/L of 0.083 with a normalized critical load of 1.7. Through previous research it has been shown that the actual bifurcation occurs just after the maximum load. This figure also shows the fundamental path which would be followed if localization would not have occurred. Figure 8 shows the normalized load curve for width displacement at a maximum u/w of 0.054.
The plane stress case had similar results to the plane strain case. Figure 9 shows the plate with plane stress elements at the end of the analysis. The main difference between plane strain and plane stress is that in plane stress the shear bands are more pronunciated as can be seen by the slopes of the plate's edges at the end of the diagonals. This can also be observed by looking at the slope along any edge from the boundary to the point were localization occurs. In the plane strain case this slope is less gradual when compared to plane stress. Figure 10 shows the normalized curve with a maximum of u/L 0.089 very similar to plane strain. The maximum normalized critical load is of about 1.45. The curve after the bifurcation point which seems to be somewhat unstable does not provide any real physical meaning since a real specimen would be in the process of fracturing at this time for which a whole new analysis is needed.
The hyper elastic (Arruda Boyce) material was used to explore necking without subsequent localization. Figure 11 shows the stages from beginning to end where no strain localizations can be observed. Figure 12 has the normalized curve for the width displacement which is not conclusive. This curve is linear up until u/w of 0.13 where it becomes an exponential curve. Given that Abaqus had to stopped the analysis after the elements width had become to small to continue integrations the second part of the curve could be due to errors of the finite element analysis. This behavior may also be due to the materials non-linearity. I was not able to find information of this type of analysis with a hyper-elastic material. Figure 13 shows the normalized curve for longitudinal displacements. This plot is linear with a change of slope. Again this change in slope which occurs at around u/L of 0.75 might signal an error. Nevertheless it can be concluded that for small displacements the Arruda-Boyce model shows an increasing linear behavior for necking.
A complete analysis of necking and localization for plane strain and plane stress elements with different materials was presented in this report. The elasto-plastic (steel) material underwent necking and strain localization while the hyper-elastic (Arruda Boyce) only experienced necking as expected. The steel specimens reached a maximum load or critical load where bifurcation occurred and strain localization started. The critical load was reach at about a longitudinal displacement of 8.5% of the initial length, and it varied from 1.45 to 1.7 for plane stress and plane strain elements respectively. This strain localization culminated with the formation of shear bands. For the Arruda Boyce material only necking was observed due to the materials properties. The plane stress / strain tests had similar results. The analysis of 3-D models would have provided a better understanding of these phenomena but substantial computational power is needed to run this type analysis in a realistic time frame. Finally the results of this analysis where compared with past research in this field and there was a strong correlation for the elasto-plastic plane stress/strain results.

Methanotrophs have been known as gram-negative and aerobic bacteria and they use only methane for their carbon and energy source [3]. The initial oxidation of methane to methanol is catalyzed by methane monooxygenase (MMO) that can be expressed differently depending on the environmental factors. The most well known factor is copper concentration by which two different MMOs can be expressed: a soluble cytoplasmic MMO (sMMO) and a membrane-associated, or particulate, MMO (pMMO) [1,3,4,6]. Under low ratio of copper to biomass (≤ 0.9 nmol of Cu/mg of cell protein), the sMMO is expressed; at higher value, the pMMO is [6]. That is mainly because pMMO is a copper-based enzyme [4].
While copper plays an important role in the physiology of methanotrophs, the mechanism of copper uptake system by methanotrophs is still unclear [2]. However, sMMO's mutant of Methylosinus trichosporium OB3b [5, 7] which can express either sMMO or pMMO depending on copper concentration, suggested the presence of an extracellular copper-binding compound (CBC). The CBC was further isolated from M. capsulatus Bath and was shown to be small polypeptides with a molecular mass of 1,232 Da [5, 8, 9]. The two strains, M. capsulatus Bath and M. trichosporium OB3b, have been known to make CBC, but other methanotrophs have never been reported to make CBC. Since methanotrophs such as Methylomicrobium album BG8 and Methylocystis parvus OBBP need copper to express pMMO for the oxidation of methane and appear not to possess CBC, they either have a different copper uptake mechanism or have to utilize the CBC made by other methanotrophs.
This independent study was performed to achieve two goals. The first one was to determine if there was an interaction among methanotrophs for copper uptake; i.e. CBC, the sole copper uptake mechanism. To do so, three methanotroph strains were chosen, M. trichosporium OB3b, M. album BG8, and M. parvus OBBP to compare their growth trends in response to the presence of CBC in the growth media. The second goal of this study was to acquire the basic knowledge and technology of molecular analysis. Hence, the characteristics of the bacterial strains were investigated using naphthalene assay for a verification of sMMO expression, trichloroethylene (TCE) degradation, polymerase chain reaction (PCR), gel electrophoresis (GE), and capillary electrophoresis (CE).
M. trichosporium OB3b, M. album BG8, and M. parvus OBBP were grown at 30℃ on the agar plates of nitrate mineral salts (NMS) medium with the presence of 10 µM copper as Cu(NO3)2 5H2O under a methane-air mixture (1:2 ratio) [10] and cells were transferred to a fresh NMS liquid medium. The liquid culture medium of NMS did not exceed 15 % of the total flask volume to prevent mass transfer limitations of methane and oxygen from the headspace to liquid medium [10].
Whole-cell sMMO activity of M. trichosporium OB3b was examined using the colorimetric naphthalene assay of Brusseau et al. [11]. Since only sMMO can oxidize naphthalene to 1-or 2-naphthol, it could be determined whether or not M. trichosporium OB3b expressed sMMO rather than pMMO depending on copper concentration by adding tetrazotized o-dianisidine to form a purple naphthol diazo complex [12].
NMS media with different copper concentrations, either 0 µM or 20 µM, were prepared to evaluate sMMO expression. As M. trichosporium OB3b can express either sMMO or pMMO, all flasks were acid-washed in 2 N HNO3 for 2 days and 20 µM copper was added aseptically as Cu(NO3)25H2O. Cells were grown until they were in an exponential growth phase (optical density (OD) between 0.2 and 0.5) and then 2 ml of the liquid culture was aseptically transferred to 20 ml vials. Naphthalene was then added to the cell-transferred vials, sealed and incubated at 30℃, 270 rpm for 1 hour. 35 µl of 5 N NaOH was added to the samples to disrupt cell activities. 1.5 ml of the samples were taken and centrifuged at 12,000×g for 5 min. Lastly 130 µl of 4.21mM tetrazotized o-dianisidine was added to 1.3 ml of the supernatant for the absorbance measurement at 528 nm using Milton Roy Company Spectronic 20. Duplicate samples were measured.
Trichloroethylene (TCE) degradation assay was performed with M. trichosporium OB3b given with either 0 µM or 20 µM copper as Cu(NO3)25H2O to evaluate the ability of sMMO and pMMO-expressing cell to degrade chlorinated solvents. Stock liquid culture was prepared as described earlier and methane was removed from the stock culture flask by evacuating the flask and reequilibrating with air performing seven cycles [13]. 3 ml of the stock culture was aseptically transferred to 20 ml vials with Teflon-coated rubber butyl stoppers and aluminum crimp caps, and sealed [13]. TCE concentrations for standard calibration curve varied from 0 to 33 µM in aqueous phase. 9.8 µM of TCE in the aqueous phase of 20 ml vials was added to evaluate the ability of M. trichosporium OB3b to degrade TCE. In addition, the role of formate was investigated adding 20 mM of formate in the form of sodium formate to the samples. By using a dimensionless Henry's constant of TCE as 0.42, the partitioning amount of TCE between the liquid space and the headspace was calculated [13]. The 20 ml vials containing cells and certain amount of TCE were incubated at 30℃, 270 rpm for 6 hours. Control samples for monitoring any abiotic losses were treated with 50 µl of 5 N NaOH to lyse the cells [13]. TCE analysis was performed using an Hewlett Packard 5890 Series II gas chromatograph with an FID detector and the temperature of the injector, oven, and detector were 250, 120, and 250℃, respectively. Triplicate samples were prepared and analyzed in the experiment. All gas phase concentrations were calculated by the standard curve equation.
Since M. album BG8 has been contaminated, it had to be purified before the experiment. The purification was verified using nutrient agar evaluation, and polymerase chain reaction (PCR) and gel electrophoresis. The contaminated cells were diluted 10, 100, and 1,000 times and then spread on NMS agar medium plates to pick up some colonies of M. album BG8-like cells, all of which were aseptically transferred to other NMS agar medium plates and nutrient agar medium plates at the same time and the contamination was observed for more than a week. By performing the procedure repeatedly, cells that could grow on a NMS agar medium but not on a nutrient agar medium were finally selected as potential M. album BG8.
A microscope was used to verify the rod shape of M. album BG8. A simple staining was conducted using methylene blue. A small amount of cells was placed in a drop of Milli Q water on a glass slide and fixed by heating the glass slide. The heat fixed smear was covered by 1 % of methylene blue for approximately 1 min, and the excess stain was washed off. The stained cells were observed using a microscope (Olympus Tokyo Model E 324059).
PCR and gel electrophoresis were conducted to verify if the purified M. album BG8-supposed cells possessed pmoA gene but mmoX gene. For positive controls and a negative control, both M. trichosporium OB3b and M. capsulatus Bath, and Escherichia coli were used, respectively. DNA samples of the bacteria were prepared using a freezing-thawing method and a beadbeater-using method, both of which were developed earlier [14] were performed and the effectiveness of the methods were compared each other.
In the freezing-thawing method, approximately ten loopful-amount of cells in 1 ml of TE buffer that played a role in repressing the activity of DNA degrading enzyme were frozen at-70℃ for 30 min, boiled for 10 min, and mixed strongly for 3 min using a vortexing device. This cycle was repeated 3 times and the samples were finally centrifuged at 13,000 rpm for 5 min. The supernatants were taken as whole-cell DNA samples. In the other method using a beadbeater, 0.1 mm glass beads were filled half of 2 ml Ependoff tube and TE buffer was also filled two-third of the tube. The same amount of cells used in the freezing-thawing method was placed in the tube, shaken at 5,000 rpm for 30 sec, and cooled in ice water for 1 min. Total 6 cycles of the procedure were conducted and the samples were finally centrifuged at 13,000 rpm for 5 min. The supernatants were taken as whole-cell DNA samples. The recipe of PCR is shown in Table 1 and the condition of temperature control is indicated in Table 2. Thermocycle device made by a company automatically performed the temperature cycles and it kept the samples at 4 °C after finishing the thermocycles.
After completing the PCR, 5 µl of sample loading buffer made of 20 % of glycerol and bromophenol blue was added to each sample and DNA molecular weight marker (DNA molecular weight marker VIII, 19~1114 bp, Roche) which was prepared with 20 µl of Milli Q water and 5 µl of the marker. All samples were briefly centrifuged to be mixed and loaded into 1.8 % of agarose gel with TAE electrophoresis buffer (Tris-acetate-EDTA made of 0.04 M Tris-acetate and 2 mM EDTA, pH 8) and 0.5 ppm of ethidium bromide which could bind very tightly to DNA molecules and form a strong fluorescent complex resulting in being visualized by exposure to UV light. Current was applied as 125 volts for 45 min. The separated PCR products in the agarose gel were exposed to a strong UV light using UV device.
Gel electrophoresis is able to show clear bands of PCR products under UV light but it usually requires a relatively long time to see the results of PCR. However, much more precise and rapid results of PCR products can be obtained using capillary electrophoresis (CE).
P/ACETM MDQ capillary electrophoresis system from Beckman Coulter was used under the condition of the reverse polarity mode with either 4 kV or 6 kV applied voltage. The CE separation buffer was prepared as described earlier in Han and Semrau [10]; 50 mM HEPES sodium salt (N-2-hydroxylethylpiperazine-N'-2-ethanesulfonic acid), 65 mM boric acid, 0.5 % HPMC (hydroxypropylmethylcellulose), 6 % mannitol, and 1 µg/ml ethidium bromid. The final volume of the buffer was adjusted to 100 ml. Since 6 % mannitol made the buffer so sticky and bubbling that the buffer had to be degassed overnight by a stirring device using a low stirring speed. The buffer was then sonicated for 30 min to completely remove remained bubbles and stored in a refrigerator before CE analysis. The capillary used was an uncoated silica capillary whose total length, effective length, and inner diameter were 31 cm, 21 cm, and 75 µm, respectively [10].
Both 0.2 N NaOH and 0.2 N HCl as well as the separation buffer were used to rinse the CE capillary before analyzing samples. Each rinsing solution rinsed the capillary for 5 min in order of 0.2 N NaOH, 0.2 N HCl, and the separation buffer under a pressure of 25, 25, and 30 psi, respectively. A sample was then injected at 1 psi for 50 sec and measured at 254 nm of UV detector. To be sure of the CE result quality, all CE analysis was performed after confirming that the current was maintained consistently during CE analysis.
Copper analysis was performed using an atomic absorption spectrophotometer (Perkin-Elmer, model Z5100) with a furnace mode. The amount of sample injection was 20 µl with 5 µl of dilution Milli Q water. All samples were properly diluted to a final concentration within 100 ppb because the detection range of the AA instrument was from 0 ppb to approximately 100 ppb. Energy lamp strength was always maintained over 50 %.
Copper samples were analyzed under the presence of nitric acid that could dissolve copper bound to cell materials. Since heavy metals tend to be precipitated with ligands depending on pH conditions, copper might also be bound to cells or precipitated in a sample, which might cause error in a copper analysis due to the unbalanced distribution of copper concentration in a sample vial. Thus, by making the sample be acidic, copper could be present in a dissolved form in the liquid sample. The proper concentration of nitric acid was investigated analyzing a known copper concentration solution with different concentration of nitric acid ranging from 2 % to 10 %.
MB was isolated from the spent medium of M. trichosporium OB3b and treated with either copper or EDTA: copper-bound MB and EDTA-treated MB. The copper concentrations of the treated MB and the original MB were analyzed. In addition, the copper concentration of a fraction of MB eluted from HP20 column after regeneration was measured as well. Stock solutions of each sample were prepared dissolving 10 mg of each sample into 1 ml of Milli Q water. The samples were diluted 1,000 to 3,300 times depending on the copper concentrations before injecting to the AA instrument.
M. parvus OBBP was grown under the presence of CBC to evaluate the effect of CBC on the growth of M. parvus OBBP. A stock liquid culture was prepared as described earlier including 10 µM of copper as Cu(NO3)2 5H2O. After harvesting cells in the exponential growth phase, cells were washed with pre-warmed fresh NMS medium to remove loosely cell-bound copper, and resuspended on 50 ml of NMS medium samples with different MB and copper concentration.
Two kinds of MB were added to the M. parvus OBBP cultures: copper-bound MB and EDTA-treated MB. Since MB could be destroyed by heating, it was added to the culture after autoclaving the medium. The copper concentrations for the samples were either 0 or 10 µM. In addition, a negative control was prepared adding neither copper nor MB to the M. parvus OBBP culture and a positive control adding just 10 µM copper to the culture. An optical density of each sample was measured at 600 nm. After completing the growth monitoring, copper concentrations of the spent media were analyzed.
Table 3 indicates the results of optical density at 528 nm depending on different copper concentration. The naphthalene assay is based on the fact that only sMMO can oxidize naphthalene to 1-or 2-naphthol. While the color of the 20 µM-copper sample was not changed, the naphthol diazo complex samples of 0 µM copper were in bright purple color. Based on the results, 0 µM copper condition made M. trichosporium OB3b to express sMMO and oxidize naphthalene to 1-or 2-naphthol; on the other hand, 20 µM copper condition to express pMMO that could not oxidize naphthalene. These results were well matched with the known fact that M. trichosporium OB3b can express either sMMO or pMMO depending on copper concentration [3].
The standard concentrations for the calibration varied from 0 to 33 µM in the gas phase. Figure 1 shows a standard curve between TCE concentration in gas phase and peak area. Figure 2 indicates TCE concentrations in the gas phase of the 20 ml tube samples after 6 hour incubation with the initial TCE concentration of 4.12 µM in the gas phase. Although all gas phase concentrations were calculated by the standard curve equation, two data of GC peak area, 0 µM and F 0 µM samples in Figure 2 (b), were converted to the concentration using just two points of the standard concentration because the peak areas were too small to be calculated by the standard curve.
Since M. trichosporium OB3b has been known to express either pMMO or sMMO depending on copper availability [3], M. trichosporium OB3b was expected to express pMMO under 20 µM copper concentration (Figure 2 (a)) and sMMO under 0 µM copper concentration (Figure 2 (b)). As shown in the two figures, sMMO showed approximately 10 times higher rate in TCE degradation than pMMO and TCE degradation efficiency by M. trichosporium OB3b grown with 20 mM formate was 7 % higher than that with no formate. Table 4 shows the TCE degradation rate of the samples. These results were also well matched with those of the naphthalene assay.
Two kinds of potential M. album BG 8 could be picked up from contaminated plates; one was from a very small colony in light yellow, and the other from a streak. The two potential samples were spread on NMS and nutrient agar media at the same time. After 5 days of the incubation, the colony samples showed much higher cells on NMS media and much less cells on nutrient agar media than the streak sample, suggesting that the colony sample had contained much more methanotrophs than the streak sample. The colony samples were then diluted and transferred to fresh NMS media repeatedly. Finally cleaned cells from the contaminated cells showed no growth on a nutrient agar medium but on a NMS medium.
The purified M. album BG 8-like cells and M. album BG 8 that had been stored in a refrigerator for 6 years were observed using a microscope. Simple staining with 1 % of methlylene blue showed the shape of the cells. Both of the cells seemed to be a round shape and seemed alike much. Also there were no other shapes of cells except for the round type, so they might not to be contaminated by other microorganisms.
The PCR product bands shown in Figure 3 were obtained using the freezing-thawing method. While pmoA gene indicated strong bands of both M. trichosporium OB3b and M. album BG8, mmoX gene rarely showed any bands except for one weak band (marked with a circle) in Figure 3 (b). After some consecutive results of no PCR products in mmoX gene such as Figure 3 (a), the quality of the primer set of mmoX gene were suspected, so they were replaced with another one which was also not a new one. With the replaced mmoX primer set, a weak PCR product of mmoX gene of M. trichosporium OB3b could be obtained (Figure 3 (b)). However, no PCR product of mmoX gene of M. album BG8 was shown in all the PCR experiments. The unexpected results of mmoX gene of M. trichosporium OB3b might result from either the poor quality of the primer set or not proper method of DNA sample preparation, or both of them.
With the same DNA preparation method, a freezing-thawing method, the following PCR products shown in Figure 4 and 5 were obtained using three different methanotrophs: M. trichosporium OB3b, M. capsulatus Bath and M. album BG8. Both M. trichosporium OB3b and M. capsulatus Bath were used as positive controls in producing the products of pmoA and mmoX genes at the same time. As shown in Figure 4, M. capsulatus Bath showed two clear bands of pmoA and mmoX as expected, but no bands of M. trichosporium OB3b was indicated. However, the reverse results of those in Figure 4 were obtained in Figure 5; M. trichosporium OB3b showed two distinguish bands of pmoA and mmoX, but M. capsulatus Bath did not.
Although one of two positive controls for the detection of pmoA and mmoX genes did not work properly at the same time in the two PCR experiments and nothing was surely confirmed, a clear PCR product of only pmoA gene of M. album BG 8-like cells was shown in all PCR experiments and also the negative control of E. coli showed no products of pmoA and mmoX genes. Therefore, the cleaned M. album BG 8-like cells obtained another positive evidence for real M. album BG 8.
CE performance for DNA molecular marker VIII from Roche (0.25 µg/µl, molecular size 1,114 ~ 19 bp) was conducted to make sure of the separation quality of the marker prior to an analysis of PCR products of pmoA and mmoX genes. Han [10] applied 1.0 psi for 50 sec and 4 kV for 25 min for sample injection pressure and separation voltage, respectively. With the same conditions as Han [10], the separation of DNA molecular marker VIII was performed and Figure 6 showed the results and a semi-log graph between molecular size (bp) and retention time (RT). According to the composition datum of the DNA marker offered by Roche (Figure 7), the separation quality using Han [10]'s method was quite good except for the baseline at the end of the analysis. So, better separation quality and faster method was investigated changing the sample injection pressure and separation voltage based on the Han's method [10].
Figure 8 indicated the results of the modified method of Han [10]. With 0.5 psi for 30 sec and 6 kV for 15 min for sample injection pressure and separation voltage, respectively, the separation quality was good enough to distinguish each peak, it was faster to separate all peaks, and the baseline during the analysis was fairly good. The current was maintained as-20 µA during the CE analysis. In addition, the DNA marker separation was conducted with the same method as the modified method except for the applied voltage 8 kV instead of 6 kV to make the analysis much faster. As shown in Figure 9, the separation of the peaks performed at 8 kV was good enough and the analyzing time was shorter, but the base line was not as good as that of 6 kV.
PCR products of pmoA and mmoX genes of M. capsulatus Bath were detected using CE analysis. The growth condition of M. capsulatus Bath was the same as shown in Materials and method. DNA samples were prepared using the beadbeater-using method. Although the same method as used in the previous PCR experiments was applied, PCR products did not clearly shown on both gel and capillary electrophoresis (data not shown). So, each PCR chemical and DNA samples were analyzed using CE to figure what shape of peaks could be indicated on CE. Figure 10 shows each peak of the chemicals and DNA samples.
One suspected thing from Figure 10 was that the peaks of the primer sets were much smaller than those of dNTPs. Actually the primer sets were not fresh ones and also they had been stored even in a refrigerator with no power for about 2 days due to the US-nationwide power failure in 2003. Hence, the concentration of the primer sets was adjusted to 0.4 µM (proper concentration range of a primer set is 0.1 ~ 0.6 µM) and the amount of each primer set was recalculated according to the information shown on the labels of the primer sets. The information on each label was as follows: pmof 334 (MW=6,885.5); 2.3 OD260 = 10.8 nmol = 0.07 mg. That information was not enough to calculate the required amount of the primer sets, so the total volume of each primer was assumed 1.8 ml which was the volume of the primer-contained vial. Based on the information, the required amount of each primer set was calculated as 0.4 µM in 25 µl of a sample. The amount of each primer of pmof 334, pmor 640, mmoxf 882, and mmoxr 1403 were 1.77, 0.64, 1.11, and 0.49 µl, respectively. Figure 11 shows the results of CE analysis of pmoA and mmoX genes using either the pre-existed amount of each primer (Figure 11 (a)) or the recalculated amount of each primer (Figure 11 (b)).
With the same PCR products as shown in Figure 11 (b), CE analysis was conducted and the peaks of the PCR products are indicated in Figure 12. As shown in Figure 11 (b) and Figure 12 (a and b), clear bands of pmoA and mmoX genes appeared on both GE and CE results. The sizes of pmoA and mmoX genes were 327 bp and 547 bp according to the DNA marker. The two obvious results were the clearest results after changing the DNA sample preparation method and adjusting the concentration of the primer sets. In addition, both gel and capillary electrophoresis results were well matched each other.
5 % of nitric acid showed a proper concentration of copper, so all copper analysis were performed under the presence of 5 % of nitric acid. Copper concentrations of four different MB are shown in Table 5 and Figure 13. According to the results, MB was proved to have a high affinity to copper binding approximately 260 time-higher copper amount of original MB. Also, MB could still retain about 11 % of the total copper amount even after the ethylenediaminetetraacetic acid (EDTA) treatment.
M. parvus OBBP growth was monitored under the presence of 20 mg/L of MB. Figure 14 and 15 shows the growth curve of the optical density and the relative optical density, respectively.
As shown on the graphs, M. parvus OBBP grown with MB-Cu did not show any growth in both 0 µM and 10 µM copper concentrations of NMS media; however, M. parvus OBBP grown with MB-EDTA was grown well similar to the positive control, M. parvus OBBP grown in 10 µM copper-NMS, except for longer lag phase than that of the positive control. In addition, the growth rates of the three samples were so similar one another even though the growth rate of the positive control was a little higher than those of the others (Table 6). After completing the monitoring of growth rates, whole-cell copper concentrations of the samples were analyzed to compare copper concentrations between initially intended and actual concentrations. Table 7 and Figure 16 indicate the actual copper concentrations of M. parvus OBBP-containing samples.
Basic molecular analysis on methanotrophs was performed to understand the characteristics of the bacterial strains and acquire the analysis techniques. To examine MMO expressions in M. trichosporium OB3b depending on copper availability, naphthalene assay and TCE degradation assay were conducted using non-copper NMS medium and 20 µM copper-containing NMS medium. Since M. trichosporium OB3b expresses either sMMO or pMMO under low copper and high copper concentrations, respectively (the critical concentration was reported as 0.89 µmol copper per gram cell dry weight [10, 13]), M. trichosporium OB3b was expected to express sMMO at non-copper NMS medium and pMMO at 20 µM copper-containing NMS medium. In addition, while the substrate specificity of sMMO is lower than that of pMMO, halogenated hydrocarbon degradation rate of sMMO is higher than that of pMMO [3, 10, 15]; in other words, sMMO can oxidize much more kinds of halogenated hydrocarbons at faster degradation rate than pMMO. In both of the assays, sMMO-expressing cells showed much more degradation efficiency of naphthalene and TCE than pMMO-expressing cells (Table 3 and 4, Figure 2). Also, 20 mM formate played a role in an outside source of reducing equivalent that facilitated the TCE degradation rate. Thus, making methanotrophs express sMMO and adding a proper reducing equivalent in a chlorinated compound-contaminated site would help in situ bioremediation strategy be optimized. During the experiments, it was important for better activities of cells to transfer a part of a liquid stock culture to sample vials when cells were in the initial exponential phase rather than other phases.
PCR and gel electrophoresis were performed to verify a purified M. album BG8. Since both M. trichosporium OB3b and M. capsulatus Bath possess two types of MMO genes, they were used as positive controls to show the PCR products of pmoA and mmoX genes on agarose gel. However, initial continuative experiments could not show clear bands of PCR products of either pmoA or mmoX gene but indicated a thick and blunt band on agarose gel. Three explainable reasons were investigated changing the components of PCE experiment. The first one was the method of DNA sample preparation; using a beadbeater with 0.1 mm glass beads rather than a freezing-thawing method to disrupt cell membrane showed better PCR results. Although the freezing-thawing method could also indicate quite good PCR products, it sometimes failed to show proper products (Figure 3, 4, and 5). The reason why the freezing-thawing method sometimes failed to show good results might be either that the method was too strong to remain a proper gene size or that it was too weak to disrupt the cell membrane. In Figure 3 and 4 using the freezing-thawing method, both pmoA and mmoX genes did not appear at the same time but one of the gene products was shown, suggesting that the preparation method was not too weak to disrupt cell membrane. Thus, the former reason might be a better explanation. The second reason could be the quality of the primer sets that were exposed to room temperature for about 48 hours, because oligoneucletide primers were vulnerable to freezing-thawing cycles and room temperature. After changing another primer set of mmoxf 882 and mmoxr1403, a mmoX gene product could be obtained (Figure 3). The last possible explanation would be not proper amount of the primers used in PCR experiment. Although PCR recipe in Jong-In's lab note [14] suggested taking 0.5 μl of each primer whose concentration should be 20 μM, the concentration of the primers used in this study was so doubtful that the concentrations of the primers were recalculated based on the label information and the required amount of each primer was then calculated. As shown in Figure 11, PCR containing recalculated amount of each primer showed very clear product bands on 1.0 % agarose gel (Figure 11 (b)) even though the DNA sample was the same as used in Figure 11 (a).
According to the PCR results and nutrient agar assay for the verification of the purified M. album BG8, it not only indicated a pmoA gene product in all experiment (Figure 3 (lane 3), Figure 4 (lane 2), and Figure 5 (lane 2)), but also showed no growth on nutrient agar plates. As M. album BG8 can express only pMMO, the M. album BG8-like cells might be purified based on the two assay results. However, the purified M. album BG8 did not grow as well as other methanotrophs in a liquid NMS medium (data not shown). It took more than 8 days to start to grow in the liquid medium, suggesting that the purified M. album BG8-like cells might not be M. album BG8. Thus, it is not clear whether the purified M. album BG8-like cells were completely purified or not. Since the M. album BG8-like cells have been contaminated on agar plates for a long time, it might be possible that they could not be easily accustomed to the liquid medium. Based on the assumption, the M. album BG8-like cells will be continuously tried to grow in the liquid medium and 16S rRNA analysis will be performed if necessary in the future study.
As a more convenient and faster analysis of PCR products, capillary electrophoresis (CE) was used to separate the PCR products of pmoA and mmoX genes. CE method was modified and developed from Han's method [10]. Faster and better peaks could be obtained using 6 kV separation voltages with 0.5 psi sample injection pressure for 30 sec; especially the peaks of 900 and 1114 bp of the DNA molecular weight marker were sharper than those of the previous study [10]. Also, 8 kV separation voltages were applied to achieve much faster results, but as shown in Figure 9, the baseline was not as stable as that of 6 kV voltages. With the modified method, the PCR products of pmoA and mmoX genes were separated and the size of the genes were estimated. Two distinguishable peaks could be observed in Figure 12 (a and b). A pmoA gene product appeared at 10.34 min of the retention time and a mmoX gene product at 12.03 min; the gene sizes were 327 bp and 547 bp for pmoA and mmoX, respectively. These results of CE analysis were well matched with those of gel electrophoresis analysis but showed much faster and sharper peaks.
Methanobactin (MB) is a copper-binding compound (CBC) which was isolated from the spent medium of M. trichosporium OB3b [9]. Although the role of CBC is not known, it is assumed that it might play a role in up-taking copper outside cells because CBC has high affinity for binding copper. Table 5 showed the high affinity of MB to copper. One unusual and interesting thing in Table 5 was that the copper concentration of EDTA-treated MB was higher than that of the original MB. Since EDTA is a strong chelating agent [16] resulting in binding copper much more strongly than MB, the copper concentration of MB-EDTA was expected to be lower than that of the original MB; however, the reverse was true, suggesting that MB could retain the bound copper against the strong chelating agent, EDTA. In addition, M. parvus OBBP could utilize the copper bound to MB-EDTA and showed almost the same growth pattern as that grown under the presence of 10 μM copper. The results indicated that the EDTA-treated MB did not badly affect on the growth of M. parvus OBBP but assisted the growth supplying copper to M. parvus OBBP for the expression of pMMO. However, the lag phase of M. parvus OBBP with MB-EDTA was longer than that of the normal growth of M. parvus OBBP, probably because MB still affected on the growth to some extent, so it took much time for M. parvus OBBP to be accustomed to the MB environment. Also the two growth of 0 μM-ED and 10 μM-ED in Figure 15 showed exactly the same pattern each other. It might be explained that M. parvus OBBP could be grown normally once copper concentration in NMS media met the minimum of the required amount of copper to express pMMO. On the other hand, M. parvus OBBP could not survive under the presence of copper-bound MB (0 μM-Cu and 10 μM-Cu in Figure 14), suggesting that copper-bound MB seriously badly affected on the growth or killed the cells. The cause(s) might be either the effect of MB or the toxicity of high copper concentration, or both of them. According to the copper analysis of the whole-cell and NMS medium (Table 7), the copper concentration of 0 μM-Cu and 10 μM-Cu were 22.0 and 22.9 μM, respectively. 22 μM of copper was usually not a high concentration to methanotrophs, so it could be hypothesized that MB might play a serious role in impeding the cell growth.
Two methanotrophs, M. parvus OBBP and M. album BG8, were purified from contaminated plates but it will be necessary to verify them using PCR and CE with the recalculated amounts of primer sets (Figure 11 (b) and Figure 12 (a and b)). Also, based on the preliminary data of the growth of M. parvus OBBP under the presence of different-copper concentrations of MB, it will be required to determine if M. parvus OBBP is able to show the same growth pattern with the same copper concentrations under no presence of MB. It will be able to suggest which of the factors caused no growth of M. parvus OBBP under the presence of copper-bound MB in Figure 14. In addtion, the growth monitoring of other methanotrophs that can express pMMO but not produce CBC under the presence of the treated MB will make the role of CBC better clear. With the results, it will be possible to give an answer to a hypothesis if there are some important groups that provide copper-needed methanotrophs with CBC among methanotrophs. Ultimately, the way that methanotrophs interact one another based on the necessity of copper will be figured out to understand the copper uptake mechanism of methanotrophs.
Methanotrophs oxidize methane to methanol using either soluble cytoplasmic MMO (sMMO) or a membrane-associated, or particulate, MMO (pMMO). pMMO expression requires high ratio of copper to biomass (> 0.9 nmol of Cu / mg of cell protein), but the copper uptake mechanism remains vague. One of the assumptions of the copper uptake system lies in copper-binding compounds (CBC) that are small extracelluar polypeptides and have high affinity for binding copper. In this independent study, preliminary data suggesting the effect of CBC on the growth of Methylocystis parvus OBBP were obtained and the characterizations of some methanotrophs were performed as well.
Methylosinus trichosporium OB3b showed the effect of copper concentration on the expression of either sMMO or pMMO using naphthalene and trichloroethylene (TCE) assays. Non-copper environment caused M. trichosporium OB3b to express sMMO and the cells then not only showed to oxidize naphthalene to naphthol changing the color of the NMS medium to bright purple in the naphthalene assay, but also indicated approximately 10 times higher TCE degradation rate than those grown with 20 μM copper. Also, 20 mM formate played a role in an outside source of reducing equivalent that facilitated the TCE degradation rate.
Polymerase chain reaction (PCR), gel electrophoresis (GE), and capillary electrophoresis (CE) were performed to verify purified Methylomicrobium album BG8. The purified cells showed a distinct PCR product of the pmoA gene on GE, but no product of the mmoX gene, suggesting that the purified cells might be M. album BG8; however, the cells were not grown as well as other methanotrophs in a liquid nitrate mineral salts (NMS) medium. Thus, the cells need to be monitored continuously in the liquid medium and further analysis such as 16S rRNA analysis will be required to verify them. In addition, the method of CE analysis could be modified from Han's [10] using 0.5mpsi sample injection pressure for 30 seconds and applying 6 kV separation voltages; it showed as better peaks as Han's [10] and faster analysis.
Methanobactin (MB) could, to some extent, hold bound copper against a strong chelating agent, ethylenediaminetetraacetic acid (EDTA). EDTA-treated MB assisted the growth of M. parvus OBBP supplying copper to the cells even though the lag phase under the presence of EDTA-treated MB was approximately 20 hours longer than that of a normal one. However, copper-bound MB killed the cells; it might be caused by either MB toxicity or high copper concentration, or both of them. With the results, it is assumed that MB might be able to play a different role according to a certain environment in an interaction among methanotrophs based on the copper nessecity.

This program was revised based on the beam-column program. After the modification, it can solve any 2-D structure assembled by any number of rectangles (Figure 1). In this revised program, lots of effort was put on the preprocessor modification, such as structural assembly and mesh generator. The 4-node rectangular C0 element was used in this FEA program. In this program, it is assumed that the applied loads are only applied at the nodes. In other words, this program does not consider consistent nodal loads due to body or surface loads.
The program has the following structure:
In the preprocessor, it allows user to assemble any number of rectangles to form a structure and gives the degree of freedom to assign different material properties (Young's modulus, e, and Poisson's ratio, v), element types (plan stress and plan strain), and thickness (t) in different rectangle (however, elements within the same rectangle have the same properties). Another feature within this program is the mesh generator. This function allows user to create specific mesh in each rectangle. The preprocessor has the following structure:
In plotting the "Meshing result for the first rectangle", nodal number is also plotted aside each node (Figure 2). This information makes it easier to assign the boundary conditions and nodal loads in the following procedure.
The algorithm in the "Insert another rectangle" can be shown as follow,
Figure 3 to 6 show the ability of this preprocessor by displaying the assembling process of a frame-like structure.
As shown in figure 9, a 4-node rectangular C0 element has 8 degree of freedoms. The displacement field within the element can be interpolated by the shape function as follow.
(1)
where
The stiffness matrix can be calculated through
(2)
To check the correction of this program, a square plate (2 inch in thickness) stretched by a uniform force was analyzed (Figure 10). It is expected that the convergence problem in this simplest loading configuration is limited. Hence, this configuration was computed using FEA by only one 4-node rectangular C0 element to check the correction of this FEA program. The simulated geometry, meshing, and boundary and loading conditions are shown in figure 11. The support conditions are fixed in x-and y-directions in node 1 and fixed in x-direction and free in y-direction in node 3.
Table 1 and 2 give the comparison between computational and theoretical results. As can be seen, the results are almost identical except some truncation error. Figure 12 shows the deformed shape (dot line, 2000 times) of this problem.
A cantilever beam problem was conducted in this section to do the convergence studies. This cantilever beam problem is identical with homework 2 and 3 (Figure 13) with 35 inches in length, 10 inches in depth, and 2 inches in thickness. The material is isotropic and linear elastic. Poisson's ratio is 0.3. Enforce plane stress condition.
Table 3 shows the computational result of nodal displacement in the y-direction for the right-bottom corner (dy of the black dot in figure 13) calculated with different number of elements. This result is compared to the theoretical values and the value calculated from ANSYS (The element type used in ANSYS analysis was chosen to be the 8Node-82 quad element).
Two theoretical theories were used to calculate the theo. deformation and give comparisons with computational results. The first one is the Classical beam theory, in which the shear deformation is not considered.
The second theoretical calculation considered the shear deformation in the cantilever beam; however, it still assumes that plan remains plan in this calculation. This consideration is probably important for this case because the Depth to Length ratio is 3.5, which is kind of a deep beam rather a slim one.
Figure 14 plots the results in table 3. As can be seen, the computational results (both FEA program and ANSYS) converge to the theoretical value (shear consideration) as more and more elements were used. Second, the convergence speed of deformation is faster in ANSYS than in FEA program. This result can be attributed to different element type used in FEA program (4-Node rectangular C0 element) and ANSYS (8Node-82 quad element). Third, shear consideration is necessary in this analysis (deep beam). Forth, a careful examination of the FEA result shows that the convergence speed of deformation is faster when more elements are created along the horizontal direction than along vertical direction. For example, dy increases from 0.022 to 0.023 when mesh number increases from 4*1 to 4*2. However, dy increases from 0.023 to 0.028 when mesh number increases from 4*2 to 8*2. This is because the geometry, boundary, and loading configuration make the deformation more sensitive to the meshing density along horizontal direction. Figure 15 displays the sequence of deformation with different number of elements (100 times magnitude).
After checking the correction (section 4) and the convergence study (section 5), we have more confidence to use this FEA program. In this section, two complicated structures were computed to demonstrate the ability of this FEA program.

With increased urban development, the fraction of impervious surfaces has dramatically increased throughout the United States. When soil surfaces become impervious, the degree of infiltration of water into the soil substantially decreases, causing increased urban runoff. This runoff is typically either discharged directly into receiving waters resulting in degraded water quality and erosion, or is treated at publicly owned water treatment plants prior to discharge which requires substantial capital investments and often still results in poor water quality during storm surges.
As a response to the concern over increased urban runoff, all new private developments located within Washtenaw County with impervious surfaces totaling more than 5000 square feet must adhere to the Washtenaw County Drain Commissioner (WCDC) procedures and design criteria for storm water management [1]. Further, the City of Ann Arbor requires all such developments to apply on-site stormwater management plans through both structural controls, such as trenches or detention ponds, and non-structural controls, such as vegetated swales or natural storage [2]. The combined effect of both the County and the City stormwater requirements places the burden of stormwater management on the site developer rather than the City, requiring the detention of stormwater on site although not mandating treatment.
As a result, developers must provide stormwater management plans that detail the practices used to treat, prevent and reduce the volume of stormwater on site. As guidelines for the creation of stormwater management plans, Best Management Practices (BMPs) have been developed to characterize effective, efficient and both logistically and economically practical methods for managing stormwater. These BMPs focus on reducing the volume and improving the quality of stormwater as well as reducing the need for capital investments and improvements in the City water management infrastructure. [2]
Although stormwater management plans are intended to require detailed descriptions of the controls implemented, there is no specific requirement on the type of control that should be implemented for a given site plan. The permitting system is intended to allow for innovative, unique and site specific solutions to managing stormwater cost effectively by placing this burden on the developer. However, this requires the developer to understand the technical challenges of stormwater management and the appropriateness of various controls.
As a result, there is a need for consolidated design guides and synthesized case studies to aid developers in selecting appropriate water treatment and detention methodologies. Additionally, characterizing the fate and mass transport, removal pathways, and removal rates of contaminants is critical in designing treatment systems and implementing appropriate site specific management practices. This work is intended to supplement a design guide presented to both the City of Ann Arbor and to developers, describing the suitability and design process for constructed wetlands as part of stormwater management plans.
This paper describes the physics based numerical models used to quantify mass transport in constructed wetlands. A case study is presented of a proposed wetland for West Park, located in the City of Ann Arbor. Finally, a parametric analysis is presented to compare the influence of the design and model parameters (such as the wetland length, diffusion coefficient and contaminant decay rate) on treatment efficacy.
A wetland is an area of land covered either all or some of the time by standing water during the growing season, is made up of predominantly undrained soil, and serves as a transition between aquatic and terrestrial ecosystems [3]. Wetlands differ from rivers and lakes both in water depth and in average water velocity.
Climate and hydrology, or water saturation, dictate the types of soils and plants that can be found in wetlands. Wetlands are classified into two main categories: tidally influenced and inland wetlands. As can be expected, tidally influenced wetlands are typically found along coastlines and surrounding bays. Whereas inland wetlands typically border lakes, streams and rivers. Types of wetlands include marshes, wet meadows, swamps, bogs, prairie potholes and fens. [4]
It has been estimated that over 50 percent of the wetlands in the contiguous U.S. have been lost [5]. Between 1986 and 1997, the U.S. Environmental Protection Agency estimates that 58,500 acres of wetland were lost in the lower 48 states each year, with an estimated 105.5 million acres existing in 1997 [4]. This rapid decrease in wetland area is due in large part to drainage, dredging, deposition of fill, logging, construction, mining, damming, tilling, overdrawing groundwater aquifers, and many other human related factors [4].
Constructed stormwater wetlands are water treatment wetlands designed to improve the water quality of urban stormwater while increasing on-site detention to mitigate the effects of storm surges on water treatment plants or riverways [6].Constructed wetlands can also be used to treat municipal wastewater prior to discharge [7], or for treating industrial wastewater such as wood waste or landfill leachate [8].
Constructed wetlands are considered to be favorable control mechanisms for storm water management because they reduce stormwater contaminant loading, provide increased detention capacity on site, and increase recreational opportunities and wildlife habitat. Although not always suitable as a sole stormwater management control mechanism, constructed wetlands have gained increased attention as an option for controlling stormwater within a diverse stormwater management infrastructure.
This section outlines the typical composition of urban stormwater, the contaminant removal pathways, and design considerations for the construction of treatment wetlands.
The composition of urban stormwater is influenced by land use and varies throughout the year. Typical stormwater pollutants include nutrients (nitrogen and phosphorus), solids (sediment), pathogens (bacteria and viruses), metals (lead, copper, cadmium, zinc, mercury, chromium, aluminum), hydrocarbons (oil, grease, napthalenes), organics (pesticides, PCBs, synthetics), and salts [9].
The solids collected from stormwater can be classified as litter (greater than 6.35 mm) or non-litter (less than 6.35 mm) as shown in Figure 1. Litter is then further classified as gross, wet, or dry, then floatable or non-floatable, and finally biodegradable or non-biodegradable. Non-litter particles are classified as sediment, gravitoidal, colloidal, and dissolved. [10]
Release rates of other pollutants are impacted by the particle dynamics of suspended solids; additionally, rainfall intensity directly impacts the ability to mobilize particles as well as the size of the particles mobilized and occurs as a random process [11]. Figure 2 shows the fraction of dissolved copper, lead, nickel and zinc measured from state-wide California highway runoff characterizations from 2001-2003 [11]. Although the median colloidal and dissolved concentrations are between 30-60%, the concentrations can vary from 0-100%, making modeling of the fate of suspended solids non-trivial.
The removal of stormwater pollutants involves a complex interaction of physical, chemical and biological processes. Stormwater wetland contaminant removal pathways include sedimentation, adsorption, filtration, and microbial, plant and algae uptake [6]. A description of several removal mechanisms for macrophyte based wastewater treatment wetlands, where macroscopic plants play a critical role in the water treatment process, is provided in Table I.
Water soluble organic compounds are typically removed by bacteria attached to plant and soil/sediment surfaces. The diffusion of oxygen from the atmospheric/water interface and photosynthesis within the water column, along with leakage of oxygen from plant roots support the aerobic removal or these organic compounds. [12]
Nitrification and denitrification converts nitrates into nitrogen gas. As with the processing of organic compounds, the oxygen required for this process is provided from the atmosphere or from leakage by plant roots. Plant uptake is typically a less dominant removal mechanism than denitrification, but does occur. Depending upon the water pH levels, ammonium can be converted to ammonia gas and released. [12] The typical nitrogen cycle in a wetland is displayed in Figure 3.
Phosphorus removal occurs via adsorption, complexation and precipitation reactions with aluminum, calcium, iron and clay in the sediment layers. For low concentrations of phosphorus, sedimentation and filtration can be a significant removal mechanism. [12] The typical phosphorus cycle in a wetland is displayed in Figure 4.
As a result of these removal processes, wetland soils typically contain a shallow oxidized soil layer over a reduced soil layer, creating constituent concentration gradients. Figure 5 graphically displays the soil profiles of reduced manganese, iron and sulfur along with the redox potential.
Due to the complex nature of contaminant transport and the dynamics associated with the physical, biological and chemical interactions of contaminants in the wetland system, a trial and error approach has been taken in designing treatment wetlands. Numerous case studies have been published to detail wetland designs and treatment effectiveness. From these case studies design guides have compiled general rules of thumb for designing treatment wetlands that remove particular pollutants, in certain concentration ranges, for specific climates. A brief summary of some of these design strategies is provided here.
There are four basic stormwater wetland designs [6]:
Numerous factors such as the pollutant removal capability, land consumption, site water balance, contributing watershed area, maintenance requirements, and wildlife interactions play a role in the selection of an appropriate wetland design for a specific site. It is however, important to note that constructed treatment wetlands have a different functionality than constructed wetlands designed to mitigate the loss of natural wetlands. Additionally, treatment wetlands should not be located near or adjacent to natural wetlands due to their protection by local, state and federal regulations. [6]
Figure 6 provides comparative profiles of the stormwater wetland designs.
Important objectives to consider in the design of stormwater treatment wetlands are [6]:
For optimal pollutant removal, design recommendations are to provide a [6]:
These sizing criteria for the design of stormwater wetlands are summarized in Table II.
Additionally, it is suggested that the water depth in the deepwater cell and forebay be one to six feet below normal pool, the lo marsh should be 6 to eighteen inches below normal pool, the hi marsh should be zero to six inches below normal pool, and the semi-wet region should be zero to 2 feet above normal pool [6].
Lists and descriptions of wetland plant species suitable for surviving exposure to certain pollutants, draught and inundation, along with tolerance to certain water depths are outlined in [6] and [7]. The selection of particular plant species and modeling of the complex dynamics associated with their pollutant removal capabilities is beyond the scope of this work and will not be addressed.
The Allen Creek is currently plumbed beneath the surface of the park. The proposed wetland serves to daylight the creek, improve water quality, increase detention capacity within the Allen Creek Watershed, increase habitat, and educate residents on the use of constructed treatment wetlands within the context of stormwater management. A site analysis is provided in this section along with a description of the proposed wetland design.
The fraction of impervious surface throughout the Allen Creek watershed has dramatically increased in recent decades, causing a significant burden to the existing stormwater infrastructure. Street, yard and house flooding events occur throughout the area, along with blown man-hole covers, placing increased attention on West Park and its potential to reduce the flow of stormwater during and following precipitation events.
West Park is located along the western edge of downtown Ann Arbor, a city with a population of approximately 114,000 full time residents, as shown in Figure 7. An aerial view of West Park along with five foot topographic contours is provided in Figure 8.
Single and multi-family residential housing borders the park. The park is located in a bowl, with 15-20 foot ridges along the northern and southern boundaries. The city of Ann Arbor has located concrete and wood structures along with vegetation to stabilize these slopes and reduce or prevent erosion and land slides. There is a 30 foot elevation difference from the western to eastern boundaries.
There are many existing landmarks within the park, such as a marker indicating the site of a Native American trail, an outdoor amphitheater, playground, water fountain, pergola, several hundred year old trees, and the City's oldest baseball field. The park is frequented by residents and is considered to be an integral part of the local community.
Unfortunately, West Park has its' own localized flooding concerns. Much of the soil in the park is considered undisturbed and little modifications to the topography have been made over the years. Due to the difference in elevation of the park with respect to the surrounding neighborhoods, overland stormwater flow collects in several locations within the park. Spot fixes including vegetated swales and depressions have been created to store this stormwater in small quantities, however does not provide a large enough storage volume to prevent the large expanses of turf grass from remaining saturated many days after a rain event.
To reduce the amount of space required by the wetland system, increase stormwater detention capacity, and enhance the removal of suspended solids, a pond/wetland system was selected. First, the amount of space to be allocated to the treatment wetland, the location of the wetland within the park, and the general water flow path were determined. Then the surface area to volume ratios were calculated according to the design criteria set forth. Finally, the site was regraded to achieve the recommended side and bottom slopes as well as to accommodate the necessary wetland cell depths.
There should be little elevation change in the hi and lo marshes. The largest surface area available for the marshes and thus the critical sizing parameter, was located on top of the existing baseball field. Although considered to be a desirable amenity, the baseball field was removed and used for the marsh system. The outlet of the wetland system must be located close to the Huron River and thus was placed on the eastern edge of the Park. Due to the elevation, the inlet to the wetland system must then be located along the western boundary, either from the northern or the southern corner. Because the amphitheater restricts the available space for the forebay and deepwater marsh, the southern corner was selected for the wetland inlet.
Following the recommended design criteria, maintaining a 30 foot flood buffer zone, while minimizing the necessary regrading of land, the surface area to volume ratio were achieved with the constraint that the forebay and deepwater marsh must be located on the shelf leading towards the existing baseball field. The resulting volumes at normal pool level and full bank are listed in Table III.
A sketch of the wetland boundaries (solid lines) and the buffer zone (dotted line) superimposed on an aerial view of the park is shown in Figure 9.
Treatment wetland models are typically constructed for one of four main reasons, to: examine the hydrologic response of the wetland to storm surges, investigate the biological response, explore contaminant transport and removal processes, or describe the wetland hydraulics for use in designing control structures. The degree of complexity of these models varies from static algebraic expressions to high order dynamical equations accounting for the conservation of mass, energy, and/or momentum.
Water budgets are used in hydrologic models to characterize the movement of water through the wetland system. The objectives of these models are often related to stormwater detention, and thus focus on the inputs and outputs to the wetland and treat the wetland itself as a lumped parameter single volume. Konyha [13] employed a first order lumped parameter model using
where Q is used to denote volumetric flow rate, and S for the stored volume. Water enters the wetland via
, (2)
where P is used for precipitation, SRO for surface runoff, DRN for subsurface drainage, B for baseflow, and G for groundwater seepage and springs. The output flows are characterized by
, (3)
where AET is used for evaporation and transpiration, Rp for flow over the spillway, Re for flow over the emergency spillway, L for lateral seepage, and D for deep percolation. Additional models are then used to relate these variables to physical properties, such as expressing evaporation as a function of temperature. This methodology is not unique and has been described in various forms in standard hydrology textbooks on flood routing [14]. The implications of the use of these models depends upon the degree of complexity incorporated. Additionally, they focus on hydraulics and often neglect the presence of pollutants.
Walker [15] applied a two-dimensional momentum balance along with continuity, to formulate a numerical model of the flow processes in a constructed wetland. The momentum balance in the x direction and continuity are described by
, (4)
where
The processes of sedimentation or uptake are typically modeled as first order decays using specific removal rate constants for given constituents (BOD, TOC, nitrogen, pathogenic microorganisms, heavy metals, and trace organics) regardless of the actual removal mechanism [16], [17]. Kadlec [17] compared the impact of assuming the wetland volume of interest behaved as a plug flow reactor (PFR) or a continuous stirred tank reactor (CSTR) using dye tracer studies and found that the wetlands investigated were best described by series and parallel combinations of PFRs and CSTRs. The PFRs were modeled assuming steady-flow, first order irreversible reactions using,
, (5)
where C is used for concentration, k is a decay rate constant, t represents the nominal retention time, and Cin is used to denote the inlet concentration. From inspection, it is clear that this equation was derived by neglecting diffusion/dispersion and advection. An advantage to this formulation is that it can be easily tuned with experimental data using input and output measurements of concentration. The decay rate can then take on a functional relationship according to statistical analysis or flow regime characteristics [18].
Kadlec [17] went further to describe long narrow reactors (wetlands) by modifying the PFR with a dispersion coefficient of the form,
, (6)
where D represents the dispersion coefficient, u is the average velocity, and L is the length.
Walker [15] used the following two-dimensional transport equation to describe the spatial and temporal evolution of a constituent concentration,
, (7)
where C represents the constituent concentration, U and V are the component velocities in the x and y directions, D is the water depth, and S is the dispersion coefficient. The dispersion coefficients are then represented as a function of the component velocities. The intent of these numerical models was to quantify the residence times and predict the flow pattern. The algorithm incorporated by Walker in modeling sediment transport is shown in Figure 10.
Fig. 10. Flow chart for modeling sedimentation [19]
The relationship of the design parameters and flow characteristics on contaminant transport was of interest in completing this work. A simple zero or first order relationship, as described in Equations 5 and 6, between constituent concentration and time is not capable of simulating the dynamic response to changes in inlet concentration or water velocity. The two dimensional transport model, although accurately quantifying transport in constructed wetlands as compared to the low order models, is computationally intensive. As a result, a one dimensional advection diffusion reaction equation was employed of the form,
, (8)
where U is the average velocity in the direction of the flow path, k is a decay rate constant, D is the diffusion coefficient, and C is the constituent concentration. The inlet concentration was assumed constant for this analysis, and the initial concentration in the wetland was assumed to be zero. The location of the outlet, x = L = 800m, was set to be equal to greater than two times the actual wetland length (350m) to ensure the boundary condition did not have a significant influence on the solution profile.
Johengen [20] published data quantifying nutrient removal in a stormwater treatment wetland. These data included nitrate, ammonium, and phosphate removal efficiencies. Although these data were taken by comparing inlet and outlet concentrations, the diffusion coefficient and decay rate were tuned such that these efficiencies were achieved. It is important to note that there is not a unique combination of parameters that satisfy the removal efficiency requirements.
The model was simulated using the "pdepe" initialboundary value solver in Matlabr for parabolic and elliptic one-dimensional partial differential equations.
For this analysis a general constituent was modeled, rather than a specific constituent such as phosphorus, nitrogen, or sediment. The model developed in Equation 8 assumes the diffusion coefficient and decay rate are constant parameters that depend on the constituent of interest. This analysis examines the impact of varying these parameters throughout the range of values expected for urban stormwater pollutants.
A minimum flowrate of 0.0006 m3/s is recommended throughout the wetland [6]. The smallest average cross sectional area in the wetland is approximately 2.3 m2, resulting in a average minimum velocity of 2.6x10−4 m/s. Assuming the maximum average velocity to be ten times greater than the minimum velocity, or 224.6 m/day, a constituent concentration was simulated for the length of the wetland assuming a uniform decay rate constant and diffusion coefficient. The resulting steady-state velocity profile assuming three different values for the velocity is given in Figure 11. For the range of velocities considered, the concentration profile does significantly depend on the velocity and therefore should not be neglected from the model.
The impact of the decay rate and dispersion coefficients on the contaminant transport were then considered, as shown in Figure 12. As expected, the decay rate has the greatest influence on the concentration profile and the diffusion coefficient has very little impact on contaminant transport for the range of parameter values considered.
Tuning the parameter values to achieve measured inlet and outlet concentrations is rather straightforward. However, assuming a single decay rate and diffusion coefficient for the entire wetland constrains the solution of the concentration profile. It has been well documented that particular pollutants are targeted for removal within certain wetland cells [6]. For example, the large majority of suspended solids are removed in the forebay and deepwater marsh. As a result, if a more accurate prediction of the actual concentration profile is required,
different parameter values should be used for each wetland cell.
By assuming that each wetland cell (forebay, deepwater marsh, hi/lo marsh, and micropool) has an individual constant lumped decay rate and diffusion coefficient, different pollutant dynamics can be incorporated for each wetland cell. Figure 13 and 14 display the simulation results and the sensitivity of the concentration profile to the individual parameter values. The nominal decay rates assumed were
As expected, similar inlet and outlet concentrations can be achieved with significantly different concentration profiles. Additionally, the concentration profile in upstream wetland cells, such as the forebay, is not impacted by changing the decay rates in downstream cells. Finally, manipulating the diffusion coefficient in the various wetland cells has little impact on the concentration profile as compared to the decay rate. As a result, it is recommended that a constant diffusion coefficient be incorporated with a spatially varying decay rate to model the concentration profile through the length of the wetland.
The temporal and spatial evolution of concentration was simulated for a 10 day period, as shown in Figure 15 with a constant diffusion coefficient and velocity throughout the wetland and employing the four nominal decay rates for the individual wetland cells.
By increasing the decay rate in the forebay to
Wetland design guides typically provide qualitative assessments of various wetland designs for a wide range of applications, such as stormwater, wastewater, greywater, commercial, and residential treatment wetlands. While easy to follow and replicate, these guides provide little assurance that the proposed design will function as intended and typically target Best Management Practices, not design and control. Existing models for characterizing contaminant transport in treatment wetlands vary greatly depending upon the application of interest. Typically, these models focus on wetland and watershed hydraulics, assume zero or first order transport relationships and are tuned using measured input and output pollutant concentrations. The models that do capture the complex interactions of transport phenomena are difficult to tune and cumbersome to use in practice.
Using a one dimensional advection diffusion reaction equation to describe the pollutant transport through the proposed wetland, both the reaction rate and the diffusion coefficient impact the spatial concentration distribution. Surprisingly, the average water velocity does influence the concentration profile and should not be neglected. If a simple transport model is sought, it is recommended that a spatially varying reaction rate be employed with a constant diffusion coefficient.
Future work could incorporate hydrologic models to assess the impact of stormwater surges on contaminant transport. Additionally, data for expected stormwater constituents in the City of Ann Arbor, along with information on particular species could be used to provide a more detailed analysis of particular constituent concentration profiles.
I would like to acknowledge the help and support received from Danielle Kahn, a graduate student in the Landscape Architecture Program at the University of Michigan. Her role in this project was instrumental in developing an innovative and unique final design for West Park, which incorporated many of the design elements presented here.

It is widely understood that education is an essential component to the development of any society. A more educated population leads to a more advanced health care system, more efficient use of resources and, generally, more wealth for the entire economy. Therefore, it is vital that the adult population of any developing nation today invests in the education of their children if they wish to expand their economy. Over the past twenty years, conditional cash transfer programs have begun to address this issue by offering stipends to families with school-age children, provided that these children have documented regular attendance in school. These programs have been especially popular in Latin America where Progresa, the cash program in Mexico, exhibited wonderful success in its preliminary stages starting in 1998. Since then, these programs have spread like wildfire throughout Latin America, with individual countries adapting the model slightly to meet their own needs. The hope is that these conditional transfers can replace the wages children might earn from working or helping with the family business, and thus reduce or eliminate the family's opportunity cost of sending these children to school.
Still, questions remain regarding the magnitude of the opportunity cost of attending school, and regarding the components that create this opportunity cost. Is the opportunity cost of attending school higher for families with fewer children, where each child's work has a greater marginal return to the family's income? Or, do these families have fewer children because they do not need as much help generating a sustainable income, hence diminishing the effect of the children's contribution to family income? Is family size simply a result of the family's income level prior to having children; that is, are a family's fertility decisions independent of the future income these children may generate? These are the types of questions that this analysis seeks to answer in the context of Costa Rica. The paper proceeds as follows: Section II describes the theoretical and empirical motivation for studying this question, and specifically, for studying Costa Rica. Section III describes the data and methodology used to answer these questions, Section IV describes trends in the data, Section V presents and interprets regression analysis results, and Section VI concludes.
Gary Becker's theory on the quantity and quality of children describes a negative relationship between the quantity of children a family has and the quality, or amount of investment, which the parents put into each child. Becker and Lewis argue that this inverse relationship results from the increasing shadow price of a child as the level of quality of children increases (Becker, 280). Taken in the context of education, this theory implies that families who desire higher levels of education for their children will have fewer children due to the fact that they will have to pay more for each child's education as the level of education rises. Empirical studies have also found this relationship to be true. For example, Lam and Duryea find a negative relationship between parents' schooling and fertility in Brazil. They link this relationship to an increase in the level of schooling of each child in the family. At lower levels of schooling, this decrease in fertility results from an increase in child quality, which is measured by an increase in schooling (Lam and Duryea, 176).
A study of this phenomenon in Costa Rica is unique because of specific characteristics of Costa Rica, some which separate it from some its Latin American neighbors. First of all, its literacy rate is one of the highest in Latin America at 96 percent. According to the CIA World Factbook, the 2006 literacy rate in Brazil is 88 percent, 91 percent in Mexico, and 67.5 percent in Nicaragua. Secondly, it has a young population, with nearly 30 percent of its population under the age of 14, and a median age of 26.2. With a population growth rate of 1.8 percent in 2007, the relationship of family size and education level certainly affects a large subset of the population. To compare, growth rates in Brazil and Mexico are 1.0 percent and 1.1 percent, respectively, whereas the growth rate in Nicaragua is 1.9 percent (CIA Factbook). Therefore, while Costa Rica has literacy rates that are similar to more developed countries in Latin America, it behaves like smaller, less developed countries in terms of population growth. For these reasons, it poses an interesting case to study.
Additionally, the relationship between family size and education in Costa Rica is interesting because a conditional transfer program, called Superémonos, implemented in 2001 found that providing one monthly food coupon to selected Costa Rican families if all children between 6 and 18 attended school increased school attendance among these families by 2.9 percentage points, an effect equivalent to increasing the mother's education by six years. However, when comparing the mean attendance rate of families who received transfers to those who did not receive a transfer, the difference was not found to be significant (Duryea and Morrison, 11-12). While there could be several explanations as to why there is no significant difference in mean attendance rates, this impact evaluation is perhaps the greatest motivator for analyzing the relationship between family size and educational attainment. Regression analysis captures a bit of the opportunity cost of sending a child to school, but the lack of significant difference in means could suggest that perhaps one food coupon per family, regardless of the number of children between 6 and 18 might not always be an effective motivator to send children to school. The next sections explore family size in Costa Rica in more detail, in hopes of deepening the understanding of this relationship.
In order to look at these questions, this paper uses IPUMS census data from Costa Rica in 1973, 1984 and 2000. The three years of data will provide a cross-sectional analysis of education and family size over time. To begin, it is important note drastic changes in demographic trends over time. Latin American women averaged approximately 6 births in 1950, but fertility rates have decreased rapidly over the past fifty years to hovering just above replacement fertility in 2000 (Population Reference Bureau). Therefore, each year will provide an interesting contrast in comparing family size and education, holding other factors constant. The entire population of individuals with education information available in 1973 is 154,885. The sample size increases to 208,958 in 1984, and 343,642 in 2000. The mean age of the sample in 1973 is 26, increasing to 27 in 1984 and 30 in 2000. In other words, the population is aging slowly over time.
Variables included in the IPUMS dataset that are especially important to this analysis include educational attainment, representing the current (or completed) level of education of every person in the household, and the variable indicating the number of people in each household. Those who are have no education information listed, most likely children under age 6, are excluded from the sample. In this analysis, the number of children born to a child's mother is used as a proxy for the number of siblings that child has, and in this case the number of children born is interpreted as a measurement of family size. It is important to note the assumptions when using number of children born to the mother as a proxy for the number of siblings. In certain cases, some of those children born to the mother may no longer be alive, or some of these children may be half brothers or sisters that do not live in the same household. Obviously, these absent children have no impact on the observed child's educational attainment. Still, this variable attempts to capture some of the possible competition between siblings for the opportunity to go to school, if parents need one child to stay at home or work. In additional specifications, the number of people per household is understood to be another measurement family size, although in some cases multiple generations may live in one household, or the household may contain people who are not directly related to children in the family. One additional model is run using this measure of family size as a sort of robustness check to the effect of number of siblings.
Educational attainment is grouped into four categories: less than primary, primary, secondary and university. Most analyses will use years of school in place of this variable for a more accurate interpretation of effects. When years of school is used, 6 years of school can be compared to completing primary school, and 11 years of school generally is understood to correspond to completion of secondary school. Any additional years of education indicate participation in some type of specialized technical education or university. Other variables to be used as controls in regression analysis include an indicator for whether families live in an urban or rural neighborhood, and a proxy for income. While the IPUMS data on Costa Rica does not include information on household income, this analysis uses home ownership, availability of electricity and possession of a refrigerator as proxies for income. Certainly, these variables cannot be interpreted as any monetary equivalents of income, or even as a continuous distribution of income levels. They simply group the population into a higher income and lower income bracket. In other words, if a household owns its home rather than rents its dwelling, has electricity and has a refrigerator, it is assumed to be a higher income household than a household that does not possess these basic durable goods.
Analysis will focus on children ages 9 to 15 as a whole, and the older subset of this age group, children 14, 15 and 16. On average, children ages 9 to 15 are most likely both still living with their parents and attending school, so there may not be as much variation in educational attainment in this population group. However, as children reach their early teenage years, their probability of working increases, especially after the completion of primary school. Children in this age group are old enough that, should the family need extra help earning money or working in a family business, parents could pull these children from school to work more easily than they could pull younger children from school.
These variables will be used to study the relationship between family size and educational attainment by comparing trends of family size over time, by level of education and with control variables. Then, multivariate ordinary least squares regressions will estimate effects across the entire age group by year, including only children ages 14-15 by year, analyzing children aged 16 separately, pooling all years together. One final robustness check will number of people living in a household as a robustness check for the effect of family size.
Before analyzing the effects of these variables on educational attainment with regression analysis, it is important first to gain a picture of the relative trends and distributions of families in Costa Rica. To begin, Table 1 presents the mean values of important variables in the sample in each of the three years studied. As seen in this table, the population in 2000 is older, more urban, with smaller families, more schooling and more household durables than the population in 1973 or 1984. The following figures investigate these changes further.
Figure 1 shows the changes in the distribution of family size over time. This graph shows a relatively normal distribution of children born across all years, although this distribution is slightly skewed to the left. Furthermore, this graph shows a clear decline in the variance of the number of children born to women in Costa Rica over the three years surveyed. In 1973, approximately 44 percent of women in Costa Rica had at most five births over their lifetime; by 2000 this percentage increased to nearly 78 percent, with 40 percent of those women having only two or three births. This shows quite clearly the women's changing preferences of childbearing over time.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica.
When looking at the mean number of children born to mothers Costa Rica by years of education, it is clear that schooling adds another dimension to this relationship. Not only is the number of children born to Costa Rican women decreasing with her level of education, but the average number of children born to women at each level of education is decreasing over time. While this graph suggests that households where the mother has higher levels of educational attainment are generally smaller than those where the mother has less education, it does not shed light on the causal relationship between these two factors. This highlights the need to separate the effects over time from the effects of educational attainment to determine if smaller households result in family members attaining higher levels of education, or if these two factors are simply correlated, but caused by some third variable, such as income.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica.
These two graphs naturally beg the question of the distribution of educational attainment over time. Because it appears that family size is negatively correlated with educational attainment and over time, it is very plausible that the population of Costa Rica is becoming more educated over time, thus explaining these two results. In fact, this is exactly what the data shows. Figure 3 shows the highest level of education attained by mothers of children ages 9-15 over time. The sample was limited to the education of mothers with children ages 9-15 due to the direct impact of their education on the children to be analyzed in the regressions in this paper. As seen below, approximately 15 percent of mothers completed six years of education, or primary school, in 1973. This number grows to 25 percent in 1984 and 35 percent in 2000. Approximately 10 percent of mothers complete secondary school in 2000. Additionally, the percentage of mothers completing more than 11 years of education increases from approximately 2 percent in 1973 to 6 percent in 1984 and 12 percent in 2000. This shows that mother's education is increasing over the same period of time during which they are experiencing decreasing fertility.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica. Mothers represented here are those with children ages 9-15 in the year of the survey. Schooling level shown is the maximum level of schooling the mother in each household has attained.
In order to examine whether the level of schooling has risen unambiguously in Costa Rica over time, Figure 4 shows a Lorenz Curve of the years of completed schooling for individuals over age 15. Because the top line in this graph represents the year 2000 and the bottom line represents 1973, it is clear that the distribution of education in Costa Rica becomes increasingly more equalized from 1973 to 2000 because these lines do not cross. Still, the magnitude of the gains in equality over the entire period is relatively small. This graph confirms the aforementioned conclusions that the population of Costa Rica has become more educated over time. However, all three curves remain very close together and are clearly concave; indicating that more work needs to be done to equalize education in Costa Rica.
It is also useful to briefly analyze the breakdown of educational attainment and family size across different control groups. For example, Table 2 shows the mean years of schooling and standard deviations across the three income proxy variables and across the indicator for urban households. This provides a basic understanding of how educational attainment varies across different income groups, even if income is not explicitly described by these proxies. Also, this table represents mean years of schooling over all three census years.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica. Yrschl=Years of Schooling. Sample is limited to individuals over the age of 15, to be interpreted as individuals who have completed their education. Top curve in the graph represents 2000, the middle curve represents 1984, and the bottom curve represents 1973.
Intuitively, as shown by the previous figures, one can assume that these means would be higher if the sample were limited to include only 2000, and lower if the sample included responses only from 1973.
As shown by this table, the average number of years of schooling nearly doubles in households that have a refrigerator or electricity. While the effect is not as large for a household owning its dwelling, there still is a positive effect. This confirms that the three binary variables selected serve as a good proxy for income, since literature shows that higher income families generally have higher levels of education. When looking at the corresponding means for urban locations compared to rural locations, Table 2 shows that households in urban areas have, on average, 2.5 more years of education than households in rural areas. Again, this relationship is paired with smaller households in urban areas compared to rural areas, as seen in Figure 5 below. Nearly 40 percent of the urban population of Costa Rica has a household size of 4 or 5 people, compared to approximately 30 percent of the rural population. These two distributions are marked by similar skews as the previous figures.
Source: Integrated Public Use Microdata Series, Costa Rica. Compiled by University of Minnesota and the National Institute of Statistics and Censuses, Costa Rica. Relationship above presented across all three years of the sample. Again, Table 1 shows that the households become smaller and increasingly urban over time.
Hence, all of these trends and summary statistics indicate that households in Costa Rica that are smaller, richer and urban have higher levels of education than those that are not. The next section employs regression analysis in order to tease out the magnitude of these effects, as well as the importance of family size in consideration with these other factors.
Initial ordinary least squares regressions were run on years of education, controlling for number of siblings, age, age squared, sex, mother's schooling, father's schooling, urban areas, owning a refrigerator, owning a home, and having electricity. Separate models were run for children ages 9-15 in each year of the survey. The coefficient of greatest interest for this analysis is the coefficient on the child's number of siblings in the household, and the other variables simply serve as controls. The results of this regression are shown in Table 3 below.
The first, most striking observation is that all of these variables significantly explain some of the variation in years of schooling. Additionally, the negative coefficient on the number of siblings is increasing over time. While the coefficients are statistically significant, the substantive significance of these statistics is quite small. For example, the coefficient on the number of siblings in 2000 means that increasing the number of siblings a child has by one decreases the number of years of schooling of a child ages 9-15 by 0.09 years. In other words, increasing the number of siblings by 10 decreases the years of schooling of a child ages 9-15 by one year. However, as seen previously by Figures 1 and 2, the average number of children born to a woman in 2000 is less than eight, across any level of educational attainment. Therefore, it is unlikely that a child's number of siblings would have a large effect on their educational attainment.
In Figure 4, children ages 16 were examined separately to investigate if older children's education is more sensitive to changes in the number of siblings. The coefficients on number of siblings for sixteen year olds are slightly larger when compared to those from children ages 9-15, but the coefficients suffer from the same questions of substantive significance as in the previous model. However, it is interesting to note that coefficient on being male is negative, and that the magnitude of this coefficient is larger for 16 year old boys than it is for boys ages 9-15. This coefficient demonstrates the diminishing returns of schooling for boys; after age 15, the opportunity cost of attending school is significantly larger than it is for younger children. Hence, many boys over age 15 drop out of school to work, or are encouraged to do so by their families. This fact is compounded slightly by one additional child in the family, increasing the negative effect of being male on schooling in larger families.
In Table 5, the samples from all three years are pooled into one model, and dummies for 1984 and 2000 are added to account for variation in education levels over time and other unexplained year effects that could affect educational attainment in a given year of the survey. As different controls are introduced into the model, and interesting result emerges. While in the first specification, the coefficients on years 1984 and 2000 are positive, indicating that the level of education is increasing over time, the signs on these coefficients are reversed once parents' education and the number of siblings are included in the model. Not only is the sign reversed, but the magnitude of the effect of years 1984 and 2000 increases. This is contrary to what is expected, one would expect these additional controls to absorb some of the variation of educational attainment over time. This changing sign suggests the possibility of the presence of multicollinearity in the regression; that is, the fact that parents' education is highly correlated with the later years of the sample, 1984 and 2000.
In fact, Figure 2 exhibits this multicollinearity through the fact that the number of children born to mothers is decreasing both over time, and by the mother's years of education. This illustration, combined with the changing signs on coefficients as shown in Table 5 make a strong case for the presence of multicollinearity as the reason for the confusing the results. Again, the coefficients on number of siblings are negative and significant, but substantively small. In order to examine one more measure of household size, the variable for number of children was replaced by the variable indicating number of people in a household. Results are shown in Table 6 below.
Here, the coefficient for adding one additional person to the household has more variation than the siblings variable had, but the overall magnitude stays within the range of the sibling variable. While the coefficient is smaller in 1973 than the coefficient on one additional sibling for children ages 9-15, the coefficient on adding one additional household member in 2000 is approximately equal to adding one additional sibling to the family of a 16 year old. Hence, the measure of siblings and the measure of overall household size, regardless of the individuals' relationships within the household, capture essentially the same effect. Other control variables in this regression also have similar directions and magnitudes as in the sibling model. The fact that similar results were found when using two separate measures of family size proves that this negative correlation is a robust finding.
In short, OLS regression results presented in these models predict that the negative effect of larger families on education for children ages 9-15 increases over time, judging by the increasing coefficients on number of children from 1973 to 2000. This means that having an additional person in the household in 2000 decreases an individual's years of schooling by a greater magnitude than having an additional child in 1973. The effect of other control variables, such as parents' education on an individual's schooling decreases slightly over time, when using the household size variable as the dependent variable, but increases when using the sibling variable. As the distribution of educational attainment spreads out over time, becoming less concentrated in fewer years of schooling, the possible outcomes become more variable. The changes in the magnitudes of these coefficients account for the increasing diversity of the Costa Rican population over time. Still, the overall effect of household size on education in comparison with other control variables is small. In the end, while these regressions show that there is indeed an effect of adding an additional person to a household on schooling, this effect is small in comparison to a multitude of other factors that influence a household's educational decisions.
Based on the figures and tables presented in this analysis, it is clear that households in the Costa Rican population have become both smaller and more educated during the past thirty years. However, a cursory glance at these facts reveal very little about the actual changes in the population that have occurred. While there is a clear negative correlation between the size of a household and the level of schooling of individuals in that household, the direction of the causality is more complicated. In fact, not only do smaller households attain higher levels of education, but urban households, households with more educated parents, and households with electricity also attain higher levels of education. Due to the fact that all of these factors develop simultaneously as a society grows, it is likely that they all play a role in increasing education of the individuals within that society, to some extent.
The ordinary least squares estimates in this analysis reveal that all of these factors do indeed significantly influence the amount of schooling that a child in Costa Rica achieves, including the effect of household size, which is the focus of this analysis. As expected by classical theory, the years of education a child receives decreases as the number of people in that individual's family increases. For 9-15 year olds, adding one additional sibling to a family decreases the years of schooling for that child by between 0.04 and 0.09 years. The effect is slightly larger for 16 year olds, but still remains substantively small. In fact, this effect is smaller in magnitude than the effect of several control variables, for example, increasing mother's schooling by one year. Other significant predictors of educational attainment include household income, which is replicated here by the presence of electricity, a refrigerator, owning a home, living in an urban area, and being female. Despite the fact that the magnitude of the effect of increasing family size is relatively small, this nevertheless settles part of the debate of the causal relationship between household size and educational attainment.
It is difficult to place a magnitude on the opportunity cost of schooling in Costa Rica based on these results. However, the regression estimates here can be understood to mean that the opportunity cost of schooling is higher for families with more children, and for families whose parents have lower income. As Becker suggests, this could result from a tradeoff between the quantity of children, and the quality, or investment that parents instill in each of their children. The larger negative effect found for older children (age 16) demonstrates the diminishing returns to education for children in large families, especially for boys in large families. As their earnings from labor increase with age, the value of schooling decreases in comparison to these wages. While the actual costs associated with schooling, costs like books, shoes and pencils, may be relatively small, the cost of wages these children could have earned had they not been in school is much higher. This certainly increases as children age and are able to perform more difficult, or valuable, tasks. Additionally, if parents have fewer years of education, they most likely have lower wages, which would make the children's contribution to income even more valuable.
The story is less clear for households with many children. Children's wages could be more highly valued because there are more mouths to feed in the household; or, parents might decide to have more children in order to help support the family. Regardless of whether this decision was made before or after educational decisions, the effect is the same: larger families tend to value children's wages more than smaller families, a fact which increases the opportunity cost of attending school. The OLS results confirm these suppositions, and inform possible changes to a variety of conditional cash transfer programs in Latin America. While a fixed monthly stipend or coupon may encourage a family with one or two children to send their kids to school, the fixed amount of the transfer may not always be enough to overcome the opportunity cost of education for children in large families.
Despite the possible limitations to education that children in larger families face, the data from Costa Rica mainly presents positive changes over time through increased education, and suggest that the country is continuing to grow and attain even higher levels of education. Every generation of children that grows up to be more educated parents can help make informed decisions to serve as a model in the community and throughout the region.

The article, "Economic Development in Brazil, 1822-1913", by Nathaniel Leff attempts to explain the poor performance of the Brazilian economy from its independence until the 20th century, and then further attempts to explain the dramatic shift to a period sustained long term economic growth. Leff claims that the original poor performance was due the fact that the domestic agriculture sector of the economy was a very large, if not dominant, part of the economy and that its performance was hindered by extremely high transportation costs which kept it from being profitable. Leff explains the shift to long term growth with the introduction of a new constitution in Brazil which shifted it to a decentralized federal republic and allowed for greater public investment that spurred the economy.
The information presented in Leff's article is important because it provides evidence that suggests that states that have tightly controlled governments can restrict economic growth. It further shows how a switch to a decentralized federal republic can provide necessary public finance that allows for investments that can help instigate economic long term growth.
In his article, Leff argues that transportation costs could have been reduced by the introduction of an extensive railroad system, but this system was late in coming because difficult terrain made the construction costs very high and the tax revenues generated by the government through the taxation of imports and exports was simply not enough to fund it. Taxing the domestic agriculture sector of was unattractive economically because of the great distances involved, poor communications, and low literacy rates. Taxing imports and exports, however, had much lower administrative costs, and thus provided the bulk of Brazil's public funds. When the government adopted a new constitution in 1889 which changed Brazil from a centralized imperial regime into a federal republic it allowed for greater opportunities for overseas borrowing. These increased public finances led to an increase in government spending which help jump start the Brazilian economy into a period of long-term economic development.
In order to demonstrate the size of the domestic agriculture sector Leff explains that at 1820, 70 percent of the population were free people, and that free labor was seldom employed in export activities. He also explains that the "limited information available on the sectoral composition . . . suggests a large fraction . . . was engaged in domestic agriculture." One last piece of evidence he uses to support his estimate of the size of the sector is his claim that from 1911-1913 exports only accounted for an estimated 16 percent of GDP, leaving an obvious gap to be filled by the domestic agriculture sector. The evidence he presents to support the governmental change causing the economic shift is simply history. When Brazil abandoned its absolutist imperial regime for the new constitution of decentralized federal republic, the states could and did raise more in tax money and overseas borrowing. He further supports this by charting out the increase in public expenditure dedicated to transportation, namely the railroads. He demonstrates this increase with the year and the corresponding amount of railway track existent in Brazil, with it showing that after the time of the new constitution the length of railway track dramatically increases, and that this increase corresponds with the boom of the economy.
As just discussed, one of the main points of Nathaniel Leff's article, "Economic Development in Brazil, 1822-1913," is that the dismal performance of the economy of Brazil from the time of its independence from Portugal until the late 19th century was the result of the poor performance of the domestic agriculture sector. Leff argues that the domestic agriculture sector could have this effect because it was a very large part of the economy. I think that the evidence presented by Leff to support his estimate of the size of the domestic agriculture sector is very weak, and his argument could have been vastly improved if he had done additional research to produce a more accurate estimate of the amount of the population invested in activities other than domestic agriculture from 1822-1913. In addition to the actual size of the domestic agriculture sector, I believe that his argument involving its unprofitability due to the high transportation costs of Brazil is also weak. Leff provides very little explanation about why the complex system of rivers found in Brazil, including the Amazon River, could not be utilized to provide cheap transportation. In addition to the system of rivers, I think that a further discussion of the condition of existing roads in Brazil would be a worthwhile undertaking.
If the domestic agriculture sector was actually in fact much smaller than Leff claims, his argument that Brazil's poor economic performance was derived from its poor performance is less credible because the smaller the sector can be proven to be, the less of an effect it would have on overall economic performance. Accurately measuring the amount of labor actually invested in the domestic agriculture sector is therefore crucial to his argument. In addition to minimizing the domestic agriculture sector's size, it would have an equal detrimental impact on Leff's argument if it could be proven that it wasn't actually an unprofitable sector to be engaged in. Investigating the transportation costs would demonstrate that if he was in fact correct about the size of the sector, then the fact that it was so unprofitable would accurately explain why the economy did so poorly until the time when railroads were introduced (which effectively cut transportation costs).
In order to test my hypothesis I am going to examine the levels of urbanization in Brazil during this period with the intention of determining the extent to which people were engaged in the non-agricultural sector. Leff claims that as late as 1890 only 11 percent of the population resided in urban centers of 10,000 or more inhabitants, meaning that the number of people working in transportation, commerce, crafts, manufacturing, and government must be small. I believe that urban centers of much less than 10,000 were plenty and provided many opportunities for the inhabitants to be engaged in activities other than agriculture. Further I believe that non-agriculture activities were not limited to the urban centers and could instead also be found in rural areas. First I am going to address the issue of transportation costs, specifically addressing the Amazon River, and then addressing the rivers in São Paulo and Minas Gerais. I am going to focus my research on the non-agricultural sector into two case studies of Minas Gerais and São Paulo.
Through the consultation of three online encyclopedias, Britannica Online, MSN Encarta, and Wikipedia, I have determined which rivers were the most significant in São Paulo and Minas Gerais. These sources are valuable because they are providing facts about the rivers that are not debated. The information has been provided by actual documentations of the real conditions present.
The article by William R. Summerhill, "Railroads in Imperial Brazil, 1854-1889," appears in book Latin America and the World Economy since 1800. The main point of Summerhill's article is to evaluate the economic consequences of railroads and the role of government policy in Brazil from 1854-1889. Through his analysis he provides useful information about the existing transportation costs in Brazil.
The primary purpose of the article, "Monarchy, monopoly and mercantilism: Brazil versus the United States in the 1800s," by Zanella, Ekelund, and Laband draw support to the notion that the difference in growth rates between the two countries are due to relative factor endowments and institutions. The article claims that political and economic structures present in Brazil allowed the persistence of monopoly restrictions, and that the United States did not, accounting for the differences in growth in the 1800s. The article is particularly relevant to this research paper in its discussion of monopoly and the Amazon River.
The book, Slavery and the Economy of São Paulo, 1750-1850 by Luna and Klein provides a vast array of valuable information. Its primary purpose is to outline the growth of the economy and society of São Paulo from the time of its colonization by Portugal, to introduction of coffee in the mid-19th century. The book claims that there had been a lack of interest in this topic because the most substantial social and economic data surviving were from the first national census in 1872, but because of the discovery of a "vast store of previously unknown and unused population and production censuses in the state archives that go from at least the 1760s until the 1850s," the authors of this book could now undertake such a topic.1 The previously unknown censuses they use provide valuable information for this paper such as the breakdown of agricultural and non-agricultural heads of household in São Paulo, the households engaged in liberal professions and the military, as well as information about those engaged in commerce, transport, and as day laborers. Furthermore they give us population estimates so that we can determine the percentages of the population engaged in the non-agricultural sector.
Another source for São Paulo that will be addressed is Elizabeth Kuznesof's Household-Economy & Urban Development: São Paulo, 1765 to 1836. Her claim is that between 1765 and 1836, the household economy of São Paulo was "transformed from subsistence to a market-oriented economy."
The article, Freedmen in a Slave Economy: Minas Gerais in 1831,by the Herbert Klein and Clotilde Andrade Paiva also uses two unpublished 1831 censuses from two major municipios; those of Campanha in the southwestern part of the province, and Sabara in the central zone near present day Belo Horizonte.5 This article is mostly about the freed colored population, but the analysis provides many insights to the fraction of the population not engaged in agriculture.
The first issue to address now will be Leff's claim that extremely high transportation costs were what caused the domestic agriculture sector to be unprofitable. This argument seems counterintuitive if one considers the extremely complex system of rivers that can be found in Brazil, including the world's second largest river, the Amazon. Within his article, Leff does concede that rivers and coastal shipping were used for transportation, but he claims that "some of the country's rivers (the Amazon, for example) were poorly located from the viewpoint of promoting economic development," and further he argues "other rivers flowed in a direction that was not advantageous from the perspective of production for markets."
The key river with the highest economic potential was that of the Amazon River. The Amazon River is the second largest river in the world (second to the Nile River), measuring 4,000 miles, with roughly half of it located in Brazil.7 Over 200 of its tributaries are located in Brazil, and the main body of the Amazon is navigable by ships as large as ocean liners over two-thirds of its length.8 The ease at which goods could be transported on the Amazon is undeniable, but upon further investigation it can be seen that Leff's denial of its lack of influence in Brazil's 18th century economy can become clear. First we will address the part of Brazil actually able to utilize the Amazon River.
As mentioned earlier, the article by Zanella et al addresses the Amazon River and the monopolies granted in regards to transportation of goods. They argue that even by the mid-19th century, the Amazon was largely unexplored despite its tremendous economic potential. With the permission of the Brazilian government, the U.S. Navy Department was given permission to explore the Amazon in 1850. However, Matthew Fontaine Maury, the first head of the United States Naval Observatory and Hydrographic Office caused alarm to the Brazilian government by speaking openly and enthusiastically about the economic potential of the Amazon River to both businessmen and the U.S. Congress. As a result, the alarmed Brazilian king granted monopoly rights over the navigation of the Amazon River to Baron Mauá to protect Brazilian interests in the Amazon (from the United States).9 As a result of these rights, Mauá was receiving freight-rates on transported goods of 18-30% of their value. While this would have been exceedingly profitable for Mauá, those wishing to take advantage of the Amazon to transport goods cheaply found themselves not much better off than if they had been transporting them overland. However, the monopoly rights held by Mauá were ended in 1867. Bearing this in mind, we have an idea of why the Amazon did little to help high transportation costs in the 19th century. First being that it was simply unexplored, and second were the high rates charged by Mauá until 1867. The reason why the ending of monopoly rights on the Amazon did little to help the overall Brazilian economy will become clear after examining the Amazon River's location.
Figure 1 shows a map of Brazil and the Amazon River and its tributaries. It can easily be seen that the Amazon proper is strictly isolated to northern Brazil. The tributaries, the Xingu and the Tocantins are somewhat more central to Brazil, but still do not reach the southeast, specifically those regions of Minas Gerais and São Paulo. Summerhill argues in his article that "although the north was well served by rivers, by the end of the eighteenth century most of the population was well south of the Amazon basin."
Even though the Amazon was not available to the southern territories does not mean there were no other rivers located in these regions. In São Paulo there are the Tietê River and the Grande River. The Tietê River is about 700 miles in length before joining into the Paraná, and several of São Paulo's largest cities are located on it, but it has relatively poor navigability due to frequent falls and rapids.13 The Grande River, or Rio Grande, which flows through Minas Gerais and along the border of São Paulo is also hindered by waterfalls and rapids.14 The São Francisco flows through Minas Gerais and is navigable for about 850 miles on its middle course, but the rest is interrupted by rapids.15 There are various other rivers located in the southeast of Brazil, but like those already mentioned, their economic viability is eliminated due to the existence of waterfalls and rapids. It seems at this point that Leff's argument about the complex system of rivers of Brazil doing little to provide cheap transportation can be taken as credible, although it could have been strengthened within his paper had he explained about the overwhelming existence of waterfalls and rapids in the southeast.
Because of the lack of navigable rivers, states like São Paulo had to make do with roads. Kuznesof tells us that by 1802 roads had greatly improved but were still very costly and that it was between 1802 and 1836 that a new important road had been constructed between Cubatao and Santos, and another road which would be passable by cars was in the planning stages.16 Summer further tells us that as a result of São Paulo's efforts to improve roads, such as paving them with macadam, by 1864, São Paulo had the lowest freight charges in all of Brazil at the time.17 However, in most other areas of Brazil, freights were hauled by mules over unimproved trails and the charges were drastically higher than those of São Paulo.
The next important issue to tackle is the size of the domestic agriculture sector. As we have determined, because of the high cost involved in the transportation of goods, the domestic agriculture sector would have been indeed quite unprofitable. This is why it will be important to determine that the domestic agriculture sector was as large as Leff claims, and its unprofitability because of transport costs would indeed explain the poor performance of the Brazilian economy at this time. To do this I am going to specifically address Minas Gerais and São Paulo and by examining their urbanization levels, try to determine the extent of those who were engaged in other activities besides agriculture. First I will examine São Paulo.
The choice someone entering the non-agriculture sector of São Paulo had included many diverse activities. There were merchants, store owners, civil construction workers, potters, metalworkers, woodworkers, spinners and weavers, clothing workers, shoemakers, and leather workers. There were also educated liberal professionals such as clergymen, government officials, lawyers, doctors, and teachers of all types. They could also be members of the military.18
This table, taken from Luna and Klein's book, shows the breakdown of agricultural and non-agricultural heads of household by color and sex for the 41 counties of Sao Paul in 1829. It can be seen that at least at this time, more than half of the heads of households in São Paulo were engaged in activities other than agriculture. If taken at simply this it might be determined that of the 16,278 households in São Paulo at this time, roughly 41% were engaged in the non-agricultural sector, and that this percentage would carry over to the population as a whole. However, it must be taken into account that only 22% of non-agricultural households owned slaves, compared to 29% of households owning slaves in the agricultural sector, on top of which most non-farmers averaged about half of the total number of slaves as agriculturalists.20 This pushes the amount of total amount of the population engaged in non-agricultural activities to a lesser degree, but it would still be more than the figures alluded to by Leff. Leff argues that by 1890 only 11% of Brazil's population resided in urban centers of 10,000 or more inhabitants, and because jobs in the fields of transportation, commerce, crafts, manufacturing, and government, were typically located in cities, that this figure suggested a large fraction of the population was engaged in agriculture.21 However, Luna and Klein point out that a "large proportion of the population in even the most rural communities did not work the land, but rather provided crucial services for those who were engaged in agriculture." Despite this statement, Luna and Klein tell us that most non-farmers were concentrated in the first few districts of an urban settlement.22 This, combined with the amount of households engaged in non-agriculture, suggest that Leff's figure of 11% urbanization by 1890 is perhaps not correct. Luna and Klein, however, also consider those engaged in export activities as members of the agricultural sector, so if they are accounted for, the figure for those engaged in the domestic agriculture sector decreases.
One theory presented in Luna and Klein is that as export activities grew, such as sugar and coffee, former subsistence farmers (who would have been counted as agricultural) were forced off of their lands and into urban centers to become either skilled, or more likely, unskilled laborers. One of the examples they list is the fact that several families who had been listed as subsistence farmers in a 1777 census were listed as weavers and spinners or leather workers in the next census in 1798, which they claim resulted from the increase in sugar production in that area. Furthermore, Luna and Klein point out that although non-agriculturists in São Paulo could sometimes attain great wealth, on the whole they tended exhibit more characteristics of those in poverty than farmers and planters. 23 Keeping this in mind, a counter-theory to Leff for the explanation of Brazil's poor economic performance can be formulated. Perhaps because the non-agricultural sector was larger than he thought, and that most of those engaged in this sector were in relative poverty, this would instead change the explanation from the dominance of the agricultural sector to the dominance of the impoverished non-agricultural sector.
Now it is time to switch focus from Sao Paul to Minas Gerais. Minas Gerais is another state of Brazil located in the southeast. In Freedman in a slave society, it is claimed that "at least half of the rural population engaged in non-agricultural activities" in the 19th century.24 This estimate does not take in account those rural households engaged in both agricultural and non-agricultural activities, but at least demonstrates that participation in the non-agricultural sector was prevalent. In 1833 the total population of Minas Gerais by administrative divisions was 768,666 and by 1872 the population was up to 2,102,689.25 Bergad does not provide any information on the occupational categories of the population for 1831, but he does however provide a selected breakdown for both slave and free populations in 1872:
When comparing this graph to the figure I just listed for population in 1872 (2,102,689) it can easily be seen that the graph does not account for the entire population, and instead only a sample. From this sample, however, we can see that farmers only make up 37%. The non-farmer population it accounts for totals to 830,414. If the population was indeed 2,102,689, then the non-agricultural sector in Minas Gerais at this time was at least 40%. While the farming population is obviously underrepresented in this table, the non-agricultural sector, at least, is not overrepresented, and could also possibly be underrepresented as well. What is known is that during the 19th century, Minas Gerais had the largest slave population (382,628) accounting for 18.2% of its total population.27 Typically the type of "farming" that slave labor was engaged in were export activities, which should not be counted in the domestic agriculture sector. Bergad also gives us another occupational breakdown based on a limited sample of 3,062 slaves and finds that only 21.6% are engaged in agriculture, while the rest are in many other non-agricultural occupations such as: carpenter, blacksmith, coachman, cook, barber, shoemaker, tailor, muleteer, seamstress, stonemason, miner, and others.28 While Bergad tells us that the sample is too small to indicate any general occupational data of slaves in Minas Gerais because only 2.7% had occupations listed in the inventories, it still provides us with an idea of the vast amount of other positions besides agriculture that the population was engaged in.
It can be seen through the analysis of the this paper that Leff's argument about transportation costs in 19th century Brazil are founded, and have been strengthened by the evidence presented. Even after dealing with the problem of monopoly pricing, the Amazon River was still restricted to northern Brazil. Where there existed rivers in southern and southeastern problems, they were on the whole not navigable to any profitable degree because of the abundance of rapids and waterfalls. There were roads in existence in Brazil, but even the ones in the best condition, those of São Paulo, were still highly costly. The argument that high transportation costs would have rendered the domestic agriculture as unprofitable stands credible and has been further substantiated.
Where Leff's argument seems to lose credibility is his dismissal of a substantial non-agriculture sector. One of his claims is that because of the lack of large urban centers, claiming that only 11% of Brazilians resided in cities of 10,000 or more by 1890, very few people would be engaged in activities considered non-agricultural. However, by examining the cases of Minas Gerais and São Paulo this paper has shown that there existed a large and significant population that was engaged in the non-agricultural sector, in both rural and urban areas. Because most of those engaged in this sector were in relative poverty, it is possible to explain at least a portion of the poor economic performance of Brazil in the 19th century to the fact that this sector was so large, instead of the domestic agriculture sector.

It is well known that high school graduates may attend college to signal their ability to potential employers, regardless of any benefits intrinsic to the educational process itself. Spence's classic paper on job market signalling demonstrated that agents of inferior quality may choose to acquire high levels of education in order to join the pool of college graduates, and gain high wages [1]. But colleges themselves are heterogenous: Western Michigan is not Yale, and employers know this. Choosing which college to attend is thus a matter of choosing which signal to send, and it is a problem that is complicated by competitive admissions processes.
Students and their families across the United States face this problem every year. Getting into the good schools is tough, but clearly would provide a head start in the job market. Students know their own attributes, but are uncertain about how the schools weight them. In the face of application costs in the form of both time and money, how high should one aim? The admissions process forces agents to self-select the appropriate range of schools to apply to. It is this feature that makes the college attended a sensible screening device for firms hiring new graduates.
This is similar to the ideas in Arrow's paper, "Higher Education as a Filter" [2]. He considers the impact of college admissions processes in the simpler case where colleges are completely homogenous. In his formulation, higher education filters students in two ways: through admissions, and through the graduation process. Provided that the skills required to perform well in college are positively correlated with the revenue product of labor (the "positive screening assumption"), firms know that students that have successfully negotiated these two obstacles are, on average, of greater value to the firm. Hence, in a competitive labor market, college graduates are paid higher wages.
His concern is with the social value of higher education. In a one-factor world, where all labor is homogenous, and where higher education does not actually add to productivity, higher education is simply a drain on societal resources. In contrast, if one extends the model to allows for multiple factors of production (say "skilled" and "unskilled" labor), higher education may allow firms to screen their workers and deduce which type of job they are best suited to. This may increase allocative efficiency and thus be good for society.
In this paper, our focus is on the filtering and screening properties of higher education in the case where colleges are heterogenous. We analyze in detail the effect of allowing students to apply to only one of two colleges with different reputations. In deciding which school to apply to, agents must weigh the future benefits of a diploma from the top college against the risks of not getting in, and thus not attending college at all.
We may view this as a process whereby agents choose which pooled signal they would like to try to send. An agent that graduates from Harvard can expect to be paid more than his peers at other institutions, because attending Harvard sends a signal to potential employers that the agent has high ability. Joining the top pool (top school) pays off in the job market. It is clear that all agents would like to attend the top school, and free-ride off the school's reputation. It is thus crucial that their college choice is mediated by a competitive admissions process. Agents must balance the increased payoff of getting in to the better school against the increased risk of not getting in to college at all.
It is this institutional feature that distinguishes this model from the approach taken in Spence's original paper. In his model, low ability agents could mimic high ability agents (acquire the same amount of education as them) provided that they were willing to bear the costs of acquiring that level of education (and their costs were higher than those of the high-ability agents). Here this is not possible. Weaker agents may try to apply to the top school, but the admissions process means that they have very low odds of getting in and mimicking the top agents. We argue that it is this feature that forces a type of separating equilibrium where all the top agents apply to the top schools, and lower ability agents do not try to mimic them.
The importance of institutions that allow agents to pool has recently received attention in the literature. Pradeep Dubey and John Geanakoplos have shown that quantity limits on contributions to insurance pools can guarantee the existence of equilibria in insurance markets [3]. This existence occurs because high-risk agents, who wish to hold a large amount of insurance, have to join pools with high quantity limits and thus declare their type. Something similar occurs with college admissions, where a limit on the admitted class size forces low ability agents to apply to less competitive schools, and thus declare their type.
Other pooled institutions include sporting tournaments and academic journals. In the case of golf tournaments, for example, professionals often have to choose which tournament to attend on a given weekend. One may offer greater reward, but is bound to be more competitive. Similarly, in submitting a paper for publication, authors must assess the quality of their paper in order to decide whether to submit it to a top journal. If they aim too high at first, they are likely to face delays in getting their paper published.
The model presented here is focused on college choice, looking at the case where there are only two colleges. The basic model of college choice is presented in Section 2, and the resulting equilibrium is analyzed. Section 3 introduces some dynamics and examines how educational institutions may endogenously become differentiated over time. Section 4 applies the model to race-based admissions, and Section 5 concludes.
We assume that agents may choose between only two colleges, college 1 and college 2. For the moment, we abstract away from the wage generation process of firms (examined in the subsection below) and assume that the wages paid to graduates are given exogenously as w1 and w2 . Let college 1 be the top school, so that w1 > w2 . Agents that get into college receive payoffs of w1 and w2 respectively; while those that don't are assumed to receive a wage of 0, for simplicity1
There are a continuum of agents, indexed by their ability level θ, where θ has positive (possibly unbounded) support, has a finite mean, and a continuous cumulative distribution function F (θ). Each college computes a score for every agent applying to that college S . This is given by
where ε has support ( −∞, ∞), continuous cumulative distribution function G(ε) and E[ε] = 0. Assume that the error distribution G is independent of the ability distribution F . The colleges are assumed to have a capacity of measure Ci ≥ 0, i = 1, 2. After ranking the applicants by score, the top Ci students are offered places. Agents may apply to only one college.2
While the proof of this result is surprisingly lengthy , the intuition behind it is not. Consider the first claim. In this case, because the total college capacity is sufficient to accommodate the agent population, in equilibrium those applying to the unattractive college, college 2, are guaranteed a place. As a result, only agents that are sufficiently likely to get in to college 1 are willing to take the risk of not being accepted, in return for the wage premium w1 − w2 . There is precisely one point at which an agent is indifferent; this point is
In the second case, the agents face aggregate risk, since the total number of places at college is insufficient for the population. One might imagine that in this case, some of the "good" agents would choose to apply to college 2 to make sure of their acceptance to college. But given that behavior, some agents of lesser ability may take a chance on college 1, since "good" agents are applying to both colleges and getting in will be difficult in any case. Thus, in this case, we would not have a separating equilibrium. The theorem makes clear the circumstances under which this intuition fails. Where a marginal increase in ability always enhances the agent's proportional odds of getting in to the top college relative to the bottom one, good agents always find it preferable to take advantage of this and apply to the top college. Similarly, weaker agents have an incentive to apply to the bottom college, and a separating equilibrium obtains.
This result proves formally that the college applications process acts as an effective screening device in a setting where agents choose between colleges. Due to the separation, the eventual outcome of this game is a set of graduates from each college that are markedly different in ability level. Firms may use information about which college an individual attended to improve their estimate of agent ability. Thus in this model screening is obtained endogenously as a result of strategic agent behavior, within the given institutional framework. Notice that the admissions process itself plays a minor role in the screening, although on average the better applicants tend to be accepted. Rather, it is the presence of a fairly reliable testing mechanism that induces agents to reveal their type through their own college selection.
It is also clear from the proposition that, in general, the screening properties of college admissions will be enhanced when the size of the college-going population is less than the available number of places. This seems likely in two scenarios. Firstly, in countries where tertiary institutions have some freedom in setting tuition fees (as in the United States), we would expect the "price" of tuition to be such that the market clears. This suggests that the supply of college places will be roughly equal to demand. Furthermore, in reality colleges tend to admit more applicants than they actually expect will accept their offers. This necessitates them having excess capacity to cope with years when they have high acceptance rates. These facts would support the contention that college capacity is indeed sufficient for the college going population and the conditions of the proposition hold.
Secondly, in countries where the government regulates tuition fees or waives them completely, it is often necessary to pass a standardized test to be allowed to apply to any of the colleges at all. For example, in South Africa, one requires a certain number of points in the matriculation examinations to be eligible for college. Governments may use this type of policy to equate the size of the population applying for places and the number of places. Thus it seems that the requirements for the existence of a separating equilibrium are not unduly restrictive.
In this section, we tackle the question of how firms behave. For simplicity, we assume that the labor market is perfectly competitive. In this model, we assume that college serves only one educational purpose: to provide a diploma and a set of skills that allow the graduate to enter into the "skilled" sector. Education does not add value, in the sense that it does not increase the workers' ability level. Unskilled workers are indistinguishable, and are paid a reservation wage r, which, without loss of generality, may be normalized to 0. In contrast, firms looking to hire skilled workers receive information about which college the prospective employee has attended. Based on this limited information Ω, they pay some function of the expected ability of the agent, w = w(E(θ|Ω)) > 0.
It is clear that this simple model misses important aspects of the wage generation process. Firstly, the role of education here is somewhat at odds with the human capital literature, where education actually increases the agent's existing ability, rather than simply providing a skill-set. In section 3 below, I explicitly consider this modification to the model. More importantly, firms observe far more information about the agent than simply a college dummy variable. A typical resum ́e of a college graduate might include his or her grades, ma jor fields, extracurricular accomplishments, and summer job experience. These are all individual signals. Clearly, this will reduce the effect of the pooled college signal. This model should therefore be viewed as an extreme case.
Notice immediately that with the firm behavior as given above, we may apply the results of proposition 1 to deduce agent behavior. Thus it is now possible to analyze the game that results when firms set wages endogenously after observing college outcomes.
This result is in some ways trivial. Consider the case where the cut-off level,
Unfortunately, this may be the only such equilibrium. To see this, note that for an agent of ability
where we are using the fact that firm's optimal wage offers depend endogenously only on θ∗ (as well as exogenously specified distributions). But notice that the L.H.S. is monotone increasing in
This may indicate that important aspects of the model have been left out. For example, if one included the possibility that education added to human capital and that colleges with small enrollments were better able to provide such education, agents may choose the low-wage college in order to benefit from a smaller enrollment. The payoffs may then be adjusted in such a way that an interior solution for
It also seems reasonable to say, however, that this result is in some sense a product of the solution concept. Firms are assumed here to know the structure of the model, and in equilibrium to successfully guess the separating point
Suppose that agents believe that the firm's wage generation process is, on average, static: that is, their best guess of this period's wage offer is last period's. This seems like a plausible behavioral assumption, in light of the fact that each generation of agents plays the college admission game only once, and must look to the past for indications of how firms might behave. In this case, agents will have wage expectations given by last period's wages w1(t −1) and w2(t−1) , and behave as in proposition 1. In particular, provided that the requirements given in proposition 1 are met, the unique equilibrium will be a separating equilibrium.
Suppose also that one year after hiring them, firms observe some imprecise measure of the true productivity of their new hires. Hence when making their wage offers, their only new information (as compared to last period) is how last period's hires from each college did. For simplicity, assume that firms thus set their wage offers as a moving average of the observed productivities of graduates of each college over the last 5 years. That is,
where pit is the observed productivity of college i students graduating in year t. Again, this represents a sensible strategy for firms, since the observed productivity signal is not entirely accurate, and thus averaging over the past five years reduces error while still responding to new information. We now make things more concrete by considering a specific case.
Given this explicit formula, it is easy to do comparative statics and show how the exogenous parameters affect the separating point. Notice that
We use the framework above to simulate the evolution of wage offers to graduates of different colleges in an economy. Of necessity, we consider a situation with a finite number of agents (in this case 1000), and assume that they behave as if there were a continuum of them. We use admissions error bounds given by a = 0.1, capacity at each school of 0.5 (half the population) and errors in firm's observations of true productivity drawn from a uniform distribution on [ −b, b], where we set b = 0.2. These may also be thought of as macroeconomic productivity shocks. These parameters conform to the requirements of proposition 3 above for the range of wage offers generated. In order to generate initial wage offers, we create a history of the last 5 period's productivity levels, where college 1 has been better over those 5 periods. The differences are rigged to be very slight (on average, productivity in college 1 is around 0.005 higher than that of college 2). Notice that in all other respects, the colleges are identical.
The simulated wage path shown in figure one above demonstrates a key result: initially almost identical colleges can endogenously become differentiated over time as a result of random initial fluctuations in productivity. This is a striking example of "tipping". In this case, agents seeking to profit from a small initial wage differential cause a separating equilibrium. Firms notice the difference in productivity levels across the colleges, and change their wage offers accordingly. Suddenly, the small wage differential becomes large, and one college attracts the best students. College 1 emerges with a reputation for producing top quality graduates while the other college languishes behind. One should note that firms cannot tell whether this is because of the separating effect argued for here, or because college 1 really does provide a better education. Thus even if Yale were to provide exactly the same education as Western Michigan, firms and society at large might incorrectly attribute the difference in productivity between Yale and Western Michigan graduates to differences in education, and conclude that Yale was a better school.
This model seems apt for looking at students' choice of graduate school. In this environment, the pooled signal conveyed by the college that students graduate from is, in general, regarded as very important by potential employers. The academic job market for positions in top-ranked schools tends to be closed to graduates of schools with weaker reputations. Furthermore, since graduate students are often either not assigned grades, or the grades are seen as unimportant, this pooled signal is one of the few "objective" (easily measured) signals available to potential employers.
Turning to potential students, we notice that students take the rankings of graduate schools in publications such as US News and World Report surprisingly seriously. The prestige of the school is also very important. This model suggests a potential reason for this. Essentially, these factors act as coordinating devices much in the same way as wages do in the model we presented, and allow agents of similar ability to group themselves together, and thus maximize the value of their graduate experience. The model predicts that some schools will attract the top students year after year, since these agents will self-select the top schools.
This result is not surprising. It is interesting, however, that the model shows how hard it may be to reverse the trend. Small initial fluctuations result in large differences between colleges, once students self-select and separate. Now imagine that the weaker college, college 2, hires superb new faculty, and in fact this means that graduates of college 2 emerge with higher productivity than they started with. College 1 continues to give no value added. Yet, unless the value added by college 2 is very large, this will not change the outcome. College 1's reputation attracts the top students, and these students will remain the top students despite college 2's best efforts. They will get the top jobs, and college 1's reputation as the better school will remain intact, and the top students will continue to go there. The logic is relentless.
Figure 2 shows this scenario. After period 51, college 2 graduates are assumed to receive a bonus of 0.25 to their productivity (which is, on average, doubling it). Despite this, they simply aren't as good as those of college 1, and college 1 remains the top college. What might one do to try and catch up? Part of the answer is to reduce the size of the incoming class, and thus make admissions more competitive. The quality of the admitted students thus improves. The effects of this adjustment are shown in figure 3, where college 2 is assumed to cut its enrollment from 0.5 to 0.2 in period 51.
Notice that this has almost as much of an effect as the quality improvement depicted in figure 2. Together, these strategies can work to reverse the trend. Specifically, suppose that college 2 has access to a fixed endowment, and that the exiting productivity of students is a function of the amount spent on each student. For example, dropping the enrollment from 50% of the population to 20%, and raising per capita spending accordingly may perhaps allow college 2 to add a productivity bonus of 0.3. The result is that college2 is able to catch up and then surpass college 1, as is shown in Figure 4.
In fact, this suggests that temporary increases in spending combined with selective admissions can have a permanent effect on a college's reputation. The time-scale required to effect such a change depends on how quickly wages (or school rankings) rise to reflect the better quality of college 2's graduates. Once wages have fully responded, college 2 will start attracting the best applicants and may reduce spending to earlier levels.
This sort of strategy is not, in general, viable. The schools with access to large endowments tend to be the top schools, and are, in general, far more likely to be able to provide a high quality of education. Furthermore, while wages may respond to an increase in value added by college 2 fairly quickly, it is not clear whether intangibles such as prestige will respond at all (how do you dispel the Ivy League mystique?). Finally, this sort of approach entirely neglects the strategic aspect of this "reputation game"; one would expect college 1 to match college 2 in spending, knowing full well that a temporary increase now will pay off by cementing its reputation for the long term.
An alternate strategy may be to target individual students, acting on preferences not captured in the model by offering them fellowships or access to better facilities. But the model shows that the additional incentives offered will need to be large to counter reputational effects. It is perhaps, therefore, unsurprising that in most disciplines the top schools remain relatively constant.
The question of race-based admissions policies has recently come under legal scrutiny. The U.S. Supreme Court decision in University of California Regents v Bakke ruled that racial quotas were inequitable and should not be permitted, but that other forms of admissions that take race into account may be allowed in the interests of diversity. The University of Michigan's admissions policies are currently under legal challenge, with the Bush Administration, amongst others, arguing that the university's points system for admissions constitutes a de facto quota. It is with this in mind that we apply the model developed above to analyze a simple quota system.
We set up the model as follows. Suppose that a minority group (hereafter to be termed "Blacks") constitutes 20% of the overall population. However, not all members of the total population are eligible to go to college. In particular, suppose that the college-going population is comprised of only 10% Blacks and 90% Whites. This may be true for a variety of reasons, a lower percentage of Blacks completing high school, a failure to pass standardized eligibility tests of the type mentioned earlier, or access to credit. All members of the college going population are otherwise identical.
Now, suppose that in the interests of diversity, all colleges adopt a policy whereby they reserve 20% of their capacity for Blacks (i.e. their percentage in the general population). Notice that this means that the capacity available for Black students vastly exceeds the college-going Black population. This constitutes a quota system. But the outcome would be similar in any case where there was an admissions process that virtually guaranteed entry to Black applicants with certain qualifications 6.
For the simulation, we set the capacities as C1 = C2 = 0.5, as before, which means that the quota of 20% at each college is 0.1. This is a particularly simple version of the model, where the college-going Black population is also of measure 0.1and thus all of them will apply to college 1 and get in. Thus the resulting capacity for white applicants at college 1 is C1=0.4. Choosing parameters a=0.1, b=0.2 and using 1000 agents (900 White, 100 Black), we may run the simulation as before.
The figure above shows the wages earned by the graduates of college 1 and 2 by race. Notice that since all Blacks apply and get into college 1, we have no wage series for Black graduates of college. The wages paid reflect the fact that race is observable to potential employers. Thus the wages paid to Blacks are on average 0.5, since the admissions process yields no information about ability (it is neither separating nor selective). Wages paid to Whites are higher than they would be usually, since with a quota system, getting into either college is more difficult. Clearly though, this is not necessarily beneficial to the average white student, since his odds of getting in are reduced.
This deserves more explicit analysis. Suppose that socioeconomic status and race are highly correlated, so that it is possible to enforce the quota system by basing it on socioeconomic factors rather than race. Since socioeconomic factors may be unobservable to potential employers, under this system the employers may not be able to determine whether the job applicant got in to college 1 on merit or through a quota. Then we have three possible policy alternatives: no quota, a race-based quota, and socioeconomic quota. The key difference between the latter two is that with a race-based ("observable") quota, the employer, who may easily observe the race of the applicant, may use this information in deducing the appropriate wage offer; whereas with a socioeconomic ("unobservable") quota, this may not be possible8.
In the table below we report the average wage of both white and black graduates from college 1 (where they differ) and the a priori expected wage of a white applicants with ability 0, 0.25, 0.5 and 0.75 respectively under each of the three systems9. In all cases, measurements are taken from the steady-state.
It is interesting that for the given parameters of the simulation a quota seems almost Pareto-improving. Yet there are definite winners and losers. Consider first the college going Black population. If the quota is unobservable, the effects are unambiguously positive (average wage goes from 0.5 to 0.769). But if the quota is observable (which is almost always the case), then the quota has no effect on the average Black student, as firms can extract no information from the college signal. In particular, above average Black students are hurt by the quota (since they could have attended college 1 and earned on average 0.769), while less able students are advantaged (since they would have gone to college 2 and got 0.252).
The effects on the college going White population are more complicated. For the observed quota system, both college 1 and college 2 become more competitive. This is reflected in the higher average wage for graduates of college 1 (0.801). Further, the white student of ability 0.5 actually does better, since he now finds it preferable to apply to college 2, and since college 2 is more competitive than before, he ends up with a higher wage. But there are two main groups of losers. The first are the students of ability around 0.6 (not shown in the table), who used to be almost certain of getting in to college 1, and are now borderline applicants. Their ex ante expected wage will fall. One may argue that in a more realistic model, with many colleges and the possibility of multiple applications to limit risk, this change will be small. Fair enough. But the second group to lose out are the White students of low ability (θ = 0 in the table). There the changes are dramatic. Suddenly a certain place in college 2 has become uncertain, as better White applicants now apply to college 2 because of the quota system. The ex ante expected wage falls from 0.252 to 0.048. Moreover, the effects on this group are unlikely to be mitigated in a more complicated model -- there is simply no space for them in the college system anymore.
Turning to the unobserved quota case, the results are similar. Now college 1 White students are on average paid less than their ability, since the firms are less able to determine ability from the pooled college signal. Furthermore, this system is in fact even worse for the bottom White applicants, as the lower wages in college 1 (relative to the observed quota case) mean that more people apply to college 2, further diminishing their chances of getting in.
The major effects of a race-based admissions policy are captured in this simple model. In particular, if the quota system is based on easily observable characteristics, it has little effect on average wage outcomes since employers can and will discount the credentials of the favored group. Furthermore, in this case it is actually prejudicial to the top minority applicants, although benefiting weaker applicants. The model identifies the main losers from a quota system in college admissions as the members of the college going majority group that used to barely get in to college, and now cannot. It is interesting that these are almost never the people showcased as the victims of these policies. Rather, it is the students that fail to get in to top colleges (like the University of Michigan) who are seen as the victims. This is because the impact on the weaker White students is indirect: it is the resulting change in the behavior of their White peers that actually knocks them out of the system, rather than the quota system itself. Overall, this suggests that the overriding concern of policymakers worried about the effects of any preferential admissions system should be the weakest applicants of the group that does not receive this preferential treatment.
Pooled signals matter. The reputation of the college a student attends will almost certainly affect his post-graduation wage. In this context, it is critical to understand how agents may behave when confronted with colleges that they see as heterogenous in terms of reputation. This model makes a start in this respect, establishing the circumstances under which a simple separating equilibrium will exist. It also explains why colleges that are similar in most respects can still attract vastly different calibers of students if they differ in reputation. The key is to recognize that both firms and students are making decisions based on signals: students choose the school with the better reputation in order to signal that they are of high ability, while firms pay more for the graduates of the top school based on their pooled signal. Admissions processes are important in ensuring that low ability agents do not try to mimic high ability agents in attending the top school, and thus perform a valuable screening function.
The analysis of a race-based quota system shows how important it is to look at the overall picture when attempting to analyze these policies. It was shown that the people who are hurt by a race-based policy are the most able of the minority group and the least able of the majority. This is in contrast to the perception that it is the students that get shut out of top schools due to the quota that are most damaged by the policy.
Looking forward, it seems clear that this model has wide applicability. Institutional structures where agents compete for some prize (or to send some signal that yields a prize) are all around us. From sporting tournaments to academic journals, agents must repeatedly make choices about which pool they should attempt to join. Future work could include looking at the effect of having more than two such pools, the possibility of multiple applications, and attempts to characterize the solution in the case where the number of applicants exceeds the number of places, and the likelihood ratio condition fails.

1. Often, we would notice from financial bulletins that the world's key stock indexes generally move in tandem with each other, in particular in the event of a major crisis where major stock indexes plunge drastically or a major economic breakthrough where stock markets boomed. Possible reasons contributing to this trend include (i) the strong interlink between the world's major financial markets where buyers and sellers are operating across the globe like in a single market, (ii) the wide implications to the entire world economy due to a major event such as a terrorist attack or a financial credit crunch and (iii) the general knee-jerk reaction of speculators.
2. While there is a general trend of stock markets moving together in major events, there are exceptions of stock indexes contradicting the trend. These then could possibly due to internal economic factors such as internal good/bad economic performance and bilateral economic relationships with closely linked economies.
3. To analyze this inter-relationship, this report looks specifically at Singapore's Straits Times Industrial Index (STI), the key stock index in Singapore and how the moves in STI is correlated with other major stock indexes and the economic relationship variables.
4. Essentially, the first question we would like to analyze is the correlation between STI and the major stock indexes.
5. Dow Jones Industrial Composite (New York, United States), Financial Times Stock Exchange 100 (London, United Kingdom), Nikkei 225 (Tokyo, Japan) and Hang Seng Index (Hong Kong SAR, China) are selected to compare against the STI index. As world's major financial hubs, the selected stock indexes listed in New York, London, Tokyo and Hong Kong are the main stock indicators and are good representations of the individual cities' general financial landscape. In addition, the selected countries/regions are of similar economic development stage as Singapore (i.e. developed financial markets as opposed to emerging financial markets).
6. The next parameter to set is the time period of analysis. Specifically, there are factors such as availability of data, updated and sufficient data to reflect latest current trend, stability of economy (as opposed to choosing a period of turbulence which may over-skew the data and reflect wrong correlation due to a rare occurrence or low probability). Taking into account the above factors, the selected period is from Jan 2004 to Aug 2007. There are sufficient data points between this selected period as stock markets operate on all working days and that this period does not contain extreme world events such as 911 terrorist attacks. Also, this is also the period which all the selected economies are generally enjoying good growth with minimal economic disruptions or imbalances.
7. To measure the impact, the percentage changes in stock indices are being put into the regression model as opposed to having the absolute value of stock indices which would not reflect the changes. During the selected period between 2004 and 2007, there are more than 900 data points. As different countries have different public holidays, the days of operations vary, on such days, the rate of index is then computed as taking the value of the previous opening day1. In this analysis, only days which all stock exchanges were in operation are being used as the purpose of the report is to look at the correlation between STI and all other major stock indices.
8. Before running the regression model, it is assumed that the model of the correlation between STI index and other stock indexes (namely DJ Composite, FTSE100, Nikkei and Hang Seng), the model in the population is estimated to be as
9. This time series regression model takes into the linear time trend, commonly found in time series analysis where
10. Table 1 below summarizes the regression results of the first model.
11. In analyzing the robustness of the coefficient estimates, there are statistical tests needed to ensure the unbiasedness in estimating the regression model. These are serial correlation and heterskedasticity.
12. Under the Gauss Markov Theorem, there are assumptions made to the time series regression, which only when satisfied, the OLS estimators are BLUE (Best Linear Unbiased Estimator).
In the sample, no independent variable is constant nor a perfect linear combination of the others.
For each t, the expected value of the error ut, given the explanatory variables for all time periods is zero,
In the sample, no independent variable is constant nor a perfect linear combination of the others.
The variance of ut, is the same for all t,
The errors in two different time periods are uncorrelated,
13. In the presence of serial correlation, the OLS estimator is no longer BLUE and the usual OLS standard errors and test statistics are not valid even asymptotically. If there is autocorrelation, the expression is as follows:
14. The model in equation (1) is being tested for serial correlation with the results being tabulated in Table 2. It shows that there is indeed serial correlation with H0:
15. In correcting the serial correlation, we continue to assume the Gauss Markov Model Assumptions 1 to 4 but we relax the Assumption 5 and takes into account the error term model,
16. The OLS on the transformed data is being tabulated in Table 3. This GLS estimator is BLUE and the errors in this transformed equation are serially uncorrelated, of which t-statistics and F-statistics are valid asymptotically.
17. Similarly in cross sectional applications, heteroskedasticity in time series regression models while not causing bias or inconsistency in the coefficient estimate ß, invalidates the usual standard errors, t-statistics and F-statistics. In the test for heteroskedasticity, one key assumption is should not be serially correlated; any serial correalation will generally invalidate a test for heteroskedasticity. After the serial correlation is corrected, we will apply the test for heteroskedasticity.
18. The test for heteroskedasticity for time series regression is the same as the cross sectional analysis which (in the equation (2)) is regressed with the variables, serial_dj, serial_ftse, serial_hs, serial_nikkei where
19. Based on the results on F-statistics (Table 4), we fail to reject H0 for significance level of 10%. Therefore, we can conclude that the transformed equation is not heteroskedastic.
20. The regression model based on the results shown in Table 2 is estimated to be
21. The model illustrates that there is correlation between the percentage changes in STI index and Dow Jones Industrial, FTSE100, Hang Seng Index and Nikkei Index.
22. With regard to the correlation between the stock indexes and the STI index, there are a few observations. First, it is in line with the common notion that there is some correlation between the markets in Singapore and the financial stock indexes in US, UK, HK and Japan (i.e. when Dow Jones suffers a loss, there may be a ripple effect on STI index to react on the following trading day). From the magnitude of the coefficients, we can observe that 1% change in Dow Jones has a return of 0.05% change in STI index. This is relatively small and reasonable correlation as a large correlation is not expected given the differences between the comparing indexes in terms of structure, composition and economic performance and fundamentals. The correlation with the Asian stock indexes is stronger, possibly due to closer proximity in terms of economic structure. The recent economic diversification efforts initiated by the Singapore government to rely less on the major world markets such US have also reduced this correlation relationship. This could also be attributed to the particular time period selected for analysis. This regression model is based on the time period from 2004 to Oct 2007, where there are no significant turbulence economic shocks and even if during certain economic shocks i.e. sub-prime housing loan, the duration is not sufficiently long enough to capture the relationship effectively.
23. While efforts are being made to make data sets for analysis as robust as possible, nonetheless, there are some weaknesses in this selection. Only 4 other stock indices are being selected to conduct the regression model with STI index. These stock indexes are computed using different approaches (ie price weighted or scale weighted versus STI which is weighted-value stock index) and are therefore not an exact like-for-like comparison of stock indexes. There may be missing variables such as neighboring stock indexes (e.g. Kuala Lumpur Stock Exchange Index, KLSE) and emerging stock indexes with growing influence (e.g. China's Shanghai Stock Exchange).
24. The model can also be improved by separating into 2 periods where one of which is a stable period while the other could be a turbulent period, such that we can compare the correlation under different situations. Using the same analogy, the model can be expanded by separating into developed stock markets versus emerging stock markets.

Although its large population has provided China with a sufficient labor force, compared to its limited resources, it is widely believed that China's economic development and ability to modernize to a great extent depends on its success in curbing the population (Tsuya & Choe, 1988). Moreover, education has always been a crucial aspect of China's severe gender disparity. On one hand, China's huge population along with its increasing fertility and decreasing mortality could intensely limit China's further development. On the other hand, China's market transition and economic growth dating from the late 1970s along with the increasing population growth may have actually slowed down the progress toward gender equity instead of accelerating it (Hannum, 2005). The slowing progress is caused by the rising cost of education and increasing population which were resulted from the market transition and economic development. The combination of these factors may result in even more intense competition between boys and girls for parents' investment onto them, which boys are more likely to "win."
Therefore, by investigating the relationship between the educations of women aged 41-64 and their fertility behavior I could possibly answer whether or not China would be left in this "economic growth-> population growth and enlarging educational gender gap-> increasing population growth-> slower economic growth " trap. In this way, the answer may also provide an insight into how the relationship between education and fertility contributes to China's educational gender gap and a possible prediction of China's future development. Furthermore, to investigate the relationship from a cohort perspective could enable us to investigate the dynamics of the relationship, approach the changing historical background of fertility policies in China, and also compensate for the unavailability of data for other than 1982 while helping to forecast what relevant changes may lie in years ahead.
From the perspective of other social context of China, although its large-scale one-child policy did not start until 1979, the wan-xi-shao policy dating from 1970 might actually cause the relationship itself as well as its evolution over cohorts.1 Firstly, the wan-xi-shao policy may drive the relationship itself. This is because on one hand, fertility fell dramatically under the policy – the policy is negatively correlated with fertility in this way; on the other hand, in more developed regions and for the majority ethnicity group Han female education tends to be higher and the policy tends to be implemented in a more rigorous and far-reaching way – the policy is positively correlated with female education in this way. Therefore, the policy might be the intermediary part joining female education and fertility together and drive the relationship between them. Secondly, the wan-xi-shao policy may also cause the evolution of the relationship. This is because for women aged 41-64 in 1982, the younger of them may have not yet completed their fertility at the time of the policy and thus were more likely to be influenced by the policy, while the older of them may have already completed their fertility at the time of the policy and thus were more likely to be exempted from the policy.2 The different levels of policy effect may result in different patterns of the relationship across female cohorts of interest.
From the perspective of economic theory, the negative correlation between the quantity and quality of children in a family is one of the central features of economic theories of fertility (Becker & Lewis, 1973). Willis (1973) further extended the theory and argued that because education increased the value of women's time, women with higher education tend to substitute toward quality and away from quantity of children. This suggests a negative correlation between women's education and their fertility levels (Willis, 1973; Lam, 2005). Lam and Duryea (1999) have more explicitly attributed the negative relationship to the results of two trades-offs: trade-offs between quantity and quality of children and trade-offs between women's time allocation between housework and labor force participation. China, a country with more than one fifth of the world's total population and severe gender inequality of education (Hannum, 2005), provides a critical context for testing the theories and investigating how the underlying mechanisms of the relationship operate.
Specifically, this paper will investigate the following questions: How did the education of women in China aged 41-64 in 1982 correlate with their completed fertility? How would the relationship change across cohorts? What caused this relationship itself and its change over time? My empirical analysis is based on retrospective fertility histories of 89,594 Chinese women aged 41-64 in 1982. I document the strong negative relationship between education and fertility and summarize trends in education and fertility across cohorts. By relying on both the general trends across cohorts and including cohorts in the regression model, I demonstrate that the relationship for older cohorts is getting stronger and the relationship for younger cohorts is getting weaker. Analyzing the mechanisms through which education affects fertility, I first show that for both younger cohorts who were more likely to be influenced by the fertility policy and older cohorts who were more likely to be exempted from the fertility policy, the relationship between education and fertility remains negative. Combing this pattern with the above results indicating varying strength of the relationship, I conclude that fertility policy is not likely to be the cause of the negative relationship itself but the cause of the evolution of the relationship. Furthermore, by examining the relationship between women's education and their children's survival fraction as well as the relationship between women's education and their labor force participation, I find evident positive relationships in both results. My interpretation of this limited evidence, based on the rich research on the "two trade-offs", is that the "two trade-offs" are likely to be the cause of the negative relationship itself.
Many studies have focused on the relationship between women's education and fertility in developing countries. Bongaarts' (2003) conclusions were based on the data from Demographic and Health Surveys in 57 less developed countries and he investigated the relationship from the view of fertility transition that these educational differentials in fertility are slightly larger in the earlier than in the later stages of the transition. 3 He provides the perspective to analyze the relationship in China under such a macro background of fertility transition. Jain's (1981) research was based on the data from the First Country Reports of the World Fertility Surveys for eleven developing countries and reached the conclusion that advancement in female education can be expected to influence fertility behavior even without simultaneous changes in other factors such as increasing opportunity for participation in the paid labor force in the modern sector. 4 Lam and Duryea (1999) also confirmed the negative relationship between women's schooling and fertility in Brazil, a developing country experiencing rapid fertility decline in the absence of a major family planning effort.
Moreover, there are many studies paying attention to the mechanisms beneath the relationship. Graff (1979) concluded that "a more basic, critical, and realistic conceptualization of the role of education is required. Education ... should be seen ... less directly and less linearly, functioning and mediating through and with other structural and attitude-shaping factors." Weinberger (1987) reached a similar conclusion that education influences fertility through its effects on the intermediate factors, for example, age at marriage, breastfeeding and contraceptive practice. Both of their conclusions stressed the importance of finding out the mechanisms driving the causality from education to fertility and of controlling for other relevant variables. Martin (1995) argued that education enhances women's ability to make reproductive choices which resulted from their intensified bargaining power. This argument was consistent with Lam and Duryea's (1999) conclusion that education would increase women's household productivity. Martin (1995) also concluded that in some of the least-developed countries, education might have a positive impact on fertility at the lower end of the educational range, which was different from Lam and Duryea's (1999) finding that low levels of schooling are associated with large declines in fertility. Both of the above similarity and discrepancy raised the necessity to empirically investigate the evolution of the relationship and to test the mechanism beneath the relationship. Some articles especially emphasized the importance of controlling for other relevant variables. Freedman (1979) suggested controlling for changes in life conditions and changing perceptions, while Bongaarts (1978) preferred controlling for intermediate fertility variables (exposure factor, deliberate marital fertility control factors and natural marital fertility factors). Instead, Lam and Duryea (1999) controlled for socioeconomic factors including husband's schooling, region, race, marriage age and husband's income.
This paper will provide a complementary perspective compared to the above literatures by examining whether the policy effect is the driving force of both the relationship between education and fertility as well as its evolution. Furthermore, although all of the above studies were on developing countries, none of them were on China, for which my investigation might be a necessary complement and improvement.
Moreover, all of Jain (1981), Freedman (1979), Bongaarts (2003) and Lam and Duryea's (1999) articles emphasize the importance of controlling for other relevant variables. However, both Freedman (1979) and Bongaarts (2003) ignored the socioeconomic factors which are more fundamental since they actually determine life condition, perceptions and fertility variables in the first place. Therefore, I will turn to Lam and Duryea (1999) for reference on which socio-economic variables should be controlled for.
Table 1 presents the summary statistics of key variables of interest in the study. The dataset I will use is the IPUMS International China 1982. The sample will be restricted to Chinese women aged 41-64. By this restriction, women who are too young or too old for a research on completed fertility can be excluded. For those too young, their fertility or even education pattern may still be changing frequently and intensely; for those too old, their relevant patterns would already be very fixed, thus, including them may contribute little to the general analysis. The average age for women of interest is around 51.27. Therefore, their fertility behaviors can hardly be influenced by the large-scale on-child policy starting from 1978 since it is highly likely their childbearing age had already ended at the time. However, as discussed in the previous sections, the wan-xi-shao policy dating from 1970 might dissimilarly influence the fertility behaviors of the women of different cohorts. To capture the policy effect and the general evolution of the relationship between education and fertility, women aged 41-52 and those aged 53-64 will be analyzed separately through regressions. 41.86% of the women are aged 53-64, which generally guarantees the representativeness of both age groups. Furthermore, relationship changes across female birth cohorts will be investigated as a more detailed examination of the policy effect and relationship evolution.
Within the women of interest, 79.27% of them were illiterate or semi-illiterate, 15.05% of them had primary schooling, 3.56% of them had junior middle schooling, 1.57% of them had senior middle schooling, and only 0.56% of them had undergraduate schooling or higher. For their employment status, 53.44% of them were employed, 0.009% of them were unemployed and 46.55% were inactive. This under-representativeness of higher educational groups and unemployed groups may make the conclusions shaky. The women had a mean family size of 5.18 and had born 5.31 children on average of which 4.24 had survived. Therefore the average fraction of children surviving is as high as 83.25%.
Moreover, since most minority ethnicity groups were exempted from the fertility control policy, the patterns of the relationship between education and fertility for majority and minority ethnicity groups are also good indication of the policy effect. In the sample, as high as 93.91% of the women belong to the majority ethnicity group -- Han. This under-representativeness of the minority ethnicity groups may influence the reliability of the conclusions.
Specifically, to investigate the first research question that how did the education of women in China aged 41-64 in 1982 correlate with their completed fertility, firstly, descriptive statistics of the education level and number of children ever born to women aged 41-64 will be used to demonstrate the general trends and relationship between education and fertility of women of interest. Then a regression of the number of children ever born to women aged 41-64 on their educational levels will be done in order to achieve the partial and incremental correlation of each educational level on the number of children ever born. Both of the above descriptive and regression analyses can be done using the 1982 IPUMS International data for China, and the variables used are educational level and number of children ever born. Since the universe of educational level is all persons aged 6+, the universe of number of children ever born is females aged 15 to 64, and the dataset is based on the China census in 1982, there should not be serious self-selection problem. The variable indicating educational level will be treated as five dummy variables and their partial correlations with number of children ever born as well as their incremental correlations beyond the lower one will be shown.
To address the second research question that how would the relationship evolve across cohorts, firstly, descriptive statistics of the educational levels and fertility levels of female cohorts will be plotted to demonstrate the general evolution of the relationship between women's education and fertility across time. Next, two regressions of number of children ever born on educational levels will be done respectively for age group 41-52 and 53-64. In this way, general changes of the relationship across time can be achieved. Then two more inferential analyses will be done by regressing the number of children ever born to women aged 41-64 on the educational level and female cohorts respectively with and without interactions of each cohort with educational level, in order to demonstrate how the relationship between education and fertility changes across different cohorts. An F-test will be done between the above two regressions so as to examine whether the changing relationship between education and fertility across cohorts are statistically significant. Both of the above descriptive and regression analyses can be done using the 1982 IPUMS International data for China, and the variables will be used are educational level, number of children ever born and female birth cohorts. The eight 3-year female cohorts will be treated as seven dummy variables. Note that educational level will be treated as a categorical variable here for the convenience of interpretation.
The third research question that what caused the relationship and its evolution is highly important in that it investigates the underlying mechanisms driving the relationship between education and fertility. To examine whether the relationship itself is the result of the policy effect, the analyses for the second research question could be applied to demonstrate whether the directions of the relationship are different from younger to older women in 1982. Furthermore, since most Chinese minority groups were exempted from the wan-xi-shao policy (Tsuya and Choe, 1988), two regressions of number of children ever born on educational level, ethnicity with and without interaction of ethnicity with educational levels may also be instructive to test the possible causality carried out by the policy change. Ethnicity group will be treated as one dummy variable with minority ethnicity groups as the reference level.
The two trade-offs have provided insightful explanations for the negative relationship across many literatures. By attributing the declined fertility to the increased productivity of women in household and labor market as a result of the generally improved educational status of women, the rationales behind the negative relationship emerge in a clear and reasonable way.
To examine the trade-off between children quantity and quality, I will use both descriptive statistics of education, children quantity and children quality, in which children quantity is indicated by the number of children ever born, and children quality is indicated by number of children ever born now living and fraction of children alive. The analysis can be done using the 1982 IPUMS International data for China.
To investigate the trade-off between women's time allocation between housework and labor force participation, I will use descriptive statistics to show the general relationship between women's education, wage and labor force participation. With the increase of wage, if women's labor force participation does not change much, the research focus should be turned to the first trade-off that women, with even more improved market productivity, may tend to stay in housework and try to invest more on children quality. The opposite causality might also be achieved through empirical test. However, data needed for this analysis are not entirely available in IPUMS 1982. I can only use women's employment status and education to examine with higher education, whether or not women are more likely to participate in the labor market. To determine the pattern of the relationship between women's education and income, conclusions from Xie and Hannum (1996) will be referred to. However, since Xie and Hannum's paper was based on the 1988 Chinese Household Income Project (CHIP), the conclusions may not be quite comparable in this article due to the fact that China has experienced fast economic transition between 1982 and 1988. Moreover, CHIP and IPUMS data are based on different universe and survey design. As a result, reliance on their conclusions is quite limited.
Figure 1 shows the number of children ever born and the number of children ever born now living at the time of the survey, classified by different educational levels, for all Chinese women aged 41-64. The figure demonstrates the large differences in fertility across different educational levels. Illiterate or semi-illiterate women (about 79 percent of the women aged 41-64) report an average 5.8 live births. This falls rapidly with the educational levels from illiterate to undergraduate, at which mean number of children born alive was 2.5. Figure 1 also shows the large differences in child survival across educational levels. Illiterate women lost an average of 1.2 children, a survival rate of 78 percent. Women with graduate education report an average of only 0.07 children death, for survival rates over 97 percent. This leads to the issue of child health and child survival which indicates implicitly the interaction between education, fertility and investment in child quality. Therefore, it is a good starting point for my later investigation into the causes beneath the relationship between education and fertility. However, from undergraduate to graduate schooling, the declining trend of mean number of children ever born starts to be weaker, which is corresponded to by the following regression analysis.
Although family size does not entirely correspond to fertility levels, it does reflect the fertility and labor participation decisions made by women. Figure 2 shows the number of persons in the household at the time of the survey, classified by different educational levels, for all Chinese women aged 41-64. The figure demonstrates the large differences in family size across different educational levels. Illiterate or semi-illiterate women report an average family size of 5.2. This falls relatively fast with the educational levels from primary school to senior high school, while declining gradually in both educational levels of illiterate to primary school and senior high school to graduate schools.
To investigate the relationship between education and completed fertility, it is necessary to determine firstly whether education had shown a certain trend during the period of interest. As discussed by Lam and Duryea (1999), educational inequality is associated with the low mean and high variance of mean educational levels. Table 2 shows the trends in educational levels for female cohorts born between 1918-1941, based on the cross-sectional relationship between educational levels and age in the 1982 IPUMS International China. Mean educational levels for women has increased tremendously across cohorts by eight times over the years shown. The most rapid increase in the mean occurred for the cohorts born from 1930 to 1941. The role of these cohorts in the fertility decline as well as the evolution of the relationship between education and fertility will be investigated in the next section. However, Table 2 also demonstrates the enlarging educational inequality with the increase of standard deviations. Moreover, as seen in Table 2, the coefficient of variation declined steadily during the cohorts shown, which indicates that the educational distribution has a general tendency toward equality relative to their mean levels.
Table 3 represents the estimated correlations of educational levels on the number of children ever born to Chinese women aged 41-64. It is clear that both for the age group 41-64 in general, and for the separate age groups 41-52 and 53-64, a significant negative relationship exists between women's educational levels and their fertility level, that is, the higher educational levels women have, the lower their fertility levels are. Moreover, for the age group 41-52, the negative relationship does seem stronger than both the age group 41-64 in total and the age group 53-64 since for all of the educational levels, the absolute values of the coefficients are larger. This to some extent indicates that the wan-xi-shao policy did influence the fertility behavior of those women who were still in their active childbearing age at the time when the policy were implemented despite their respective educational levels. Therefore, it might be meaningful to treat different cohorts as dummy variables to be regressed on along with educational levels by fertility levels, in order to test whether or not the policy effect are significant.
Furthermore, although the negative relationship between educational and fertility levels is estimated to significantly exist, the marginal differences of the estimated coefficients5 of educational levels on fertility levels are actually diminishing, that is, for women aged 41-64, when the educational level turns from illiterate or semi-illiterate to primary school, the number of children ever born is estimated to reduce by 0.925, however when the educational level turns from primary school to junior middle school, the marginal correlation is only -- 0.905; from junior middle school to senior middle school, the marginal correlation is -- 0.622; from senior middle school to college undergraduate, the marginal correlation is -- 0.662 and from college undergraduate to graduate, the marginal correlation is only -- 0.001. The diminishing trend is the same for women aged 41-52, though with a larger marginal correlation from college undergraduate to college graduate. The trend is even more evident for women aged 53-64 for whom the marginal correlation from college undergraduate to college graduate gets a quite large positive value 0.445, which indicates that for this group of women, the higher education, instead of leading to lower fertility level, is actually resulting in higher fertility level. However, different from two other age groups, there is a rebound of marginal correlation from senior middle school to college undergraduate.6 Lam and Duryea (1999) attributed this diminishing trend of marginal educational correlations to the increased household productivity of women whose fertility decisions were mainly based on the trade-off between "quantity" and "quality" of children. Therefore, those results indicate the importance to investigate the driving forces of the relationship between education and fertility. However, these diminishing marginal correlations may also be due to the under-representativeness of women with higher education.
Figure 3 shows the fertility and education trend across female birth cohorts. As can be seen, there are changing relationships between educational and fertility levels longitudinally. It is clear that from the cohort 1927-1929, evident negative relationship between the average educational and fertility levels started and with an increasing speed. Before cohort 1927-1929, this trend of changing relationships was rather weak and gradual, and the relationship between education and fertility almost kept positive. However, both trends before and after 1927-1929 do not indicate the causality between the educational and fertility levels and also does not show whether the observed negative relationship was getting stronger or weaker across different female birth cohorts. To address this problem, regression analysis of fertility levels on educational levels and different female birth cohorts might be helpful.
Model 1 in Table 4 shows the regression results of number of children ever born on education which is treated as a categorical variable for convenience of interpretation, and cohort dummy variables which treat cohort 1918-1920 as the reference level. Interaction terms between education and cohorts are included in Model 2. As can be seen in Model 1, the negative relationship between education and fertility still holds. Moreover, along with the cohort evolving from 1921-1932, the average differences in number of children ever born across educational levels with the reference level of cohort 1918-1920 have been increasing. However, from 1930 to 1941, the differences in average fertility level between the cohort of interest and cohort 1918-1920 starts to decrease with an increasing speed. Especially, the difference falls below zero for cohort 1939-1941. In general, Model 1 shows the changing pattern of the average fertility level that although from cohort 1918-1932 the average fertility level keeps increasing, it increases at a decreased speed. For Model 2, the negative relationship between education and fertility also holds. This also shows that for those illiterate or semi-illiterate women, the average fertility level keeps rising from cohort 1918-1932 and then starts to decline. However, the changes of the negative coefficients on the interaction terms indicates that along with the cohort evolution, the negative correlation between education and fertility is getting stronger from cohort 1921-1932 and then begins weakening with the absolute values of the negative coefficients getting smaller. An F-test between Model 1 and Model 2 is done with an F-value of 6.96, which indicates the changing strengths of the negative relationship across cohorts are statistically significant.
As can be seen more clearly in Figure 8 and Figure 9, although the average fertility level is estimated to increase before cohort 1930-1932, it is with a stronger negative correlation between education and fertility; however, although from cohort 1930-1932 the average fertility level is estimated to decrease, it declines with a weaker negative correlation between education and fertility.
Model 1 and Model 2 also show that before cohort 1930-1932, the negative relationship is getting stronger with an increasing average fertility for all educational levels; however, from cohort 1930-1932, the negative relationship is getting weaker with a decreasing average fertility. Especially, both in Model 1 and Model 2, from cohort 1930-1932, the coefficients of cohort dummy variables decreased fast and eventually turn negative for the cohort 1939-1941. These results demonstrate that fertility policy might be the reason for the evolution of the relationship.
Figure 1 and Figure 6 show that with higher education, the number of children ever born tends to be lower and the average fraction of children ever born now living tends to be higher. This generally indicates women with higher education do substitute toward quality and away from quantity of children. The first trade-off is likely to drive the relationship itself.
Figure 7 respectively shows that women's labor force participation tends to increase with higher education. Moreover, Xie and Hannum (1996) concluded that education is estimated to be positively correlated with earnings in China during 1988. If the conclusion is used here, it indicates that women with higher education actually reacted actively to the higher income by participating more in the labor force. Therefore, the second trade-off might also be the key driving force of the negative relationship.
Based on the above descriptive statistics and linear regressions of fertility on educational levels with controlling for other key relevant variables, it can be concluded that the higher the educational levels of the women, the lower the fertility levels. This negative relationship holds true for both older and younger cohorts. More specifically, for older cohorts, the negative relationship is stronger but with an increasing average fertility for all educational levels; while for the younger cohorts, the negative relationship is weaker but with a decreasing average fertility for all educational levels. Moreover, the average fertility level for Han is estimated to be lower with a stronger negative relationship than that of minority groups, although the difference of the slopes is not statistically significant and the negative relationship remains for both majority and minority ethnicity groups. Furthermore, both the number of children ever born now living and labor force participation of women tend to increase with higher education, which generally follows the rationales of the two trades-offs.
Based on the above empirical results, the following conclusions can be drawn:
From the above conclusions, some implications about China's social reality can be reached. In the first place, decreasing China's gender gap of education and improving women's education tends to be an effective way to curb China's population and mitigate the intense competition for resources among its people. However, in terms of China's already limited resources and increased gender inequality resulted from the economic transition, this is a quite long and hard way to go. Relatively, implementing fertility control policy is effective not only in restricting China's population but also in weakening the reliance of fertility on women's education. Moreover, since it is implemented imperatively by the government, it is also more efficient in the short term. However, to improve women's education, in the long run, is more significant since this can fundamentally pull the relationship between education and fertility out of the vicious circle from large gender gap of education to increased fertility.
However, the above conclusions and implications should be taken cautiously due to the following reasons. Firstly, the regression of fertility on education has not been done repetitively by treating each educational level as the reference level in order to statistically test whether their respective correlations with fertility are significantly different from one another. Secondly, women with higher education and unemployment, as well as minority ethnicity groups are under-representative in the used sample, which may affect the reliability of the conclusions. Thirdly, to investigate the two trades-offs, descriptive statistics and regressions to examine the relationship between women's education, wage, labor force participation and their children's education should be achieved aside from the reference to other literature's conclusions. These analyses will be done for future study and will be based on a combined dataset of China Study (1964) and Chinese Household Income Project (1988, 1995). However, the three datasets are different in their specific universes and survey designs; moreover, although the time periods of the dataset covered 1982, in terms of China's intense transitions during 1980s, the possible results achieved from the above dataset may be unrepresentative and inconsistent with the situation in 1982, and thus lack the convincing power. Fourthly, except for cohorts and ethnicity, more variables should be controlled for in the model to secure the estimates to be unbiased while making the conclusions more reasonable and convincing. Last but not least, the conclusions should be related more deeply with China's social context in order to provide more insights and implications for gender gap of education and sustainable economic development of China.

Using a one-sector model, Lucas (1990) argued that it was a paradox that not more capital flow from rich to poor countries. His reasoning goes as follows. Let y = f (L, K) be a constant-returns-to-scale production function where y is the output produced using labor L and capital K. Let p be the price of the good, and w and r be the returns to labor and capital, respectively. Firm's profit maximization problem gives
Assuming that the product price is equalized across countries under free trade, the law of diminishing marginal product implies that r is higher in the country with a lower
capital-labor ratio. As an illustration, Lucas calculated that the return to capital in India should be 58 times as high as that in the United States based on their factor endowment. Facing a return differential of this magnitude, one should observe a lot more capital to flow from rich to poor countries. That too little flow is observed in the data has come to be known as the Lucas paradox. In fact, in the data today, this paradox is prevalent if not aggravated. In table 1, it shows clearly that the total value of capital inflows into rich countries exceeds those into poor countries manifolds. This trend has even been stronger in the more recent periods.
Lucas (1990) discussed three possible explanations (within a one-sector framework): (a) A worker in a rich country could be several times more productive than her counterpart in a poor country; (b) Human capital may be a missing factor and is likely much higher in a rich country; (c) Political risk and hence required risk premium may be substantially higher in a poor country.
The paper will mainly focus on the third explanation, namely whether the quality of political institutions plays a role in capital inflow. It discusses the motivations and empirical evidences in the more recent literature. The question of whether political institutions matter in terms of attracting capital inflow is crucial for policy makers especially in the developing countries. As indicated in table 1, poor countries often have difficulty in attracting capital inflows which might be crucial to their growth process. Thus, if policy makers had a better idea of the underlying cause of the lack of capital inflow, such as bad quality of institutions, they could improve the source of the problem.
Interestingly, Lucas himself dismisses the quality of political institutions as possible explanation for the lack of capital inflow into rich countries. His reasoning is that prior to 1945, most of the colonies were ruled by European countries who instituted the same laws in their colonies as in the mother country. Thus, if a British lender, for instance, wanted to lend money to an Indian, the British could expect the same contract laws to apply in India. Reinhart and Rogoff (2004) challenge the validity of Lucas' dismissal by pointing out the number of rebellions in the colonies at that time. They argue that despite having the same laws in the colony and mother country, it was far more likely that the laws in the colonies would be disobeyed due to instable governments. In fact, Reinhart and Rogoff Reinhart argue that the set of countries with frequent default on their external debt have instable governments and are generally low income countries.
A more extensive empirical study is by Alfaro, Kalemli-Ozcan, Volosovych (2005). Using data from the IMF on balance of payments, Alfaro, Kalemli-Ozcan and Volosovych (2005) find that good institutional quality is a key determinant of total capital inflows. In their study, they use the extensive dataset by IFS/IMF on foreign direct investment (FDI), portfolio equity investment, and debt inflows from 1970-2000. For their total sample of 98 countries, they construct a measure of institutions as a composite index which is the sum of the indices of investment profile, government stability, internal conflict, external conflict, no-corruption, non-militarized politics, protection from religious tensions, law and order, protection from ethnic tensions, democratic accountability, and bureaucratic quality obtained from International Country Risk Guide (ICRG). Alfaro et al. run the OLS regression of capital inflow on log GDP per capita and the institution index, i.e.
Alfaro et al. argue that if α, the coefficient on log income per capita is positive, it indicates the existence of the Lucas' Paradox. As seen in table 2, when regressing capital inflow on log GDP per capita by itself, α is statistically significant and positive (columns 1 and 3). When adding the institution index as a regressor, however, the coefficient estimate on log GDP per capita becomes statistically insignificant and β, the coefficient estimate on institutions, is positive and statistically significant (columns 2 and 4). Thus, Alfaro et al argue, the quality of institution provides an explanation for the Lucas Paradox. To get a better sense of the magnitude of β, Alfaro et al. point out that based on the estimate, if a country moves up from the 25th percentile (Guyana) to the 75th (Italy) in the distribution of the index of institutions, based on column (4), there will be an increase of $187.54 in inflows per capita over the sample period on average. This represents a 60% increase in inflows per capita over the sample mean which is $117.34. Alfaro et al. also run a separate multiple regression by including other explanatory variables such as log average years of schooling, log average distantness, and average restrictions to capital mobility. These three regressors represent plausible barriers to capital inflows, namely, human capital, asymmetric information and government policies, respectively. The results in Table 3 show, however, that none of these three additional regressors can by itself explain away the Lucas paradox. The coefficient on log GDP per capita is only insignificant when adding the quality of institutions index.
The two main potential problems with this simple OLS regression are multicollinearity and endogeneity. The multicollinearity stems from the fact that log GDP per capita and the institution index are highly correlated. The authors perform extensive diagnostic tests and simulation exercises to make sure that the results are not spurious and that they capture the direct effect of institutional quality on capital inflows. For instance, they regress the residuals from the regression of average inflows on average institutional quality against the residuals from the regression of log GDP per capita in 1970 on average institutional quality and vice versa. The slopes of the fitted lines match exactly their counterpart in the multiple regression as proposed by the Frish-Waugh Theorem if all conditions are met. In addition, they perform perturbation exercises and none of the robustness regressions show any big sign and magnitude changes, which are typical indicators of multicollinearity.
The other potential problem is endogeneity which can be caused in two ways. First, it is possible that the capital inflows affect the institutional quality of a country. Since most institutional quality measures are constructed ex-post, and there might have been a natural bias in "assigning" better institutions to countries with higher capital inflows. With this errors-in-variable type of endogeneity, one would expect attenuation bias, which means that the actual effect of institutions on capital inflow should be even higher.
The second source of endogeneity can come from the possibility that both inflows and institutional quality are determined by an omitted third factor. By conducting various robustness tests, Alfaro et al. include that omitted third factor is not an issue.
To control for the first possibility of endogeneity, Alfaro et al. use an instrument for institutional index. As the instrument, they pick log settler mortality, which was suggested originally by Acemoglu, Johnson, and Robinson (2001, 2002). Acemoglu et al. find that the mortality rate of settlers indicates the conditions in the colonies. More specifically, mortality of European settlers in the countries they colonized shaped their decision to settle or not. When they settled, they brought with them effective European institutions, whereas when they did not settle, they instituted systems of arbitrary rule and expropriation of local populations. The results from the first stage regression shows a large R-squared of 0.39, and as expected, the second stage estimate on institutional quality becomes even larger than the one without instrument.
Glaeser, La Porta, Lopez-de-Silanes, Shleifer (2004) raise the interesting issue regarding the construction of the institutional index and the validity of the instrumental variable. Their critique is a follow up on Acemoglu et al.'s paper which suggest that institutions play a role in the economic growth of a country. Glaeser et al. point out that institutions are long lasting constraints rather than brief policy changes as constructed via the institutions index. The other main issue that Glaeser et al. criticize is the use of the instrument log mortality rate employed in Acemoglu et al's paper. Contrary to Alfaro et al, Acemoglu et al. run a regression of log GDP per capita on the institutional quality variable. Glaeser et al. point out that human capital is an omitted variable Acemoglu et al's regression. In fact, when including human capital via average log years of schooling, in a two stage regression, the impact of institutional quality on log GDP per capita become insignificant but the estimate on schooling become significant instead. Alfaro et al. included average log years of schooling in their multiple regression as well, but it did not have explanatory power on capital inflows. Thus, although Glaeser et al.'s critique on Acemoglu et al's approach is relevant, it is not applicable to Alfaro et al.
The dilemma with this stream of literature on capital inflows is that apart from mostly OLS based empirical papers, there is no concrete model for the actual problem.
The basic setup of a model is
(1)
If agents can borrow and lend capital internationally and if all countries share a common technology, perfect capital mobility implies instantaneous convergence of the returns to capital, i.e. for countries i and j,
where f(.) is net of depreciation production function per capita and k denotes capital per capita. Based on Lucas' three different explanations of why capital does not flow into poor countries, different approaches to model capital returns, 1. Missing Factor, 2. Government Policies, i.e. impediment to flows, tax policies, capital controls, and 3. Institutional Structure and Total Factor Productivity.
In the missing factor explanation, there might an externality in the production process that affects the returns to capital but are generally ignored by the conventional neoclassical approach. For example, if human capital positively affects capital's return, less capital tends to flow to countries with lower endowments of human capital. Thus, if the production function is given by
Where Z denotes another factor that affects the production process, then (1) misrepresents the implied capital flows. The true return for countries i and j is
A second explanation of the lack of capital inflows might be that government policies are impediments to the flows. For example, differences across countries in government tax policies can lead to substantial differences in capital-labor ratios. One can model the effect of these distortive government policies by assuming that governments tax capital's return at rate τ, which differs across countries. Then the true return for countries i and j is
The last explanation that inhibits capital inflows might be the quality of institutions. Institutions can affect economics performance through their effect on investment decisions by protecting property rights. Weak institutions can lead to lack of productive capacities or uncertainty of returns in the economy. These differences can be captured in the parameter A, which depicts differences in overall efficiency in the production across countries. The problem with this approach is that one cannot differentiate between the effect of institutions on investment opportunities versus that of the Total Factor Productivity (TFP). The true return for countries i and j is
Clearly, much work lies ahead in constructing a sound economic model that shows the effects of institutions on capital inflows. The model would enable us to focus on particular data sets instead of averaging numerous countries and only taking into account the cross-sections. Thus, the next step could be to identify particular instances, where given the events, we are sure that the lack of capital inflows was due to a certain change in institutions. This way, we can test this particular event with a specific dataset. As of now, the empirical evidence is rather generalized, and the cross-section evidence is sensitive to various econometric problems.

Asymmetric stock price response to earnings announcements has long been an interesting and important topic in accounting, finance and economic research. This paper empirically investigates the existence of asymmetric stock price response to abnormal earnings both at aggregate and industry levels. These asymmetries are identified by comparing the response of the stock price to good news to the response of the stock price to bad news. In order to uniformly measure asymmetry in stock price response, the difference between actual earnings and analysts' mean forecasts are defined as abnormal earnings. Positive abnormal earnings are categorized as good news and negative abnormal earnings as bad news. This method gives an ideal measure for information about firms' earnings because it measures "quality" as well as quantity of information. In addition, the dependence of the asymmetric stock price response on the industry that the firm belongs to is also examined. This paper investigates if the average asymmetric stock price response in an industry is related to industry characteristics, such as the Herfindahl index, CEO (Chief Executive Officer) compensation, credit rating information, and firm size.
There has been few quantitative research performed at the industry level to detect the asymmetric stock price response. This paper gives an interesting motive to investigate the reason for the different stock price responses between industries. Quantitatively, this approach has some advantages in that asymmetric stock price response in each industry is easily detected using a simple OLS specification, and the interaction of the industry-specific variables with abnormal earnings can be analyzed to identify the effect of each industry-specific variable on the asymmetry in stock price response. In fact, there is a large body of research on asymmetric stock price response, and an equally large one on the relationship between key industry-specific factors and voluntary disclosure or earnings management. However, there has been little work linking these two studies. This paper examines an empirical link between these two studies.
I find that there exists asymmetry in stock price response in aggregate level. The econometric specification shows that the coefficient of good news is 0.1110 and that of bad news is -- 2.421. This implies that overall bad news has a larger impact on the stock price change than good news does. However, another econometric specification performed in industry level suggests that there is huge heterogeneity in asymmetric stock price response across industries. These asymmetries are more likely to be related to the variance of the stock price change than to the magnitude of the stock price change itself. The empirical link between the key industry-specific factors and stock price response is examined. The consistency of the results with prediction of relating theories is also checked. Most of the key industry-specific variables are likely to follow the pattern that the corresponding theory predicted.
The remainder of the paper proceeds as follows. Section 2 reviews related empirical or theoretical literature. Section 3 contains corresponding data. Section 4 discusses the asymmetric stock price response in aggregate level. Section 5 discusses the heterogeneity across industries. Section 6 characterizes the sources of heterogeneity discussed in Section 5. Section 7 concludes this paper with a discussion of some implications of the results for future research.
For asymmetric stock price response in earnings announcement, Kross and Schroeder (1984) provide an empirical investigation of the effect of quarterly earnings announcement timing on stock price returns. They found that early quarterly earnings announcements contain better news and are associated with larger abnormal stock returns. MacKinlay (1997) shows that both good news and bad news have strong impact on the cumulative abnormal return. He shows that the absolute value of the statistics for testing the impact of news is higher for good news than bad news.
Asymmetric stock price response might be attributed to a few factors such as earnings management and/or voluntary disclosure occurred before earnings announcement. First of all, what earnings management is should be clarified. Notice that there is no consensus on defining earning management1. Following Schipper (1989), I define earnings management as "a purposeful intervention in the external financial reporting process, with the intent of obtaining some private gain." If earnings management is an unobservable component, it is reasonable to assume that investors and other firms in the same industry cannot unravel the effect of earnings management on reported earnings.
Analytical or empirical research on managers' voluntary disclosure has shown that there are three different effects on stock price according to which information managers are more likely to reveal before the quarterly earnings announcement. First, if firms tend to voluntarily disclose more good news than bad news, the stock price response to positive abnormal earnings in actual earnings announcement is expected to be smaller than the response to negative abnormal earnings. Lev and Penman (1990) document that earnings forecasts are used by managers of "good news" firms to screen themselves out from other firms with "bad news" in a signaling scenario. However, they do not find that their results are consistent with the negative price reaction implication for the nondisclosing firms in the same industry. Second, if firms are more likely to disclose bad news than good news, the stock price response to positive abnormal earnings in actual earnings announcements is larger than the response to negative abnormal earnings since quarterly earnings announcements that convey large negative earnings surprises are preempted by voluntary disclosure while other earnings announcements are preempted less(see Skinner (1994)). Lastly, if firms are willing to disclose good news and bad news equally, the stock price response to either abnormal earnings in actual earnings announcement is same. According to Pownall, Wasley, and Waymire (1993) forecasts are less informative than earnings announcements for their full sample and differences across forecast forms are not significant at conventional levels. They find that voluntary disclosures are associated with a stock price response which is, on average close to zero.
Theoretical analysis on incentives for voluntary disclosure associated with industry-specific factors has shown that key industry-specific factors could explain a large portion of asymmetry in stock price response. A highly concentrated industry (high Herfindal indexed industry) is expected to have large stock price response regardless of signs of abnormal earnings. Darrough and Stoughton (1990) predict that competition in the product market encourage voluntary disclosure. The effect of voluntary disclosure might lower the stock price response in earnings announcement. The degree of asymmetry is, however, not yet analyzed. In a similar study, Dontoh (1989) supports that in an N-firm oligopoly, firms have incentives to disclose unfavorable information about future outcome as well as favorable information depending on the types of firms. Thus it is expected that an industry with a higher HHI will have lower stock price responses with almost no symmetry in earnings announcements.
Murphy (1998) supports that CEOs who receive relatively small bonus payments have potentially less explicit incentives to manipulate income so as to maximize bonus-based compensation. Murphy and Zimmerman (1993) point out that the first class of discretionary behavior -- reflecting the managerial horizon problem -- is likely to be relatively more pronounced in firms with good corporate performance and routine retirements while the other two classes of discretionary behavior -- outgoing CEOs covering-up poor performance and incoming CEOs taking a big bath -- are likely to be more pronounced in firms with deteriorating economic health. The irresolvable links between performance and discretionary behavior make it difficult to disentangle the effects of poor performance from the effects of managerial discretion. Note that earnings management is not necessarily tied with poor economic performance, but is highly correlated with deteriorating corporate performance. Thus higher portions of bonus payments in CEO compensation leads to larger amounts of the stock price response due to higher incentives in earnings management. While poor economic health is more likely to encourage CEOs to manage earnings.
By simple logic, lower earnings management is expected with higher credit rated firms since the better firm's credit rating is, the better performance of a stock and dividends are made. So there might be a negative relation between the stock price response and firms' credit rating. For theoretical support, Bagnoli and Watts (2000) indicate that firms may exaggerate their earnings in a world driven by multi-firm-comparisons because they expect other firms to do so. They also find that the equilibrium amount of earnings management depends not only on the earnings management method itself but also on the proportion of long-term investors' in the firm. This implies that firms with a higher crediting rating for long-term issuer's credit rating do less earnings management under the assumption that investors' decision is based on all the information available including the firms' credit rating.
In this section, quarterly earnings announcements are considered. I investigate the information content of quarterly earnings announcements for the entire US firms in the COMPUSTAT® Industrial data over the 5-year period from January 1997 to December 2001. These announcements correspond to the quarterly earnings for the last quarter of 1996 through the third quarter of 2001. For each firm, the daily stock prices for the corresponding earnings announcement are collected from the same source. For each firm and quarter, three pieces of information are gathered: the date of the announcement, actual announced earnings measured in earnings per share, and the analysts' mean forecasts. A measure of the deviation of the actual announced earnings from the market's ex-ante expectation is required in order to analyze the impact of abnormal earnings on the stock prices. I used the analysts' mean quarterly earnings forecasts from the Institutional Brokers Estimate System (I/B/E/S) to proxy for the market's expectation of earnings. I/B/E/S compiles forecasts from analysts for a large number of firms and reports summary statistics each month. In order to control the change in firm's economic performance and macroeconomic shock, the industry average of stock prices on the corresponding dates and the S&P 500 Indices from COMPUSTAT® Industrial data are included. All firms are classified by their industry classification codes. From this sample 22 industries were dropped due to insufficient financial data in the regression.
The COMPUSTAT® Industrial CEO compensation data taking into account options granted, firms' sales data, and S&P Long Term Issuer's Credit rating data as firms' credit rating information are used in another regression. CEO compensation is comprised of the following: salary, bonus, other annual, total value of restricted stock granted, total value of stock options granted (using Black-Scholes), long-term incentive payouts, and all other total. The quarterly average CEO compensation is calculated under the assumption that the annual average CEO compensation could be evenly spread throughout the years. The incentive variables are defined as the ratio of incentive related compensation to total compensation. This could measure how the CEOs' discretionary intention affects the stock price in the event of earnings announcements. For the Herfindahl index, I computed the market share of each firm using its quarterly sales data relative to the total industry sales. Note that the Herfindahl indices were computed in the original data set simply because the more available sales information obtained the better the results. Information about firms' credit rating was obtained from COMPUSTAT® Industrial data after comparing this with data from Moody's Default Risk Services database. According to the data manual in COMPUSTAT®, credit rating information is defined as a current opinion of an issuer's overall creditworthiness, apart from its ability to repay individual obligations. The S&P Long Term Issuer Credit rating was employed. From this sample 52 industries were dropped due to insufficient observation in the regression. Lastly, firms' quarterly sales data is used as a proxy for firm size.
I examine the stock price response of firms in aggregate level to the announcement of earnings in every quarter over 1997-2001. If stock price response to positive abnormal earnings and negative abnormal earnings differ systematically, it is likely that there exists asymmetric stock price response. To explore this, I examine the stock prices, abnormal earnings, industry average stock price and the S&P 500 Index. For example, when we observe positive or negative abnormal earnings ex-post, the corresponding stock price responses could either vary equally in both cases or represent some asymmetry. I test whether there is asymmetric stock price response to their earnings announcement through aggregate level OLS specification.
In order to detect the stock price response, I use a modified event study of earnings announcements repeatedly. The usefulness of a modified event study is that, given rationality in the marketplace, the effect of an earnings announcement will be reflected immediately in stock prices. The event's economic impact can be measured using the stock prices observed over a relatively short time period. The period of interest expanded to multiple days including the day before the announcement and the day after announcement. This captures the stock price effects of announcements which occur after the stock market closes on the announcement day. I assume that there exists no correlation between the reported earnings by firms and the mean value of analysts' forecasts. In order to control common shock in the stock market, I use the S&P 500 indices and to control industry-specific shock I include the average industry stock prices. The equation to be estimated has the following form for each industry:2
(1)
where
(2)
The model suggests that positive coefficient on the difference in reported earnings by firms and the analysts' mean forecasts would be expected when the positive abnormal earnings lead to positive impact on stock prices. But negative or 0 coefficient on the positive abnormal earnings variable could be observed when the abnormal earnings have negative impact or zero impact on stock prices. Note that the interesting part is not the value of the coefficients themselves but the difference between two coefficients. The difference in coefficients of two opposite signed variables that represent asymmetry, explains the strength of asymmetric response of the stock prices.
However, this gap reflects not only asymmetric stock price response from abnormal earnings but also other forces resulting from changes in the economic performance of the firm's industry. This justifies including the average industry stock prices and the S&P 500 indices to separate the effect of asymmetric stock price from the effect of industry-specific economic factors mentioned above. Since the number of observations available for estimation is large enough, the use of Ordinary Least Square (OLS) is justified for each industry. The scope of investigating abnormal earnings is constrained by the limited disclosure of information in quarterly financial announcements.4
A summary of the sample data for the regression is presented in Table 1. The total number of industries collected from the data source is 393 and 366 out of 393 industries remain since 27 out of 393 industries were dropped due to insufficient observation. Table 1 part A reports the summary statistics before the differences of the variables were made. The sample size is 96,784, which is calculated based on the number of actual EPS observations. Table 1 part B informs the sample statistics after the differences were made. The sample size is 42,358. This provides summary statistics for all the variables used in the OLS specification. The OLS regression, performed in the aggregate level is presented in Table 2.
In the aggregate industry level, the results support the asymmetry in stock price response to earnings announcements. These results also indicate that all the coefficients are statistically significant at 1-percent significance level with and without time dummies and industry dummies. The OLS regression with dummies shows that positive abnormal earnings variable has a positive coefficient. In other words, the stock price is more likely to rise when earnings announcements contain good news. The magnitude of the coefficient implies that, for instance, when a firm in this industry reports positive abnormal earnings about 1 percent of the analysts' mean forecasts, the stock price one day after the earnings announcement date is expected to rise by 11 percent more than it would otherwise. Also the fact that the coefficient of negative abnormal earnings is larger than that of positive abnormal earnings in absolute term implies that the stock price responds more to bad news than good news.
In this section, I study the stock price response of firms to the announcement of earnings in different industry level in order to see whether there is heterogeneity of stock price response across industries. At this stage, the same econometric specification and equation as in the previous section is used. Based on the arguments provided at the end of the previous section the following hypotheses for each industry relating to the asymmetric stock price response are tested:
No asymmetric stock price response exists across industries.
As in the previous section, I find that there is asymmetric stock price response in aggregate level by using OLS specification. Now I test the hypothesis that no asymmetric stock price response exists in earnings announcement in different industry levels. If there is no asymmetry in the stock price response, those two estimated coefficients of the abnormal earnings in every industry have the same magnitudes. Graphically, two estimated coefficients are scattered on the 45 degree line if no industry is expected to have the asymmetric effect on abnormal earnings. As shown in Figure 1 and Figure 1-1, there exists clear asymmetric response of stock prices in many industries. This finding supports that there is heterogeneous stock price response in different industries. Figure 1-1 represents that the absolute magnitude of the estimated coefficients of the positive abnormal earnings is larger than that of the negative abnormal earnings overall and some points are clearly located outside of the 45 degree line. Figure 1-1 also shows that coefficients of positive abnormal earnings are scattered broader along the axis than those of negative abnormal earnings. Namely, many industries are more likely to have larger stock price response to good news than to bad news. The asymmetry is related not to the magnitude of stock price change but to the variance of this change. Summaries of the sample data for the industry level regression are presented in Table 3 and Table 4. The number of industries that show the greater stock price response to negative abnormal earnings is 128 while 238 industries report that the stock price response to positive abnormal earnings would be larger than that to negative ones. However, only 61 industries show that they have statistically significant asymmetric stock price response. Results for the industry-wide OLS regression of the difference in stock prices on the abnormal earnings and the controls for rational stock market conditions are shown in Table 3. Table 3 exhibits that 23 out of 366 industries show the asymmetric stock price response, which is statistically significant at the 1-percent level. Up to 10-percent significance level, 61 industries out of 366 have the difference in estimated coefficients that are statistically different from 0. The detailed information about other industries which represent asymmetric stock price response is provided in Table 4.
In this section, sources of heterogeneity across industries are explained by the corresponding theories of key industry-specific factors, which are provided in Section 2.
The evidence of including voluntary disclosure is consistent with the idea that managers have incentives to preempt the announcement of large negative earnings surprises, and they have incentives to distinguish themselves by declaring good economic performance. This voluntary disclosure would affect the stock price immediately so that it will lead to lower impact of quarterly earnings announcement on the stock prices. Accordingly, unfavorable voluntary disclosure will reduce the impact of negative earnings surprise in earnings announcement. So if firms voluntarily disclose favorable information more often than they do unfavorable information, the stock price response to the negative abnormal earnings will be larger than the response to the positive abnormal earnings. Also if firms are more likely to disclose unfavorable information than favorable information, the stock price response to the positive abnormal earnings will be larger than the response to the negative abnormal earnings. If they disclose voluntarily both favorable and unfavorable information, the asymmetric stock price response is not expected.
Also, including voluntary disclosure justifies the phenomena that the stock prices could rise (fall) when we have bad news (good news). For example, when we observe the positive abnormal earnings ex-post, the stock price one day after earnings announcement date is expected to rise without prior information about future income or any earnings management. The stock price is, however, expected to fall with previous voluntary disclosure which already carried very favorable news about earnings and no more promising news, but note that this response could be much larger than it is in the case we observe negative abnormal earnings. Similarly, when we observe negative abnormal earnings ex-post, the stock price one day after earnings announcement is expected to fall without prior information or any earnings management by the same magnitude when we observe positive abnormal earnings. Also the stock price could even rise with previous voluntary disclosure to preempt the negative earnings surprise.
The results predicted by corresponding theories are following: A highly concentrated industry is less likely to have asymmetry in stock price due to less voluntary disclosure. Since CEOs who receive relatively small bonus payments have potentially less explicit incentives to manage earnings5, higher portions of bonus payments in CEO compensation lead to larger amounts of the stock price response due to higher incentives in earnings management. The amount of discretionary compensation is expected to explain the asymmetry in stock price response. The better credit rating would be related to less incentive to manage earnings, thus less asymmetry in stock price if other conditions are the same.
In this section, I investigate the source of heterogeneity in different industries using simple econometric method. Once asymmetric stock price response has been detected both in aggregate and industry levels, I examine the stock price response of firms in each industry to the announcement of earnings, taking into account the interaction terms between abnormal earnings and the four industry-specific factors, such as a change in number of competitors in the market which can be represented by a Herfindahl index, a change in firm CEO compensation, a change in firms' credit rating and a change in firm size proxied by quarterly sales. For example, I examine if an industry is more concentrated, an individual firm in this industry more likely to have large/small response in stock price. Also I check if changes in credit rating or CEO compensation explain the degree of stock price response to earnings announcements. Since CEO compensation includes the bonus payments and the total value of options granted, adding this variable could explain how much the asymmetry in stock price response is affected by the amount of discretionary portion of compensation. I also estimate the effect of incentive variables defined as the discretionary portion of CEO compensation. I assume that the previously detected the asymmetric stock price response is determined by a linear combination of some observable industry strategic factors, and also an unobserved component. Thus asymmetric stock price response which might be explained by key industry-specific factors has the following form:
(3)
where
(4)
where difeg12it :
None of the industry specific factors will explain the stock price response.
Table 5 provides summary statistics for the key industry-specific variables used in the OLS specification. Note that we are interested in studying how the interaction terms of the variables of HHI, CEO compensation, incentive variable, credit rating information and firm size with abnormal earnings would affect the difference in the stock prices. This industry-wide regression makes an attempt to ascertain whether the key industry-specific factors could explain the difference in the stock prices through the asymmetry in the stock price response. It repeats the industry-wide OLS regression specification with and without incentive variables. I assume that any interaction among the key variables should be zero because each variable was measured independently and no correlation exists. By looking at the estimated coefficients of each industry-specific variable jointed with the abnormal earnings, I test the hypothesis that none of the industry specific factors will explain the difference in stock prices. If the difference in stock prices shows no response to any interaction terms of four industry-specific variables, this supports the hypothesis that none of the industry specific factors will affect the difference in stock prices. Otherwise, this supports that at least one of the industry-specific factors significantly affects the difference in stock prices.
Table 6 shows the OLS specification, taking into account interaction terms of industry-specific variables and the abnormal earnings in the aggregate level. It shows that the CEO compensation interaction term and the credit rating interaction term are significantly different from 0 at the 5-percent level and at the 1-percent level, respectively. With incentive variables, only credit rating interaction term is statistically significant at 1-percent level. For the OLS specification without including incentive variables, 75 industries show significant effect on the stock price response. Total number of industries successfully regressed is 164, and 134 observations are statistically significant up to the 10-percent level. The effect of each industry-specific factor on the stock price is represented in Figure 2, Figure 3, Figure 4, Figure 5, and Figure 6, respectively. For the Herfindahl index interaction term, the less concentrated industry is more likely to have higher effect on the stock price as seen in Figure 2-1. As theory predicts, highly concentrated industries tend to have lower stock price response in earnings announcement. For the CEO compensation interaction term, Figure 3-1 provides that small compensation would be more related to higher response in the stock price. This result is also consistent with the theory that higher compensation CEOs have less incentive to manage earnings in the event of earnings announcement. Figure 4, 4-1 presents the effect of incentive variables on the stock price. The pattern is somewhat different from that in CEOs: the medium portion of discretionary compensation not the small portion of it represents higher response in the stock price. This means that the higher portion of CEO compensation could be used as discretionary, the more likely to show higher stock price response, except top compensation notch. For the credit rating interaction term, Figure 5 implies that neither a very good rating nor a bad rating has large effect in the stock price. Finally, the firm size interaction reveals that small firms tend to have larger stock price response to earnings announcements.
The principal result of this paper is that the asymmetric stock price response at both the aggregate and industry level has been detected and is empirically related to key strategic industry-specific factors (Herfindahl index, CEO compensation, credit rating and firm size). My results indicate that on average, the stock price responds more to negative abnormal earnings than to positive abnormal earnings. The difference in responses, although small, is statistically significant. Another interesting finding is that, the asymmetry in the stock price response is more related to the variance of the stock price change than to the magnitude of the stock price change itself (Figure 1). The effect of industry-specific variables on the stock price is consistent with what the previous theory predicts. For instance, the stock price has a larger effect in a relatively more concentrated industry. However, for the effect of the credit rating interaction on the stock price, further development of theory is needed. The fact that the variance of stock price response has a larger impact on the asymmetry in stock price response will naturally lead to time series econometric specification for future research. Furthermore, it seems worthwhile to control voluntary disclosure in this model.

This paper is an exploratory analysis of whether or not an organizations' gains from out-sourcing on-site clerical jobs have any impact on the clerical workers' compensation. For the purposes of this discussion, we consider a worker who works on site at company x, but paid by company y as "out-sourced." This analysis excludes the more classical definition of "out-sourcing" as a job sent overseas. The first part of this paper discusses whether economic gains from compensation drive firms' decisions to out-source. The second component examines where contracting firms locate and raises questions about the spatial mismatch. Finally, the third and primary sectino of the paper explores whether or not indirectly employed clerical workers receive lower compensation than do their directly employed counterparts. None of these analyses are conclusive, so the future research section sets an agenda for an inferential spatial analysis of the interaction between the demand and supply of clerical contractors, a model of spatial mismatch, and a longitudinal analysis of clerical workers, focusing on the importance of flexibility in the workers' decision to work indirectly. Ultimately, I hope to determine whether or not the longitudinal trend towards out-sourcing will negatively affect the low-skill female work force.
In the past decade firms increased their purchases of services significantly more than they increased their direct hires. From 1988 to 1997 "business services" grew by 5.8% per year. The rate of growth in business services increased at twice the rate of the rest of the economy over the past 2 decades. Business services is the larget category under the NAICS 54 to 56 categories. These include professional, scientific, and technical services, (legal advice, accounting, bookeeping, architecture, consulting, research, etc) management of companies and enterprises (establishments holding sufficient securities to administer and oversee a company, and administrative and support and waste management (office administration, human resources, clerical, security, cleaning, and waste disposal.) Personnel services, a subcategory of business services, is the largest employer in the cateogry. The second largest is computer services.(Clinton:1997) The fastest increasing sub-sub sector is the temporary help industry (largely clerical), which grew 11 percent annually from 1979 to 1995, five times more quickly than all other non-farm employment.(Autor:2000)
Contractors are gradually displacing their traditional direct-employment counterparts in some of the biggest sectors of the labor market. One-fifth of all wage and salary workers in the US are in administrative support jobs like clerical work.(Clinton:1997) This change is enough to transform the entire female low-skill labor market. Out-sourcing has already transformed the maintenance (janitorial) sector; by the late 1990's, 90% of all maintenance workers were subcontractors.(Clinton:1997) If this trend continues to diffuse, the effects could be enormous. If out-sourcing has different effects on compensation for low-skill and high-skill workers, there could also be an effect on overall inequality. Between 1973 and 2000 the average real income of the bottom 90% of Americans fell by 7% while the capital gains for the top one percent rose by 148%. (Piketty) Organizational theories suggest that high skill jobs might be outsourced to tap into intellectual economies of scale or because of variable demand for flexible services. If the factors that drive low skill outsourcing are different; namely if they are savings firms compensation costs, outsourcing could widen the ever-growing gap between high and low skill labor.
If organizations are increasingly hiring workers indirectly, it must somehow be more efficient to outsource. However, it is uncertain that these gains come from saved compensation costs. While this is intuitively the most logical motivation, there is surprisingly little evidence that indirect hires earn less than direct hires of comparable ability and more surprising, there is also little evidence that firms realize any economic gain beyond maintaining a flexible labor force.
John Benson's qualitative analysis of four Australian manufacturing firms suggested that in the long run, outsourcing does realize returns through reducing compensation costs. However, this does not cause any harm to the indirect employees who maintain the same earnings. All saving must therefore come from the firm being able to adjust the workforce as the workload fluctuates.(Benson:1999) In contrast, Young and MacNeil (studying two food processing companies) found that out-sourcing can incur unexpected management costs, negating any negligible benefit from cutting compensation(Young:2000)
Broader data anlyses also find mixed evidence. Analyzing the BLS's Industry Wages Survey, Abraham and Taylor found that higher wage companies were more likely to outsource their janitorial, machine maintenance, engineering, and accounting services.(Abraham:1996) Similarly, Gramm and Schnell found that organizations with higher wages are more likely to contract out.(Gramm:2001) In contrast Deavers found evidence that they are not. (Deavers:1997) Deavers argued that technological change, a focus on organizational competencies, and labor flexibility are the primary motives for outsourcing. Howeveeer Deavers provided little empirical evidence to support the claim. Davis and Uzzi also failed to find evidence that externalizing the labor force save organizations on health insurance, pensions, unemployment insurance, and supervisory costs, or wages.(Davis:1993) Mayal and Nelson found contradictory evidence; in their sample of 882 firms, they found that firms with higher benefit levels were much more likely to outsource to avoid these costs.(Mayall:1982) Mangum, Mayall, and Nelson found that firms use temporary and contingent workers to save money on the higher compensation associated with full time long-term employees. (Mangum:1985) Some of the variation in these findings is related to the different methods; some of these studies are qualitative, some estimate benefit levels based on the firm's interpolated likely payout. Most of these studies are cross sectional, and all focus on one of the two important building blocks of the argument: 1) does the firm outsource in response to economic pressure and 2) does the outsourced worker receive less compensation.
The problem with these studies is that they need to prove both to come close to proving that firms outsource to save money on compensation, and in the process, harm the same workers they would have otherwise hired directly. For the firm that is indifferent between indirectly and directly hiring a worker, the cost of an indirect employee (oversight search, and compensation) should equal the cost of a direct hire (compensation, search and oversight costs, and down time.) As such the firm could have incentives to outsource without reducing compensation. Similarly, compensation could decline for the outsourced worker without benefit to the employer, (for example if outsourcing increases the employer's search, training, or oversight costs, or if worker quality declines.) However, the literature generally assumes that if we can find evidence that firms with higher wages and benefits are more likely to outsource, the worker is worse off. Similarly, they attempt to show that if indirect employee receives less compensation than the direct employee, the workers are the same workers. Further, most of the literature focuses on a case study of one or two firms, failing to make generalizable arguments.
The spatial diffusion of outsourcing is also important in telling the story of workers' welfare. The firm might be more likely to outsource if contractors areavailable locally. Local contractors could reduce the firm's search cost and reduce the cost of oversight and implementation. There is a literature examining the role of space in the diffusion of various organizational practices. Knoke used a pairwise distance measure to predict the spread of municipal reform, while Hedstrom (1994) examined the spread of unions and Nyers found that riots are more likely to happen in locations close to where other riots have occurred. While many studies find that economic and social practices diffuse in a spatially correlated manner, it is difficult to measure the cause, since spatial correlation in the social world can happen through many different mechanisms from social networks to marketplace pressures.(Soule) Space isimportant from the worker's side as well. As a sector's employment moves to indirect employment in an individual's labor market, that individual is more likely to be indirectly employed. Thus, if outsourcing diffuses through the Northeast first, or through the suburbs, the Northeastern or suburban worker might suffer lower compensation or enjoy shorter unemployment spells or more flexibility. 1
Firms would not indirectly employ clerical workers if it were not in their economic interest to do so. The 1997 National Organizational Survey (NOS), a survey of 1,002 work establishments from June 1996 to June 1997 asked firms about their outsourcing practices for maintenance, clerical, accounting, IT, and core production. It used a stratified random sample from 15 million work establishments. (It was stratified based on organization size since the majority of work organizations are small.)
In the 1997 NOS sample outsourcing low-skill labor was common, but constituted a small proportion of all clerical employment in the sample. Fifty-one percent of the surveyed organizations had employees that were not on the payroll (primarily through subcontractors or temp agencies.) However, on average only .42% of the organizations' jobs were outsourced. At most,organizations contracted between 1 and 15% of their jobs. The organizations were most likely to contract out clerical (16% of organizations), accounting (12% of organizations), and IT positions (13% of organizations.) These results mirror the BLS samples reported by Clinton and Abraham, who found that more than one half of businesses contract for one or more types of business or administrative support.(Abraham:1988, Abraham:1990, Clinton:1997) Seventy-one percent of the sample (710 of 1,002) was for-profit organizations, 20% was public non-profit, and 8% was private non-profit. The median organization had between 100 and 150 employees, 87% offered health insurance, 26% had unions in the workplace, and about half believed their industry was "competitive."
The survey asked managers why they outsourced and found that the overwhelming reasons cited wither variable work demand and contractors special skills. Lowering costs was not a common response, and interestingly, few firms claimed they did it because others do. Obviously, these categories are artificial separations. A firm could not outsource if other firms were not doing the same. Similarly variable work demand and simplifying administrative tasks, and recruitment strategy are all just various forms of lowering costs-down times costs, low productivity costs, and search costs. These answers are likely to have errors since the individual answering this question was generally not the individual who chose whether or not to outsource a given position and respondents probably preferred not to answer positively to unappealing responses like "willing to do unsavory tasks" even if this was the case.
In the NOS dataset outsourcing was common among organizations, but not among employees.
First I ran two logistic regressions examining why firms outsource clerical janitorial positions.
where:
For-profit organizations are much more likely to outsource clerical positions than are larger firms. Those organizations offering medical benefits are less likely to outsource clerical positions. Health care benefits might be highly correlated with "high skill industries" since these industries are both more likely to offer benefits and less likely to contract out. But even that explanation is suspect; more likely there is so little variance in offering benefits that the results are unreliable. It would be preferable to measure the percent of employees actually receiving benefits. Organizations with seasonal work are more likely to outsource and oddly, those organizations that perceive themselves to be in a "competitive" environment are less likely to outsource.
The strongest results are those for "for-profit" status and the size of the organization. Both of these results are intuitive. Smaller organizations have less flexibility both in terms of revenue and in terms of shifting staff within the organization. Thus small firms must outsource to maintain flexibility that larger firms inherently have. There are multiple hypotheses why non-profits might be less likely to outsource-one of which is that they face a less economically competitive environment. Other reasons cross into the organizational theory literature.2
Table 3: Outsourcing clerical positions: logit
For this analysis I generated some continuous maps displaying the number of firms offering contract services in four cities across the U.S. by zipcode. I also conducted analyses, not displayed here, for the total revenue, payroll, and the number of employees in these firms. The firms are all firms falling under the NAICS code 56, Administrative and Support and Waste Management and Remediation Services. The majority of this category includes those firms I am interested in: administrative support, temporary help, employee leasing, etc. However, it also includes collection agencies, travel agencies, security services, janitorial and landscaping services, and waste management services. Thus it is a very imperfect measure of contractor accessibility. Some of these categories, like waste management services, vary almost entirely by population. However, others, like locksmiths, might distort the data. However, without access to the business census microdata, spatial analyses of firms are limited.
Kriging is an interpolation method that originated in mining. It is similar to linear regression, however the weights (coefficients) are re-calculated for each interpolated point and the variance-covariance matrix is based not on the covariance of variables, but on the variance of the single outcome variable based on the distance between all pairs of points. In essence it is a weighted average. (Wackernagel)
where d is distance and z is the variable being interpolated.
the weights are calculated by experimentally finding the variance between two points as a function of distance. For a single interpolated point,i, the weights on each observed point are calculated as the betas are calculated in an OLS regression, based on the variance-covariance matrix. The formula for variance is generated by calculating for every pair of points in the data set the distance between them on the x axis and the difference in their values on the y axis. The functional form of variance is often exponential or gaussian, declining as the distance between two points increases. V0 is the vector for the variance between the observed points and the interpolated point.
CoKriging, the second method used here, is simply an extension of kriging, drawing in another spatially correlated variable that is correlated with the variable of interest. 3 In my case, coKriging is an important extension because I used zipcodes. Incorporating population does not "control" for population in the traditional statistical sense. Rather, it uses data on the covariance between population and the number of firms to create a better estimate of the number of firms. While firm level data is by zip code, population level data is by "place." This is an important adjustment in my analysis because in dense cities the organizational correlation is distorted. For example, imagine that I am kriging the number of organizations in New York City. My dataset is bounded by Northern New Jersey, Westchester County, and Farfield County, Connecticut. In Manhattan some zip codes include only one building. Depending on that buildings zoning, it might have 50 or 100 firms in it. The adjacent building might be zoned residential and have zero firms. Thus, in the densest part of the city I will have a lot of variability -- thus I will show a lot of variability between those points that are close together. The pairs of data that are furthest apart are those on the periphery of my data set. Given the geographic distribution of organizations, the majority of these zip codes have zero organizations in them. Thus, the zip codes furthest apart are the most similar. Incorporating population takes care of this problem because it is positively correlated with the number of organizations. Thus, cokriging gives me more realistic contour maps. All analyses here used a ln transform for kriging and then transformed back for presentation. I used a ln transform because the data was heavily skewed to the right.4
It should be noted that none of this analysis is inferential. I will incorporate spatial regression and inferential statistics in future analyses. These maps are simple descriptions of the supply side of the market. The mean zip code has 12 contractors. However, establishments are strongly skewed with the median zip code only having 4 establishments and the maximum being 459 (in New York City.)In 1997 the average zip code had between 20 and 49 employees in the clerical services and waste management category. Again, this measure is skewed with some zip codes having up to 50,000 employees in the sector.
These maps show first, that the number of contractors varies by population within the city, as expected. In addition, we notice that Detroit has an overall higher number of contractors than do the other three cities -- but it also has a larger population (and CoKriging did not "control" for population. Initially Houston seems to have many contractors, but given its large MSA population, we see that we might rank the cities in terms of contractor availability as Houston, Seattle, and then Detroit and Tampa. Second, it is clear that these organizations prefer to settle in satellite cities like Bellevue and Palm Harbor rather than in the central business district. Results were consistent kriging with the number of employees, total revenue, and total payroll, using ordinary kriging, and kriging by square mile. Results were inconsistent when I kriged New York City since New York is so densely populated and zoned mixed business residential, that there was no spatial correlation and a much higher mean number of organizations.
Suburban clustering could have two effects. First, it might mean that firms in the satellite cities are more likely to use contractors. Second, it might mean that suburban workers are more likely to work at these firms. The location of the contracting organization is where the worker interviews and perhaps trains for their placement, but is not actually where they work. Thus, the worker only needs to go to the firm at the beginning of their employment spell.
Much of the spatial mismatch literature is plagued by the problem that workers simultaneously choose their place of employment and their workplace, an analysis of indirect employees avoids this problem. Who moves to a new city to be closer to a temp job? Thus, we can be certain that if there is an association between the existence of contractors in an employee's area and an employee's status as an indirect employee, the proximity of the employers is causally related to their indirect employment status.
The dispersion of outsourcing and organizations' motivations for outsourcing do not tell us anything about how these changes are affecting the outsourced worker. There are multiple positive and negative outcomes that could result from the geographic and organizational shifts. Workers in areas with indirect employment opportunities might suffer shorter unemployment spells as contractors decrease search costs and increase the match rate between firms and employees. Workers in these areas might enjoy more flexibility, choosing to work fewer weeks or hours. Indirect hires might be paid less assuming that they are willing to pay for more flexibility or if they are lower quality workers. The firms might pay more in compensation and contractors' fees if they value flexibility or lower search costs; but they will pay less if the workers are lower quality. While this preliminary analysis will not be able to parse out the reasons behind pay differential -- it will examine whether or not there is a pay differential between indirect and direct employees. Insofar as the total demand for clerical workers is not increasing, any pay differential could suggest an overall shift over time in clerical pay -- and a regional shift in clerical pay where outsourcing practices have diffused.
For this final analysis I analyzed compensation differentials for direct and indirectly employed clerical workers. I pooled data from the 1994 through 2000 March Current Population Survey (CPS), randomly selecting one observation for each respondent, and using a sub-sample of women reporting paid employment as secretaries, typists, receptionists, administrative support, and data entry workers. I split this data into two groups, those reporting that their employer's industry as a personnel supply services company, management services, or miscellaneous business services, and those reporting working in some other industry. There is some automatic error in this classification scheme since even the contractors do hire some direct clerical workers. The final sample of clerical workers under 30 included 710 indirectly employed clerical workers and 20,409 directly employed workers. Because I was unable to control for labor force attachment beyond hours and weeks worked, some analyses focus on female clerical workers under 30. All numbers presented are unweighted because of the narrow population of interest; however results using CPS person weights were almost identical. All wages are in 2000 dollars using the CPI (not the Urban CPI)
Indirectly and directly employed clerical workers are not identical groups -- in fact indirect hires seem to be a higher skill group. Indirect hires have significantly more education, are about three years younger(36 versus 39), and are much more likely to be single. Indirect hires are more urban; only 10% of the indirect hires live in non-metro areas and 32% are in central cities while 20% of the direct hires live in non-metro areas and only 23% are in the central city. Indirect hires are also about twice as likely to be black than direct hires.5
Education of female clerical workers, all ages
Marital status of female clerical workers, all ages
On average, the indirect employee earns less compensation than does the direct employee and is less likely to receive a pension.
One possible reason for the difference in compensation is labor force attachment. Since this is cross sectional data, I was unable to analyze how long teach type of worker remains in the clerical sector, though it is likely that the contracted worker might be more transient worker, moving onto some other career. Thus, they might accept lower wages for a short term position, just as a new college graduate accepts a position as an assistant in a prestigious firm, ultimately moving up to a better position in the firm. Without the promise of higher future wages, the career clerical worker may expect higher wages in the current period.
While I was not able to examine long term attachment to clerical work, I was able to determine their short-term labor force attachment. Indirectly employed clerical workers fewer weeks per year but the same number of hours per week. This seems to suggest that indirect employees are under-employed, not voluntarily seeking more flexibility. The contract worker who is pursuing a secondary career as an artist or the part time mother likely wants fewer hours regularly, to cover bills and to have free time with their children or secondary career. Intermittent full time work seems to speak to an under-employment problem.
Labor force attachment for clerical workers under 30 years old
Of course indirect and direct clerical employees are not exactly the same populations. Controlling for their individual characteristics indirect employment might not influence their compensation. Because of the minimal labor-force attachment data in the CPS, I ran a simple OLS regression on the subset of female clerical workers under thirty years old.Initial results suggest that indirect clerical workers earn lower hourly wages. However, using cross-sectional data, this data is no more than suggestive of some possible inequality.
Female Clerical Workers under 30, Compensated less? OLS: R-square = .11 Logit: LogLikelhiood is significant with a test statistics of 1348 and a pseudo R-square of .20. 6
These regressions show that, at least in a cross sectional analysis, young female clerical contractors earn 87 cents less per hour than their direct counterparts (or about 9 percent less.)They are also about a third as likely to be offered a pension, though the average worker in both groups is unlikely to be offered a pension. A similar analysis of all female clerical workers (relaxing the age constraint) shows that the indirect employee earns $1.80 less per hour than the direct employee and that the indirect employee only has 11% the chance of getting a pension as does the direct employee. Compared to other important factors, like firm size, education, and rural/ urban markets, only urbanity is more important than contractors status in determining compensation.
Of course, these results are far from conclusive. The CPS offers insufficient control for worker quality and labor force attachment. A future analyses should use panel data, controlling for the worker's work history, their tenure in the clerical workforce. Further, it would be interesting to consider a hazard analysis of indirect workers, testing the chance that they move to direct employment. Perhaps the indirect labor market works as a matching mechanism, bringing inexperienced workers into the marketplace and eventually matching them with firms. If this is the case, the worker accepts lower compensation in exchange for an ultimate placement with a direct placement.
Based on this cross sectional analysis, I cannot answer any of the above hypotheses. However, given the longitudinal trend in the out-sourcing market, these "positive-spin" stories seem unlikely. Assuming that there has not been a dramatic increase in the overall demand for secretaries (and given the innovations in office computing -- it seems likely that there might even be a decline)there has been a dramatic shift among secretaries towards indirect employment. Did clerical workers always have an unsatiated demand for a more flexible work schedule? for job matching? Rather, it seems more likely that the firm faces pressure to minimize variance among its office employees, particularly in fringe benefits. Hiring a worker indirectly can relax these pressures, allowing the firm to compensate low skill workers less, without facing legal, bureaucratic, and social pressures to offer less dispersed compensation.
This analysis also fails to consider the spatial mismatch question posed in the second part of this paper. Indirect employees are more likely to live in urban areas with the indirect employers located in satellite cities. Does the relative location of the firm and the contracted worker influence the worker's decision to work at that firm? This question can be pursued using two sorts of data sets, the first is a set of workers linked with their firms, with addresses of both. Also, given a data set including workers' addresses I could use the data used in section two to krig a "job acessibility" measure for each worker. This measure would simply be the spatially weighted average of the number of indirect employers divided by total employers in the worker's commuting area. The spatial weight would use a decay function, giving extra weight to those organizations closest to the worker and less weight to those at the perimeter of their commuting area (say forty miles out.) Access to both types of data is limited because of the confidential address data. I am currently working on two applications for access to data, one to the RDC to use the LEHD to work with firm-worker data. The second application is to work with my advisor's datasets on Chicago Health. (This survey includes extensive information about employment in Chicago and includes addresses.)
This analysis raises more questions than it answers. These questions fall in three overlapping categories: wage inequality, space and the labor market, and organizational practices diffusion.
The first set of questions this analysis creates relate to inequality. Clearly, we found that outsourced workers are compensated less. It is uncertain whether or not this differential is due to workers desiring flexibility or to some sort of selection. However, it seems likely that outsourcing release the firm from bureaucratic and social pressures to offer low-skill workers higher compensation, no par with their other workers. If this is the case, what does the trend toward outsourcing mean for low skill laborers? As more firms outsource will low skill laborers be less likely to earn a living wage, receive pensions and health coverage? If the answer to these questions is "yes" there are clear public policy concerns related to retirement benefits and public health insurance.
The second set of questions motivated by this analysis relate to space. First, why do contractors locate in satellite cities? Is it the lower overhead costs? Are they closer to the firms they contract with? To the workers they employ? Do these firms' locational decisions contribute to the spatial mismatch? If workers find permanent positions through these firms, will inner city workers be unable to take advantage of these firms because they interview and train in satellite cities? Are suburban firms more likely to outsource because they are closer to contractors? If inner city firms are less likely to hire contractors, this might negate the spatial mismatch problem.
All of the above questions rely on the question of how this organizational practice diffuses. Has it already diffused in the clerical market? If not, what is its equilibrium point? What types of organizations will maintain direct hires and where will they be located? If this phenomena has not played itself out, what will the low skill labor market look like when it does?
In this analysis we established that for profit organizations and organizations with variable workflows are significantly more likely to outsource their clerical workers, contracted clerical workers earn significantly less than their direct hire counterparts, and that the contractors are located disproportionately in satellite cities. Given the strong longitudinal trend towards outsourcing, it seems unlikely that the compensation differentials are entirely explained by workers' demands for flexibility. Furthermore, if the firm also desires flexibility, why does the contractors' fee primarily come out of the worker's paycheck and not from the firm? In conclusion, this analysis seems to provide suggestive evidence that this trend towards outsourcing low skill jobs like janitorial and clerical services might exacerbate the trend towards growing inequality.
I still have not attacked the research question that motivated this project, an analysis of whether or not the density of contractors encourages firms to outsource. (In other words, is there a spatial covariance between the decision to outsource and the number of contractors? The predictor of interest would be the percent of business in the region that is business services. Spatially weighted (by zip) economic controls will include the average wage rate, average unemployment, average education, etc. Individual level predictors will include classic controls like education, work experience, etc.
There might be positive outcomes associated with more employment through business services organizations. The most likely positive outcome is a more fluid labor market. This might be related to shorter unemployment spells for clerical workers. I think I'll probably propose a hazard model testing whether or not individuals in area with more outsourcing are more likley to have shorter unemployment spells.
1 The question of economic gains from lower benefits is more complicated than that of wages because it encompasses questions of tax law. US Code Title 26, subtitle A, Chapter1, Subchapter D, PartI, Subpart A, Section 401 a(4) states, "if the contributions or benefits provided under the plan do not discriminate in favor of highly compensated employees (within the meaning of section414(q)). For the purposes of this paragraph, there shall be excluded from consideration employees described in section 410(b)(3)(A) and (C)."
This section of the US code says that if an organization offers benefits to the only the top 20% of its paid employees, or only to those employees paid more than $50,000, their expenditures on pensions, health insurance, and life insurance will be taxed 25%. This law prevents discrimination in favor of the high skill worker but not against the low-skill worker. At the same time -- the threat of additional taxes combined with the administrative costs of managing multiple plans and economies of scale in bargaining for single plans, might encourage firms to provide consistent benefits to all employees. As such, it is easy to imagine that a firm might realize gains in benefit costs by directly employ their workers whom they wish to compensate with one level of benefits, and out-sourcing those who they do not wish to compensate with comparable benefits.

Traditional models of international assets typically assume that countries and currency areas are identical in size. The major contribution of the paper that I am going to discuss demonstrates analytically and empirically the implications of relaxing this assumption. The primary finding is that the differences in sizes of economies have important implications for international asset returns. More specifically, the author's model predicts that larger countries' bonds must pay lower excess returns in equilibrium. This is because the bonds of larger countries provide better insurance against shocks that affect a larger fraction of the world economy. The author interprets the results of his model to declare that uncovered interest parity (UIP) fails unless countries are identical in size. He conducts a set of regressions to test whether his model's predictions are empirically significant. I describe below the author's motivation, theoretical development, empirical strategy, and his results while providing a critical assessment of each of these parts at the same time. I focus in this paper on the intuition of the author's model and its pros and cons as well as providing my assessment of his empirical strategy.
The author's research question is motivated by some recent empirical findings that document the existence of significantly better hedging properties of large country assets. For instance, he cites: 1) Campbell, de Medeiros, and Viceira (2007), who find the Euro and the Dollar to be better hedges against the risk faced by a global equity investor than other currencies; and 2) Lustig and Verdelhan (2007), who suggest that portfolios of bonds denominated in low interest-rate currencies tend to be good hedges against US consumption risk. These citations do not necessarily justify the expectation that it is primarily the sizes of the economies which lead to these empirical regularities. One could possibly argue that there is an unobserved effect that is correlated both with country size and low interest rates, or some other omitted factor. To preempt such an argument and to further motivate his question, the author runs a couple of regressions with time fixed effects (although one would also like to see individual effects included) to show that there is a strong negative correlation between a country's share in total OECD output and interest rates. While this first pass at the data propels us to care about country size, the author's interpretation of these regressions as a significant departure from UIP is premature, mainly because the simple empirical approach here is unable to capture the assumptions of a UIP framework. Another deficiency with the motivation of the research question and the "descriptive" regressions is that none of the model's key arguments are communicated at this point to describe why we would see such a negative correlation between country size and interest rates. The motivation seems to arise solely due to an empirical observation for interest rates but with no theoretical inquiry about why earlier models have not cared about country size and we now should. As a result, the author cannot motivate well enough why we should care about this question theoretically as well as trying to explain the data better.
The author carries out two main exercises to study the effects of differences in country size on international asset returns. The first and major model of the paper is a standard Lucas-tree endowment economy with complete asset markets which includes non-tradable goods. In this respect, it is very much in the spirit of Backus and Smith (1993), Tesar (1993), and Stockman and Tesar (1995) -- the author only relaxes the assumption of identical country sizes and derives the implications for asset returns under this case. The second exercise is introducing monetary shocks to the previous model. The author departs from the complete asset markets assumption in this part to establish the desired relationship between country size and interest rates under a weaker set of conditions.
The introduction of non-traded goods in consumption has the primary role of allowing the consumer price index to differ across countries. Since the real exchange rate is defined as the ratio of two countries' consumption price indices, shocks to a country's endowment of the non-traded good is the driver behind the fluctuations in its exchange rate with the other country. The author skillfully makes use of this regularity to demonstrate how stochastic shocks to a country's endowment of the traded and non-traded goods affect its asset prices. More specifically, the author solves the competitive equilibrium problem for a representative household, where, taking prices as given, the household solves1:
where Q(ω) is the first period price of a state-contingent security that pays one unit of the traded good if state ω occurs in the second period, and W1(i) is the NPV of household i's endowments (net of transfers). Notice that the household exhibits constant relative risk aversion (CRRA) utility. The consumption and price indices are given by:
Before discussing how the endowment of non-tradable goods affects λT and how the author demonstrates the implications of country size differences, a few remarks are in place. After solving for the Euler equation of the household, the author presents Lemma 1 of his paper which relates the difference in the (log) expected returns between two arbitrary assets to the difference between the covariances of the payouts of these assets with λT. The lemma basically says that households prefer assets that pay off high whenever additional traded goods are sorely needed (i.e. marginal utility tradables consumption is high), which implies that the asset that has the higher covariance with λT must pay a lower return in equilibrium.
Appendix A shows that the return to an asset is found by summing up the prices of state-contingent securities from the households' FOCs, and the final result is found by using statistical identities that make use of the assumption that the payouts of the assets are log-normally distributed. While there is a nice intuition that arises out of this lemma, the author actually possesses the required findings to solve for asset return differentials directly from his model. He is able to do this since he solves for the marginal utility from tradable goods in the planner's problem, the equation for which is given in (14) on p.12. Then plugging this in to (10), the equation for the price of a state-contingent asset, he can solve for
With the exact same setup, Backus and Smith (1993) solve for the interest rate differential between two countries in Example 1 of their paper (p.306). This example not only demonstrates how to derive the interest rate differential directly from the model, but also shows that the relationship between the interest rate differential and non-tradable endowments crucially depends on the assumptions on functional form. Backus and Smith (1993) assume that α = 0 and γ = 0, thereby allowing for additive separability.3 Correspondingly, the state utility function form is the Cobb-Douglas case. Their only assumption on the planner's weights is that
The second primary identity that the author establishes shows why we should care about differences in country size. This identity is the closed-form solution for the marginal utility from tradable goods, λT, from the planner's problem. The planner's problem is cast as: max
How does this condition compare to earlier findings in the literature? In her paper focusing on a two-country version of the complete markets economy model with non-tradable goods, Tesar (1993) notes the importance of the assumptions on the functional form of utility, just like Backus and Smith (1993) demonstrated the implications of different forms. She shows that if utility is separable, then the realization of the endowments of the non-traded good does not affect the optimal allocation of the tradable good. If utility is not separable between traded and non-traded goods, however, the allocations of the traded good will depend on the endowment of the non-traded good. This is the case that the author implicitly assumes throughout his paper. Tesar (1993) shows (equation (6) in her paper) that consumption of the traded good in the home country is an increasing function of the home country's endowment of the non-traded good when the sign of the cross-derivative of utility is greater than zero. That is, if this cross-derivative is positive, the home country wishes to consume more of the traded good when its endowment of the non-traded good is large, because its marginal utility from tradable goods will be large. Now, contrast this with the author's findings above. While Tesar (1993) relies on a sufficiently high degree of complementarity between the tradable and the non-tradable goods, the author requires a high enough substitutability between the two. Indeed, focusing on the author's equilibrium levels of the consumption of the tradable good in the home country and λT, we see that a higher level of a country's endowment of the non-traded good means a lower level of its consumption of the tradable good and a lower value of λT, under Condition 1. To see the sharp contrast between the two papers more explicitly, note that under the identical isoelastic preferences and CRRA utility setup, the requirement on the parameters for Tesar's (1993) results is
It is thus highly important for the author to address the importance of the functional form of utility and the assumptions on the parameters of the model. Going back to Example 1 of Backus and Smith (1993), we see that the assumptions α = 0 and γ = 0, which allow for separable utility, do not agree with Condition 1 of the author, but with Tesar's (1993) condition. In the case of α = 0, the direction of the inequality will be determined by whether γ>1 or not. If γ>1, as the author assumes, then Tesar's (1993) condition is no longer satisfied.
After solving for the important variables in the model, the author solves for the prices of the traded and non-traded goods by calculating the Lagrange multipliers associated with the planner's problem, which I do not report here. The implications on the prices actually follow from the discussion about the relationship between consumption of tradables and the endowment of non-tradables from earlier on. Whenever the world average endowment of non-tradables is high, more tradables are delivered to the domestic economy, which diminishes the relative supply of the non-traded good within the country and hence makes it relatively more expensive. These implications are again dependent on Condition 1, which stand in contrast to the condition in Tesar (1993). The author finally presents the spread on international bonds, which depends on a country's price level since the country's risk-free bond pays in terms of the home consumption bundle. The intuition is well posed: when the domestic endowment of non-tradables is high, the home consumption bundle is relatively cheap and the ex-post payoff from the risk-free bond is relatively low. The author presents his Proposition 1:
, which directly reveals that there will be an interest rate differential unless countries are of the same size. The author argues that this points to the failure of the UIP, and this result is derived in a neat and analytical way under certain assumptions. We see this result independent of the fact that the equation above corrects for expected movements in the exchange rate, although it does not account for the presence of an exchange risk premium (I turn to this in greater detail below). However, the direction of the interest rate differential is again dependent on whether Condition 1 holds, which is contestable given the earlier literature and the indeterminant results surrounding what the value of γ should be. Taking this equation to the data might potentially return ambivalent results given that countries will differ in their respective degrees of risk aversion and elasticities of substitution. Indeed, some of the robustness checks that the author provides in his empirical results show no particular difference from zero in any direction.
The second exercise that the author carries out is to introduce monetary shocks to the model economy that is constructed previously. This extension is of secondary importance to the results of the paper, and seems to be constructed to support the analytical findings of the earlier model in a different setup. The value added from this extension, however, is to establish the point that Condition 1 is no longer needed to arrive at the author's conclusions. This is done by switching to an incomplete markets economy, where only a subset of households in each country is allowed to trade complete state-contingent assets in a Calvo-style formulation, and all households are subject to a cash in advance constraint. The results of the model will be very similar to the ones presented above, with the exception that home country assets will now be priced by the ratio of the marginal utility of "active" households, i.e. those household which can access complete markets.
The competitive equilibrium solution to the household's problem in the monetary extension case (not reported here) indicates that inflation at home country will lower the tradable and non-tradable consumption of the inactive households, thereby functioning as an inflation tax. After having established this result, the author solves the planner's problem in the case of incomplete markets for active households by adding the ratio of the active households in the population to the Lagrangian presented above. The new equilibrium solution for the marginal utility from tradables becomes:
, where the upper bar again denotes the world average. Whenever domestic inflation exceeds the world average, the non-traded good becomes relatively more abundant at home, leading to a decrease in its relative price and thus to a depreciation of the domestic currency in real terms. It follows from here that a larger country's risk-free bond is a better hedge against consumption risk, so it should pay lower returns in equilibrium. Notice that these conclusions are actually much stronger than the ones presented for the previous model. These results do not require any restrictions on the parameters of the model like Condition 1 does. The reason is that since inflation directly affects the amount of tradables available to active households, it must always lower marginal utility, giving the required condition to establish the author's results. Hence, he no longer needs additional conditions to be imposed on the model. This is surely a strong result, which comes at the cost of leaving the realm of complete markets, though.
The final piece of the author's theoretical work concerns the formation of a currency union. This additional step is not motivated theoretically, but from an empirical point of view to account for the fact that the bonds of most European countries started to be denominated in Euros from 1998 on. The author simply extends one of his earlier propositions to say that the formation of a currency union should lower the expected return on risk-free and nominal bonds as well as stock returns in the non-traded sector of all participating countries. This claim and earlier predictions about interest rate differentials are tested in the empirical part of the paper.
As noted before, instead of solving directly for the relationship between the interest rate differential and the stochastic discount factors, the author chooses to provide identities that are more amenable to econometric estimation. Specifically, the theoretical discussion of the paper produces four testable predictions: 1) bonds issued in the currencies of larger countries should pay lower expected returns; 2) the introduction of a currency union should lower expected returns on bonds within the union; 3) stocks in the non-traded sector of larger countries should pay lower expected returns than those of smaller countries; 4) the introduction of a currency union should lower expected returns on stocks in the non-traded sector of participating countries. The main interest is on the first prediction here, as it is the closest test of the UIP. The author uses quarterly data from OECD for the period 1980-2007 and presents his empirical results for several specifications and sets of right hand side controls. The main empirical findings support the author's predictions, although the econometric setup still has room for improvement. Instead of focusing on these minor econometric details, I will provide below a main discussion about the author's argument that UIP fails and argue why this implication is still immature. I describe below why his econometric strategy is not a complete test of his theory, thereby weakening his paper despite the strong theoretical results. I also provide a suggestion to the author that could either strengthen his case or tell him more about the interaction between country size and interest rate differentials once carried out.
Underlying the UIP concept is that under perfect capital mobility, interest-rate differences must be offset by expectations of exchange-rate movements. So for instance, the domestic interest rate can exceed the foreign interest rate only if the domestic currency is expected to depreciate at a rate equal to the interest-rate differential. This is in essence a no-arbitrage principle. The author argues that UIP fails unless the countries are of the same size, since larger countries tend to have lower risk-free interest rates. The theoretical derivation of this result is well-constructed and demonstrated (although still subject to my comments above and below). However, when it comes to estimating econometrically whether this effect is significant, the author reveals the weak side of his paper and does not seem to present convincingly whether a structural or reduced-form estimation method is relevant. Here are some of my concerns.
There are several reasons why the author's findings would come about even when capital is perfectly mobile within the framework of UIP. The first reason is that the rational expectations hypothesis might not apply in this context. A quick check for this would be to look at the serial correlation in the error terms of the regressions that the author runs. If serial correlation is detected, this could pose a serious problem to the assumption that the shocks that are realized this period are not correlated with any future shocks. Indeed, the variables under the author's focus, such as quarterly interest rates or exchange rates, are likely to follow time trends and the dataset that is available is essentially of a panel form. Given the time-series properties of these variables, it is highly likely that one will detect serial correlation in the error terms. Pooled OLS regressions that do not take such concerns into account as in this paper are likely to return inefficient estimates where standard errors are miscalculated5.
Another reason is the closely related issue of econometric implementation. The author notes (p. 23): "Since the model developed in this paper has only two time periods, I interpret the panel as a series of cross-sections and make the appropriate econometric adjustments." Firstly, it is not clear which econometric adjustments are carried out in order to be able to interpret the panel as a two-period system. The author uses quarterly data over the period 1980-2007 and compressing this relatively long panel into a two-period system has its drawbacks which are not mentioned. Second, rational expectations are imposed on the econometric estimation and the author rules out any concerns of autocorrelation to start with. Interpreting the panel as a two-period set of observations effectively prevents one from estimating HAC (heteroskedasticity and autocorrelation consistent) standard errors. Yet, testing for autocorrelation and correcting the standard errors for such a possible problem is what the author needs to argue that the interest rate differential comes from differences in country size as opposed to other possible factors. This brings me to my third point that this is not merely an econometric concern.
The third reason why the author's results might still be consistent with perfectly mobile capital in the UIP framework is the presence of exchange risk premia. While the author uses a constant relative risk aversion (CRRA) utility form to solve for household optimization, he bypasses the role that risk premia play in the determination of exchange rates, and correspondingly interest rate differentials. UIP holds under the assumption of no risk premia. If there exists exchange risk premia, then (part of) the cross-country difference in the interest rates can be attributed to the risk aversion of agents who want to be compensated for the exchange rate uncertainty they face. Hence, the author should be able to isolate the effect that differences in country size have on interest rate differentials on top of the portion that risk premia have in explaining these movements.
The current regressions included in the paper fail to account for the presence of risk premia. Hence, I would suggest the author to do a simple test of whether such premia exist. The author can easily carry out this test given the data set he has, and here is how to proceed6:
UIP implies that the interest differential between two countries equals the expected exchange rate change: i.e.
As a result, the lack of a theoretical argument about exchange rate uncertainty and exchange risk premia as well as an inconclusive econometric setup weaken the author's ambitious argument that UIP fails merely due to differences in country size. Testing for the presence of risk premia is relatively easy and certainly doable for the author. A more careful treatment of the data and econometric estimation can certainly help the author strengthen the implications of his theoretical model.
Overall, the author presents a well-written, clearly worked out, and a very intuitive piece of work especially in the theoretical parts of the paper. Results are demonstrated in a nice and clear way, and the implications of the model are well posed. However, the paper is weak on the side of highlighting its contribution and especially linking it to the previous literature on complete asset markets models with non-tradable goods. The author fails to identify how his results and theory are a step forward from previous papers, not providing any discussion about whether his results stand in contrast or complement earlier work on closely related topics. Moreover, the author does not check how functional forms or other parameter restrictions affect his model and only focuses on a single case. However, earlier literature suggests that these concerns are definitely in place. Another drawback of the paper is that the econometric estimation seems to be done only for expository purposes -- an econometric framework that aims to truly test the implications of the model is missing. More thorough work on the theoretical part that puts the author's findings in context with the literature and discusses its implications can be more telling than the regressions. However, improving the econometric framework and confirming the main model's predictions would also be a huge success.

There has been a large body of research regarding underreporting real outputs to the IRS to evade or to avoid corporate tax. There have been also large amount of research on the relationship between the overstatement of the reported output and management compensation. While we have plenty of works done for either of the falsifications, however, it is relatively in recent years that both falsifications are dealt as an agent's simultaneous decision problem under information asymmetry, or in principal-agent problem context.
Erickson, Hanlon, and Maydew (2002) examined the extent, if any, to which firms pay additional income taxes on allegedly fraudulent earnings. They found that firms are willing to sacrifice substantial cash to inflate their accounting earnings, which is not assumed in pure tax evasion problem. Desai and Dharmapala (2004) also analyzed the relationship between the compensation and tax sheltering, and concluded that the increases in incentive compensation tend to reduce the level of tax sheltering. Crocker and Slemrod (2003), adopting the optimal insurance model developed by Crocker and Morgan (1998), suggested the optimal contract for the CFO under information asymmetry between the CFO and shareholders and the possibility of falsification of taxable income report.
In many cases, both the overstatement of output to the market and understatement of taxable income to the IRS must be decided at the same time. In addition, and one incentive acts in the opposite direction to the other one. In this paper, adopting the implication of an embellishment under rational expectation from Kwon and Yeo (2005), I analyzed a relationship between the level of both reports -- tax and output -- under different marginal tax rate, intensity of compensation, and the level of output by calibration.
An agent (e.g. CEO) is risk-neutral and is assumed to have private information regarding the level of non-negative effort a and that of non-negative manipulation ma. The real output of the company y is determined by
(1)
where ε is an unobservable state noise such that E(ε) = 0.
After the real output is observed by the agent, she announces the reported output for her firm to the public. The announced output, which is public, is the sum of the extent of manipulation and real output as
Now, there is a risk-neutral principal (e.g. a representative shareholder) and he wants to make a contract with the agent. The distribution of the real output y is known to the principal, but he only knows the reported output. The support of y is assumed as
Finally, the company has to pay corporate income tax, which depends on the level of income. Note the reported income in this case is not necessarily the same as that released as output announcement. The company report different level of income yt = y + mt to IRS, and the corporate income tax rate t is determined as
where τ is the marginal corporate tax rate and r is a constant.
The timing of this model is as following: First, the agent chooses her level of effort a. Second, after the real output y is revealed, she decides the level of manipulation ma and mt. Third, the output is announced as ya, and separately report the income yt to the IRS, and, based on it, marginal tax rate t is determined. Fourth, stock return for the firm ys is determined by the market, based on the reported output ya minus the level of market conjecture on the level of manipulation μ and the income tax rate
The agent's expected utility is the following function of effort, manipulation, tax, and market-conjectured level of manipulation as:
(4)
Under risk neutral utility assumption we can rewrite this equation as
(5)
where c(a, ma, mt) means cost of effort and manipulation. We assume that c(a, ma, mt) can be separable into c(a), c(ma), c(mt),and c(ma-mt), which are convex (i.e. c' > 0 and c" > 0) and satisfy c'(0) = 0 and c'(∞) = ∞. These assumptions about cost functions of the agent imply that costs of putting effort or manipulation costs increase with respect to the size of overstatement. In addition, we assume that the wider the gap between the overstatement and the understatement is, the higher the cost is. For simplicity, we assume quadratic form of cost functions as
where km and kd are exogenous constants.
The principal's payoff Π(a, ma, mt | y) is based on the anticipated real output ys. Stock returns are also determined based on it. After the returns are determined, the agent gets paid proportional to them and shareholders will receive the rest of them as:
We solve any multi-stage game backward. At the end of the game, with the realized output y after choosing the optimal effort a*, the agent's problem is
In other words, the agent's problem at the final stage is to choose the optimal level of manipulation for the market and the IRS, respectively. The first order condition is
To simplify, let r = 0. It makes sense because the corporate tax rate is zero when the reported income level is the least. Now the equations (9') and (9") can be rewritten as
From Kwon and Yeo (2005), under rational expectation, ma* should be equal to μ*, and from the equation (2) and (3), we can derive dma = dya, and dmt = dyt. The equation (10') and (10") would be written as
Under assumption that the support of y is
With two unknown variables ma and mt, and two equations (14') and (14"), we can find the optimal levels of manipulations. Unfortunately, however, an analytical solution does not exist. I did calibration for different level of marginal tax rate τ, the strength of incentive β, and real output y to see the cross relationship between these factors and optimal levels of the understatement of taxable income and overstatement or output. Followings are three findings from the calibration.
First, the level of overstatement on output ma decreases with respect to marginal tax rate τ. Beyond one point of marginal tax rate, the announced output is even lower than the real output. On the contrary, the level of understatement of taxable income mt increases with respect to marginal tax rate.1 Second, the amount of falsification for both the understatement of taxable income and the overstatement of output gets higher with respect to the strength of incentive β. Third, as the real output y goes higher, the level of overstatement on output goes lower while the level of understatement on taxable income goes higher. Detailed results of calibration following are attached as tables and graphs.
This paper investigates the effects of the marginal tax rates and the strength of monetary incentives of CEOs, and the level of realized output on the two optimal levels of falsifications; the understatement of taxable income and overstatement of output in the context of principal-agent model. While the calibration results show very intuitive ones, several future works are worth pursuing. First, I need to investigate the effect of those factors on the optimal level of effort a, or the amount of labor to get better idea on the optimal progressivity of corporate income taxes. Second, stronger theoretical results are necessary. Even though analytical solution is not achievable, more rigorous calibration and comparative statics are needed to generalize the model. Third, detailed analysis of the calibration is necessary to derive policy implications. Especially, it is necessary to quantize the marginal changes in the level of falsification and that of effort with respect to the changes in the marginal tax rates, the strength of monetary incentives, and the true level of output, respectively. Finally, an empirical model to test the results from Desai and Dharmapala (2004) will be worthwhile. These future works would make this paper have more general and concrete results.

The primary concern of this paper is whether household consumption values constructed from wealth and income data in the Health and Retirement Study are accurate enough to be of use in estimating an Euler equation describing consumption in a model with uncertain lifetimes. This paper is a companion to Perry (2005) in which I actually attempt to estimate that equation.
People's beliefs about their own life-expectancy have not been extensively studied-mainly due to lack of data. It is not clear that people actually have consistent beliefs about their own chances of survival at any time. Even if they do, measuring them in a meaningful and convincing way is difficult.
The life-cycle hypothesis makes a simple prediction about the relationship between a person's perceived risk of death and their consumption: those who think they are less likely to die will have less consumption growth over time. Simply put, if you expect to live a long time, you will conserve your resources early in life in order to have enough later-this means earlier consumption will be lower than it would have been if you had thought your chances of survival were worse, ceteris paribus. In this way, a higher expected chance of survival should have the same effect as a higher interest rate or a lower degree of impatience.
The Health and Retirement Study (HRS) has elicited subjective life-expectation data from its respondents since the study's inception in 1992 (12 waves of the HRS have been completed-1992-2002, every two years). The questions are of the form "What is the percent chance that you will live to be 75 or more?" (the target age-75 in this case-can vary).
The HRS, however, does not elicit consumption data from respondents. Instead, it provides measurements of assets, income and capital gains. These can be used to deduce a consumption level for the time periods between survey interviews. This process leads to a large amount of measurement error, though, as the assets, income and capital gains are all measured with non-trivial measurement error to begin with.
While my primary goal is to study the relationship between consumption profiles and subjective survival beliefs, this study is a first step in which I test constructed HRS consumption data against three alternative models of consumption-specifically, models of the covariance structure of changes in log-consumption. I find that a model describing consumption as an individual-specific constant plus a time-varying random shock best describes the data. This can either be interpreted as showing that the consumption values have too much error to be of significant use for this application or as showing that that process describes real respondent consumption in this dataset. In either case, the result indicates that this dataset is probably not of much use in testing the primary relationship of interest.
In section one I describe the three models of consumption that I test the data against. In section two I describe the data and how I produced my values of consumption. Section three contains the results of fitting the data to the three models.
My method is to propose a model for the consumption data; use it to derive conditions on the covariance between the changes in log-consumption in different periods; and then use those conditions to fit the data to that model using generalized method of moments as described in the appendix of Abowd and Card (1989).
The first model I propose to test is a model in which an individual's log-consumption equals a person-specific constant plus a random shock that changes each period:
This implies that the change in log consumption between two periods is given by:
As stated in the introduction, this model can thought of as a model of pure measurement error, or alternatively as actually describing the process that describes real consumption. The only parameter value to be estimated is
The second model I test is a variation on the random-walk model of consumption proposed by Hall (1978). The variation is that I propose (for convenience) to test whether log-consumption follows a random walk. The proposed model is
I ignore the issue of whether there is an additional trend term as that will not be identified by the GMM approach I take. Equation (4) implies
Therefore, the covariance structure of changes in log consumption is:
Similar to the measurement error model, the only parameter to be estimated is
However, this model implies zero covariance for elements that will have non-zero covariance in the measurement error model.
The third model I test is based on the life-cycle implication stated in the introduction. The implication that consumption growth should rise with a rise in a person's mortality risk comes directly from the Euler equation of an agent maximizing the sum of additively separable utility over his lifetime (my formulation is borrowed from Kuehlwein, 1993):
Here, p is the probability of surviving to the next period, r is the interest rate and
is the rate of time-preference. Lowering p has the same effect as raising
or lowering r-it privileges current consumption over future consumption. In this way, because I have no measurement of
, the effect of survival expectations will not be separately identified from the effect of time-preference. I ignore the issue of the utility value of a bequest upon dying.
I assume a felicity function
, and take the logarithm of each side. Also, because my main concern is with life-span uncertainty, I assume that the income stream is known. This means that there should be no uncertainty about realized consumption in period t+1, given that the respondent survives to that period, so I dispense with the expectation operator:
Adding a term to account for measurement error in the change in log-consumption gives:
The only variables from (9) that I have measured variation in are consumption and subjective survival expectation. Therefore, I make the possibly unfounded assumptions that the difference between log rt and log
is distributed randomly in the population given log(pt), and that
is constant (or distributed randomly) throughout the population. This leaves the relationship that I examine:
This equation implies the covariance structure:
The GMM estimation of this model will fit values of both
, then this reduces to the random-walk model.
The HRS is a nationally representative panel study of persons over 50 in the United States. Beginning in 1992, respondents were interviewed every two years, covering health, finances, physical and mental capabilities, family structure and relationships and job history. A study called AHEAD (Assets and Health Dynamics of the Oldest Old) began in 1993 and focused on older respondents. In 1996, the AHEAD study merged with HRS. New cohorts were added to HRS in 1998 so that the survey would remain representative of those over 50. The last wave of data available for this analysis comes from interviews done in 2002. I employ all HRS waves, but I do not use AHEAD data that was taken prior to the merger with HRS.
Table 1 shows descriptive statistics of the population that I will use for this analysis. In each survey wave this population consists of all respondents who answered at least one subjective survival question in that wave and who were single throughout the time 1992-2002. P(75) and P(85) refer to the mean values of the probability responses to the subjective survival questions that ask about target ages of 75 and 85 respectively1. These statistics are meant simply to make it clear what the population of respondents is like in any particular year. They cannot be used to make accurate inferences about the evolution of households or singles in the HRS population over time because those respondents who have a valid answer to at least one subjective survival question are a highly non-random group. This is due to both self-selection (it takes a certain mental capacity to give a sensible answer to a probability question) and due to survey variation (exactly which sets of respondents have been asked which questions has varied over time in the HRS).
Each wave of the HRS contains detailed questions on household assets (both real and financial), household income (separate from capital gains), and capital gains. The survey does not contain any consistent measure of household consumption. In order to test the implication of survival expectations on consumption profiles, I use the HRS data on assets, income and capital gains to infer a measure of consumption for each respondent for each period between survey interviews.
The basis of the calculation is the relationship:
That is, consumption between two measured points in time equals whatever the household took in, in earned income and capital gains, minus the amount that their asset level grew during that period. It is ambiguous in the HRS whether respondents give pre-tax or post-tax income levels and so there is no way to account for income tax. I do, however subtract property taxes from inferred consumption.
First, I use the HRS income data to estimate household income over the period between survey interviews. I divide the study's constructed household income variable-which estimates total household income in the one-year period prior to the interview-by twelve to get an estimated monthly income and then multiply by the number of months between interviews. This procedure will add measurement error to the extent that actual household income during the period between interviews differs from income during the period just prior to the interview. Additionally, all financial variables in the HRS include imputed values which increase the level of measurement error, but also substantially increase the number of data points available. To exclude the imputed values from this analysis would entail dropping a majority of the available data since almost all respondents require imputation on at least some financial variables.
Second, I use the capital gains section of the survey to estimate capital gains between survey interviews. Respondents are asked whether they have put money in to or taken money out from their various assets. This information, combined with the asset values reported in the earlier and later waves, allows for inference of the respondent's capital gains over the period. This is straightforward except that housing capital gains are not well-measured for respondents who buy or sell a house during the period, so those respondents are dropped.
Finally, I calculate respondents' change in assets between the survey interviews by subtracting the later survey-interview household assets variable from the earlier survey-interview household assets variable. I do not include housing assets on the assumption that people-particularly retired people-do not generally monetize housing assets for the sake of consumption. Descriptively, this assumption is probably alright for the period 1992-2002, but may be less so now.
Adding income and capital gains and subtracting asset growth and property taxes and then deflating by the CPI-U yields the measure of consumption in 2002 dollars that is used to test the life-cycle prediction.
Using this strategy I have measures of consumption for the periods between the survey waves 1992 and 1994, 1994 and 1996, 1996 and 1998, 1998 and 2000 and 2000 and 2002. These five sets of consumption data can be used to calculate four cross-sections of log-consumption growth, the statistic of interest in the Euler equation. Table 2 shows summary statistics for the measures of consumption in 2002 dollars and log-consumption growth. Panel A shows consumption measured for all households consumption measured for households composed of singles. Panel B shows log-consumption growth measured for all households and for singles. The values in Panel A can be thought of as approximately 2-year levels of consumption for those households as that is approximately the time between survey interviews. Panel C, most relevant for this study, shows consumption and log-consumption growth for those singles who exist throughout the entire panel period and who have no negative consumption values-these are the respondents the model is estimated for. This group is different from the overall singles group in two major ways. First, consumption levels are much more steady through the period-perhaps because it is always the same group of people. Consumption for each approximately two-year period is always approximately $50-60K. Second, for both of the other groups (all households and all singles) consumption growth is negative for the first three periods and positive in the last. For this group of singles, consumption growth is negative in the first period and positive thereafter. I have no explanation for that difference.
Two elements of this table suggest large measurement error. First, in each year a large proportion of households have negative values for this measure of consumption-typically 11-15%. Because actual consumption cannot be negative, these cases are necessarily mis-measured. The proportion of negative cases is a lower bound on the proportion of mis-measured cases in each year. The large number of negative values also explains why the number of cases is significantly lower in Panel B and Panel C than Panel A-if there is a negative value in either the early period or the late period, then log-consumption growth cannot be measured.
Second is the fact that, for both the whole population and for singles, measured consumption drops substantially for the period 1998-2000 and then rises substantially for the period 2000-2002. This may be due to unreported capital gains appearing in the change in asset level. If a respondent had substantial capital gains in 1998-2000 (as many did), then did not report them as capital gains and did correctly report their total assets, this would result in measured consumption being biased downwards. The reverse is likely for the period 2000-2002. Poorly measured capital gains are probably not restricted to these time periods-they just show up strongly in these periods because asset values fluctuated substantially.
The HRS does provide a few variables that can be used to corroborate my deduced consumption values. In 1996 and 1998 the survey asked each household what their total spending-including all debt payments, utility bills, rent, transportation, entertainment, food, clothes and any other expenses-was in the previous month. Also, in 2002, the survey asked three food consumption questions: how much did the household spend in the past week on all food; how much did it spend having food delivered; and how much did it spend eating out. The left side of table 3 shows mean values for these measures and for deduced consumption values measured on a monthly basis for the same time period. The HRS survey levels of consumption are substantially lower in both cases. Also included are mean values of inferred consumption with negative values removed-this exacerbates the difference between the HRS measure and my measure. It is questionable how accurate a respondent is likely to be in making a fast estimate of monthly spending, so there is no guarantee that the HRS measure is very good. One possible, partial explanation for the large difference between the values reported by respondents and the calculated values is that I have not accounted for income tax. If respondents generally report pre-tax income, then my calculation will count their taxes as consumption. It seems likely that few respondents would include income tax in their response to the 1996 and 1998 HRS consumption question.
The right side of table 3 shows correlation coefficients and respective significance levels between the HRS measures and my inferred levels of consumption for the relevant time periods. For both HRS consumption measures, the correlation is substantially higher when the negative cases are removed from the deduced consumption numbers. This is unsurprising as those cases almost certainly represent particularly egregious cases of measurement error. Furthermore, it is encouraging that the inferred consumption shows such a high correlation with the HRS measures of consumption given the likely presence of substantial measurement error in both. Interestingly, of the food consumption measures in 2002, only the measurement of what a family spends eating at restaurants is significantly correlated with my inferred consumption measure. Also interesting, though I have not shown it, is that these measures of food consumption correlate very little with each other-again probably due to measurement error.
Table 3 shows that despite its weaknesses, my inference about household consumption do match up to a substantial degree with the limited information the HRS survey provides about actual household consumption.
In the Euler equation that provides the hypothesis tested, p represents the agent's subjective assessment of his probability of living to the next period. The HRS provides answers to questions of the form "What is the percent chance that you will live to be 75 or more?" These questions are asked twice in each survey wave with different target ages, although some respondents may only be asked once or not at all. From 1992 to 1998 respondents were asked the questions with 75 as a target age and then with 85 as a target age. In 2000 and 2002, the first question remains the same and the second question has a target age that varies from 80 to 100 in five year increments depending on the age of the respondent (the target for anyone under 70 was 80, for those 70-74 it was 85 and so on).
A response to one of these questions does not imply directly any particular value of the respondent's expected chance of living to any particular date other than the target age. In order to use the survey responses to calculate a value of p (in the life-cycle model the probability of living to the next period; in this analysis the probability of living through the next period of measured consumption) for each respondent, some assumptions are necessary. In the appendix, I describe an algorithm I use to produce values of p from the responses to the subjective survival questions in the survey.
The object I use to fit the three proposed models to the data is the covariance matrix of log-consumption growth. This is shown, along with the corresponding correlation matrix in table 4. Significance levels are shown for the correlation coefficients as well (a value of 1%, e.g., indicates that the correlation is very significant). As can be shown relatively easily, the pure measurement error model implies a correlation matrix with -1/2 on the elements one-removed from the diagonal in the correlation matrix and zero for the other off-diagonal elements. Prima facie, the correlation matrix of the data appears very close to that-with one-off diagonal elements close to -1/2 and highly significant and no other significant correlations.
The results of GMM estimation are presented in table 5. Results are shown both for optimal minimum-distance (OMD) estimation, using the inverse of the variance matrix of the vector of covariance elements as a weighting matrix, and for equal-weighted minimum distance (EWMD) estimation, using the identity matrix as a weighting matrix. In both cases, we cannot reject the measurement error model at standard significance levels and we can reject the other two models. Indeed, the random-walk model and the life-cycle model both fit the data very poorly-producing very high chi-square statistics. (Note in table 5, I use the term p-value to indicate the probability of observing a
There are two other results to note in table 5. First, the value of
that minimized the test statistic for the life-cycle model is zero. This reduces the life-cycle model to the random walk model-indeed the values for
are the same for each model. The models produce different
values in the EWMD case because although they minimize the same statistic, the test statistic for that minimization depends on the first derivative of the vector of covariance elements with respect to the parameter vector. In the life-cycle model this derivative depends on the covariances of p (the subjective survival probabilities). In the random walk model, this derivative is a vector of constants.
Second, OMD and EWMD produce very similar parameter values for the well-fitting model, but very different values for the poorly-fitting model. Abowd and Card note this issue and that is their justification for using both-it serves as another test of how well the model fits.
The primary implication of these results is that this data is unlikely to be of much use in estimating the life-cycle model that is really the object of interest. Because of the procedure used to produce the consumption data, I conjecture that the results of this study indicate a large amount of measurement error in the data, rather than a real consumption process for survey respondents.
As the main goal of this research program is to investigate whatever link may exist between people's stated survival beliefs and their decision-making, it will be necessary either to derive a testable result that does not rely on consumption data or to find a dataset that measures both survival expectations and consumption.

This research examines the intersection of race and gender in shaping opportunity to pursue careers in sciences, technology, engineering and mathematics (STEM) fields. The modest goals of gender and racial/ethnic parity have both been reached between the national undergraduate student population and the US population as a whole, but African Americans, Latinos, and American Indians remain underrepresented in graduate programs (See Fig. 1 and Fig. 2). Although the data show women now overrepresented in degree-granting institutions overall, aggregated data such as these mask both persistent disparities within particular disciplines in the academy, as well as the unique nature of inequality borne of intersecting dimensions of identity.
When one looks at the intersection of race, gender, and academic field, the underrepresentation story looks very different. In STEM fields -- and particularly within the disciplines of physics, chemistry, and engineering -- women of color are so underrepresented that many statistical compilations leave their data out entirely to protect their anonymity. There are sub-disciplines in which women of color have never earned a doctorate. One's equal access to careers in STEM fields neither amounts to nor ensures equality in those fields, but equality includes requires equitable access; thus, improving access is both a precondition to and part of the realization of deeper equality. The purposes of this research, then, are (1) to examine the ways that race and gender intersect to shape access to careers in STEM disciplines and (2) to consider the roles that two programs at the University of Michigan have in increasing this access -- the Undergraduate Research Opportunities Program (UROP) and Women in Science and Engineering (WISE).
Race is a socially constructed way of dividing the human population by phenotypes that has been politically reproduced since the colonial era. Race was created to justify social inequality, and became a marker of inequality, specifically, the subordination of those with darkerwho are not White skin to the domination of those who are Whiteith lighter skin. Race, or "the color line" (DuBois, 1903), has been the basis for both individual prejudice and governmentally sponsored subjugation, including denial of personhood, rights, citizenship, and countless other manifestations of freedom. DuBois called it "the color line," which is to say the separation of some from others on the basis of skin color.
Genders are the socially constructed meanings attached to the sexes, including the functional norms that dominate people's understanding of masculinity and femininity. In the last ten or so years, dialogue on gender has begun to open up to include not only traditionally female and male orientations toward self and the world, but also categories such as transgender (i.e., one's physical sex does not match gender identity/role), pangender (i.e., one cannot be labeled as female or male), and genderqueer (i.e., one takes a "both/and" approach to female and male) identities and roles. Like race, gender has been used as the basis for systematic domination of one portion of the population (in this case, males) over another (females).
From these definitions, it is clear that race and gender, like all socially constructed categories, are dynamic in meaning and laden with values. One's status -- and I believe it is fair to say, one's freedom -- is shaped by the salience of one's race, class, gender, national origin, sexuality, and religion in a particular temporal, geographic, and social context. More importantly that the ways that an individual's race, gender, and other dimensions of identity play out independently, however, are the ways that they intersect to shape experience on an individual level. Poor, uneducated Irish immigrants, for example, may initially have been excluded from the white racial category despite their light skin color and European origins, but within a few generations had assimilated the values of privilege and the discourses and behaviors of domination that permitted them to claim status within the hierarchy (Marable, 2000, p. 245).
The significance of the cell in which the female gender row and the African American race column intersect is in what it means for one's opportunities and experience not only to be Black and to be a woman (as separate dimensions of identity), but also to experience what statisticians would call the "interaction effects" that accrue from being a Black female. That is to say, The effect of identifying with both a subordinatedthe effect of these identity dimensions' intersectionrace and a subordinated gender is greater than the sum of the individual effects that identifying as Black or female has for one's experiences and opportunities. A theoretically robust description of the meaning of intersectionality for Black women is advanced by Patricia Hill Collinss (1998), who introduced the concept of intersectionality to modern social science thinking as a paradigm that can account for social reality's complexity (Collins, 1998).
If one's race is white or gender is male, inIntersectionality functions for some and in some contexts to counterbalance the oppression or discrimination one experienceds as a result of the otherone dimension of identity being subordinated (e.g., being female, but also being white). However, for or interseAfrican American women, intersectionality may functions as a double or "interlocking oppression" (Collins, 1998)., such as being African American and female.
In the context of higher education and the pursuit of scientific careers, to be a Black woman means that one is in a small minority whether or not gender or race is most salient. While African American men and women are both critically underrepresented in the sciences, (Pascarella, Wolniak, Pierson, & Flowers, 2004)Pascarella, Wolniak, Pierson, & Flowers (2004) found that gender was significantly more important in predicting graduate school aspirations for African American women than it was for Hispanic and White students at the end of their third year of college. Controlling for all other influences in their regression equation, African American women had just one tenth of the odds of planning to earn a graduate degree as their male peers.
Add to this structural disadvantage the history of Black women being viewed as objects, not agents, of knowledge (Collins, 1998, p. 97), and it becomes clear that it is not simply a matter of being in a minority among one's peers or colleagues in STEM fields, but of defying normative roles and resisting the force of history.
It is possible within the academy to think of one's discipline as a third dimension of identity with which race and gender intersect. Disciplines not only advance understanding of different types of knowledge, but they do so according to particular epistemologies and ontologies. In sociology, Patricia Hill Collins could advance a theory grounded in her experience as a Black woman due to the discipline's relatively expansive epistemology and recognition of multiple realities. Rhonda Dzakpasu, a Black female physicist at UM whom I know, is very unlikely to have opportunity to integrate her standpoint into the content of her scholarship because of the narrowness of physics' epistemology and emphasis on a single, measurable reality. Much like race and gender, then, disciplines present unique ways of being in, understanding, and valuing the world; thus, for scholars they have power to shape one's experience and view of the world.
Intersect best understood as an individual level theory than in explaining group experiences. Group analyses quickly become too complex to conceptualize, but "groups do not operate as individuals do" (Collins, 1998, p. 208); therefore, they are more difficult to study – another reason that intersectionality is best-suited for individual level analysis.
(Add to the identity matrix in higher education the axis of one's disciplinary identity -- means something different for Black women sociologists than chemical engineers)
It is possible within the academy to think of one's discipline as a third dimension of identity. Disciplines not only advance understanding of different types of knowledge, but they do so according to particular epistemologies, carry with them unique ontologies, and operate under disparate axiologies. In short, much like race and gender, disciplines present unique ways of being in, understanding, and valuing the world; thus, they have power to shape one's experience.
Taking individuals out of the state of nature and making them citizens of a civil society is the expressed purpose of the social contract, which was codified as a state sponsored arrangement through the Constitution. In this arrangement, citizens are inherently equal and inherently free, and the power of the government derives from their will. The individuals whom the Constitution tacitly empowers as free and equal citizens, however, are its authors and signatories: White, property-owning, males. While the gender and race-neutral language of "individuals" embedded in the Constitution gives the appearance of a gender and race-neutral social contract, "unwritten presuppositions about the proper role of women and the relations among the races limit the apparent universality of constitutional language" (Starr, 1992). This contradiction has had the effect of permitting de jure and de facto domination of women and people of color, such that "Modern contractual [domination] both denies and presupposes women's [and non-Whites'] freedom and could not operate without this presupposition" (Pateman, 1988).
Pateman and Mills argue that the creation of the social contract via the Constitution implicitly generated sexual and racial contracts that both formally excluded women and people of color from the social contract and established structures of domination. Mills quotes a 19th century French imperial theorist, who eloquently expresses this idea with regard to race: "The subject people are not and cannot become citizen in the democratic sense of the term...It is necessary, then, to accept as a point of departure the fact that there is a hierarchy of races and civilizations, and that we belong to the superior race and civilization" (quoted in Mills, 1997, p. 25). Add to the collectively imagined hierarchies of race and civilization a hierarchy of sex, and you have the grounds for white supremacy and patriarchy.
Under the sexual and racial contracts, systems of slavery are intrinsically justifiable and serve as logical (i.e., effective and efficient) mechanisms for maintaining patriarchal white supremacy. At this level, the significance of race and gender's intersection viz a viz the social contract becomes clear, for "The body itself...is the foundation for all other levels" (Mills, 1997) of domination. "Exactly what form subordination takes, to what use the body is put or what kind of access is granted, depends on whether a man or woman is constituted as a subordinate" (Pateman, 1988, p. 231). Clearly, black women have been "constituted as subordinates" and the nature of their bodily subjugation has historically been more dehumanizing than that suffered by white women or Black males. For example, female slaves endured physical, sexual, and psychological violation that no wife of a slaveholder would ever be forced to experience. Between the sexual contract's fundamental appropriation of the female body as property and the slave system's formal, lawful possession of Black person's body as property, the Black woman's body was doubly not her own. Even today, not only are Black women subjugated as women and subjugated as Black, but the two are mutually reinforcing. Denigrating a person on the basis of race permits an even deeper disrespect of her body as a woman.
Among scholars who have analyzed the underrepresentation of women and people of color (and, in a few instances, women of color) in science, technology, engineering, and math (STEM) disciplines, a clear distinction can be made between literature on the pipeline to careers in the sciences and critically-oriented literature that acknowledges the ways that women of color are systematically excluded and marginalized in the sciences. Research based on pipeline assumptions views the problem as one of inequitable access to resources and an inadequate supply of scholars from historically oppressed groups, whereas the small, critically-oriented literature views the problem as inherent in the pipeline itself and in the use of the pipeline metaphor. These latter scholars contend that the process of becoming a scientist was developed by and for white males, and that the power structure established in the sciences since the mid 19th century (i.e., in research universities) systematically excludes women and people of color and marginalizes even those who make it through the pipeline. This literature review will critically discuss extant literature on women of color in STEM fields, including an extended analysis of the role that undergraduate research programs play in providing aspiring scholars of color with early socialization to careers in the sciences.
Scholars have very rarely attended specifically to the underrepresentation and experiences of women of color in STEM fields (an assessment of the literature with which Clewell, 1991 and Orn, 2005 concur). Scholarship on female scholars of color in education and the social sciences is abundant in comparison, and from that body of work the entire theory of critical race feminism has emerged as an intellectual response to the experiences of double marginalization that women of color in the academy have endured. Indeed, it is from this work that intersectionality literature originated. In my review of the literature, however, only Orn Ong (2005) employs intersectionality and critical race feminism in exploring underrepresentation of women of color in STEM fields. Just as a body of scholarship exists about women of color in the academy, in general, and in education and social sciences, more specifically, there is a large literature examining the underrepresentation of women in STEM fields; however, the vast majority of this does not explicitly consider the added impact of race, or it does so only tangentially (such as in concert with other individual-level characteristics). Less research has been conducted on the underrepresentation of people of color in science than women of science and here, too, only a small fraction of it analyzes the ways that gender compounds race-based inequities. The paucity of literature on women of color in STEM fields is easily apprehended through a Venn diagram, where each of the circles represents a subject of study and the size of the overlapping areas and of circles themselves represent the relative size of the literature on those subjects:
Fig. 1: The Depicting the relative size of existing literature on the intersection of race and gender in STEM fields
Pipeline scholars have identified barriers to participation in the sciences for women and people of color as separate challenges; however, intersectional analyses are absent from this literature. (Oakes, 1990)Jeanne Oakes (1990) gives a prototypical assessment of this perspective. "Careers in science and technology result from students passing through a long educational 'pipeline.' Doing so successfully involves three critical factors: opportunities to learn science and mathematics, achievement in these subjects, and students' decisions to pursue them" (Oakes, 1990, p. vi). Women tend to leave during senior high school and college, she finds, and they do so because they choose not to pursue scientific careers. Students of color start out with greater interest in science during elementary school, but because of lower achievement, tend to be funneled into remedial tracks presenting limited opportunity for science related experiences (vii). That is, of the three factors that facilitate progress through the pipeline (i.e., opportunity, achievement, and decisions), she finds that women choose not to pursue science careers while people of color are impeded by a combination of achievement and opportunity. Oakes advises, therefore, that educators should intervene at the places we know these groups tend to leak from the STEM career pipeline. She suggests altering the way science and math are taught at the elementary level will spur girls to consider science, and providing additional science exposure and role models in and out of the classroom to retain students of color (viii).
(Clewell & Anderson, 1991)Clewell (1991) reviewed the literature published between 1959 and 1990 on young women of color in the sciences and mathematics between grades four and eight, and found four general barriers impeded potential female scientists of color: student attitudes, student achievement, student selection of courses, and students' career interests and aspirations. Her review showed the tendency of the literature to explain underrepresentation in terms of student deficits. In this same vein, but on the other end of the pipeline, (Baker, 1998) Baker (1998) analyzes persistence in science and engineering Ph.D. programs among women and students of color, finding that when he controlled for "ability" (questionably operationalized in terms of GRE scores and grade point averages), that sex and race differences in degree completion are significantly reduced. He does not indicate the effect for women of color, specifically, but does indicate that a gender gap persists even after controlling for GRE scores and GPA. His analysis conflates ability with test and academic performance, however, and uses measures shown to systematically vary by race and gender due to stereotype threat.
An alternative perspective shows that structural and cultural sources best explain attrition in STEM fields, not individual student adequacy or effort. This view sheds light on the external pressures on women and people of color that ultimately deter and exclude them from careers in the STEM disciplines. For example, (E. Seymour, 1992, 1995)(E. Seymour, 1992, 1995) actively debunks what she calls "attrition myth theories" such as Oakes', which explain students' leaving of STEM majors for the social sciences and humanities to students' lack of ability, effort, or application. The "widespread acceptance of this theory functions to allow schools and departments to focus on weeding out those least fit to survive, and to regard their leaving as a sort of 'natural selection' process" (Seymour, 1992, p. 237). Exploring reasons for switching from science to non-science majors, she finds that "switchers" are just as academically successful, hard-working, and capable of facing the difficult conceptual content of STEM courses, but they less frequently use "situational resources" to overcome the challenges also faced by non-switchers (Seymour, 1992, p. 232). In addition, the structural barriers she cites include the need to work long hours to pay tuition, high school preparation, length of the major, teaching quality, and approachability. Non-switching seniors cited coping strategies as a reason for their persistence in the major. Seymour's perspective is a clear improvement on the deficit-based "attrition myth theories," but it does not explore the impact of race as strongly as it does class and gender-based structural explanations; furthermore, it takes for granted that the pipeline metaphor best describes the reasons for women's underrepresentation, even as it posits alternative explanations for the pipeline's leaks.
A multidimensional critique of pipeline explanations for women's underrepresentation in sciences and engineering is advanced by (Xie & Shauman, 2003) Xie & Shauman (2003) in his widely acclaimed book, Women in Science. The pipeline framework, Xie contends, inherently limits the scope of how we study gender disparities by conceiving of becoming a scientist as an overly linear, stage-based process that "equates noncompliance with the normative career trajectory to 'leaking' or 'dropping out'" (p. 9). Its narrow conceptualization also prevents alternate career trajectories from being seen as legitimate and overlooks the role of one's family, assuming that one's educational and occupational attainment are independent of other major life events. In addition to problems with the ways that the pipeline metaphor unduly restricts conventional wisdom about career processes, most pipeline-oriented research suffers from methodological problems. His comprehensive analyses of the dynamic and multidimensional processes by which women enter the science and engineering labor force are guided by a life course theoretical framework, which is informed by a combination of structural allocation and self-selection theories. While Xie's (2003) research only tangentially studies the impact of race, it makes a critical contribution to the literature on access to STEM careers, the vast majority of which leaves the assumptions of pipeline logic unquestioned.
Orn (2005) also introduces fresh perspective to the research literature through a critical race feminist perspective. Moreover, she is the only scholar I could identify who acknowledges that intersectionality of race and gender in access to science. She finds intersecting identity has largely been ignored in the literature, and thus takes a consciously intersectional perspective in relating the findings of her longitudinal, qualitative study of female students of color in physics. Women of color in physics "sense that their belonging and competence in science are questioned because their bodies do not conform to prevalent images of the "ordinary" white male physicist" (p. 593). To persevere, her participants described two coping strategies: (1) gendered and racial passing, and (2) manipulating others' stereotypes of minority black women in the sciences by seeking to perform with superiority to their white and male peers. Lacking females of color with whom to identify in their own fields, these true pioneers in their discipline persist by avoiding and transcending stereotypes.
To address persisting disparities for women and students of color in the sciences -- particularly at the graduate school level and in the labor force -- a variety of interventions have been developed to retain students in the sciences at all levels. One prominent approach is involvement of college students in faculty-mentored, original research. Among other outcomes, undergraduate researchers have higher baccalaureate attainment rates (Nagda, et al, 1998), greater interest in science careers (Campbell and Skoog, 2004; (Campbell & Skoog, 2004; Kremer & Bringle, 1990)Kremer and Bringle, 1990; Russell, 2007), and higher graduate school enrollment rates (Bauer & Bennett, 2003; Hearn, 1987; Russell, Hancock, & McCullough, 2007)(Bauer and Bennett, 2003; Hearn, 1997; Russell, 2007). The Boyer Commission (1998) thus urged research universities to create opportunities for all students to engage in research as one of ten recommendations for improving undergraduate education. Comparison group analyses, however, show that undergraduate research has a significantly stronger effect on the retention, aspirations and post-baccalaureate choices of students of color and first generation students' than those of white students and students whose parents attained a four-year degree (Hathaway, et al., 2002; Ishiyama, 2002; Nagda, et al., 1998; Russell, 2007).
Interestingly, in their program design, activities, and objectives many undergraduate research programs tend to reflect characteristics of both pipeline logic and the need to fundamentally rethink and restructure the process by which women and people of color approach the sciences. Consistent with pipeline-minded scholarship that emphasizes shoring up individual students otherwise at risk for attrition from the sciences, programs provide support structures to encourage academic success and increase students' self-confidence, for example. However, they do so through early socialization as a research scientist and extended engagement with faculty sponsors who can open doors that might have otherwise been closed – a particularly powerful experience for students from populations that have historically been marginalized in the sciences. Some programs, such as the Meyerhoff Scholars Program and McNair Scholars Program, explicitly emphasize connecting undergraduates with scholarly communities of color and role modeling, with the goal of increasing faculty diversity across the country. Such programs affirm the pipeline principles of access and retention; however, they seek not only to plug the pipeline's leaks, but also to alter the entire process by which scholars pass through it and to prepare students for what to expect on the other end as a scholar of color in the academy.
Although numerous programs exist specifically for students underrepresented in their disciplines based on their race/ ethnicity, gender, and/or socioeconomic status, scholarship on undergraduate research has not approached students' experiences with an eye to intersecting identities. A mixed methods of how female graduate students in the sciences perceived the long-term impact of their undergraduate research experience (Campbell & Skoog, 2004) (Campbell & Skoog, 2004) emphasiz emphasized the mentoring role that the research experience provided -- both from their faculty mentor and graduate students working in their lab. Through their early experience with research, these women also gained self-confidence as a scientist, a solid understanding of the time demands a lab-based science career would require, and had their career aspirations influenced in the direction of scientific research. Of course, such a retrospective methodology among those who have persisted suffers from the drawbacks of selection bias; we should expect relatively positive outcomes of research to be reported when the sample does not include undergraduate research participants who did not persist in pursuing scientific careers after college.
Among the many types of undergraduate research that are proliferating, the Undergraduate Research Opportunities Program (UROP) here at the University of Michigan has a strong national reputation and scholarship on UROP is some of the undergraduate research literature's most methodologically rigorous. UROP was specifically developed to improve retention of students of color by "broker[ing] intellectual relationships between faculty and first-year and sophomore undergraduates through research partnerships" (Nagda, et al., 1998, p. 58). Since most undergraduate research programs serve upperclassmen, UROP's program design is unique, and a rigorous quantitative study confirms the program's success in meeting its primary objective: the attrition rate for African American participants in the 1993-1994 year was approximately half that of African American non-participants in a control group (Nagda, et al., 1997, p. 62). African American program participants demonstrate significantly higher retention outcomes than participants from other racial/ ethnic backgrounds. Although it has since opened participation to students of all racial/ethnic backgrounds, the program leadership retains a particular emphasis on the success of women and students of color in the sciences and has an emerging focus on facilitating graduate education and postgraduate research.
In1 another study of UROP outcomes, (Hathaway, Nagda, & Gregerman, 2002)Hathaway, Nagda, & Gregerman (2002) undertake an inquiry into whether undergraduate research participants and non research participants at the University of Michigan differ in their pursuit of graduate education, use of faculty recommendations for post-baccalaureate opportunities, and continued contact with faculty after graduation. In order to overcome the selection bias problem, researchers matched university alumni who had participated in the Undergraduate Research Opportunities Program (UROP) with non-participants based on major, race/ ethnicity, GPA, and graduation date (N=291). They were surprised to discover that, among non-participants, enough respondents indicated participation in non-UROP undergraduate research that they were able to construct a third comparison group (which they called other research students).
Through Chi squared analyses, they found that UROP and other research student alumni were significantly more likely to have pursued graduate education, to be involved in ongoing research, and to use faculty for recommendations. Overall, UROP and other research students were significantly more likely to pursue graduate education than students of color (African American and Latino) who had not participated in undergraduate research. UROP students were also most likely to pursue doctoral degrees -- including law and medicine. While White and Asian student were most likely to pursue doctoral education across the entire sample, UROP students of color were as likely to pursue doctoral education as White and Asian UROP students.
By offering a sub-environment in which students engage the research university's mission and scientists critical in carrying out that mission, undergraduate research can counteract two institutional factors that deter students from the sciences: lack of faculty contact and a lack of community with other students. Clearly, UROP participation has particular benefit to students of color over and above the benefit of research participation, in general, a finding that motivates the need for deeper understanding. While the article offers a more methodologically sophisticated analysis of research outcomes for students, like most scholarship in this literature it is retrospective. Thus, it cannot provide evidence to show whether or how specific structures of research influence students, nor does it explain how the research experience may stimulate students' consideration of specific graduate degrees and/or careers.
In general, literature on the impact of undergraduate research participation suffers from two methodological problems: selection bias and retrospective analysis. Selection bias is endemic in the literature, as most studies sample only from within a given research program, neglecting to compare learning outcomes with a matched comparison group of non-participants. This practice makes it difficult to ascertain which outcomes are indeed a function of research participation and which are correlates of the predispositions that lead students to participate in research in the first place. Another weakness is a reliance on retrospective analysis. By asking past research participants to reflect back on their experience we learn valuable information about its long term impact. Although such analysis does not necessarily weaken the validity of findings, it does prevent us from understanding how participants make meaning of the research experience while they are in it and how it is that research effects the beneficial outcomes that retrospective analyses have identified. In their methodological design, Hathaway, et al. (2002) overcomes the selection bias problem and is thus able to offer sound empirical support for the claims to research impact that many have posited without sufficient evidence. Applying this type of analysis to a set of longitudinal data would present an even stronger advance in the scholarship on the impact of undergraduate research.2
Like the two studies examining the impact of UROP participation on retention and graduate enrollment, (Barlow & Villarejo, 2004)Barlow & Villarejo (2004) likewise found that undergraduate research increased the likelihood that students of color majoring in a biological science would persist to graduation at University of California-Davis. Moreover, their participants were more likely to continue on to graduate education than graduates from the institution overall. The impact of undergraduate research experience on graduate school enrollment is well established for students in general, but particularly for students of color (Hathaway et al., 2002; Ishiyama, 2002; Russell et al., 2007; Elaine Seymour, Hunter, Laursen, & DeAntoni, 2004)(Hathaway et al., 2002; Ishiyama, 2002; Russell, Hancock, & McCullough, 2007; Elaine Seymour, Hunter, Laursen, & DeAntoni, 2004). However, in a large survey of 4500 undergraduate research participants and 3400 STEM degree recipients who had conducted undergraduate research, no significant differences were found by gender or race in program outcomes. "No formulaic combination of activities optimizes the undergraduate research opportunity, nor should providers structure their programs differently for unique racial/ ethnic minorities or women" (Russell, et al., 2007, p. 549). Rather, the duration of their research experience was most strongly correlated with reporting expectations of earning a Ph.D. Among STEM field researchers, 73% said that the research experience raised their awareness of graduate school and 68% reported an increased interest in a STEM career as a result of their undergraduate research opportunity (Russell, et al., 2007).
Although women of color have made great strides in their educational and occupational attainment in the last forty years, access to careers in STEM fields represents a major frontier for race and gender equity in education. While the pipeline metaphor dominates the literature, the most current research advances alternative explanations for disparities that may empower scholars and practitioners to innovative scholarship and programming that extends opportunity to women scientists, especially those of color. Among these innovations are undergraduate research programs that use concrete research experience to stimulate the educational and career aspirations of students. Of particular relevance to the current analysis are findings from multiple studies showing that students from historically marginalized backgrounds disproportionately benefit from these programs, and that participation almost entirely erases baccalaureate attainment and graduate school enrollment gaps for women of color who major in STEM fields. However, resolving undergraduate retention and graduate school enrollment disparities does not resolve the persistent disparities in graduate degree attainment rates and attainment of both tenure-track faculty positions and tenure, itself.
Based on my interests in the role undergraduate research plays in enhancing the opportunities of students of color and in access to STEM careers more specifically, I elected to conduct two interviews: one with Sandy Gregerman, director of Undergraduate Research Opportunities Program (UROP), and one with Cinda-Sue Davis, director of Women in Science and Engineering (WISE). WISE and UROP were founded explicitly to improve the retention and achievement of women and students of color, respectively, but over the years have opened participation to students regardless of their demographic background. UROP expanded its participation to White students in the late 1990's as undergraduate research became nationally recognized as a beneficial mode and context for learning, and WISE opened to men just last year as a result of Proposal 2. In both cases, though, the program directors continue to derive their passion for the programs from the impact they have in increasing access and achievement. Moreover, both programs are funded through a combination of state general funds and federal grant funds, permitting the continuance of some programming specifically for women and students of color. Both admitted at the outset of our interviews, however, that they have much less expertise regarding the intersection of race and gender as it pertains to STEM career access than with women and students of color, as separate populations. Although women of color come through both programs, little programmatic attention is paid to the meaning of this intersection for their unique experiences.
WISE was founded in the late 1970s when Davis was pursuing her Ph.D. at UM in Biochemistry. Informally run for years by women faculty, they petitioned the central administration in 1980 to have a formal program established. The university's provost at the time, Billy Frye, had two daughters, and Davis believes this was critical to his willingness to have the program established. Early activities emphasized a group of female faculty Davis called "survivors of the system" teaching the next generation of female students and young faculty the rules of the science game, assuming that if they knew these rules that they would succeed. After several years of working in the research faculty as she started a family, Davis took the position of WISE director in 1984 and led the program in efforts to augment their pipeline efforts with deeper structural transformations within the university. This work included engagement with curricular and pedagogical committees and bringing to light the inhospitable climate women faced in many STEM departments on campus.
UROP was created as an outgrowth of the Michigan Mandate. As the number of students of color increased and clear retention disparities were observed between white and non-white students, Psychology professor John Jonides looked to the research literature for academically oriented, non-remedial programming that might support students of color. Drawing on Tinto's theory of academic and social integration viz a viz student retention, they developed UROP. Although it was developed with an eye to retention and research showed its success in accomplishing this (Nagda, 1998), it became clear than an unintended outcome of the program was enhancing its participants' interest in graduate education (Hathaway, et al., 2002) and, because so many UROP projects have a biomedical focus, particularly in medical school.
Davis and Gregerman, who are both white, independently confirmed my assessment that the research literature is weak on the intersection of race and gender. I shared with both of them my assessment that the two primary perspectives in the literature about the access women of color have to STEM careers seem to be grounded in either pipeline or critical race feminist assumptions, and asked them which of these views seems to resonate with their professional experience. Davis grinned and answered, "It depends on the day."
Congruent with the pipeline and critical perspectives, but underacknowledged in the literature, they both emphasized the importance of having a sense of community within the discipline for women of color and, if not structural diversity is not present, a clear sense of inclusiveness in the closest groups with whom students work (e.g., one's research lab). Confirming this, Gregerman spoke about the specific majors to which women of color tend to gravitate, such as biopsychology for the structural diversity and to engineering for the strong community of color. Davis affirmed the strength of the community in engineering, which she says is most clearly manifested through active student organizations such as Movement of Underrepresented Sisters in Engineering and Science (MUSES), Society of Women Engineers, and the National Society of Black Engineers as well as the University's MEPO and WISE programs. MUSES takes a particular interest in reaching out to female middle school students of color through mentoring, tutoring, and campus tours. Activities like these meet the desires women of color have to reach back and for community engagement, even as they pursue highly technical, frequently impersonal, academic fields. Although they indirectly make critical social contributions, engineering and sciences disciplines do not market themselves like social work, education, and psychology as disciplines whose work benefits society, and Davis believes this public image problem deters women of color. Having opportunities through groups such as MUSES then, presents an opportunity to for women to stay involved in social change issues interpersonally, and not just through developing technologies that will improve people's lives. At the same time, MUSES offers a mutually supportive network that enhances their own academic success.
In the post-Proposal 2 environment, groups like MUSES are more important than ever, since the policy has removed important resources that were used in the past to encourage student success. Of course, given the language of Proposal 2 emphasizing race and gender, women of color are once again uniquely caught in the crossfire. Particularly devastating for their efforts to retain current students, Gregerman and Davis say, is the withdrawal of designated scholarships for women of color. However, Davis spoke at length about the new challenges of recruiting students of color having positive effects for her program. With the new university-wide focus on outreach and community engagement, which is work that WISE has been involved in since 1990, "Suddenly, we've become legitimized." Along with other university programs that have received minimal recognition for their decades of outreach to students in the K-12 system, WISE leadership is now at the forefront of the university's outreach strategies. Davis has been a primary consultant in the development of Tony England's Office of Engineering Outreach and Engagement (OE2), and both she and Gregerman are key players in the development of the M-STEM Academy, which will offer a first year living-learning community to students admitted to UM through OE2.
The University of Michigan goes back and forth with Georgia Tech as the institution enrolling the most women in STEM fields, and only HBCU's graduate more African American engineers than UM. Maintaining its national leadership in promoting STEM career access in the post-Proposal 2 environment has created a sense of urgency that Davis believes has and will stimulate new creativity and strategies. It has also generated the political will in the administration to use every mechanism at the university's disposal to foster access. "We're far from where we need to be," Davis concluded, but she and Gregerman remain optimistic about the university's capacity to maintain a real commitment to the success of women of color. Indeed, they remain in this work because they believe well structured learning environments can make a difference that transcends the forces of policy and history.
I am struck in considering Davis and Gregerman's comments by their implicit assumption that institutional commitment to access is the primary force by which said access is achieved. Institutional commitment does not ensure commitment at the level of individual departments/ majors or by individual faculty for their students' success. The institution plays a critical role in recruitment and retention, but individual students' primary contexts for learning and career advancement are within smaller organizations, such as research groups, departments, and disciplines. Thus, in addition to crafting and supporting co-curricular programs that bridge academic and student affairs such as UROP and WISE, the university has an obligation to vigorously cultivate inclusive cultures within departments. After all, what we want women of color to have access to in their careers as scientists and engineers is not simply the job of scientist or engineer. Fundamental among the institutional resources we must push for is access to the freedom to participate in these fields as respected equals. Although I do not know how this is to be achieved, in part because of my own standpoint as a white woman outside the STEM disciplines, there are two qualities to the sort of academic freedom I envision:
In higher education, we traditionally think of academic freedom as a cornerstone of both the academy's reason for being and its modus operandi. It is a powerful value that lures many scholars into academic careers. Thus, while we attend to the disparities by race and gender that continue to exist and work to eradicate them, an important part of that work must be to improve access to the most basic forms of academic freedom. I am convinced that it is no accomplishment for women and people of color to be technically included but marginalized as unequal participants. As a result of this course and this research, therefore, I intend to take my own study of educational inequality to a deeper level by studying the ways that research universities' organization of knowledge inequitably structures opportunity.

American higher education is becoming more diverse now than at any previous time (Zhao, Kuh & Carini, 2005). Behind this trend are international students who constitute an increasingly relevant and important source of diversity on college campuses. During the past decade, American colleges and universities have witnessed a steady increase in international student enrollments. From 1958 to 2005, the population of international students enrolled in U.S. higher education institutions increased from 43,000 to over 560,000 (Open Doors, 2005). Among that population, Asian students comprise the largest proportion, approximately 58 percent, of all international enrollments (Open Doors, 2005).
This increasing population of international students, especially from Asia, in the United States faces special challenges in terms of adaptation to a new living and learning environment at host universities and colleges (Perrucci & Hu, 1995). In addition to academic pressures, many international students tend to experience a variety of adjustment concerns, including language difficulties, insufficient financial resources, social integration, challenges in daily life tasks, homesickness, and role conflicts (Mallinckrodt & Leong, 1992). Interestingly, several studies suggest that students from Asia have more difficulty adjusting to life in the United States than international students from non-Asian countries (Abe, Talbot & Geelhoed, 1998).
These difficulties in adjustment among Asian students may stem from the greater differences in cultural norms, values, and languages between their home countries and the United States, which results in alienating Asian international students from the host society. In order to avoid potential social alienation, developing interpersonal relationships may provide a powerful coping resource to overcome barriers to adjustment in a new environment for these students (Mallinckrodt & Leong, 1992). In addition, based on the appreciation of cultural differences, supportive social interaction with diverse others has the potential to enable Asian international students to develop intercultural maturity by promoting positive changes in their sense of self and relationships with others (King & Baxter Magolda, 2005).
This paper aims to address the importance of international students' building social support networks with diverse others as a coping strategy for adapting to American life. In addition, this paper will examine how effective these social supports are in facilitating their identity development and intercultural maturity. To this end, this study reviews existing resources that identified major difficulties in adjustment encountered by international students while studying in the United States and the impact of such social interaction on their adjustment and further identity growth. Drawing from the review of the literature, this study then interviewed first-year Asian graduate students who were granted Fulbright scholarships to examine the role of social support in their adjustment, identity development, and intercultural maturity.
For most international students, the experiences of studying abroad can be an overwhelming personal and cultural transition. Many researchers have sought to identify specific adjustment problems that international students have experienced. English language proficiency was recognized as one of the major adjustment issues for international students (Mallinckrodt & Leong, 1992; Surdam & Collins, 1984). Studies have found that international students encountered significant problems in communicating with Americans in English especially in academic settings.
Poor language skills were found to be detrimental not only to academic process, but also for social interaction (Huntly, 1993). Research has demonstrated a high correlation between poor English language skills and a lack of interaction with American students and the surrounding community as a whole. By contrast, students who are fluent in English and have American friends tend to have fewer adjustment problems during their stay in the United States (Schram & Lauver, 1988; Surdam & Collins, 1984).
In addition to language challenges, international students were often faced with the need to adjust to a variety of other cultural and social challenges as well, which often entails considerable psychological stress. Fatima (2001) found that many international students experienced significant adjustment-related problems in immersion to a new culture, manifested as anxiety, frustration, loneliness, helplessness, distrust and hostility towards members of the host culture, disruption of one's identity, and loss of self-esteem. These are common symptoms of the early stages of cultural shock (Zhao et al., 2005).
Students from non-Western countries were significantly more likely to experience more of these acculturation stresses than those from Western countries (Surdam & Collins, 1984). Compared to White and Black international students, Asian students were less engaged in active and collaborative learning activities and were less satisfied with their campus environment (Zhao et al., 2005). In the same vein, Abe et al. (1998) found that students from Asian countries experienced more challenges in adjusting to college life than students from non-Asian countries.
Using the University Alienation Scale (UAS), Schram and Lauver (1988) studied the relationship between international students' adaptation and social alienation. The UAS, designed to measure alienation of students, included items assessing powerless, meaninglessness, and social estrangement as aspects of alienation. They found that students from Asia had the highest alienation scores, reflecting minimal social interaction with Americans and other international students. The concept of "cultural distance" (i.e. the degree of difference between home culture and host culture) has been suggested as having considerable explanatory power for these differences in adjustment (Parr, Bradley & Bingi, 1992).
Researchers agree that social support plays an important moderating role in protecting international students exposed to a new environment against the deleterious effects of acculturation stress (Cemalcilar, Falbo & Stapleton, 2005). Since international students tend to lack their familiar support system when they come to a new culture, developing a new social support system may fulfill their desire to achieve a sense of belonging. In particular, for students facing stressful transition to an unfamiliar culture, Mallinckrodt and Leong (1992) found that developing interpersonal relationships in a new environment provided a powerful coping resource to overcome barriers to adjustment. They found that social support from significant others was effective in buffering the impact of life stressors and promoted psychological adjustment. Schram and Lauver (1988) suggested that international students who have a strong social support system tend to adjust to college life in the United States more quickly and effectively than those who do not.
The patterns of seeking social support among international students have been categorized into three potential sources: host nationals, co-nationals, and other international students. Several studies have indicated that more frequent and closer interaction with host nationals is a predictor of successful social and cultural adjustment (Perrucci & Hu, 1995; Schram & Lauver, 1988; Surdam & Collins, 1984). Surdam and Collins (1984), who defined adaptation as the satisfaction of social and academic demands, found that spending more leisure time with Americans was significantly correlated with the adaptation of international students. A Similar study reported that international students who had more interaction with host nationals also felt that they had better cultural, academic, and social adjustment (Heikinheimo & Shute, 1986).
Schram and Lauver (1988) found that social interaction with Americans was the best predictor of alienation, while less interaction was predictive of alienation. Based on this result, they recommended developing an orientation program that would encourage international students to become acquainted with Americans and provide opportunities for such interactions.
Despite the benefits of social interaction with host-nationals, many international students experienced only superficial contact with Americans, and give up hope of establishing deep cross-cultural relationships (Mallinckrodt & Leong, 1992). As was discussed above, English language deficiencies represented a significant barrier to meaningful relationships with host nationals. In particular, some studies have shown that Asian students have reported language difficulties as their major concerns in contrast to students from other regions (Heikinheimo & Shute, 1986).
In addition to language differences, another potential barrier to cross-cultural relationships is perceived differences in core cultural values and communication styles (Lam, 1997). Asian students who were from collectivistic cultures, which highly value interdependent ties with their family and community, perceived American students to be more individualistic and competitive compared to their own culture (Perrucci & Hu, 1995). Furthermore, Asian international students struggled to communicate clearly with American that their indirect communication style may be perceived as an unwillingness to disclose their private lives in contrast to the direct style of Americans (Lam, 1997).
Lack of opportunity to interact with Americans, the level of receptiveness to foreigners displayed by members of the host culture, and American ignorance about their home cultures may also significantly impact international students' social interaction with host nationals (Abe et al., 1998). Particularly, perceived prejudice and discrimination were negatively related to international students' adjustment and acculturation (Yoon & Portman, 2004). To overcome these barriers, Heikinheimo and Shute (1986) recommended that international students need to be receptive to cross-cultural learning, endeavor to learn English, and develop common interests to facilitate interaction with host nationals.
One of the major criticisms of previous research is the implicit assumption that social contact with people from the host culture is the only way to avoid social isolation and the adjustment-related problems, thus ignoring the importance of social support from co-nationals and other international students (Perrucci & Hu, 1995). Considering that most of this research was conducted before mid-1990s' when relatively few international students attended the U.S. institutions, this assumption may have been valid previously due to difficulties in organizing co-national student groups or international student groups as a whole. Currently, however, the numbers of international students, especially from Asia, have grown enough to provide social, cultural and academic assistance independent of the host culture (Perrucci & Hu, 1995). Due to this fact, growing social interaction among international students without involvement from host nationals is also significant in understanding adjustment of international students.
Faced with the challenges of forming ties with host nationals, many international students rely heavily on co-national students for social support. Enclaves of co-nationals provide support in coping with the challenges of adjustment by providing a sense of belonging and psychological comfort (Schram & Lauver, 1988). Due to the likelihood of shared language and values, support from a co-national is qualitatively different from that offered by Americans (Rajput, 1999). Establishing strong relationship with others from a common cultural background can raise self-esteem and consequently affect the adjustment of international students positively (Myles & Cheng, 2003).
Despite its significance as source of support, frequent social interaction with co-national may insulate international students from opportunities to engage with host culture (Lam, 1997). The convenient opportunities to meet fellow nationals who share common language and cultural values may result in reduced social interaction with Americans, thus retarding adjustment to the new culture (Cemalcilar et al., 2005). Due to their greater difficulties in English proficiency and perceived cultural differences, Asian students tend to withdraw from social relationships with Americans and maintain insular mono-cultural social networks with co-nationals (Rajput, 1999).
With few students from their countries, support from enclaves of co-nationals becomes difficult to obtain. In this case, "shared foreignness," sharing the experience of being a stranger in a new culture, encourages these international students to build social ties with other international students (Lam, 1997). In a comparison study of friendship preferences between international students and American students, international students tended to be friend with other international students, over 40 percent of whom had no American close friends (Rajapaksa & Dundes, 2002). In addition, sharing similar cultural background and geographical regional proximity positively influenced the establishment of social relationships with other international students (Rajput, 1999).
The literature reviewed for this study suggest that cross-cultural adjustment of international students is enhanced when they experience relationships with co-nationals, or establish connections with host-nationals and other international students. Unfortunately, Asian students seem to have more difficulty establishing intimate social ties with host nationals. As a result, interaction with co-nationals or other international students often becomes their primary source of social support.
Engaging in multicultural networks with host-nationals and other international students from different cultures, in addition to mono-cultural relationships, enables international students to develop intercultural competences. By interacting with diverse others, international students may come to understand and accept cultural differences.
To promote intercultural maturity in college, King and Baxter Magolda (2005) proposed three comprehensive dimensions of intercultural maturity: cognitive, intrapersonal, and interpersonal maturity. Achievement of intercultural maturity in these three dimensions is demonstrated as follows: 1) cognitive maturity: complex understanding of cultural differences using multiple cultural frames; 2) intrapersonal maturity: the capacity to create an internal self that openly engages challenges to one's view and considers social identities in a global context; and 3) interpersonal maturity: the capacity to engage in meaningful, interdependent relationships with diverse others based on an appreciation of human differences (King & Baxter Magolda, 2005, p.576).
The third dimension, development of interpersonal relationships with diverse others, is especially relevant in the explanation of international students' patterns of social interaction. At the initial level of development, social relationships are grounded in one's primary social identity or affinity groups (King & Baxter Magolda, 2005). This level may be applied to explain international students' initial experiences in the United States when they had few social relationships with host nationals, thus relying primarily on social support from similar co-national groups adapting to a new environment.
At the intermediate level, there is a greater capacity to explore cultural differences and to interact effectively with others, acknowledging the legitimacy of multiple perspectives and realities. The highest level of the interpersonal dimension is achieved by engaging in intercultural interactions based on cultural understanding, enhancing one's identity and role as a member of society (King & Baxter Magolda, 2005). When international students, who move between different cultural perspectives, construct meaningful relationships with host nationals and other international students that are grounded in respect for cultural differences, this mature level of interpersonal competence enables them to promote intercultural maturity.
The interrelationships among the three dimensions of students' development lead to interpersonal competence facilitating cognitive and intrapersonal competence in a multicultural context (King & Baxter Magolda, 2005). Beyond dependent relationships with similar co-nationals, building interpersonal relationships with diverse others will lead to cognitive and identity development of international students. International students' development of interpersonal relationships can be examined through the lens of intercultural maturity model in relation to their cognitive and identity development.
In summary, the literature on international student adjustment, social relationships, and intercultural maturity suggest that interpersonal relationships have a significant impact on cross-cultural adjustment and development of intercultural maturity. Engaging in social support networks with diverse others plays a key role in their acculturation process and obtaining intercultural competence based on the appreciation of cultural differences. Although most literature seems to indicate that interpersonal relationships with host nationals have the most positive effect on cultural adjustment, interaction with co-nationals should not be ignored since they provide a sense of belonging and comfort. In addition, despite a lack of literature, social relationships with other international students may also be important sources of support in adjustment, and provide enough contexts for developing intercultural maturity.
Despite the positive impact of cross-cultural relationships, the literature reviewed in this study consistently suggested that students from Asia tended to encounter more difficulties in cross-cultural adjustment and developing interpersonal relationships, due to the greater perceived cultural distance between Asia and the United States compared to the other Western countries. This indication of particular cross-cultural challenges for Asian students motivated me, an international student from Asia, to identify whether this result is valid and thus applicable to explain the experiences of other Asian international students. Hence, this study attempts to investigate Asian international students' cross-cultural adjustment problems and interpersonal relationships in particular.
Based on the previous review of literature, two data-gathering methods were used: individual interviews and participant observation. During the Fulbright Enrichment Seminar held in Chicago from March 23rd to 26th, 2006, I conducted participatory observations to examine the process of developing social relationships of international students. Under the objective of Building Trust in Diverse Communities, the U.S. Fulbright recipients who had study abroad experiences and international Fulbright recipients from 67 countries participated in this seminar. The participation in this seminar provided a rare opportunity for me to explore the process of developing social relationships among this diverse population. In particular, two outreach programs, a public school visit and a home hospitality event, were designed to encourage international Fulbright recipients to develop meaningful relationships with Americans in Chicago.
The public school visit gave international Fulbright recipients an opportunity to visit Chicago area high schools in small groups to experience a U.S. public high school and speak with local students. This visit allowed international students to share information about their home country and culture with local students and their teachers through exchanging ideas, perspectives, and experiences. The home hospitality program was organized by Chicago supporters of the Fulbright program who invited international Fulbright recipients for a dinner in their homes. This program provided a unique experience for international Fulbright recipients to be involved in typical U.S. family lives.
In addition to participant observation in the seminar, data were collected via individual interviews with six first-year Asian international graduate students who enrolled at U.S. colleges and universities, all of whom were participants of the Fulbright Enrichment Seminar. Interviews were conducted in informal locations during the breaks at the seminar. The interviews included the following questions on the participants' patterns, preference, perceived differences of the relationships, influences on intercultural maturity and changes in identity:
Several themes emerged from the analysis of interviews and observation, including the process of adjustment to U.S. life, motivation to develop social relationships, patterns of building relationships, and influence of social support on identity growth and intercultural maturity.
Recalling their initial sojourn experiences in the United States, most of the interviewed participants expressed some degree of loneliness and social isolation due to leaving from their home culture, loss of familiar customs and behavior, and reduced contact with their family and friends. For them, loneliness was felt most acutely during the first few months of the stay. The fact that all participants were unmarried seemed to influence their feeling of loneliness the more.
Separation from their culture and society motivated the students to develop new social relationships in the host culture. Because they needed to receive both emotional and practical support to adjust to their new environment, such as asking for a ride to an airport or to go grocery shopping, all of the participants sought assistance from a caring, trusting, and reciprocal relationship with new people, regardless of their nationality. They all agreed that they could develop interpersonal relationships with people from different countries if they shared similar personal characteristics, experiences, values, and beliefs.
If they desired, all participants were able to establish some form of social relationships with Americans, co-nationals, and other international students. Unlike the literature that indicated particular challenges for Asian students in making American friends, most of them were not afraid of interacting with Americans. This contrasting tendency stems from their earlier living experiences in the United States before they came to attend graduate schools. Three of the participants had lived in the United States during middle school with their parents. Another two participants had benefited from undergraduate exchange programs which enabled them to study at U.S. institutions for a year. Their unusual earlier experiences in the United States contributed to their ability to communicate with Americans and experience less cultural conflict between their home country and the United States, which further facilitated the development of relationships with Americans. In this sense, these participants' experiences confirm the previous literature (Schram & Lauver, 1988; Surdam & Collins, 1984) that highlights the positive correlation between language fluency and interaction with Americans.
For the participants with strong English proficiency, the Fulbright Enrichment Seminar provided enriching opportunities for developing interpersonal relationships with Americans. Throughout the seminar, interaction with the U.S. Fulbright recipients seemed to be facilitated by their cross-cultural understanding. Due to their past experiences of being international students, the U.S. Fulbright recipients were open-minded to international students and displayed a particular interest in learning about their culture and language. Some U.S. Fulbright recipients who had stayed in Asian countries, such as Thailand and Japan, were quite familiar with Asian culture and religion, and enjoyed asking questions about the participants' countries. In this case, their acceptance of foreign culture seemed to become a strong factor to facilitate cross-cultural relationships.
Furthermore, the home hospitality event facilitated caring and lasting interpersonal relationships between Americans and international Fulbright recipients. By inviting international Fulbright recipients to their home, the American supporters took the initiative to interact with international students, which was critical in building relationships with them. One of the female participants reported that her American host, who was a New Age musician, gave her one of his recent albums as a present. Because he played New-Age music that stems from the Asian spirituality, he enjoyed discussing the Asian cultural values surrounding his music with her. His interest and understanding of Asian culture were likely to build the long and lasting interpersonal relationships between them.
In their visit to public high school, each participant gave an informative presentation about their home country and culture in the classrooms. When the participants showed maps and pictures brought from their own countries, American students and their teacher seemed to be intrigued by the apparent differences, such as the appearance of people, landscape, and geographical distance. During the presentations, however, it was notable to observe the movement of their focus from cultural differences to intercultural similarities between their home and the United States. For example, when one of the participants from India discussed the caste system which divides people into separate social classes, American students, most of whom were African Americans, seemed to have much sympathy for the issue of social stratification. Comparing the caste system to their experiences of racial discrimination in the United States, the American students and the participants shared the thought that all human beings should be valued equally regardless of their social class, race and ethnicity. By exchanging their ideas and perspectives with local students and their teacher, the participants seemed to be inspired by sharing common interest and similar concerns surrounding each country, rather than simply focusing on specific cultural differences.
Through these various trust-building activities, all participants seemed to enjoy developing caring relationships with Americans. The intercultural relationships between Americans and the participants stemmed from mutual interests as well as different, but similar experiences, along with respect for each other's culture. Indeed, these enriching opportunities for interaction with Americans, American acceptance and interest in foreign culture positively influenced establishing meaningful ties with Americans. These findings support the previous literature of Abe et al. (1998), indicating that opportunities for interaction, the level of receptiveness, and ignorance about foreign cultures significantly impact international students' social interaction with host nationals.
In addition to facilitating relationships with Americans, the seminar provided a diverse context to develop cross-cultural friendships among international Fulbright recipients from 67 different countries. In comparison to relationships with Americans, a "shared foreignness" seemed to connect the participants more strongly. Facing challenges in adjusting to a new life seemed to create a sense of connection and solidarity among them. In particular, having common concerns in academic achievement, relationships with their American peers and faculty enhanced the participants' supportive interaction by producing abundant topics to discuss. Some participants who pursued Doctoral degree shared their anxieties on passing qualifying exams and keeping up with American peers in their academic studies.
Most participants showed a slightly higher preference to engage with other international students from regions that were geographically close to their home country. For instance, some participants from southeastern Asia appeared to enjoy more communicating with students from neighboring nations than those from distant regions. Despite their comfort with intercultural diversities, the cultural familiarity surrounding the region still seemed to unconsciously attract them like a long lost friend. As noted by Rajput (1999), similar cultural background and geographical proximity was an important factor that promoted social interaction with other international students.
Similarly, common cultures and languages played a key role in developing interpersonal relationships with co-nationals. Living in a foreign country, co-national friendships provided important opportunities to maintain a sense of belonging and identification with their home culture. Some participants reported that support from co-nationals was particularly helpful when they initially came to the United States without any familiar resources and social relationships. For instance, they received substantial assistance from co-national friends when searching for a new residence and selecting courses. In this sense, their experiences confirm the observation of Schram and Lauver (1988), who noted that relationships with co-nationals support the adjustment process by providing a sense of belonging and comfort. In addition, during the seminar, the participants from the same countries were more likely to be close friends than those from different countries even if they met the first time at the seminar.
For the participants, a social network with co-nationals served as a secure base upon which they were able to build relationships with Americans and other international students. One of the participants reported that the presence of his co-national student organization in his school provided a sense of pride in his own country, which significantly increased his interpersonal competence when communicating with Americans and other internationals. In contrast with the previous literature that suggests potential insulation from the host culture, these participants' engagement with similar co-nationals further facilitated interaction with diverse others. In this sense, the social ties with co-national were not a barrier, but a critical foundation in developing positive relationships with the host culture for successful cross-cultural adjustment.
During their stay in the United States, the participants appeared to achieve considerable personal identity growth by engaging in multicultural interpersonal relationships. All participants reported enhanced self-efficacy in managing relationships with diverse others. This was boosted by their increased sense of self by mastering the intricacies of a different culture and language. One of the participants reported that she could raise a sense of intercultural competence by coping with everyday cross-cultural challenges and engaging with relationships with diverse people from diverse countries.
In addition, a greater sense of appreciation for their cultural heritage, as well as for the host culture led to more balanced perspectives of both cultures. This made possible for them to integrate multiple perspectives into their identity development. One of the participants from Cambodia reported that he once regarded the United States as a far superior society to his home country. His perspective was changed when he wore his traditional costume on the Halloween party, where his American friends showed much interest in his costume and culture. He felt like wearing the costume and representing his own country and culture, which contributed to developing his identity with balanced perspectives on his home country and the United States.
The most notable outcome from the interviewees' diverse interpersonal relationships was to realize that people from different cultures were more similar to them than different, as manifested by interaction with American Fulbright recipients and programs such as the public school visit and the home hospitality event during the seminar. For most participants, interaction with Americans and other internationals throughout the seminar enabled them to acknowledge fundamental universal human characteristics across cultures. Such transformative experiences would be an important sign of intercultural maturity. Indeed, their participation in the enrichment seminar significantly contributed to their personal growth in their identity and intercultural competence.
Observations of the Fulbright seminar and focused interviews with six Asian international graduate students provided insights into the key role of social support in cross-cultural adjustment and intercultural maturity. Despite the relatively small number of interviewees, each of them offered valuable perspectives for examining the interpersonal relationships with diverse others and subsequent changes in their identity and intercultural competence.
By being separated from their home countries, participants suffered from loss of familiar systems and social relationships. This motivated them to seek new social relationships with new people. Due to the interviewees' high English proficiency and earlier living experiences in the United States, they found it easy to build social relationships with Americans. In particular, their interaction with Americans was facilitated by a variety of multicultural trust-building programs during the seminar. On the basis of mutual understanding and interests, the participants developed caring relationships with Americans who showed sincere hospitability and acceptance toward them. Because Americans with multicultural interests are effective sources of support for international students, the findings of this study highlight the importance of providing opportunities for long-term contact with Americans, such as the Fulbright Enrichment seminar.
However, at the same time, the participants' earlier experiences in the United States and higher English proficiency separated them from the other Asian international students whose experiences and language skills may be different from them. Because the participants were not the general representatives of Asian international students in the United States, the opportunities for such interpersonal relationships with Americans may have disproportionately benefited these "extraordinary" students compared to the "ordinary" international students. This limits the findings of this study in generalizing to other Asian students whose experiences are similar to the students described in the literature.
The diverse members of the seminar from all over the world provided a rare opportunity for them to engage in friendships with other international students because the shared foreignness and common concerns created a sense of connection. Particularly, relationships with co-regional students were attractive in that they gave the participants a sense of cultural familiarity and comfort in a new environment. Seeking similarity encouraged them to develop interpersonal relationships with co-nationals, which was especially helpful during the initial adjustment period. For the participants, connection with co-nationals served as a secure ground in building relationships with culturally different others, which provided an impetus to overcome cross-cultural barriers and develop intercultural maturity.
Despite the limitation of the study, these sources of social support that benefited the participants in the United States significantly contributed to increasing understanding of their identity development and intercultural maturity in a diverse context. With better appreciation for different cultures, they could strengthen their sense of self with increased self-efficacy in maintaining relationships with diverse others. Beyond the cultural differences, they discovered commonalities across human cultures. This was the most encouraging finding of this study.

The design features of a speedometer are not trivial details. Everyday, millions of people rely on speedometer details when driving automobiles. A few small defects in the speedometer design could distract the driver enough time to cause an accident and potentially cost many people their lives. Consequently, studying the effects of speedometer design parameters on speedometer reading time and accuracy is very important. The design parameters examined in the experiment include character color, location on the instrument panel, character size (strokewidth and height), radius and numbering scale. Each parameter was examined in order to explore the possible combinations of design features such that the speedometer average reading time would be low as well as having high reading accuracy. These design parameters, however, may have a different effect on reading time and accuracy depending on the task condition. Two task conditions were examined in this experiment. The first speedometer task condition was to find the exact speed while the second was to verify if the vehicle was above or below the speed limit. Therefore, the two objectives of the experiment were:
 -- To find the relationship between speedometer design parameters, task condition and task performance (reading time and accuracy).
 -- To find which speedometer is best for each given task condition and why.
The experiment was conducted using 13 test participants. Appendix A displays the relevant information for each test participant. All the test subjects were volunteers and students of the University of Michigan. There were many individual differences between the test participants that were considered relevant to the experiment and will be analyzed in later sections of this report. These differences include participant vision, handedness, sex and viewing distance. All other factors were considered negligible. For example, test participant age was considered to be negligible due to the small range of values (18 to 21).
The experiment used a simple projection and timing system to collect the data. This system included the following equipment:
The DRCU, PSCU, and response boxes were all custom made at the University of Michigan. This system worked such that the software in the computer controlled, directly or indirectly, all the elements of the system. The software was called RT334 and written by Prof. Chuck Wooley of the University of Michigan. The system projected various images of speedometers for an exact determined amount of time while being able to store data that was collected at the response boxes. The images were projected on a large projection screen in the front of the room. Two TVs were used to show summary data to the test participants in between testing blocks. Figure 1 displays the system's electronic connections.
Figure 1 displays how the computer, directly or indirectly, controlled the entire system.
Four unique speedometers were used in the experiment. Table gives a description of the significant differences for each speedometer. It is important to note that the measurement values are for the size of the characteristics when projected on the screen, not the actual size.
One characteristic of speedometer 4 that is not apparent from Table 1 is that the first number on the speedometer after 0 mph was 15 mph. From then on a scale of 10 mph was used. All the speedometers were circular in shape. The pointing needles of the different speedometers had negligible differences. Figure 2 displays the instrument control panel layout, common to all the speedometers, shown on the projection screen (without negligible and non-effecting details).
The speedometer was found in either of the large, inner circles in Figure 1. The other circles represent other gauges for RPM, temperature, oil level, etc. Other test materials and equipment used in the experiment include a standard tape measure, chairs, and tables.
Each experiment began with the 13 test participants seated in two rows, each with their own response pad directly in front of them on a standard classroom table. The middle of the first row was 10 feet and 5 inches from the projection screen and had 8 participants. The second row, containing 5 participants, was 15 feet from the projection screen. The participants in the second row were sitting in slightly higher (approx. 6 inches) chairs than the first row. Figure 3 depicts the testing room layout.
Each number on figure 3 represents the respective test participant's seat. Both televisions projected identical information, which was easily visible for every test participant.
The experiment was run in blocks of trials. Each trial began when the shutter opened revealing an image of a speedometer. This is also when the clock started for the trial. The participants could then begin to enter a response by pushing a button on the response box in front of them. After pushing the button, a signal would be sent indirectly to the computer. The time after the shutter opened until the response button was pushed was stored in the computer for each participant. The shutter would then close, completing the exposure duration. The participant could enter a response after the shutter closed, but only until the maximum response time for that trial. Only the first response the test participant gave was recorded. If the participant had responded with an incorrect response (error), the time as well as the fact that it was incorrect was recorded. After the maximum response time, data collection ended and the projector advanced. If the participant gave no response, the maximum response time was recorded for that participant as well as the fact that the participant did not respond (miss). The time in between the end of data collection and the shutter opening again was called the intertrial interval and was common for all trials.
Nine blocks, each with either 16 or 32 trials were conducted. In all the blocks the speedometers read 50, 55, 60 or 65 mph. With blocks 1 through 6, each participant pressed one of the four buttons on the response box that had been assigned a respective speedometer reading (50, 55, 60 or 65 mph). In blocks 7 through 9, the participants pressed 1 of the 2 buttons that had been assigned "speeding" and "not speeding". The "speed limit" was considered to be 55 mph. In both protocols, the participants were instructed to assign one finger for each button. Also, before every block the test participants were given the instruction to "be as fast and as accurate as possible". The first two blocks were practice in order to familiarize the test participants with the routine and system. Block 7 was also practice because of the new response protocol. The practice blocks had one large difference from the testing blocks. In the practice blocks no speedometers were shown, only 3 boxes, each with a word in them. Only one box actually had a correct spelling of fifty, fifty-five, sixty or sixty-five. The one correctly spelled speed was viewed as the speedometer reading for the trial. This difference was to make sure that the test participants did not get any practice at looking at any speedometer before they were tested on it. Also, between each block, the test participants were shown their mean response time and error count of the previous block on the 2 televisions in the front of the room.
The speedometers were grouped into 2 groups. Speedometers 1 and 2 were group A and speedometers 3 and 4 were group B. Only one group was shown per block of trials. Within each block, the 2 speedometers of the group as well as the reading on them would randomly change from trial to trial. Table 2 displays each block with its respective parameters.
The first step in analyzing the data was look at the summary data of each block with respect to each speedometer. Table 3 displays each speedometer's average response time for each block in which the speedometer was shown.
Table 4 displays the average error percentage rate and average miss percentage rate for each speedometer.
Comparing blocks 3 and 5 (the blocks with prolonged speedometer exposure duration), patterns become apparent. Speedometer 2 had the lowest average response time and the lowest error and miss percentage rates. Speedometer 1 was very close in average response time, but not as close in error rate. Speedometers 3 and 4 seem to be stratified from speedometers 1 and 2 in response time. One possible explanation for these differences can be in the overall size of speedometers 1 and 2 versus 3 and 4. Speedometer set A averaged much larger character strokewidth, character height, and speedometer radius than speedometer set B, accounting for the large difference in task performances.
Comparing blocks 4 and 6 (the blocks with shortened speedometer exposure duration), similar patterns become apparent. Overall, the average response times were all reduced from blocks 3 and 5. Also, as with blocks 3 and 5, speedometer set A is stratified from speedometer set B in that it has much lower average response times. Overall, average error and miss percentage rates were much higher than in blocks 3 and 5 due to the shortened exposure time for the blocks. This shows that as response time decreased, error and miss percentage rate increased. The error rates for all the speedometers were about equal, however, the average miss percentage rates for set B were double that of set A. This can also be attributed to the overall size difference of the speedometer parameters.
When the task condition changed to whether or not the car was "speeding", the patterns remained the same. Speedometer set A had lower average response times than set B, as well as having lower average error rate percentages. Overall speedometer response times as well as error and miss percentage rates did decrease due to the lower number of response choices.
The overall size increase of many speedometer design parameters between speedometer set A to set B seem to have accounted for the overall lower average response times and lower average error and miss percentage rates. However, other design differences, such as location of the speedometer on the instrument display panel, did not have such influential effects on the data. Comparing speedometers 1 and 4 (speedometer on left side) to speedometers 2 and 3 (speedometer on right side) only one small pattern was seen. Speedometers 2 and 3 had, compared to their respective speedometer set, slightly lower average response times than the other speedometer in the set. Whether or not this small difference in average response time can be attributed to the location of the speedometers on the instrument display panel was not possible.
Other major differences in design features within speedometer set B also seemed to have little effect on the data. Speedometer 4 had a different character color, location, numbering scale and character height than speedometer 3. However, the differences did not produce much stratification in the data. Speedometer 4 had slightly lower average error and miss percentage rates than speedometer 3 while speedometer 3 had slightly lower average response times than speedometer 4. From prior established patterns, the speedometer with larger character height (speedometer 4 in this case) should have had lower average response times and error and miss percentage rates. Although speedometer 4 did have lower error and miss percentage rates, it did not have lower average response times. Therefore, it is possible that the combined differences in speedometer design between speedometers 3 and 4 caused speedometer 4 to be slightly less legible than speedometer 3.
Speedometers 1 and 2 differed in location and radius size. However, the speedometers differed only slightly in average response times and error and miss percentage rates. Speedometer 2 consistently had the lowest average response time as well as having the lower average error and miss percentage rates (except in one instance). The effect of location, as discussed earlier, was not clear. However, the larger radius of speedometer 2 could of caused the small differences in the data. This inference continues the pattern concerning the size of speedometer parameters.
It is necessary to examine the effects of the test participant differences in order to find any discrepancies in the data. The relevant differences examined were sex, handedness, visibility (defined as the need of correctional lenses vs. no need), and distance from the projection screen. The first step in evaluating the effect of the test participant differences was to compare how each of the 4 modeled differences affected the average response times and total incorrect (errors and misses) responses. The discrepancies in average response times for most of the test participant differences were very small (less than 0.060 s). The same result occurred for the average incorrect responses. The sample size of test participants was also small, making almost all inferences impossible. However, the one individual difference that did stick out as possibly meaningful was sex. The average response times for males were, on average, 0.111 s faster than for females. Also, the average total incorrect responses for the males was 6.3 less than for females. One explanation for this difference can be attributed to the test sequence. Due to the 2 televisions, all the participants could see each other's average response times after every block, creating a small competition in which males are commonly known to be more aggressive and competitive. This would explain the reason for the 0.111 s lower male average response time.
In order to find the best speedometer of the 4 examined, it is required to first define which task condition is being used. For the task condition of reading the exact speed (4 choices), speedometer 2 was clearly the optimal speedometer. It consistently had the lowest average response time and average miss percentage rate. It had the lowest average error rate percentage one of the two 4 choice blocks.
For the task condition of "speeding" versus "non-speeding" (2 choices), the decision was not as simple. Although speedometer 2 had the lowest average response time, it had 6 errors. Speedometer 1 had a slightly larger average response time with 4 errors. However, the task performance measurement that becomes more important in the selection criteria for this task condition should be the average response time. This is because the most important task while driving is to have your eyes on the road. Whether you are mistaken in your knowledge of speeding or not speeding is generally not as important to your safety as the time you look away from the road. Therefore, the optimal speedometer for this task condition was also speedometer 2.
The relationships between speedometer design parameters, task condition and task performance were not obvious or clear given the collected data. However, some inferences can be drawn. For example, the larger the character height, character strokewidth and radius, no matter what the task condition, the more the task performance improved. This was shown in the stratification between speedometer sets A and B for all task conditions in the task performance measurements. This pattern was also visible when speedometers 1 and 2 were compared based on their radius size. Conclusions about the effects of location, character color and numbering scale were not possible.
Using the relationships found above as well as the data collected, the optimal speedometers for each task condition were found. Using low response time and incorrect response rate as the criteria, speedometer 2 was selected the optimal speedometer for both task conditions. One interesting point was the difference in importance between response time and incorrect response rate. This was concluded due to the natural consequence of not having your eyes on the road. The safety issues concerning the amount of time your eyes are off the road are generally more severe than incorrect knowledge of whether you are speeding.
Revisions in the experiment could prove to be very helpful. For example, due to the poor choice of speedometers, it was very difficult to assign the causes of differences in task performance to attributes of the speedometers. Speedometers with similar characteristics and one large difference should have been chosen for the experiment such that the process of assigning the causes of differences in the data could have been clearer. Also, a larger sample size of test participants as well as testing blocks should have been used to increase confidence in the inferences made. With these changes, the experiment could be much more effective in answering the objectives stated.

Anthropometry is a technique of measuring the human body in terms of dimensions, proportions, and ratios. Within the field of ergonomics, anthropometric data is extremely important in product design. Specifically, characteristics such as body size, strength, height, weight and range of motion are all considered when a product needs to be tailored to a particular user group. If no data is obtained or measurements are inaccurate, products may be designed in such a way that they are either unsafe or unpractical, which most likely would lead to a decrease in their sales. As a result, it is in the best interest of all companies to consider the appropriate anthropometric measurements when developing new products.
This report examines some of the human characteristics that are important when designing a stress ball for college students. In particular, 3 questions were examined:
One group of 12 participants, all of whom are currently enrolled in Ergonomics Laboratory 334 Section 003, was used for this study. Due to their class enrollment, all participants were volunteers and not-paid for their time.
Making up this group of participants were 8 women and 4 men who ranged in age from 20 to 22. Of the 11 right-handed and 1 left-handed participants, none claimed to have any hand injuries.
The test stimuli in this study were the protocols, or instructions for collecting data, that were used at each of the measurement stations. Since each participant had their height, weight, comfortable grip strength and maximum grip strength measured in this study, 4 different protocols were used. Each of the 4 protocols, plus a general introduction to the study, was written by the experimenter and all protocols were tested so they were as methodologically rigorous as possible. Summaries of each of the 4 protocols and the study introduction can be found in the Test Activities and Their Sequence section of this report.
The test equipment for this study included a Technasonic Weight Talker II scale, used to measure the weight of each participant, a model 0955 Invicta Plastics Limited standing anthropometer, used to measure the height of each participant and a model 5001 Grip A Takei Physical Fitness Test that was used to measure both the comfortable and maximum grip strength of each participant. In addition, a pencil and paper were used to record all participant data. No test software was used in this study.
On October 25, 2005 from approximately 6:02 PM to 6:50 PM anthropometric measurements were taken on 12 participants in room G699 IOE at the University of Michigan Ann Arbor.
The study began with the experimenter giving a brief introduction of the study to each participant. After attaining the participant's first name and thanking them for their time, the participant was told the study was being conducted by IOE Balls Models, a company that produces stress balls. They were also told that by participating in the study, which consisted of taking 4 anthropometric measurements, they would be helping IOE Balls Models to design the most effective stress ball possible. Finally, after the participant was told the study was risk free, the participant was given the chance to express their questions and concerns to the experimenter. It is important to note that throughout this introduction process, the experiment maintained a positive demeanor.
The first station the participant visited was the weight station, which held the Technasonic scale. After the scale was turned on by the experimenter, the participant was asked to "remove their shoes and any excess clothing". Next, after the experimenter tapped the start button on the scale with their foot, the participant was instructed to "step on the scale and to remain standing up straight and as still as possible until the scale verbally acknowledged their weight". After the experimenter recorded the weight value, the participant was told they could "step off the scale and collect their personal items".
The second station the participant visited was the height station, which held the Invicta Plastics standing anthropometer. After the green measurement disk was placed at the maximum height of 207 cm by the experimenter and the participant had again taken off their shoes, the participant was asked to "stand with their feet on the green platform, as straight and as still as possible, with their back against the anthropometer". The experimenter then lowered the green measuring disk so it touched the top of the participants head and recorded the height value to the closest tenth of a centimeter. Upon completion of this task, the participant collected their personal items.
Next, the participant visited the grip strength station which was equipped with a Takei grip dynamometer to measure their comfortable and maximum grip strength. Comfortable grip was measured first by means of 3 trials. For each trial, after the experimenter had reset the dynamometer to a value of 0 by turning the center knob counterclockwise, the participant was told to grip the device with their "writing hand" to hold the dynamometer "downward and approximately 6 in away from the body ". Next, the participant was told to "slowly squeeze the device until slight pressure was felt in the palm of their hand and at that point to release their grip and hand the device to the experimenter". The experimenter then recorded the strength value to the nearest quarter of a kg and reset the device for the next trial. After the 3 comfortable grip strength trails were completed, the participant's maximum grip strength was measured. This was done by having the participant hold the device in the same manner, but instead instructing them to "exert full force on the device by squeezing their hands together (as if they were juicing a lemon)". After allowing the participant to exert force for 3 seconds, which was thought to allow enough time for build up to maximum force, the dynamometer was returned to the experimenter who recorded the maximum strength value to the nearest quarter of a kg.
Finally, before the participant left the study site, the participant was asked to provide their age and that value, along with the sex of the participant was recorded by the experimenter.
Representative height, weight, comfortable and maximum grip strength values for the participants in terms of means and standard deviations are shown in Table 1. Values are shown in this format, because if one knows the mean and standard deviation of a characteristic, they can calculate percentiles values by using the appropriate z-score, which is important in product design. Table 1 also displays the means and standard deviations for the 4 measured characteristics for men and women participants separately. It is important to break the population into men and women subgroups, because in this case, women made up 2/3 of all participants which could skew the overall participant population statistics.
As can be seen from the table, the total participant population has a mean height of 165.6 cm, a mean weight of 68 kg, a mean comfortable grip strength of 10.25 kg and a mean maximum grip strength of 31.25 kg.
In terms of the differences between men and women, the 2 groups differ in each of the measurement categories. Looking at the mean values, it can be seen that the men participants in the study are an average of 9.6 cm taller and .5 kg heavier than the women participants. These findings support the idea that the average man is both taller and heavier than the average woman. Men participants also had a comfortable grip strength 1.25 kg larger than the women and had a maximum grip strength 8.25 kg larger than the women. Again, this data coincides with the idea that the average man is stronger than the average woman.
In order to determine the relationships between the anthropometric measures that were examined, the coefficient of determination was calculated for each possible measurement pair (6 total pairs). The coefficient of determination, or an R squared value, is an indicator of whether or not there is a good relationship between two variables. An R squared value of 1 indicates a perfect relationship while an R squared value of 0 indicates no relationship. Table 2 shows the R squared values for each measurement pair based off the data from all 12 participants.
As can be seen from above, the anthropometric measures examined in this study have very weak relationships with each other. The 2 strongest relationships are those between height and maximum grip strength with an R squared value of .46 and weight and maximum grip strength with an R squared value of .49. Since an R squared value of 1 means that two variables have a perfect relationship, these two relationships are only mediocre. In addition, the 2 weakest relationships are those between comfortable grip strength and maximum grip strength with an R squared value of .06 and weight and comfortable grip strength with an R squared value of .05. It is important to note that for this analysis, since the number of participants was small, the R squared values for the men and women subgroups were not examined. Furthermore, since all participants fell into an age range of only 2 years, age was not considered when determining the relationships between anthropometric measures.
In order to determine recommended dimensions for the stress ball, maximum and comfortable grip strengths of the participant population must be considered as well as hand breadth (which is the distance across the palm of a hand). Since a stress ball should be designed to comfortably fit in the hand of the maximum number of users, the 5th percentile woman measurement will be used for hand breadth. Since this particular measurement was not obtained during the study, the measurement was obtained from the Eastman Kodak Company and was found to be 6.8 cm. With respect to grip strength, the stress ball should be designed so the 50th percentile individual can compress the ball completely and the 5th percentile woman can grip it comfortably. By using the mean and standard deviation values found above, as well as the percentile formula (percentile = mean + z*standard deviation), the 50th percentile value for maximum grip strength was found to be 31.25 kg and the 5th percentile woman comfortable grip strength was found to be 3.17 kg. Thus, in order to appeal the largest number of users, the stress ball should be designed so that it is 6.8 cm in diameter and made of a material that starts to compress at 3.17 kg of strength but does not completely compress until 31.25 kg of strength have been applied.
After collecting anthropometric data from 12 participants, the data was analyzed in order to determine representative height, weight, comfortable and maximum grip strength measurements for young adults. From the data attained the mean values of these 4 measurement categories were 165.6 cm, 68 kg, 10.25 kg and 31.25 kg, respectively. Since these values are based of a small population, they most likely would change dramatically if more participants were measured.
Since it is important in product design to know the measurement differences between men and women users, participant data was separated by sex and reanalyzed. As can be expected, on average the men participants were taller, weighed more, and had higher comfortable and maximum grip strengths than women participants. However, since the number of participants was low, the difference between the values was reasonably small. In regards to weight, for example, the average man participant weighed only .5 kg more than the average woman, which is likely to be much less than the true difference in weight between men and women over a larger population.
In order to determine what kind of relationships existed between the tested anthropometric measures, R squared values were calculated for each of the measurement pairs. Since the 6 obtained R squared values ranged from .05-.49, all well below the perfect R squared value of 1, it can be stated that no strong relationships existed between any of the 4 measures. Since this conclusion does not mirror ideal data where values such as height and weight would have a higher R squared value, the error can be attributed to the fact that the number of participants was small and the data was likely skewed. Thus, if a larger participant population was tested, more complex relationships between height, weight, comfortable grip strength and maximum grip strength would likely be discovered.
In order make recommendations for the dimensions of the stress ball, both comfortable and maximum grip strength of the participant population had to be considered as well as hand size. Since this study did not measure the palm size of the participants, this data was obtained from an outside source. Next, the percentiles for which the different stress ball dimensions would be designed for were logically determined, and in this case were 5th percentile women for hand breadth, 5th percentile women for comfortable grip strength and 50th percentile men/women for maximum grip strength. These percentile values were calculated using the percentile formula and the known population means and standard deviations. The obtained values lead to the recommendation that the stress ball that have a diameter of 6.8 cm and be made of a material that starts to compress at 3.17 kg of strength but does not completely compress until 31.25 kg of strength had been applied. It should be noted that the recommended dimensions would generate a stress ball that would be best suited for the study participants, but not necessarily for the larger population. In order to determine those dimensions, a larger scale analysis would need to be completed.
The results of this study might be challenged on the basis that the population tested was small and not random (all participants enrolled in the same course). Also, the fact that women and men were not equal in number could have greatly skewed the overall participant population statistics and thus the stress ball design dimensions. In addition, the protocols for each of the stations could have lead to error, since participants may have interpreted the instructions differently. Finally, errors in manually reading the standing anthropometer and grip dynamometer, experimenter inconsistencies between participants, the fact the scale was not calibrated or checked for accuracy and individual participant differences could have lead to error as well.
In conclusion, anthropometric measurements are an important factor to consider when designing a product. By knowing mean values for the body size, strength, height, weight and range of motion of a target user group, companies can tailor product dimensions to best fit the group. Unfortunately, if these measurements are not taken into account, products may become either unsafe or hard to use, which of which are undesirable to consumers. As a result, in order to keep their products safe and useful for the broadest range of users, it is important for companies to design products using appropriate anthropometric data measures.

Rubberland Inc. is a producer of rubber liquid. They have been getting negative feedback from their customers who use the liquid rubber for the production of molded parts. Rubberland's customers lose, on average, thirty minutes per day of production. This time is lost in their mold process because Rubberland's customers have to adjust the temperature settings for every new-shipped batch of liquid rubber material. On average Rubberland's customers, lose $930 for each hour of lost production.
Process Engineers, organized by Rubberland management, found that the failures from their customers internal mod inspection process are due to parts not curing properly, scorch, parts being under filled, mold flash and flowlines. They also collected a variety of sample data that included 6 input variables and 7 output variables. Our six sigma team plans to identify which output variables are most important and then identify the key input variables that are most significant. We will then analyze the significant variables to provide recommendations for the process. These recommendations will reduce the problem with Rubberland's customers, thus reducing the amount of time and money lost to adjusting the temperature setting.
To find the current state of the process, we created a Pareto Chart to identify major failures. We also conducted a process capability test and measured correlation between variables.
Negative feedback from customers is a big concern at Rubberland. Our team has thus obtained a summary of the failures from the customer's internal mold inspection process. These failures are displayed in a Pareto Chart in Figure 1 below. The concern with the greatest frequency is "Parts not Curing Properly," and it accounts for 70.1% of all failures.
After brainstorming potential issues of the process, our team evaluated the performance of the process by collecting sample data and recording several material variables and process settings. We then conducted process capability tests on each of the output variables (Tensile strength, Elongation %, Type B tear strength, T10 time, VMAX, and CIR). Below in table 1, the Defects Per Million (DPM) values for each of the output variables are displayed. Table 1 shows that the largest DPM is for CIR.
The pp for CIR is greater than the ppk, and ppk approximately equals cpk, so there is a mean problem and excessive common cause variation. The ppk and cpk values for Tears are all about equal with values of around 0.48. Because the ppk and cpk values are equal, this is most likely due to a standard deviation problem.
After measuring process capability, we measured the correlation between the input variables and the correlation between the output variables. Figure 2 below shows the correlation of the input variables. Variables with p-values less than 0.05 are considered significant. Three variables, supplier, catwt, and catdisp had correlated p-values less than 0.05 and these are highlighted in Figure 2.
Figure 2 show that the supplier and the catalyst weight are negatively correlated. In addition, the supplier and catalyst dispensed are also negatively correlated. Lastly, Figure 2 shows that catalyst weight and catalyst dispensed are positivity correlated with a value of .978.
After observing the current state of the process by performing creating a Pareto chart, performing a process capability analysis, and measuring the correlation of input variables, we performed further analysis to try to determine the reason for failures in the mold inspection process.
The first step of our analysis was to develop a cause and effect diagram. Figure 3 shows the factors that cause customer dissatisfaction.
The five categories shown in the cause and effect diagram are material, personal, environment, methods and machines. As seen in Figure 3, the categories with the most causes for customer dissatisfaction are Material, Personal, and Machines.
After analyzing causes for customer dissatisfaction, we examined the input and output variables that lead to this dissatisfaction. To identify output variables with the most problems and inputs with the most significance, we ran regression models. We ran 7 different models, with each model having the 6 input variables and one of the 7 output variables. For each regression model, if a term was not significant (i.e. its p-value was not less than 0.05), we omitted it from the model and re-ran the regression. For the tensile strength, Tear "B1", Tear "B2", and VMAX we did not get any significant values. The p-values and regression equations for CIR, T10 time, and Elongation % are listed in the Table 2 below.
In these equations, the supplier is a binary variable. Supplier equals 1 when from supplier A and 0 when from supplier B. The linear regression equation for CIR shows that the input variables, supplier and catalyst weight (catwt), affect the output variable CIR. Similarly, for T10, the supplier and catwt variables affect its output, and for Elong, the crosslinker variable affects its output. Even though Elong and T10 had significant input variables, they did not have a large DPM and therefore additional analysis was not conducted on them.
After determining what the key input variables for CIR were (supplier and catalyst weight), we then examined how they affected the output of CIR. To do this we created a box plot, with CIR as the Y variable, and supplier and catwt as X variables. The results are shown in Figure 4 below.
Figure 4 shows that as the catalyst weight increases so does the CIR. In addition, the box plot shows that at the same catalyst weights, supplier A has lower CIR values then B. To analyze these variable affects further, we created a fitted line plot for CIR vs. supplier and CIR vs. catalyst weight. These plots are shown in Figure 5.
When we ran a box plot for T10 as the Y variable, and supplier and catwt as X variables, we achieved the same results as for CIR. The T10 value increased for increasing values of catalyst weight, and values for Supplier B were slightly higher than for supplier A.
From section 2.2 Process Capability Analysis, we found that Tear "B1" and Tear "B2", which both measure the same rubber sample, have high, but different DPM . We then performed a paired t test to see if the different methods for measuring the tear strength differ. Tear "B1" and Tear "B2" both measure the same parts. The results are shown in Figure 6.
From Figure 6 we can conclude that the tear measurement methods are not statistically different. The p-value is greater the .05 and the 95% CI for the mean difference between the measurement methods contains 0. Lastly, from Figure 6 we were able to see that the standard deviations for tear strengths are high with a value of 40.59 for Tear "B1" and 40.34 for Tear "B2".
Currently, Rubberland measures 6 input variables and 7 output variables. From our process capability analysis, we concluded that the output variable with the most problems was CIR. We then ran a linear regression model to determine which of the 6 input variables significantly affected CIR. From that regression model, we found that supplier and catalyst weight were both significant.
Further analysis on these input variables showed that as catalyst weight increased, both CIR and T10 increased, and supplier B had larger values than supplier A. The specification limit for CIR is 140 +/-5. The catalyst weight is targeted at 0.2%, however if this weight increases above 0.21%, the CIR reaches values greater than 145 and out of the specification limit. This is shown in a contingency table in Figure 7 below.
Failures are caused if the CIR value increases over 145, and this occurs when the catalyst weight is 0.21% or above. From the correlation analysis conducted in section 2.3, we saw that the amount of catalyst dispensed (catdisp) was 97.8% correlated to catwt. Because the amount of catalyst that an operator dispenses affects the weight, Rubberland should thus focus on fixing the amount of catalyst dispensed. We ran a contingency table of CIR defects vs. catdisp and found that 31.8% of defects started occurring when the catdisp was 0.20%. We therefore recommend that Rubberland have operators dispense a catalyst between 0.16% and 0.2% only. If they dispense more than 0.20%, they should remove some of the catalyst or consider the part a defect and not give it to customers. We also recommend that Rubberland perform additional analysis to see how small the lower spec limit can be for catalysis dispensed.
Figure 8, below, is a contingency table of CIR defects vs. Suppliers. From this figure, you can see that all of the CIR defects occur when using supplier B. We therefore recommend that Rubberland reduce the amount of orders from supplier B and order more from supplier A instead.
From regression analysis for tear strength, we found that no input variables significantly affect the tear strength. From this result, we recommend further studies to see if the noise can be reduced so Rubberland can see what input variables affect the tear strength. In paired t-test analysis, we found that the two measurement methods, Tear "B1" and Tear "B2" are not statistically different, but both have high standard deviations for tear strengths measured. We recommend looking into methods to reduce the variance for tear strengths. We also suggest only using one measurement method because they are correlated and not statically different. This will save time, which will also save money.
The linear regression analysis for CIR showed that the only two variables that were significant were catwt and supplier. The other variables, mixtime, crosslinker, and inhibitor were not significant. We modeled fitted line plots for each of these non-significant variables against CIR, and we found that all three plots had R-Squared values of 1.4% or less. An example of mixtime vs. CIR is shown in Figure 9 below.
The low R-squared values mean that the values of the variables, mixtime, crosslinker, and inhibitor do not have an affect on the output of CIR for the given specification range. We thus recommend that Rubberland can increase the specification limits for mixtime, crosslinker, and inhibitor. Rubberland should conduct further analysis to see how CIR reacts out of the given current specification limits, and will then most likely be able to increase this range.
Rubberland currently measures 7 output variables. CIR, Tear B1, Tear B2, and VMAX all have large DPM, and thus Rubberland should still measure these variables and make sure they are within specification limits. The T10 output variable did not have large DPM and it is correlated to CIR with a value of .887, so therefore, Rubberland does not need to measure the output of T10 anymore. DPM was not high for Elong or Tensile strength either, so we also recommend that Rubberland not focus its output studies on these variables either.
For Rubberland's input measurement variables, we recommend not measuring the catalyst weight to save time and money. We recommend this because the catalyst weight has a correlation value of .978 so it is highly correlated to the catalyst dispense.
To maintain our improvement recommendations we have several suggestions. We suggest Rubberland to develop better training for the employees that are involved in the production process. To supplement the training we recommend creating work instructions.
For filling the catalyst in the jar, we recommend that they come up with a better device to control the catalyst dispense so the operators cannot add any more then 0.2%. With this control, if the operator tries adding more then 0.2%, the device will stop the dispensing of the catalyst and thus the part will not have a chance to be a defect.
In our analysis and improve section, we found that defects for CIR occurred when the material came from Supplier B. However, there were also times that when supplier B material was used, the part was not a defect. Due to capacity constraints at suppliers, it may not be possible to not order from supplier B at all, so we recommend that Rubberland and Supplier B conduct a study to figure out why some parts from supplier B are defects.
In our recommendation section, we recommended that Rubberland could reduce the amount of output variables they measure and increase some of the specification limits of the input variables. If Rubberland decides to do this, they should conduct follow up studies to make sure that their new specification limits and removal of certain measured output variables do not produce more defects.
Lastly, we recommend analysis for why there are so many wrong materials, which are shown in the cause and effect diagram in section 3.1.

The objective of this experiment was to observe the human-machine system and the work methods utilized in a simulated bagel retail setting in order to identify wasted motions and make a proposal for more efficient operations based on this analysis. In the observed operation, the employee (main test subject) would be taking orders, selecting the bagel, cutting the bagel, preparing it with cream cheese at the customer's request, and delivering it to the customer. In this simulation, currency was not directly exchanged, though the customers placed an imitation currency on the table when placing orders, which served functionally as a consent form for video taping the experiment.
Although the main components of this study were the actual motions utilized by each hand of the test subject, the results of these motions were to be analyzed in the context of the entire human-machine system in order to determine the effects of the motions. Thus, it is important to be aware of the structure of the human-machine system for this experiment.
In order to obtain a clear understanding of the human-machine system, it is important to break down the specific attributes of each of the main categories in the system for this particular experiment, which is performed in the following table (Table 1.)
Take the order from the customer, in which he / she requests one of 3 different types of bagel; retrieve this bagel; cut the bagel into 2 halves; ask customer if he / she would like cream cheese, and if so, whether prefer regular or strawberry; load this type of cream cheese on the butter knife and apply this cheese to bagel; put halves together and present to customer.
1.) Table 1 (right table on depiction), which was used for storing the bagels, was 34" high, had a rectangular surface (48" width x 30" length,) and was flush against the wall on the right.
2.) Table 2 (left table on depiction), which was used for cutting and applying cream cheese, was 29" high, had a rectangular surface (60" width x 18" length,) and was located 58" from the wall on the right.
3.) Three knives were used in the operation: cutting knife size was comparable to U.S. standard kitchen knife; two butter knives (1 for each type of cream cheese) were used, each one of standard U.S. kitchen knife size.
4.) Two containers of different types of cream cheese were utilized, each one of approx 3" in diameter, 2" in height.
5.) Paper sheets (approx 8" x 11") were utilized for presentation to customers; sheets were stored in box that dispensed them (one at a time.)
6.) Three grocery bags (12"w x 7"l x 14"h) containing three different kinds of bagels were located on Table 1, situated 5" from both x-axis table borders and 11.5" from both y-axis borders, with 1" separating each bag.
1.) Employee must fall within height range that allows for large enough work envelope to use appendages comfortably and without strain on table surfaces.
2.) Sufficient wrist strength to cut through multiple bagels (approx 2-3 per minute.)
3.) Sufficient dexterity to hold bagel and cut; hold cream cheese container and load cream cheese onto knife; hold bagel and spread cream cheese on bagel.
4.) Ability to hear orders and feedback from customer.
5.) Ability to see and distinguish types of bagels and cream cheese; see and determine the center of the bagel in order to make proper cut.
6.) Cognitive awareness to maintain quality, safety, productivity and cordiality amidst potential customer cue buildup and complaints (this skill may be more prevalent in employees with experience in food service industry.)
1.) Although there is some customer feedback designed into the operation (selection of bagel and cream cheese type), excessive feedback is a potential attribute that is unpredictable and may cause delays in the operations.
Bagels should be produced at the rate of at least 3 bagels / minute.
It is important to understand several aspects about Table 1 as it relates to this particular experiment. With regard to the human attributes, the test subject in this experiment was a male, 6'2" and 195 lbs, with some experience in food service industry. With regard to environmental factors, it is important to note that although in the long run, attributes such as temperature and humidity will be taken into account with regard to bagel storage and freshness, because this analysis has a smaller timescale scope, and is focused on the operations of bagel preparation, these attributes will not play a role in the analysis.
Below is a graphical representation of the human-machine system:
The test subject was instructed to take orders from the customers and perform the tasks described above. No specific goal-oriented instructions were given, as whether to focus primarily on delivering quality bagels with cream cheese spread evenly and plentifully, to deliver bagels as fast as possible, to perform all actions with as little risk for safety violations as possible, or whether to be courteous with customers to the point of striking up conversation. Therefore, it was assumed that the test subject would balance the goals of quality, productivity, safety, and cordiality evenly and as came natural.
Multiple cycles of this activity were observed, and six cycles of this activity were video recorded for further motion analysis.1
First, it is important to view the data recorded from the motion analysis. The first table (Table 2) below lists all of the specific elements performed by each hand. This table expounds upon the actual Therbligs themselves, providing a detailed description of the motions. The second table below (Table 3) is a more succinct listing of Table 1, with just the Therbligs listed for each hand, as well as the change in time for each motion rather than the running time on the y-axis.
Figure 3 represents a simultaneous motion chart for each hands' motions. Each different shaded segment of the bar graph coincides with the motions listed on the tables above (tables 2 and 3), and the larger segments represent motions that took a longer period of time.
After looking at the detailed data within each cycle, it is important to analyze the time segments for the major portions of the job task from all six cycles to determine data on time averages and variance. The following table lists time data for each of the major tasks for each of the cycles, as well as cumulative times for each cycle.
Based on these six cycles, the mean length of each cycle was 36.0 seconds and the standard deviation was 4.82 seconds, indicating that there was notable variation. Further, although the final two cycles showed some evidence of a learning curve, the remaining data reflected a somewhat random distribution of the results. It appears that the largest sources of variation were in the very first step, in which the test subject took the order, grabbed the paper, and positioned the bagel, and there were secondary sources of variation observed in the cutting and cheese-spreading steps.
On the following chart, the time that was spent on major tasks for the six cycles are listed in terms of time spent.
The following figure (Figure 5) analyzes this data in terms of maximums, minimums, and averages.
In order to propose a more efficient layout and operations procedure, it is important to isolate wasted motions. According to Nieble and Freivalds (2003, as cited in Armstrong, 3-2, Methods Analysis and Ergonomic Work Enhancements), non-value added elements for this operation included "position,"
Because the "position" step took place in order to increase the safety factor during the cutting process, this step should not be compromised in any way. In a similar fashion, the "hold" steps listed in the table were integral in presenting the bagel, and this fact combined with the fact that their sum only amounted to approximately 1-2 seconds, indicates that these are not where the revision efforts should be focused.
Where there was considerable potential for time saving was in the initial steps of selecting the bagel, the process of spreading the cream cheese, and the process of cutting the bagel. The greatest variance in the major steps analyzed in Table 4 and Figures 4 and 5 were noted in the first step of selecting the bagel, and was most likely due to the timing of the customer's request. Nonetheless, there was room for improvement for this step. If the bagels were taken out of their bags before the entire operation began and placed in containers that would fit on top of the Table 2, it would eliminate the need for a Table 1 and should reduce the time in this first step (prior to cutting the bagel) by approximately 2 seconds. In addition to the time saved by the physical turning around and selecting, the process should be easier on the employee, saving him / her kinetic energy and reducing head motion, thereby increasing mental focus.
By far, the longest time was spent loading the knife with cream cheese and spreading it on the bagel. It was noted that of the six cycles, three times there were two trips made to the cream cheese bin to load the knife. However, this extra trip did not necessarily increase the time spent, as one of the one-trip cycles still had a total loading / spreading time of 16 seconds, which was the second most amount of time recorded for this step. It can be argued that the subject spent abnormally more time loading the knife before the one-trip cycle than the two-trip cycles, and also that he may have spent more time spreading due to having less product with which to work. Nonetheless, it is suggested that steps be removed from this process to save time. It will still take 2.03 seconds to load the cheese, but rather than spend the average of 13.7 seconds to spread the cheese on the bagel, it is recommended that the cheese be spread on the bagel with one motion, which occurred in 0.6 seconds. This should allow for an average savings of 13.1 seconds for this step. Obviously, this will result in not much of a spread at all, and in order to maintain a high degree of quality and customer satisfaction, it is recommended that a plastic knife be served with the bagel in order that the customer can spread the cheese him / herself. It is estimated that it will take the employee an additional 1 second to obtain this item and give to the customer at the end of the transaction; thus net savings on this revision should be 12.1 seconds. It should be noted that such a significant time savings should offset the variable cost of the plastic knives.
The second longest major job task was cutting the bagel (7.83 seconds on average.) A simple way to reduce this time would be to use a sharper knife, which would eliminate the second run that the subject performed on every cycle to complete the cut. Thus, using the observed cycle as a model once again, the subject will still spend 2.30 seconds for his cut. This is a conservative estimate due to the fact that it takes into account 4 observed oscillations still within that first cut, some of which may be eliminated with the use of a sharper knife. The cut should be complete after this cut, saving a total of 4.31 seconds on this run. It should be noted that eliminating this step would also eliminate a fairly major safety hazard observed during the operation. In between the two cuts, the test subject used his hand holding the knife (right hand), while still holding the knife, to rotate the bagel. In the event that the proposed change does not go into effect, it is suggested that two steps be added in which he puts the knife down to rotate the bagel and then picks the knife back up when finished with the rotation. However, this change would not be necessary under the new operational format.
In sum, with the new layout and operations changes, a total of (2.0 + 12.1 + 4.3) = 18.4 seconds should be saved, thereby reducing the average total time in half from 36 seconds to 17.6 seconds. This change translates into a production change from less than 2 bagels per minute to nearly 3.5 bagels per minute. It also reduces safety hazards, makes the workplace layout more comfortable for the employee, and should maintain a high level of quality.

During presentations or work with a computer, it is often difficult to read characters on the display when other light sources such as a fluorescent lamp and sunlight are present in the room and thus affect the contrast of the display. However, when lights are completely eliminated (except the light from the display itself) by drawing a curtain or so, people sometimes frown in the direct light from the display, especially when the display character is bright itself over a dark background. A previous study showed that the increase in contrast threshold (%) was smallest when a background and surround were at equal luminance, and the contrast threshold increased rapidly when the surround became brighter, while it increased slightly when the surround became darker (Ireland et al, 1967). The illegibility caused by luminance discord not only disturbs visibility, but also distracts audiences' or users' attentions. To provide the best legibility for normal display viewers in order to achieve a successful presentation or undisturbed work, the best surround luminance was found out from an experiment for both displays with black characters on a white background (White BG) and white characters on a black background (Black BG). Also, outcomes were compared with the previous study to assess the value of experimental results.
Total 5 volunteers whose average age was 26.2 years (range 23 to 29) participated in the experiment. Four of them were males wearing glasses (myopia) whose visual acuity ranged from 0.9 to 1.2 with glasses on, and one female subject who wore no glasses had visual acuity 0.9. A laptop, AMILO D 7820, made by Fujitsu Siemens Computers, was used to present a letter "E" in the center of 9" height ×12" width (image size) LCD display, played by Microsoft PowerPoint version 10.0. Two PowerPoint files for Black BG and White BG were made with their first slides showing only background color in order to let subjects adapt their eyes between trials and while surrounding brightness was being set. From second slides, a letter "E" (Arial, not bold, not italic, no underlined) was placed in the center with its font size in an ascending order from 4 to 23 (total 21 slides each) and the slide number on the low right corner. Both files were programmed to automatically switch to the next slide after 5 seconds except for the first slides which were operated by a space bar. The experiment was conducted in a photo dark room in order to eliminate surround luminance variation. Luminance was measured by light meters, L-508 CINE 700M MASTER, made by SEKONIC. The display luminance was 0.3 ft-L for a black background, 1.0 ft-L for a black character and 21 ft-L for a white character/background. Surround luminance was 0 ft-L with no light except the LCD, 0.83 ft-L with a distant lamp on and white surround, 14 ft-L with a close lamp on and brown surround, 20 ft-L with the close lamp on and dark khaki surround, and 65 ft-L with the close lamp on and white surround. The lamps were aimed at the surround of the LCD. Subjects were seated with their eyes 12 ft distant from the computer display. The monitor was set vertical and its center was at the same height as the subjects' eyes as shown in Figure 1.
A subject was informed that the experiment was to examine a legibility of a display and it had two sessions (Black BG and White BG), 5 surrounding conditions for each session, and 3 trials for each surrounding condition, with 30 seconds rest between sessions. Three trials under surround luminance 0 ft-L were conducted first, followed by 0.83, 65, 14, and 20 ft-L for Black BG, and the opposite order (20 ft-L → ...→ 0 ft-L) for White BG. Before starting an each session, an instruction was given to subjects; "Say 'Yes.' only if you are absolutely sure that you exactly distinguish the displayed character." The slide number at which the subject said Yes was recorded before the slide show was stopped and the file was restarted in the slide show mode for next trial from the first slide. A post-experiment interview was done for each subject about difficulties of reading a character in various conditions.
The minimum font sizes for distinguishing a character "E "under various surround luminance conditions and Black BG/White BG are shown in Figure 2. Since different displays would have different characteristics, the minimum font sizes cannot be absolute. Minimum font sizes were consistently smaller for White BG than Black BG. For White BG, the minimum font size was smallest at the surround luminance of 20 ft-L where the surround was as bright as the background, which is consistent with the previous study (Ireland et al, 1967) even though a different luminance range and a different independent variable were used. For Black BG, however, the minimum font size was smallest when sufficient light (14-65 ft-L) existed, not when the background and surround were at equal luminance. The illegibility when both surround and background were dark (0-0.3 ft-L) can be explained by spherical aberration caused by a big pupil size and it was supported by the interview in which subjects reported that characters looked blurred and were hard to distinguish. It was left to be discovered whether a too bright surround (above 65 ft-L) degrades legibility for Black BG. The illegibility when the surround was much brighter than the background can be explained by diffraction or lack of light stimuli caused by a small pupil size. In conclusion, White BG is better than Black BG in terms of legibility, and White BG works better under the surround luminance as equal as background luminance. If Black BG is used, a proper amount of light should be provided for legibility.
Figure 2 Minimum required font size and surrounding luminance (Dot line: Black BG, Rigid line: White BG)

The worker who is 1.9 m tall and weighs 85 kg is standing with two legs and lifting the 20 kg box with two hands. Symmetric force distribution in two arms was assumed. It was assumed that the box weight was applied at the distal end of lower arm and hand link. The worker's posture is as shown in Figure 1, the feet flat on the floor.
It was supposed that the individual was flexed his shoulder to lift their upper arm to a horizontal position (Angle S = 0) as well as the elbow extension, as the shoulder flexes, so that it was horizontal in the end posture (Angle E = 0). The relationship of the two angles was 7:1 shoulder flexion/elbow extension. The equations used for shoulder and L5/S1 moment calculation are as follows. Resultant forces at elbow and shoulder did not change as elbow and shoulder were extended or flexed. The box mass of 20 kg was assumed.
Shoulder moment = (length of upper arm * resultant force at elbow +COM of upper arm * weight of upper arm) * cos(Angle S)+Elbow moment
Table 6 shows the values of shoulder and L5/S1 moment values calculated for 7 equally spaced intervals through the motion (i.e. at each 10 degrees of shoulder flexion), and the data is plotted in Figure 10. The box mass was assumed 29 kg. Note that the plots in Figure 10 show the absolute values of the moments, not resultant moments. (The resultant moments were all negative values of the same amount of corresponding value).
In calculation of moments and forces in section 1, the moment was discovered to change depending on the segment weights, segment lengths, the posture and the box weight. It was shown that not only the box weight but also the body weight affected a great amount of the force and moment for each joint and the compression and shear force at L5/S1. From the calculation of the resultant force and moment at L5/S1 in section 1, it was shown that the length and the weight of L5/S1 to shoulder resulted in the great moment at L5/S1. Also, from section 2, L5/S1 disc compression and shear forces, it was shown that the moment created by the upper body weight at L5/S1 is more than a half of that by the box, as follows.
As for posture, as shown in the calculation of moments of shoulder and L5/S1 for different angles of elbow / shoulder in section 6 (Moment of Shoulder and L5/S1 when being Flexed), the posture played an important role on the moment for each joint. It implies that the posture with extremities widely spread out to the horizontal direction might cause bigger moments at joints. In the same sense, moment is expected to be smaller for the posture with body segments closer to the center of the body in the horizontal direction. Not to mention, the moment for each joint would become smaller with the box closer to the body. The moment value will vary if the box weight is applied in different position on hands or lower arm. Therefore, if it was assumed that the worker was holding the object by embracing it in his or her arm rather than holding it with the tip of the lower arm and hand link, the value of the moments at joints would be smaller than those calculated in this report.
It was shown in Figure 10 that extremities and the box's larger deviation from the body caused greater moment created at joints. The shape of two curves in Figure 10 suggests a greater slop in changes of moments in the range of greater angle (-70 ~ -50º) than in the range of smaller angle (-20 ~ 0º), which approximately follows the cosine curve. It can be explained by considering the amount of the extremities' deviation from the body in the horizontal axis, which can be described with cosine functions of the angle. From the elbow moment equation used in calculation as follows, for example, the length, the center of mass, and the weight of lower arm and hand and the box weight were constant, while Angle E was the only variable, affecting elbow moment in the cosine function.
Elbow moment = (length of lower arm & hand * box weight / 2 + COM of lower arm & hand * weight of lower arm & hand) * cos(Angle E)
Overhead exertions (which are necessarily close to the body in the sagittal plane), however, are perceived as stressful, despite generating relatively lower moment values at the shoulder. It can be explained by physiological phenomena that the muscle force varies depending on the length of the muscle as well as the length of the moment arm. Even though loads handled close to the body typically produce lower joint moment values, as muscle length is not optimized, the force capacity that the muscle can produce is reduced. Since overhead exertions are likely to cause upper arm flexion and lower arm extension, their muscles are often in the situation where they are contracting or stretched, which is not in an optimized state. To produce the same amount of force (which was required with the optimized muscle length), it can be perceived as stressful. Muscle moment arm is expected to have the similar effect, too. As the moment arm becomes shorter, the greater force is required from the muscle, which can be stressful.
With the same position but different mass of the box, it was shown from Table 2 and Table 3 that, with a heavier box, the force and moment for each joint and the compression and shear force at L5/S1 were greater. It implies that lifting a heavy object can load excessive compression force which can lead to an injury. In this view, NIOSH established a recommended limit of the weight of the object lifted by a person, as called as action limit, in 1991. The action limit is 3400 N of the compression force at L5/S1. The maximum box mass not exceeding the NIOSH action limit was calculated in section 5 (Maximum Box Mass according to NIOSH). As shown in Figure 9, the maximum box mass depends on the length of the erector spinae moment arm when the posture and the body segments' weights and lengths were same. The longer the erector spinae moment arm, the greater the maximum acceptable box mass, which was predicted from the equation of F muscle as a function of E, as below.
To decrease F comp, F muscle is to be decreased. For F muscle to be decreased, from the following equation, E is to be increased.
With only 3 cm variation in the erector spinae moment arm, the maximum box mass varied from 4.7 to 15.8 kg. It emphasizes the importance of the effect of the length of moment arm.
Also, with different box mass, the center of mass of the body and the box was located differently. As shown in Table 4, the centers of mass of the body and the box for both 20 kg box and 30 kg box were located over the foot. However, within the foot, the center of mass was located nearer to the center of the foot with 20 kg box. Therefore, it can be said that the individual could achieve a balance to stand with the box for both cases. However, since the center of mass lied more toward the toe for the case of 30 kg box, it can be said that the balance of standing was less stable with 30 kg box (or more stable with 20 kg box). Even though not calculated in this report, it is possible that, with flexing upper arm and extending lower arm, the distance from the box and arms to the body increases so that the center of mass does not lie over the foot any more. This is more likely to happen with heavier box. From the center of mass in the vertical axis, the center of mass was lower with 30 kg box. It can imply that, in case when the individual was trapped by a low bump, it would be more stable with 30 kg box (heavier box). However, it cannot be true that the heavier the box is, the more stable from being trapped, when the height at which the box is held increases more than or to the point where increasing box mass only raises the vertical position of the center of mass.
Further analysis about the effect of the horizontal force component such as leaning or being supported would be interesting, because those horizontal force components would sometimes help reducing the moments at joints, and other times increase the moments at joints.
Also, in this report, analysis was done in only 2D scale. Thus, no z-axis component was considered. Based on the analysis done in this 2D scale, it is expected that, the greater the width of the body (length in z-axis), the greater the moments at joints, as the greater the deviation from the body, the greater the moments at joints.

Materiel Services Center (MSC) plays a pivotal role in the University of Michigan Hospitals and Health Centers' (UMHHC) mission to provide excellence in patient care by meeting the medical and surgical supply needs of the clinical staff. MSC manages a 9,000 square foot warehouse where an inventory value of approximately $1.35 million is maintained and represents an inventory turnover of 24 times per year.
Materiel Services Center's periodic automatic replenishment (PAR) stocking services are utilized by approximately 210 customers within University Hospital (UH), Taubman Center (TC), Comprehensive Cancer Center (CCC) and C. S. Mott Children's and Women's Hospitals. With the imminent closing of M-Stores and the opening of the Rachel Upjohn Building, Cardiovascular Center, and the new Mott Children's Hospital and Women's Hospital, Materiel Services Center would like to re-evaluate their PAR stocking formula to ensure adequate stocking levels. Every customer is stocked on a daily basis by a Materiel Services Center stockkeeper from the warehouse on Level B2 of UH. Despite being stocked daily, some units are placing a high number of calls to MSC for depleted items. MSC responds by creating a Materiel Service Requisition (MSR) or supplemental order. An evaluation of the PAR stocking levels has been requested to reduce supplemental order requests in an effort to improve MSC efficiency and customer service.
The objective of this project is to determine the root causes for MSC's supplemental orders from units that are PAR stocked daily and to make recommendations for reducing the number of supplemental orders and improving overall PAR levels.
Materiel Services Center currently employs sixty-nine stockkeepers and runners who are responsible for PAR stocking services and filling supplemental and custom requisition orders. All carts are PAR stocked daily during first or second shift. The supplemental is that supplemental orders process requires that orders are delivered within 60 minutes of being requested 24 hours a day. If an ordered is placed stat, it is delivered within 15 minutes. Based on supplemental order data from 11/1/05 to 10/31/06, MSC fills between 67 and 576 lines per day, a daily average of 172 lines.
Interviews were conducted with MSC management and staff to gain an understanding of the current state of their operations. Process flow maps of the supplemental order process and PAR stocking process were created and validated by Materiel Services Center's employees and management (Appendix B and C). Interviews were also conducted on the receiving end of the supplemental order process with select units to validate anecdotal stories as well as to understand potential causes for supplemental orders. A fishbone diagram illustrating these causes may be reviewed in Appendix D. The fishbone diagram led to the creation of a taxonomy of supplemental order root causes, which subsequently became the basis for further observational study and data analysis.
Supplemental data from M-Pathways' Financial Operational Data Store (FinODS) was analyzed over the time period November 1, 2005 to October 31, 2006. The conclusions drawn from the data stratification led to process studies of Materiel Services Center's customer service representatives (MSC CSR) and to meetings with MSC top supplemental order customers.
A statistical analysis of the current PAR level equation was conducted to determine if it provides a sufficient level of stock to meet the demands of each unit.
The top 10 MSR customers by lines remained consistently the same when data was evaluated on a quarterly basis. The graph below summarizes the top 10 MSR customers by lines. (Lines are defined as the number of times an item is ordered by a unit regardless of the item quantity.)
The graph below illustrates the top 25 items requested from MSC through supplemental orders by quantity and their corresponding total cost.
The following three graphs show the total number of lines shipped by month, day of week, and hour of day all for the same time period, 11/1/05 through 10/31/06. Lines by month showed only slight variation and supplemental orders peak on Tuesday. Approximately 55% of all supplemental orders are placed between the hours of 8:00 AM and 2:00 P.M.
Further analysis of two of Materiel Services Center's highest supplemental callers, UMH 5DS (a surgical intensive care unit) and Mott Pod B/C (a pediatric intensive care unit), yielded the following:
Only three months of data was used when examining supplemental orders for each unit. Interviews with nursing staff from these units revealed that any unusual item usage prior to this time frame resulted in lack of recollection for any anomalies in the data. An interesting distinction between UMH 5DS and Mott Pod B/C is that Mott Pod B/C employs an "equipment technician", whose job responsibilities include reviewing the stock levels of the nutrition room and supply room for any stockouts and calling MSC as supplemental items are needed.
Nurses from both units commented that they were not sure what some of these items were based on the descriptions listed above. Another shared comment between these two units was that many of their supplemental orders were due to their highly variant patient populations and that they were requesting items to meet a particular patient's needs that they do not normally stock in their unit.
After a review of the supplemental order data in FinODS, a report was created detailing where supplemental orders were delivered and which department was charged. As a result of the manual process used by MSC CSR, billing errors became evident and they have been quantified at approximately $50,000 or 30% in of the lines in the billing report over the 12 month period between 11/1/05 and 10/31/06. The delivery location ("ShipCust" in the raw data set) and the six digit department ID number billed ("DistType") are both manually entered into FinODS which allows for typos and other errors. Appendix E provides a detailed report of the billing that occurred during this time frame. Journal entries to correct these billing errors are time consuming and costly as well as another manual process. All further analysis using this data set assumed the delivery location ("ShipCust") to be the most accurate field, as advised by the warehouse manager in Materiel Services Center.
The PAR level equation was assessed to validate whether or not it is generating a sufficient stock level for each (unit, item) combination. The current equation M-Pathways uses to generate PAR is:
where Average is the average monthly demand, Divisor is a factor corresponding to the number of times per week that the unit is stocked, and the 1.4 constant creates a safety stock of 40%. The average monthly demand is calculated as the summation of daily quantity received for an item over a year divided by twelve. Daily quantity is defined as the total amount stocked by the stockkeeper and any additional volume brought to the unit via supplemental orders. However, credits issued to a unit are not taken into consideration in this calculation, which could result in slightly inflated PAR levels. This will be rectified in early 2007, when Materiel Picking Feedback (MPF) will be introduced into the PAR calculation software by MAIS. This update will take credits into account when calculating PAR level for a (unit, item) combination.
To reduce the likelihood of stockouts, the variance of the item's daily demand should be taken into account in the PAR level equation. However, the equation currently does not do so and sets all (unit, item) combination PAR levels to be 1.4μ, where μ is the item's average daily demand (Average/Divisor). This results in a different stockout probability for each (unit, item) combination that can be approximated by using supplemental order data. A proxy for the stockout probability
By examining the PAR stocking and supplemental order data, the assumption that daily demand is normally distributed with mean, μ. Under this assumption, the optimal stocking level for a (unit, item) combination should satisfy the equation:
where μ is the item's mean demand, σ is the standard deviation of item demand, and z(α) denotes the one-sided z-statistic of the standard normal distribution corresponding to the probability of not stocking out of the item, α.
Since both Equation 1 and 2 calculate an item's daily PAR level, they can be set equal to each other. Solving for σ yields
Since it is very hard to place an exact value on the cost of stocking out, a common alternative is to use a service level in its place. Therefore, MSC can choose a Type 1 service level, the probability of not stocking out of a (unit, item) combination, that they want to achieve. When setting their service level, MSC should consider the cost of the item versus the labor cost required to deliver the item if ordered as a supplement, the cost of obsolescence if the item is left sitting on the cart, and the opportunity cost of the space that each additional item would consume. This Type 1 service level is equal to α and can be substituted into Equation 4. For example, if MSC decided upon a 99% service level, the PAR Equation would look like
This new equation will increase the PAR level for those items that have stocked out the most, causing both the number of stockouts and supplemental orders to decrease. This calculation is shown in detail in Appendix F.
Fortunately, out of the over 22,000 (unit, item) combinations, less than 500 have a probability of stock-out of 4% or greater. Please refer to Appendix G for a sample of 534 (unit, item) combinations that were stocked out at least 14 times (or 3.84% of the time) during the three-month period selected. New PAR levels for these (unit, item) combinations have been calculated.
Through observational studies and interviews, several categories of reasons for supplemental orders were identified. A taxonomy consisting of these categories was created as illustrated in Table 1.
While a few of the "whys" are stochastic random variables and out of MSC control, many are within their capacity to influence. A preliminary study of the degree to which the stockkeppers accurately inventory and stock the units was undertaken using a randomly selected nursing unit and PAR stocking order data from FinODS. However, due to the limited resources available for this project, these potential causes were not investigated long enough for any sound conclusions to be drawn. Nevertheless, by reviewing the five 'w's -- who, what, when, where, -- and how of the PAR stocking process, MSC management can determine if this is truly a contributor to the supplemental order problem.
This reason looks at why a nurse cannot find the item on his/her cart, despite the item actually being there, or after the supplemental order arrives to their unit.
After interviewing several nursing units, the reoccurring theme among all of them was the difference in nomenclature between Materiel Services Center and Nursing. Attached to each cart is a PAR list containing each item's quantity and location. However, this document lists the items by what MSC calls the item and not how the item is referred to by nursing staff. For example, the item nursing staff refers to as a cotton swab is called an "applicator, cotton tip 6in nonsterile" by Materiel Services Center.
Additionally, nursing staff have confirmed calling for supplemental items without looking in the stock room first or after taking only a cursory look. Also, clerks do not always know who ordered a supplemental item and therefore it never reaches the person who requested the item. Consequently, this often generates further supplemental calls until the item is received by the person who ordered it.
This dimension addresses supplemental orders that are a result of problems related to the pre-determined item inventory level on a cart.
PAR levels are only recalculated on an as requested basis by the PAR Maintenance Staff in Materiel Services. As discussed in Section 3 above, the current equation used to calculate an item's PAR level is:
This category investigates why a nurse manager may not keep items stocked in MSC warehouse on their carts.
Although stocked by MSC, nurse managers are responsible for the items and quantities stocked on their carts. Nurse managers must take the initiative to adjust PAR levels due to an increase in an item's usage or to add a new item to their PAR list. Consequently, the "whys" of this dimension are generally outside MSC's span of influence.
Non-Stock Item: This classification focuses on why orders are placed for items that are not stocked in the MSC warehouse.
Supplemental calls for non-stock items are unavoidable as these low volume and/or high cost items are not kept in stock in the MSC warehouse. Unless the cost-benefit of these items rises to a justifiable level for adding them as regularly stocked warehouse items, this dimension does not require further investigation.
Other observed causes for preventable supplemental calls:
Utilizing the taxonomy framework and the quantitative analysis results, the following recommendations are made to MSC management in their efforts to reduce supplemental order volume.
Work with MAIS to implement a system which forces users to select from a predefined list of delivery locations and have the corresponding department ID automatically populate. This will eliminate the need for having the CSR manually enter department IDs and reduce the potential for error. An option to override will be made available for use when necessary.
Create a committee of nursing staff and MSC staff to produce a directory of items, alphabetized by the nomenclature used by nurses that can be merged with the current PAR list on the carts. This will allow nurses to easily locate items in the stockroom. Since each stockroom layout is unique, this change will be particularly beneficial for nursing staff that float between units.
Providing more details about each item will facilitate nurses ordering the correct item the first time.
Items often end up in the stockroom when a clerk receives a supplemental order and does not know who the intended recipient is.
This will reduce the number of unnecessary supplemental orders for stocked out items.
Establish communication between stockkeepers and units. Nurse managers (or equipment technicians) can notify their stockkeeper of any unusual needs due to their patient population that day. This will result in saving their staff time later as they will no longer need to place a supplemental order.
a. Generate a trigger to notify MSC when a supplemental order is placed more than three times for a non-PAR item.
b. Generate quarterly reports of supplemental orders to determine top customers. Review reports to detect trends and abnormalities and adjust PAR as necessary.
Work with Public Relations and Marketing Communications (PRMC) to develop a brochure to educate units on services offered by MSC (e.g. ability to substock to an alternative location or bill an alternative org code) and on how to use MSC's services more effectively (communication about unit needs, encouraging nurse managers to review and update their PAR levels when necessary).
After reviewing the current PAR level equation and supplemental and PAR stocking data, it is recommended that the PAR levels are to be revised on a quarterly basis, based on the previous quarter's data. This new equation takes into account the variance of demand as well as allowing MSC to adjust the probability of stockout based on item cost or other factors such as holding or obsolescence cost.

Portable music players have become widely popular within the past decade. The MP3 format (formally known as MPEG-1 Layer III) has been one of the primary catalysts of this development. It allows for the storage of far more songs in a smaller amount of disk space than the traditional "Red Book" standard used for uncompressed music files stored on compact disc, which defined the baseline of digital audio from its inception in 1980. MP3 offers varying levels of compression to reduce uncompressed music files to a much smaller size, often of a ratio of 20:1 or even higher. Such compression is necessary to fit a large library of songs into a portable device. MP3 is, however, a so-called lossy compression format in that some of the original audio information is irrevocably lost during compression, and cannot be recovered by any means. Thus, the proper implementation of the compression (encoding/decoding) process is essential to maintaining an acceptable level of playback quality.
The format is standardized only based on its file format and to a lesser extent the way in which MP3-encoded content is decoded. Many different encoders have been produced over the years that support the MP3 standard as it is written, although sometimes with very different operational approaches. Basic options in almost all encoders include bitrate (expressed in kbps or kilobits per second, which fixes the size of the resulting compressed file), input sampling frequency (most commonly 44.1kHz, consistent with the original Red Book standard that defined "CD-quality" sound), as well as options for monophonic or stereophonic sound.
Of course, with the proliferation of MP3 encoders and various options have come attempts to guage the quality of various encoder, bitrate, sampling frequency, and decoder options. These tests have been almost entirely based on subjective listening tests (such as that presented by Bouvigne 1998 and Amorim 2004), where the same audio track is encoded with several different options and encoders and presented to a trained listener. The audio track is then rated on several factors, mostly with respect to fidelity to the original recording.
There are several problems with such a rating system, as it ultimately relies on subjective perceptual judgments that may differ vastly from person to person. Also, there are limits on how many different option combinations that a human listener can tolerate at any one time. Listening preferences could easily change between testing sessions or even within the session. This could make the usability of a large factorial experiment, for example, quite limited. Also troubling is the potential for highly non-normal results, which could skew or make normal factorial analyses statistically unusable.
As a result of the limits of traditional listening-based tests, it was decided to form a testing situation that was compatible with designed experiment goals as well as rely on objective rather than subjective measures. The measure chosen for this experiment is based on the idea that a musical sample that has been encoded and then decoded should possess a similar frequency spectrum as the original. Two sounds that possess very similar frequency spectra over their durations sound very similar. Therefore, diversions from this ideal represent noise introduced by the encode/decode process. An example of such a relative spectrum is shown graphically in Figure 1.
Figure 1 shows a relative spectrum result between an encoded/decoded file and its original source. The relative spectrum is calculated as:
Spectrum 1 is from the original file and spectrum 2 is from a file that has been encoded and then decoded. The resulting scale is called a dBr scale by the Sigview software. In the example in Figure 1, spectrum 1 (from the original file) has a much higher representation of the higher frequencies above about 19 kHz. It appears the encoder or decoder dropped a lot of information at these high frequencies. This is not unexpected since most humans are quite insensitive to sound at such high frequency ranges. (The limitations of this sort of analysis are discussed later.) To form an overall quality metric based upon this graph, the RMS (root-mean-square) was taken, which gives an overall view of the deviations of the two files from each other. Each y value represents a very narrow part of the frequency spectrum.
These relative spectra analyses were carred out with the Sigview 1.95 signal analysis software. As a result of this analysis, a treatment combination can be compared with its original source material using a simple quantitative metric. This will be the quality metric used for the rest of this report.
Several controllable factors can be considered for the analysis for this experiment. They include the encoder, the decoder, the sampling rate, and the bitrate. They are shown in tabular form below in Table 1.
Uncontrollable factors should only occur if any of the software used does not perform the same operations on different files consistently or even if the same operation is performed differently on the same file at different times. This is assumed not to occur and no other uncontrollable factors should be present.
These factor choices were made based upon commonly available software and achievable settings within software. LAME and BladeENC are enthusiast-written MP3 encoders. MP3ENC is an earlier freely available demo version of the commercial encoder produced by Fraunhofer IIS (FHG), who holds the original MP3 patents. MAD and MPG123 are widely available free command-line MP3 decoders (i.e. they are not music players, they only decode the MP3 compression and output it to a file) All software used claimed compliance with the applicable ISO (International Organization for Standardization) directives for MP3. ISO publishes some reference software programming code for encoding and decoding, although some programs claim not to use any of the ISO code.
A general full factorial analysis was chosen because the encoder treatment levels are qualitative and contain more than two levels. Also, it is possible there might be non-linear responses for bitrate versus quality so checking three levels of this factor will be desirable and center points cannot be utilized to analyze this because there are qualitative factors. As a result, a streamlined 2k analysis is not possible. Relatively few experimental resources are needed to create and analyze these files, so there is little benefit in running a fractional factorial anyway. In total, to fully explore all treatments at all levels, 36 runs are needed per replicate.
To analyze different types of music (the inputs for this process), three different musical selections were chosen for each of three replicates, and the runs were then blocked on these replicates to account for the differences in conclusions that might result from each different music selection. In total, 108 individual runs were made. Minitab R14 was used to set up and analyze the results. Randomization of experiment execution order was deemed unnecessary since the order in which each treatment was created and analyzed would have no effect on the results of the signal analysis in Sigview. No part of the analysis could be in any way affected by the run order chosen since the runs are in effect totally independent of the order in which they are executed: there is no process "memory" at any point.
To begin, three 10-second audio samples from three different audio tracks were taken to represent general musical selections. The selected tracks were:
Each song selection was extracted from an original retail CD using Exact Audio Copy V0.95 beta 4 to copy the original 16-bit stereo 44.1kHz files from CD to a PC. These files then had the applicable 10-second portions mentioned above trimmed out of them with the audio editing package Audacity 1.2.3. Audacity was also used to create 32kHz sampling rate equivalents of these 10-second snippets. Generally, sampling rates are chosen to be roughly double or more the top end of human hearing (which is about 20kHz) to avoid signal aliasing but can be set lower to have a smaller file size. Monaural files were created from the original stereo in this experiment to avoid technical limitations with analyzing stereo files. This will only affect the bitrate settings recommended. A 32kbps mono bitrate file would be doubled to about 64kbps to achieve comparable quality results if it were in stereo for most encoders.
Each uncompressed file (either in its 32kHz or 44.1kHz sampling rate form) was then encoded using all combinations of encoder, sampling rate, and decoder. Specific command line options invoked for each encoder and decoder are shown in the Appendix. Sigview 1.95 then was used to create the frequency spectra and calculate the relative spectra discussed above, along with their RMS values.
Before proceeding with the analysis, it was decided to check to see if a data transformation might be appropriate for this data, as it was highly possible that the data was non-normal since sound intensity and frequency scales are based on logarithms. The results of the Box-Cox transformation performed on all 108 RMS readings are shown below in Figure 2.
This analysis recommends a log transformation of this data as the best fit, which was somewhat anticipated as mentioned above. The data was transformed to a natural log scale and then it was analyzed in its entirety. The resulting ANOVA table is shown below.
Prior to using the analysis above to come to any conclusions, the residuals were examined for possible departures from normality that might invalidate an ANOVA analysis. The following figure (Figure 3) shows the residuals versus the fitted values from the model. Some clustering of residuals is apparent on the right side of the figure, as well as a few rather large residuals at the top of graph, but nothing highly non-random appears. A normality plot of residuals confirmed that there is no reason to reject the normality assumption at a significance level of 0.05, with a P-value of 0.163 on the Anderson-Darling test. This analysis presents no reasons to reject the analysis on the basis of lack of normality.
Upon first inspection of the ANOVA table for this model, it is evident there are many significant terms. The main effects are plotted in Figure 4. It is important to note that lower values of the quality metric are better, as it indicates better fidelity to the original file. All of the main effects except for decoder have a statistically significant effect on the spectrum quality of the encoded/decoded music files with respect to the original files. Decoder type is the only main effect not to be significant in this analysis. Perhaps predictably, bitrate is the single most important factor (it is responsible for the largest portion of the sum of squares) in the measured quality. This makes sense as it fixes the size of the encoded files and thus also most directly controls the compression level, which will ultimately have large effects on the amount of fidelity of the encoded file as compared to the original. It also exhibits a non-linear response, with diminishing returns occurring as bitrates increase. The quality improvement going from 32kbps to 80kbps is several times larger than the similarly spaced 80kbps to 128kbps improvement.
Oddly enough, the lower sampling rate of 32kHz actually results in higher measured quality. This is understandable because the compression in 32kHz files will actually be lower as they contain less data to begin with, and thus need less compression to arrive at the predetermined size that the bitrate selection fixes. Note that fidelity measurements were taken with respect to original 32kHz or 44.1kHz files. If all fidelity measurements could have been compared to the original 44.1kHz files that would hold the 32kHz files to a higher standard, it is likely 44.1kHz encoded/decoded files would outperform 32kHz files. This was not possible, though, due to technical limitations in Sigview, where comparing two files with different sampling rates is not possible.
Finally, the best encoder by this analysis is actually BladeEnc. This is surprising because it is much older than LAME is now and also was not the "official" MP3 encoder from Fraunhofer (FHG), which actually performed worst of all! As expected, decoder type has no discernible effect on quality.
The two-way interaction plots are depicted in Figure 5 with significant interactions circled. All interactions not associated with the decoder are significant. For sampling rate vs. bitrate, it appears 44.1kHz sampling rate files exhibit far more linear total improvements in quality as bitrate increases compared to 32kHz files. 32kHz files experience more dramatic improvements in quality as bitrate increases.
For encoder vs. bitrate, it appears FHG's encoder actually performs more poorly at 128kbps than at 80kbps, causing a significant interaction to occur because the other encoders do not experience this anomalous behavior. The sampling rate vs. encoder interaction is barely significant (P=0.043) and no clear interaction pattern can be seen.
Perhaps most interesting, though, is the presence of a 3-way interaction between sampling rate, bitrate, and encoder. To analyze this, the two-way interaction plots for bitrate and encoder were stratified by sampling rate. These are illustrated below in Figures 6 and 7. At a bitrate of 32kbps, little difference is noticed in stratifying by sampling rate. Larger discrepancies are noted at the higher bitrates. Most notable is the fact that the FHG encoder at a sampling rate of 32kHz and a bitrate of 80kbps appears to outperform itself compared to 128kbps. This is unexpected as generally lower bitrates induce more compression, which lowers quality. This doesn't hold true for the FHG encoder at 32kHz. It is unclear what meaning this has, other than there might be a potential programming or algorithm error in the FHG encoder at 32kHz that produces higher than expected compression artifacts at 128kbps. The best explanation may well be that this particular encoder may not have anticipated being used in a rather unusual condition (32kHz source material is rarely, if ever, used for MP3s) and thus performed in an unexpected manner.
The blocking factor (on replicates: music clips) was significant with the P-value very close to zero in this analysis in the ANOVA table. Therefore, blocking on replicates was an appropriate experimental procedure to follow.
Finally, the unadjusted R-Squared value for this model is about 92%. This means that 92% of the differences in quality between these files could be explained by the four factors or the blocked input variable (music clip). 8% of the variation must stem from other sources not explained by this model. Overall, though, an R-Squared value of 92% means that this model is excellent and explains almost all of the variation in quality measured.
A few recommendations can be made on the analysis above. First, the highest bitrate possible should always be selected that is practical for the storage space available, although returns do diminish with increasing bitrates. Based on this analysis, a 160kbps stereo MP3 would be much higher in quality than a 64kbps stereo MP3 given any encoder, but as large of an improvement would probably not be noticed by increasing it to a 256kbps bitrate. Decoders appear to play little role in ultimate quality as long as they are compliant with the applicable standards that define their operation. 32kHz outperforms 44.1kHz on the surface, but this is tempered with the knowledge that the 32kHz encoded/decoded files were only compared with 32 kHz originals, not with the inherently higher quality 44.1kHz originals. The lower sampling rate may have simply allowed for less compression and thus less distortion as compared to the originals they were compared to. 32kHz is not widely used as a sample rate for audio and 44.1kHz should be used so as to avoid unnecessary file conversions. BladeEnc appears to be the top performer with respect to the quality metrics discussed here, but the author subjectively would not have chosen it, especially at the lowest bitrate, where several aural artifacts were more obvious than those from the other encoders. Objective measures are good, but they should be sure to be tied with a user's subjective experience because that will ultimately determine the success of a music compression format, not an objective abstract measure.
There are improvments that could be made to this line of research to remedy the limitations of this quality metric. The relative spectrum calculated in these analyses relied on an unweighted frequency curve. However, human hearing is not unweighted. In fact, it is much more sensitive in some areas than others as shown in Figure 8.
Human hearing most closely follows the (A) curve indicated above. It is clearly not weighted evenly along the 0 to approximately 20kHz scale used in this analysis. As shown in Figure 1, it can be reasonably concluded that sacrifices are made at frequency levels where the human ear is not as sensitive. Also, the quality level is limited because, even if it is not intentional, departures from the baseline in encoded/decoded files will be perceived more readily by listeners at certain frequency bands. Unfortunately, no analysis software found was able to apply weighting curves to the RMS calculations. It would be a valuable extension to the current analysis and one that may become available as signal analysis software continues to advance.
If such analysis could be totally automated, it might be desirable also to analyze many different blocked replicates to come to widely applicable conclusions. Alternatively, different musical styles or types could be compared in different experimental analyses to compare recommendations. More encoders could be examined, as well as implementing stereophonic processing in the analysis, which would allow for greater resemblance between this model and reality.

The goal of this project is to tune the parameters of a high-fidelity simulation of a HUMMWV military vehicle using experimental data from an actual HUMMWV. This model has been developed by RDECOM (at the Detroit Arsenal) to assist them in the design of future HUMMWV platforms.
Initial analysis was performed using an unreplicated full-factorial experimental design and Lenth's method to evaluate factor significance. Response surface methods were then used to determine the simulation parameter settings that resulted in a simulation that most closely matched the experimental data from the actual HUMMWV.
The model predicts the vertical acceleration of the vehicle center of mass in response to a vertical impulse displacement applied to all four wheels simultaneously. The difference between the simulation predictions and experimental data is quantified by an algorithm, AVASIM.
Four model parameters were chosen as factors, and a 24 full factorial experiment (without replication) was specified with factor levels at + − 10% nominal. The model is deterministic, precluding the use of replicates. Applying Lenth's method none of the factors were deemed significant. A response surface method was employed for further investigation of optimal parameter values, and data already obtained for the 24 factorial design was used for the required corner points. Additional center and axial points were obtained as required throughout the process. A line search revealed a region with curvature, and a second order model was used to identify a stationary point.
In general the model accuracy response is ill-behaved. It is suggested that the steepest ascent optimization scheme implemented in this study is notadequate-more sophisticated approaches are suggested. However, the steepest ascent results indicate that the accuracy of the model will be greatly improved by setting the model parameters to the stationary point values.
The objective of this pro ject is to use experimental data from an actual vehicle to tune model parameters such that the simulation accurately predicts actual vehicle response. In addition, it is desired to identify which parameters have a significant effect on model accuracy.
The system used for this pro ject is the High Mobility Multipurpose Wheeled Vehicle, or HUMMWV [1]. This vehicle is a mainstay of the Army fleet and is constantly being redesigned to better suit the needs of the Army. This constant redesigning requires that the Army have a high-fidelity simulation model of the vehicle to allow for powerful design techniques to be employed. The simulation model used for this pro ject, developed by RDECOM (at the Detroit Arsenal), is a high-fidelity three-dimensional dynamic model of a HUMMWV with full suspension characteristics. The model is currently configured to parameter values based on an older model vehicle. The Army wishes to parameterize the model to reflect the performance of the latest design in order to use the model in further design studies. In order to do this an actual HUMMWV was set up on a four-poster shaker table and instrumented to record various data. The input to the system is a vertical displacement in the shape of an impulse applied at all four wheels simultaneously. The vertical acceleration of the center of gravity of the vehicle was chosen as the output based on the test setup and input applied. The availability of this experimental data will facilitate the task of finding model parameter values that produce a more accurate simulation. In order to compare the output from the experimental setup with the output from the simulation model an algorithm called AVASIM was employed [5, 6, 7]. This algorithm generates a measure of model accuracy which varies from one to negative infinity, with one representing one hundred percent accuracy and zero representing a user-defined tolerance on the accuracy of the simulation output. This dictates a maximum-the-best optimization strategy.
Four parameters of the simulation model were identified and selected for this study based on discussions with Army personnel. These parameters were the front and rear spring rates and damping coefficients. Access to only one experimental data set was available for this project. Due to this fact and the deterministic nature of the output from a non-heuristic computational simulation, experimental replicates cannot be made. This prompted a full factorial experiment with four factors and no replications for the initial stage of this study. This experimental design was suitable to analyze both the effect of the parameters on model accuracy and the significance of each factor. Lenth's method for unreplicated experiments was employed initially to identify important factors within the simulation model. Unfortunately, this method indicated that none of the factors or interactions were significant.
After the failure of Lenth's method to identify important factors, it was decided to use a response surface methodology to tune the model parameters with respect to all four factors. The data obtained for the Lenth's method analysis were used in addition to new center points to evaluate the parameter main effects and curvature around the nominal parameter values. After finding an absence of curvature around the nominal values, a line search was performed in the direction of steepest ascent. A change in slope was detected, and a new nominal point was set where that change occurred. A full response surface analysis including the factorial, center and axial points was then used to identify a stationary point and possible optimum parameter value settings. The accuracy at the stationary point was markedly improved over the baseline accuracy.
Chapter 3 provides an overview of the AVASIM algorithm used to evaluate model accuracy, and introduces the design of experiment and response surface methods employed in this study. Chapter 4 presents the results of the investigation, and Chapter 5 summarizes the interpretation of the results and suggests opportunities for future work.
This chapter provides some detail on the AVASIM algorithm used to evaluate model accuracy, and provides an overview of the data analysis methods employed in this study.
The AVASIM algorithm evaluates model accuracy by calculating an Overall Performance Index for a model. To calculate the Overall Performance Index for a specific input and system configuration using AVASIM an output of interest must be first identified. The comparison is made between the model under investigation and the truth. In this pro ject the truth is obtained from experimental data on a real system. However, in other cases it might acquired from a full model. The full model is a more complicated model with enough complexity to provide the most accurate predictions of the systems behavior. For the output of interest, target points can be selected based on the use of the model (Figure 3.1). For example, in a model with a response similar to an ideal second order system, the engineer is usually interested in the overshoot, rise time, steady state value, etc. As many of these points as desired can be defined as targets, in both amplitude and time. For each target point tolerances are then defined.
These tolerances can be either absolute or relative (percentage, etc.) tolerances. Based on intuition and experience relative tolerances are the preferred type of tolerance. Absolute tolerances can be used if the numerical value of a target point is at or near zero (resulting in very large negative performance indices for relatively small errors), or there exists some other problem-specific reason to do so. After target points and tolerances are chosen a performance index at each point is evaluated. This is done using the following formulas:
It is important to note that a Target Performance Index of 1 at any point corresponds to 100% accuracy. It is also of importance to note that a Target Performance Index of 0 corresponds to the response of the reduced model being at the tolerance for that particular target point. If there are no target points defined, as is the case for this pro ject, this step can be skipped.
Once each Target Point Performance Index has been calculated a Response Performance Index is generated. This is done through the creation of a threshold model. This is simply amplitude scaled and time shifted version of the full model of the form:
The scaling factors a and b are chosen to be as large as possible while insuring that the tolerances at all target points are met. If no target points have been defined then a percentage tolerance can be defined directly for a and b. This leads to a maximum perturbed model that still has acceptable accuracy. Once this metamodel output has been generated, the residual sums between the reduced and the full model and the threshold and the full model are calculated using the following formulas:
After these values are obtained the Response Performance Index in obtained in a fashion similar to the Target Performance Indices using the following formula:
Finally, the Overall Performance Index for the reduced model for a specific input and system configuration is calculated as an average of the Target Point and Response Performance Indices using the following formula:
If there are no target points defined then this value is equal to the Response Performance Index. Once this value has been calculated there have been two proposed methods of assessing model validity called the liberal and conservative criterion. The conservative criterion states that all the individual Target Point Performance Indices (Jp,i ) and the Response Performance Index (Jr) must be positive in order for the model to have sufficient accuracy and therefore be valid. The liberal criteria states that it is only necessary for the Overall Performance Index (P I ) to be positive (meaning that some Target Point or Response Indices can be negative, or have insufficient accuracy) for the model to be valid. When there are no target points defined these criterion are equal.
The response of model accuracy evaluation tools is notoriously ill-behaved. The response surface is typically highly non-linear, sometimes discontinuous, multimodal, and usually numerically noisy. These properties pose substantial challenges with regard to finding an unconstrained optimum, discussed in the next section.
The nominal vehicle parameters are set to:
where A = front spring rate, B = rear spring rate, C = front damping rate, and D = rear damping rate. The factorial levels were set to + − 10% of these nominal values. AVASIM was used to evaluate model accuracy at all 24 factorial points. Since no replicates were available, Lenth's method (individual error rate approach) was selected to check factor significance. With Lenth's method the pseudo standard error (PSE) can be used to estimate the standard error [8]. The full factorial experiment also provides insight into the response of the model accuracy evaluation.
Response surface methods provide a generally efficient method for performing unconstrained optimization. The well known steepest ascent method [2, 8] uses the principal that the gradient of a function points in the direction of steepest ascent. A line search in this direction is performed until a maximum in the gradient direction is found. The gradient at that new point is calculated, and used as an updated line search direction. This process can be repeated until the gradient is determined to be the zero vector, at which point necessary conditions for optimality are satisfied (this point is called a stationary point xs ). It can be shown that the search directions for adjacent iterates are orthogonal. If the response function is highly elliptic the steepest ascent method can converge very slowly. Alternate methods such Newton's method or Quasi-Newton methods such as BFGS update are more efficient in this case.
Instead of iterating until the gradient is zero, a second order model can be fit to the data around the current point. This of course only makes sense if curvature is present in the region, which can be checked by either comparing the average value of factorial corner points to the average of center points, or calculating the p-value of the aggregate quadratic term. If curvature is present, a second order model is then fit to the data (requiring additional axial points). The stationary point of this second order model is then used to estimate the location of stationary point1 . If a second order model approximates the response well, this is a reasonable approach. Care should be exercised if the stationary point is determined to be far outside the range of sampled data.
The model accuracy response corresponding to the 24 full factorial points described in §3.2 are displayed in Table 4.1.
Table 4.1: 24 full factorial experiment results.
This data was used to calculate the main effects and interactions, and the PSE required for Lenth's method. The cutoff value for significance was found to be I ER5%,15 = 2.16, and the pseudo standard error was P S E = 0.457. Table 4.2 displays all of the effects and t test values for each effect. It was discovered that none of the factorial effects were significant according to Lenth's method.
Table 4.2: Lenth's method calculations.
The data gathered from the original 24 experiment was used to begin the response surface process for finding the optimum model parameter settings. Seven center points, generated by small (within 1%) perturbations around the nominal values, were used to perform a curvature check. MINITAB was used to fit a first order model (with an additional aggregate quadratic term) to the data, and the p-value for the aggregate quadratic term was found to be 0.971, indicating an exceedingly small presence of any curvature. It was decided to perform a line search in the steepest ascent direction. The scaled coefficients of the linear model provide the search direction:
Because steps of 2∆ would quickly bring the parameter values out of the feasible range, higher resolution 1∆ step sizes were used. The line search produced the following sequence of P I values:
A change of slope can be observed around the 4∆ response, and the investigation was directed to that point. The nominal values were updated to the 4∆ parameter values. Another 24 experiment was designed, centered at the new nominal values. The factorial points were set to + − 10% of the updated nominal values, and seven zero points were specified using small perturbations of the nominal values. Again MINITAB was used to fit a first order model (with an additional aggregate quadratic term) to the data. The p-value for the aggregate quadratic term in this case was found to be 0.004, indicating a strong presence of curvature. Since a stationary point may be near, a second order model (equation 4.1) was fit after eight axial points were specified and evaluated (using a value of α = 2.0).
The values of β0 , b, and B were found to be:
Setting the gradient of the second order model to zero, the stationary point was determined to be:
The values of P I at the stationary point x ∗ calculated with the second order model and the simulation were respectively:
The second order model was only roughly accurate in this case. To determine the nature of the stationary point the eigenvectors of the B matrix were found:
The B matrix is therefore indefinite, indicating a saddlepoint. However, note from the b vector that the effect from factor D is not very significant. If the factor D is ignored, the resulting 3 × 3 B matrix is negative definite, indicating a local maximum. Setting D to its nominal value and running the simulation again with the other factors at stationary point values, we find that P I = −11.1459, which is in fact better than the simulation response at the stationary point.
The accuracy of the model at the baseline parameter values and at the stationary point can be depicted graphically. Figure 4.1 illustrates how the dynamic time response predicted by the model using baseline parameter values roughly agrees with the experimental data. Figure 4.2 shows that the model with the stationary point parameter values predicts a dynamic time response that more closely follows the experimental data.
The AVASIM algorithm was used to quantify the accuracy of a computer simulation of a HUMMWV Army vehicle with respect to actual experimental data.
The objective of this study was to find parameter values that maximized this accuracy. A 24 experimental design without replication was used for initial analysis. This indicated a lack of curvature in the region around the baseline constraints, requiring a line search to find a region with curvature so that a stationary point and optimum parameter set could be found. A steepest ascent response surface method was employed, and a stationary point with marked improvement over the baseline performance.
The functional nature of P I merits some discussion. As mentioned earlier, it is known to be highly non-linear, sometimes discontinuous, and somewhat noisy. This makes response surface methods difficult to implement. Second order models do not fit the response well. This was demonstrated by the discrepancy between the second order response and the simulation response at the same parameter value settings presented in Chapter 4.
Other optimization approaches are better suited for this task. One suggested approach is to use a gradient-free algorithm, such as DIRECT2 [3] to find a region in the model parameter space likely to have the global optimum, followed up with a more sophisticated gradient-based algorithm, such as Sequential Quadratic Programming [4]. This approach is likely to find the global optimum more efficiently.

The entire course was centered around a single problem dealing with the manufacturing of printed circuit boards. Printed circuit boards are made up of many different types of components, including transistors, diodes, and capacitors, which are arranged in different numbers in different places on a substrate, or blank board, to form the final printed circuit board. The components are stored in sleeves in a pipe-organ setup above the area where the substrates are held, with a retrieval arm that moves between the sleeves to pick the correct component. Each of these different elements needs to be placed individually on the substrate by a pick-and-place machine. This process involves two steps: first, the correct element must be retrieved from the correct sleeve (corresponding to the type of component needed); second, the substrate needs to be moved below the insertion point so that the element is placed in the correct location.
This is a rather simplified version of the problem, however, because the setups can be torn down and reassigned, for a time cost of σ. We can immediately see that if σ is very large in comparison to the overall production time, then it will be in our best interest to put all the boards together in one setup and manufacture them according to the optimal setup for all boards. If σ is very small, or zero, then we will tear down the setup between every board and manufacture each board according to its own optimal setup. From here on out, we will denote an individual board's optimal setup cost by where b is the number of the particular board. The difficulty arises when σ is neither 0 nor exceedingly large, but somewhere in the middle. Now the problem becomes not how to order the components in the sleeves, since the optimal configuration is uniquely determined by the boards being produced together, but exactly which boards to group together into a cluster to be produced under a single setup. Since there are n ways to group n boards into clusters, or sets, of 1 or n-1,
Our first step to try and get a handle on the problem was to try and come up with a good heuristic solution. In this assignment, as in many of the others that involved implementation, the most difficult and time consuming piece was arriving at a suitable data structure. I could not figure out a way to store the clusters as vectors of the boards included, since the dimensions would change from cluster to cluster, so I settled on a system of storing the clusters as a 24-element vector with the first component corresponding to the number of the cluster that contained the first board. This could be arbitrarily set to one, but the random-generation code that I wrote was easier if it could just assign random numbers between 1 and t where t was the number of clusters into which you wanted to partition the boards. Looking back, much of the implementation would have been easier had I used the system of 24-element vectors of zeros and ones, where a one signifies that that board is in the cluster. But, using my rather clumsy data structures, I was still able to create a randomized loop which would generate 500,000 instances for clusters of size two to twenty-three, and then would save both the average cost across all instances, and the lowest cost along with the vector that generated that lowest cost.
The best answer we were able to generate from this method was 906,231, with an optimality gap of 2.22%. This optimality gap is misleading, however, because we were comparing it with the only lower bound we had -- manufacturing all boards alone according to their optimal setup with no tear-down cost -- which is clearly an infeasible solution. So in a little over two hours, through our random generation, we were able to get a solution barely 2% away from an impossibly good solution. In reality, our answer was only 0.23% off the actual optimal solution of 904,145. In nearly all industrial applications being 0.23% away from the optimal solution would be more than enough. However, though this worked fairly quickly for the particular data set we were given, we have absolutely no guarantee that it would work for other data sets, or larger data sets. On the other hand, we have seen that having a good feasible solution to start from can result in substantial decreases in the time required to solve the problem, so using a random-generation algorithm such as this might prove useful in obtaining good initial solutions. We also briefly discussed using genetic algorithms in combination with this randomized procedure to generate even better solutions. Unfortunately I do not know enough about genetic algorithms to actually be able to implement them in any consistent fashion, but from the little I do know, my guess is that we could have improved our final solution by breeding several near-optimal solutions together and looking at their offspring over several generations. Additionally, we talked several times about Dushant's local neighborhood search, and the potential it has to help generate good initial feasible solutions. I know even less about neighborhood searches than I do about genetic algorithms, but it seems like it might be an interesting area to look into, especially since outside of the realm of academia, many people are more interested in "good" solutions than in provably optimal solutions. However, being firmly ensconced in the realm of academia, a heuristic solution is not enough to satisfy our unceasing hunger for provable optimality. So we endeavored on.
In order to find a provably optimal solution we must first have a formulation. The first formulation we looked at was George Polak's model (the Polak model) from his published paper.
Since the original model was so difficult to understand, we were given the task of coming up with new models, in the hopes that we could come up with something a little easier to understand. The most intuitive way to construct a model would be to have if boards i and j are manufactured together. Unfortunately, there is no easy way to determine which boards are in a cluster, nor any way that we could come up with to sort them, given this definition of x. Oddly enough, this idea of defining whether two boards are paired together or not turns out to be a very effective branching strategy later on for the master problem, though in a different form. Setting that particular idea aside, Shital and I came up with three models, none of which held any great hope of yielding a solution. Our first model was a slight adjustment to the Polak model, using all the same variables.
This model presents a slightly more intuitive definition of where if boards k and m are in the same cluster. Unfortunately, in order to achieve this, we had to introduce a non-linearity into the model in the form of absolute value. So while it may be slightly more intuitive, the non-linearity excludes it from being truly useful in solving the problem. But thinking about this model led us to our second model which eliminates the issue of determining how many setups there are by assuming that we know there are S setups. Thus, this second model would need to be solved once for S=1,...,N, where N is the number of boards.
Here if cluster r contains board k, and 0 otherwise, returning us to the counterintuitive "0 means yes" definitions from the earlier Polak model. The first two constraints are the same as in the Polak model, and the third simply says that each board must be contained in exactly one cluster. The confusion arises from the fourth constraint; however, if we look more closely we can see that the constraint is only meaningful in the case when , i.e. boards k and m are in the same cluster, thus their setups must be the same. This is exactly what is implied by the second part of the constraint, since then it must be true that , and whether they both are zero or one is immaterial. Additionally, when the constraint drops out and becomes meaningless. Thus, this fourth constraint guarantees that all boards in the same cluster have the same setup. This model seems much more elegant and intuitive than the original model. Unfortunately, it needs to be solved N times (where N=number of boards). As we were running the random-generation code, we noticed that nearly all of the best solutions had between six and nine clusters. If this was a structure that was common across all data sets, a formulation such as this one would allow us to exploit that and only look at the solutions with the likely number of clusters. However, in order to be able to prove that a solution obtained in this manner is optimal, we must be able to show that the function of optimal solution value vs. number of clusters in the solution is convex, which we will address in the next section. The final model we tried took an entirely different view of the problem, setting a new variable if board b is manufactured according to setup s, and 0 otherwise.
Here,
Throughout the process of trying to come up with alternate formulations for this problem we realized just how difficult it can be to effectively model a problem that can be described in only a few sentences. Later, as we looked at modeling the subproblem, this was really hit home -- translating problems into math programs can be exceedingly difficult, even when the problem itself seems relatively easy to describe.
If we knew that the optimal value was convex in the number of clusters in the solution we could take formulations such as Model 2 above, and solve them for increasing numbers of clusters until we saw an increase in function value. If the function is convex, once you see an increase the function value will continue to increase, implying that our previous solution is, in fact, optimal. This has the potential to save a great deal of computation time if the optimal number of clusters is small relative to the number of boards. If the optimal number of clusters is close to the number of boards, then even if the function is convex, it will not save us a great deal of time since we will have to solve the problem for all, or nearly all values of S. Working with Shankara, we tried to find a counterexample using a set of only three boards with three components. Our flaw was assuming symmetric sleeve costs; we tried all possible combinations of boards and found that all of them (under the assumption of symmetric sleeve costs) were convex. Others in the class rejected the assumption of symmetric sleeve costs and thought they had found a counterexample, though it was later proved to be incorrect. Further, their counterexample was an extremely pathological case, with strongly asymmetrical sleeve costs. All the evidence we could find supported the conclusion that with symmetric sleeve costs, the function would indeed be convex; but we were unable to provide a proof, so it is all merely conjecture. Yet even if we could prove convexity, it would mean that we would need to solve an impossibly difficult problem slightly fewer times, but we still do not know how to solve the problem even for a given number of clusters.
Another dimension of the solution involves the integrality of the solution. Clearly we cannot have fractional amounts of clusters in an optimal solution; but solving the linear programming relaxation of the problem is vastly easier than solving the integer program. The question then becomes: If we solve Polak's model with the x variables relaxed, will the solution still be integer? If the constraint matrix is totally unimodular (TU) then yes, the solution to the LP relaxation will be integer. If the constraint matrix is not TU, then the solution may or may not be integer. So we set out to prove that the constraint matrix of the Polak model was TU. Unfortunately, our proof turned out to be flawed and we were unable to correct for the flaw, though it does seem that in most instances the constraint matrix will be totally unimodular. At the end of the course, looking at the solutions using the rank-cluster-price algorithm, most of them did turn out to be integer, especially for small problem instances, implying that we might well be correct that the formulation is indeed TU. Regardless, the solutions do turn out to be integral in many instances.
The integrality of the solution is related to the strength of the LP relaxation. If the LP relaxation is close to the convex hull of the integer program, then the solution will often be integer. Another way of conveying the idea of total unimodularity, is that a constraint matrix that is TU describes exactly the convex hull of the integer feasible region, and so all the extreme points will be integer. In fact, we do not particularly care if all the extreme points are integer, only if the extreme points near the optimal solution are integer. But we were unable to prove that even just the extreme points near the optimal solution were integer, though it might be a more efficient way to approach the proof. Regardless, the better your LP relaxation is, the fewer nodes there will be in your branch and bound tree once you start trying to solve the integer program. As we saw later, this can have a profound impact on the solvability of a problem.
Having nearly exhausted the store of knowledge in the class without additional information, we turned our attention to Dantzig-Wolfe decomposition, which motivates the technique known as column generation. The idea is that we can separate the problem into a set of subproblems which are then linked together in a "master problem." Typically these subproblems have some special structure, such as an assignment problem, that allows them to be solved quickly and easily. This technique is especially useful if you have a set of constraints that affects only a small group of variables, thus those variables and constraints can be separated out and put into subproblems, with "linking constraints" in the master problem which link all the subproblems together. These subproblems then generate a set of extreme points of the solution. And we know that the optimal solution can be expressed as a convex combination of the extreme points of the subproblems, so we can replace the variables in the master problem with these convex combinations of extreme points, and solve. If the subproblems are easily solved, this can prove to be a very powerful mechanism for solving large-scale problems. The key, however, is that the subproblems need to be easy to solve.
With the column generation technique, motivated by the Dantzig-Wolfe decomposition, in mind we set out to define a master problem and subproblem to which we could apply these ideas. We fairly quickly found our master problem:
Here, we change notation slightly where K is the set of all clusters being considered. if cluster k is in the optimal solution, and 0 otherwise. is the cost of cluster k including setup cost σ. is an index variable which equals one if board b is in cluster k. Thus the only constraint in our master problem is that each board must be contained in exactly one cluster, and we want the set of clusters with minimum cost. It seems simple enough, and the idea behind the subproblem is fairly simple. We even know what the subproblem is: to generate the most negative reduced cost column. Then we would take that column from the subproblem, enter it into the master problem, get new duals from the master problem and hand those off to the subproblem and repeat until our subproblem reports back that there are no negative reduced cost columns, showing that our current solution is optimal. Regrettably, this turns out to be much easier said than done.
The reduced cost of a column (cluster) is its true cost minus the dual associated with the boards included in that cluster. The true cost is the manufacturing cost of all the boards included in the cluster plus the setup cost, σ. Thus, we define variable if component c is retrieved from location l for board b in this particular cluster, and 0 otherwise. Additionally, we use if board b is included in the cluster, and if component c is assigned to location l. This results in the first version of the subproblem,
This model minimizes the reduced cost, subject to the following constraints: each component is assigned to exactly one location; each location is assigned exactly one component; if component c is not assigned to location l, then component c cannot be retrieved from location l for any board b; finally, if xb is one then each component must be retrieved from a location for board b. Together these constraints define a feasible column, and the problem will find the feasible column with the most negative reduced cost. Unfortunately, this problem would not yield an answer in less than 2 hours with either the x, or the y, or the x and y variables relaxed. When we relaxed all the variables, we ran into a different problem: the LP relaxation is weak. When you solve the LP relaxation with all variables (x, y, p) relaxed, the optimal solution is fractional. The reason the solution turns out to be fractional, while not immediately obvious, is because the model is splitting the components for the boards it wants into different sleeves and only paying for the cheapest one. And while it will solve to an integer solution with some dual values, we only got it to solve with made-up dual values, or the dual values from the near-optimal solution. Unfortunately, after only a few iterations of shuffling between the master and subproblem starting with the near-optimal solution, the duals become such that the subproblem will no longer solve to integrality. In fact, even with all the variables relaxed we could not get it to solve with the duals from solving the master problem with the identity matrix.
All these frustrations drove us to look at an entirely different model,
This model is exceedingly difficult to understand. In this model the xcl variables take the place of the ycl variables in the previous model, the yb's replace the xb's, and the is data simply taking the place of in the previous model. When M is large enough, and yb is one, implying that the board is in the cluster, the constraint says that
Unfortunately, as we have seen, introducing a big M into a model almost always results in a terrible LP relaxation. Here, the problem arises because the model has two constraints to determine the value of θb:
Despite all these problems, we were able to get a single instance of this version of the subproblem to solve with the duals from the identity matrix in just over twenty minutes. But returning to the big picture, this means it takes twenty minutes to solve one instance of the subproblem to optimality. In order to solve the root node of the master problem, we might need to solve the subproblem millions, or tens of millions, of times. And then, we have no guarantee that the solution to the root node of the master problem will be integer, so we might have to branch on the master problem, meaning we would have to solve potentially hundreds or thousands of versions of the master problem before we arrived at a truly integer solution to our overall problem. Given the complexity of the overall problem, a subproblem that takes twenty minutes to solve is simply worthless in the overall context. And since there is really no way to remove the big M from the model, we were forced to chuck this rather creative model, returning our focus to the seemingly less-promising Subproblem Model 1
Looking at Subproblem Model 1, we noticed that the variables were simply taking the place of xbycl, and serve only to eliminate the nonlinearity in the model. So what if we left the nonlinearity in the model? We end up with a new, nonlinear model:
This model boils down to an assignment problem, which anyone can understand quickly. Additionally, assignment problems typically solve very quickly. We used the MINOS package to solve this model, and the subproblem solved in mere seconds. Shuffling back and forth between the subproblem (in MINOS) and the master problem (in CPLEX) was regrettably too time consuming to really be able to see if the nonlinear model would yield an answer to the subproblem, however it does appear that it would be a good method to generate good columns quickly. To prove optimality in any of our other models, we need to show that there exist no more negative reduced cost columns. The worry is that this nonlinear model might return a positive reduced cost column when there were still negative reduced cost columns out there. But at the very least, we would only have to solve the long linear model a few times, possibly only once, to verify that the nonlinear model had given us all the available negative reduced cost columns. Unfortunately, before we can actually test this theory, we would need to be able to write a .run file which would allow us to switch between MINOS and CPLEX to solve the subproblem and master problem, respectively, without having to do it by hand. It would seem as though there would have to be a way to accomplish this, although we were unable to find anyone who knew how to tackle this particular issue. With a better implementation, this method would seem to hold great promise for solving this problem quickly. I think this is perhaps one of the most exciting avenues we ignored.
Since we could not find a way to get the nonlinear model to solve automatically, we were forced to return to our original model, Subproblem Model 1. We could not get Model 1 to solve with the duals from the identity matrix, so we tried solving it with the duals from the many iterations we did by hand using the nonlinear model, thinking that these duals might be "better" in some sense. Unfortunately, they did not yield a solution either. Then we had the brilliant idea of using the near-optimal solution from the first week as a starting solution for our master problem -- this eight column solution gave us duals that allowed the subproblem to solve lickety-split. This solution raised issues about the validity of the duals from a solution that did not form a basis. In order to form a basis, you need as many variables as you have constraints; here we had only eight variables, one for each column (or cluster) and twenty-four constraints, one for each board. What does this mean for the validity of the duals? After much thought, we decided that even though we do not have a full basis to start with, the duals are nonetheless meaningful, since we end up with exactly eight (the number of columns we started with in the initial solution) non-zero duals. All the other duals, corresponding to the extra variables that are needed to form a basis are simply set to zero. However, this does yield a strongly degenerate solution, where most of the variables in the solution are set to zero. This means that when we took those duals and plugged them into the subproblem the objective value stayed exactly the same over many iterations, because of the degeneracy. The algorithm was simply pivoting in different variables, which were all set to zero, so the objective function did not change. This is the definition of a degenerate pivot. Degeneracy kept coming up again and again throughout this course, and often drastically increased our computation time.
After a few iterations of switching back and forth between the master and subproblem, starting with the near-optimal solution, eventually resulted in duals that the subproblem could no longer solve quickly, at roughly the same point as it reached a full rank basis. We hypothesized that the speed of the subproblem was due to the reduced size of the problem -- with only eight non-zero duals, you really only have to consider eight boards, which makes the problem much faster. And indeed when we solved the master problem with the eight near-optimal columns plus the identity matrix, the subproblem took more than a few hours to solve, supporting our hypothesis that it was the zeros in the duals which made the near-optimal instance easier to solve. So how are we ever going to get this problem to solve? We need it to give us an integer solution, and we have already seen that the LP relaxation will always give us xb values close to ½, so eventually we're going to have to put it into a branch and bound tree. Perhaps looking at the branching strategy will give us some insight, we thought.
When we started thinking about branching, little did we know we would be spending weeks and weeks on it, and what a big deal it would turn out to be. Our first step in looking at branching was to observe what happened when we branched on different variables. We initially started branching on the xb variables, since they seemed to be the ones we ultimately cared the most about -- whether or not a board was included in a cluster. Branching on the x's changed the one particular value you set to be integer, but left all the other values fractional. As soon as you set the very last x to be integer then all the y's and p's would suddenly become integer, but you need to get all the way to the bottom of the tree before you seen any major changes, which could result in 224 (in our instance with 24 boards) branches before you get a solution. 224 is a large number, and not something you want to sort through every time you want to solve the subproblem (which might need to be solved millions of times to generate an answer to the root node of the master problem, which may or may not be integer). So we decided branching on the x's was probably a bad strategy, all things considered.
If branching on the x's will not work, then what about the y's? Setting a ycl variable to be one means that component c is assigned to sleeve l, which automatically sets all other y's in that row and column to be zero, since only one component can be assigned to each sleeve and vice versa. Additionally, you also set all pclb values containing that component or location, but not in conjunction with each other, to zero. If component c is assigned to sleeve l, c cannot be retrieved from any other location for any board; likewise no other component can be retrieved from location l for any board. So branching on the y's holds promise. Unfortunately, when we look at setting a y variable to zero, almost nothing happens; we see an infinitesimally small decrease in the objective function, nothing like what you see when you set y to be one. This is a potentially hazardous branching strategy, since it results in a strongly unbalanced tree. And here again we have the same problem that we need to completely enumerate the y values before we see the x values become integer. The objective jumps the most when setting y to be one, and the least when setting y to be zero, so perhaps there is no clear cut winner. If we have to completely enumerate the y's it will only be 216 instead of 224 branches, but 216 is still too many branches for a subproblem. But what about the p's?
Setting pclb=1 has an even greater impact than setting ycl=1, since pclb=1 then sets ycl=1 with all the concomitant effects, in addition to setting xb=1, and many other pclb's to zero. Surely this is the largest impact that any one variable can have on the problem. But there are 24*216 pclb variables, and again if we look at the other side of the tree, where we set pclb=0, there is practically no change in the objective, and no other variables are set to be integer. In the end, there is no clear winner -- all the branching strategies have their flaws, and the problem was starting to look completely hopeless. Since we were not getting anywhere with this approach we decided to go back and see if we could figure out why the optimal solution was fractional and if we could fix it in any way.
We had seen earlier that the reason the LP relaxation was fractional was because the model wanted to split the boards between several different columns and then only pay for the cheapest one. We noticed, as we were working through the branching strategies, that when you set ycl to be zero, meaning that you can no longer assign component c to sleeve l, it did not change the objective value, because the model simply moved all the components that were in sleeve l and moved them to the symmetric sleeve that had the same cost as sleeve l, changing nothing, really. This means that we were branching twice as many times as we really needed to. So how can we eliminate this waste? By creating a model that assigns two components to each sleeve, and only has half as many sleeves; then you can randomly pick which of the two components goes in each of the paired sleeves because they result in equivalent costs. This lead us to the next model.
This model differs from Subproblem Model 1 only in that the sum of ycl over all components for each location is two here, instead of one as in the previous model. Everything else remains the same. Since this problem has only half as many sleeves, the number of variables is reduced by just less than half, and results in roughly half as many branches as the original, the assumption was that it would solve faster. We ran a check to make sure that both models gave us the same answer when applied to a smaller, randomly generated data set, and indeed they did, showing that the models, in their integer forms, are equivalent. Hopefully the LP relaxation of the non-symmetric model will be stronger than that of the original. Unfortunately, even with the fewer number of variables, we were still unable to get the non-symmetric model to solve with any of the variables set to be integer using the duals from the identity matrix, implying that while it may be faster, it is still not fast enough. So again, we examined potential branching strategies. Here, instead of looking at what happened near the top of the tree following a breadth-first search, we decided to see what happened near the bottom of the tree and followed a depth-first search. We confirmed that indeed none of the x values became integer until all the y values had been set. More than that, we discovered that there was an order of magnitude jump between the next-to-last branch and the very last branch where all the values suddenly became integer. This is unfortunate because it means that pruning will be nearly impossible -- none of the nodes in the branch and bound tree will ever be above the upper bound of the incumbent solution until they are themselves integer -- implying that we are still facing the potential of full enumeration to solve the subproblem. We also tried branching on the p variables, but since every time we set a p to be one we were forcing the corresponding xb and ycl variables to be one as well, we were in effect randomly picking x's, which resulted in a very poor solution. Again we saw the problem of the imbalance in the tree -- setting whichever variable you choose to be one has a far greater impact that setting that same variable to zero, so the tree is much larger on the zero side. Eventually we decided that examining the branching strategy might not actually be fruitful, and the course came full circle as we turned our energies back towards heuristics.
Heuristics are generally not considered as elegant as traditional linear programming methods, but we were hoping that we could find a heuristic that would generate negative reduced cost columns quickly. This might be an effective strategy because we do not particularly care about finding the most negative reduced cost column; we simply need a single negative reduced cost column (or many if we want to put in multiple columns at a time). The first thing we noticed was that we can automatically throw out boards with negative or zero duals, since there is no way their contribution to the reduced cost can be negative, since we still need to pay for the manufacturing. We also know that including boards with larger duals will be more likely to yield a negative reduced cost.
The first heuristic we came up with started by sorting the duals in descending order. We automatically included the two boards with the largest duals, since we know any new column will have at least two boards (since we started with the identity matrix where each board is by itself). Then, in decreasing order of the duals, we examine each board. If adding the board to the previous cluster maintains a negative reduced cost column, we add it and move on to the next board. If the board makes the reduced cost greater than zero, then we do not include the board and output the previous cluster. This would seem to be a smart heuristic because it will almost always yield a negative reduced cost column, but it fails to take into account any measure of similarity between the boards. If boards 1 and 2 have very large duals but are entirely dissimilar, it might not be a good idea to put them together into a cluster. Additionally, we discovered that this heuristic generated very similar columns at every iteration, because the duals do not change much when you start with the identity matrix. We saw other situations where the duals changed much more rapidly, so there is a chance that this heuristic would work in different circumstances; however, it is not a good bet when starting from the identity.
Our second heuristic harkened back to the random-generation code of the very beginning. We simply generated random clusters, checked their reduced cost, and saved the smallest. If the random generation failed to produce a negative reduced cost column, then we applied our first heuristic. Though we consistently produced negative reduced cost columns, the objective value was decreasing by only a little more than 1% per iteration, so using this method would take a long time to reach a near-optimal solution.
The other idea motivating us to find good initial solutions was the thought that we could then use them in AMPL as better upper bounds, allowing CPLEX to prune the branch and bound tree faster. What we discovered with both of our heuristics was that AMPL generated an integer solution with a better objective value than that of our heuristics in less than a minute, so we were not really improving the pruning by giving it our heuristic upper bound. Perhaps had we been able to generate solutions with much lower objective values it would have made a difference, but given what we had, we made no impact at all. AMPL is much smarter than we give it credit for, it would appear.
Still fixating on the issue of solving the subproblem with the duals from the identity matrix, we decided to test whether or not it was a special property of the identity matrix, or simply the fact that we had a full rank basis. So several people tried multiple randomly generated sets of 24 clusters, and found that often the clusters did not provide a feasible solution. Adding the columns from the near-optimal solution so that a feasible solution existed, however, resulted in a problem that still took just as long to solve as the one using the duals from the identity matrix. Thus, we can firmly state that the faster speeds observed using the near-optimal solution stem not from the near-optimality of the solution, but the fact that the columns do not form a full-rank basis. This exercise did not really provide any profound insight, but at least confirmed that our suspicions were correct.
So now we've decided we cannot come up with a way to generate good upper bounds (or at least upper bounds that are better than those that AMPL arrives at within minutes), we cannot find any inspiration from branching strategies, and we are sure that this is our most promising model. It is at this point that we were all grateful that this was not our dissertation research. So what do you do?
Thankfully, we had our intrepid professor to help us along, and refocus our attention on pruning. Previously, we had agreed that any board with a negative or zero dual would not be included in an optimal solution, but perhaps there was a tighter bound we could use. If we define to be the cost of manufacturing board b according to the optimal setup for cluster C, we know that
Perhaps the most intuitive pruning technique is to look at the tree and see if adding a board to a particular cluster increases the reduced cost. If so, then you can prune that branch of the tree. But this is only a valid method of pruning if you can prove that there is no other combination of boards including the original cluster, the additional board and some subset of the remaining boards which will have a lower reduced cost than that of the original cluster. In other words, can we prove that if
So we have shown that we cannot prune the tree going down, but perhaps we can gain some insight about the future of a branch based on what is left to explore. If the reduced cost of a node is positive, and the lower (or best) bound on the contribution of all the boards left to examine does not outweigh the current reduced cost, then there is no way the branch will ever be negative and we can prune it. We define the potential of a node to be the lower bound on the reduced cost contribution of the remaining boards, or , where R is the set of all remaining boards. Since this is a lower bound on the contribution of the remaining boards, if the current reduced cost plus the potential is not negative, then we can prune the current node. This will hopefully save us some time when we near the bottom of the tree. We attempted to implement this pruning scheme, but unfortunately were stymied by our lack of object-oriented programming knowledge, and we could not figure out a way to store all the data of the tree in Matlab. Here again, formulating the data structures proved to be the most difficult part of the implementation, which is itself the most difficult part of testing the idea.
With this notion of potential in hand, we are now prepared to understand the final solution, though it probably would have taken us another semester (or at least another half a semester) to arrive at it on our own. The final solution involved a slightly modified version of this potential. The potential we defined above assumes each board is manufactured according to its own optimal setup, which we know is almost guaranteed not to happen in an actual solution. So how do you take that into account? We could say that we must account for the cost of manufacturing all the remaining boards together, i.e. if we had three boards left to examine, the potential would be
Why would the algorithm take so very long to find a solution that it already had to begin with? (All instances started with an initial set of columns consisting of the identity, the exhaustive set, all 2 board clusters, all 3 board clusters, all n-2 board clusters, and all n-3 board clusters.) The answer belongs to our old friend, degeneracy. In a situation where the optimal solution is the exhaustive set, you have n-1 degenerate variables with value zero and 1 variable (the exhaustive set) with value one. The algorithm will continue to find negative reduced cost clusters, because it thinks it can make the objective value better, except all those negative reduced cost columns enter into the basis with value zero, and so have no effect on the objective value. There are nearly as many combinations of n-1 boards as there are of n boards, and you can cycle through most of them without seeing any change in the objective. The problem is that you have to find those degenerate columns and pivot them in, otherwise you will never know that you are optimal, because there will still be negative reduced cost columns out there. So how can you avoid this seemingly endless degenerate pivoting? You can't, basically, without exploiting some sort of problem structure.
In this instance, we noticed that the cost of the exhaustive column is
So we have a solution, or more rather, a way to obtain a solution. Was that not the point of the class? Well, we have a way to find a solution to the root node of the master problem, but what if the root node is fractional? Then you have to branch (and bound) to get an integer solution. Surprisingly, branching within the confines of column generation turns out to be quite challenging. The traditional way to branch takes a fractional variable, x, and says it has to be less than or equal to
Branching in this problem turns out to be much simpler than one might initially think. If we have a cluster, say boards 2 and 3, that is fractional, instead of setting x2,3 to be zero or one, we can define the branching in terms of "togetherness." If we want to set x2,3 to one, another way of saying that is we always want boards 2 and 3 together, so whenever we add board 2, we automatically add board 3 in with it. Continuing in this fashion, we can create a togetherness web -- board 1 is with board 4, boards 2 and 3 are with board 7, board 5 is not with 4, etc. And this will eventually allow us to determine our integer solution without violating the needs of the subproblem.
It has been a long, crazy journey on the road to this solution. But in this journey as in most, what is most important is not the destination, but the path taken to arrive there and the experiences gained along the way. The two most important things I learned from this course were the importance of thinking flexibly about problems, and that implementation is a pain. We explored the depths of frustration, we mounted the pinnacles of success, and saw that research is truly an up and down process -- some of the things you try work out nicely, most of them will not; and you've got to learn to roll with the punches.
On the more practical side, this course really gave us a firm handle on what exactly column generation is and what sorts of problems we can adapt to fit it. Column generation is a massively powerful technique, yet elegant in its simplicity. I very much enjoyed getting my hands dirty with the nitty gritty of it all. We learned about degeneracy, and what a pain that can be. We learned about LP relaxations and how important a good formulation can be if you ever want to solve your problem. We saw that big M's are almost always bad news, which will definitely encourage me to look for other ways to model my problems from here on out. One of the things that will really help me in the future is the ability to look at a fractional solution, and think about why it is fractional, step-by-step. Because many of the problems you model as integer programs will, in fact, turn out to be fractional, and many of them will make no sense as to why they are fractional. Being able to analyze a problem in the detail we did in this course, I think will help me a great deal in my future research.
We also saw how to exploit problem structure, and what great advantages that can give you. From now on, I will know to always look for symmetry in my models, to see if there is a way I can reduce the number of variables, and thus the complexity of my problem. We learned about branching -- what had formerly seemed like a fairly straightforward process is now even more of a mystery. I can't say that I gained any deep insight into branching strategies, except that I more fully understand their vast complexity. I did learn, however, how to critically attack branching strategies, and the process one would go through to try and evaluate them, which I think will probably be useful in the future. We saw that using a programming language without good references can be frustrating in the extreme. The majority of my AMPL questions were answered not by the manual, but by other students in the department who had wrestled with these exact same questions.1 But perhaps most importantly, we now have a whole toolbox full of skills to use when we are attacking a problem that just will not budge -- you can take the dual of the problem and see if that gets you anywhere, you can try to reformulate it using entirely different variables, you can look to exploit problem structure, you can closely examine the branching strategy and see if you can gain any insight... Many of these approaches I never would have considered before taking this course. I feel like I am much better prepared to face the frustrations and difficulties that inevitably face researchers now. (Which is not to say I am prepared, per se, simply more prepared than I was previously.)
I really wish we had someone who knew how to write a .run file that would allow us to test the non-linear model. I would be very interested to see how it behaves and if, in fact, it would get us close enough to an optimal solution that then we could throw it at the symmetric model and prove optimality. There is no real way to know without writing that .run file, but it seems like it would be so simple, and the results so fascinating. I also feel like we had some interesting intuition about other branching strategies right there at the end that might have been interesting to explore. Not that it would have been particularly fruitful (given all the other complexities of the problem) but I would think with a little more work we might be able to prove the convexity of the optimal function; it definitely seems like it should be convex, and my dreams would be sweeter knowing that it was. Additionally, I would have liked to revisit the idea of genetic algorithms to see if we could concoct a heuristic using breeding that would get us close to the optimal solution. And while I am sure there could be multiple semesters taught on Dushant's local neighborhood search mechanisms, I would have enjoyed at least a little glimpse into that world. It also would have been interesting to look at the lagrangian relaxation -- I do not know much about lagrangian relaxations, except what I saw in 518, Intro to Integer Programming, but they seemed to be powerful tools to help solve integer programs with nasty constraints. I feel like I finally have a handle on degeneracy for the first time in my life, which is rather exciting. That said, I still would have enjoyed looking at different situations where we can exploit problem structure to avoid degeneracy, since I feel that is one of the more important skills I gained from this class. People have talked about Bender's decomposition, but I had never even heard of Bender's decomposition until this class. Dantzig-Wolfe decomposition blew my mind -- I hazard to guess what Bender's decomposition would have done to me.
And that, as they say, is all she wrote.

Efficiency measurements are widely used in various industries to benchmark performance, document operational improvements, and provide other managerial information (e.g.,, Arogyaswamy and Yasai-Ardekani 1997; Westphal et al. 1997). Conversely, water utilities exist in relatively non-competitive environments, with few quantifiable operational measurements being available to compel management efficiency. This paper describes a procedure by which the efficiency of water utilities can be assessed using data envelopment analysis (DEA), a procedure widely used to provide objective numerical efficiency rankings for comparable units.
Data envelopment analysis was first described in a landmark paper by Charnes et al. (1978), and has since experienced extensive development and growth to become a ubiquitous efficiency measurement method (Seiford 1996). DEA has been used for efficiency measurements in such industries as health care (e.g. Banker et al. 1986; Ozcan et al. 1992; Kontodimopoulos and Niakas 2005); insurance (Brockett et al. 1998); agriculture (Coelli 1995; Wadud and White 2000); food processing (Jayanthi et al. 1999); and many others. DEA has been used to measure the efficiency of engineered products (Bulla et al. 2000) and has been used to guide the selection of new technologies (Baker and Talluri 1997).
Of particular interest to our work in analyzing various operational challenges facing water utilities is related work analyzing units in sectors which similarly experience extensive regulatory presence and government control. For example, DEA has been used to examine changes and policies within public school systems (Bessent and Bessent 1980) and to examine the efficiency impact of government regulatory changes on units with different ownership models, such as banks (Bhattacharyya et al. 1997). In other relevant studies, researchers have used DEA to measure the relative efficiency of government-run publicly-owned forest management districts and to estimate the potential efficiency gains from different organizational alternatives (Kao and Yang 1992).
DEA has also been used to measure efficiencies of units within the municipal infrastructure sector. Bosch et al. (2000) used the procedure to measure the efficiency of municipal waste collection services, finding that services operating within competitive environments were more efficient than services operating within monopoly environments. Worthington and Dollery (2001) used the procedure to study the efficiency of 103 municipal waste collection units, comparing inefficiency drivers between urban and rural units and between units covering various geographical scales. These investigators estimated that inputs could be reduced by 65 percent while maintaining the same level of service if best practices were used by currently inefficient units.
A few DEA studies have in fact been performed on selected aspects of potable water supply systems. Akosa et al. (1995) reported on the DEA efficiency analysis of ten water and sewage infrastructures in Ghana, a low income, West African state. Pursuant to funding agency interests, the projects had six input variables (technical, financial, economic, institutional, social, and environmental) representing such things as community input, etc., and three output variables (reliability, utilization, and convenience) representing various levels of use. Despite the high ratio of variables measured to units compared, the analysis indicated that only one unit was fully efficient when all nine input and output variables were considered. Although a limited number of compared units were compared, the investigators were able to draw relevant inferences regarding future funding efforts to optimize benefits.
Variants on DEA-measured efficiencies have been used as input to regulatory pricing structures relevant to the work described here . The British Office of Water Services (OFWAT) regulates potable water price structures with the goal of balancing inflationary pressures and efficiency gains using DEA efficiency results as input to price evaluations. Thanassoulis, for example, has published a series of papers (2000a; 2000b; 2002) on the use of DEA calculated efficiency measures to guide the pricing structure of private water and wastewater utilities in Great Britain. In the first of these paper (2000a) a single input of operating expense was employed and five outputs were considered; i.e., number of connections, length of the mains with the distribution system, total water deliveries, measured and estimated water deliveries, and number of pipe bursts. Ten utilities combining potable water and wastewater units and 22 potable-water-only utilities were included in the study, In an earlier related study performed by OFWAT only the combined utilities (those providing both water and wastewater services) were used in defining the efficiency frontier, a fact which Thanassoulis in his paper demonstrates was a potentially a flawed premise because potable-water-only utilities tended to define the efficiency frontier. Aida et al. (1998) also performed water utility measurements using DEA to compare regional water utilities utilities, but did not evaluate the effects of ownership type. Lambert et al. (1993) used DEA to compare ownership type, but limited their study to public versus private utilities. Using a single output variable measuring total water production and four input variables measuring annual labor use, total energy use, financial value of material inputs, and total value of capital, Lambert et al. concluded that public utilities were more efficient overall. Anwandter and Ozuna (2002) concluded that neither decentralization nor the presence of an independent regulator provided any benefit in the efficiency of Mexican water utilities, and concluded that competition might have had a greater role in increasing efficiency.
None of the previous studies cited (Lambert et al. 1993; Aida et al. 1998; Thanassoulis 2000a) used non-discretionary type variables for connections, network length, or water delivery, thus tacitly assuming that the utility had some measure of control over these variables. The current study builds on these previous studies by using non-discretionary variable types for variables that are not controllable by the utilities, and by efficiency comparisons of utilities based on different water sources.
Three main objectives underlie the study reported here. The first objective was to compare the relative efficiencies of different water utility ownership types calculated using different measurement variables. Three main water utility ownership types were considered; public utilities owned by local or regional government, private not-for-profit utilities operated by non-governmental agencies, and private for-profit utilities operated by private enterprise. Ancillary water utilities, those utilities run as a side operation by a larger concern, were not evaluated because of their limited financial independence.
The second objective was to compare the efficiencies of groundwater source utilities to those of surface-water source utilities using various measurement variables. Although there is clearly a geographic constraint limiting unbounded selection of water source, unbiased efficiency data could be important for evaluation of future water resources and policy regarding development.
The final objective of this paper is to discuss variable selection by presenting a brief analysis of the measurement discernment of additional variables. In this section, we assessed differences in efficiency measurements for decision-making units analyzed with increasingly discrete measurements of output components. A detailed set of variables can discern a finer resolution of the efficiency measurement than can a sparse set of variables. However, a detailed set of variables also can artificially inflate the apparent efficiency of the decision-making units and can result in an incurred cost due to the need to measure each variable.
A series of DEA trials, each with unique input and output variables, were performed to identify variables significant to the efficiency measurement of each utility type. US EPA Community Water System Survey Data (EPA, 2002) was used as input into a DEA model to determine the relative efficiencies of water utilities. Comparisons between utility types were checked using the Wilcoxon-Mann-Whitney comparison of ranked efficiency measurements. Two comparisons were performed using the efficiency data obtained from each trial. The first comparison examined efficiency differences between water utility categories of different ownership type and water source. Three main utility ownership types are represented by the EPA Survey Data: public, private not-for-profit, and private for-profit, while water source was either ground water or surface water. Efficiency measurements varied depending on selection of input and output variables, but major trends and variables having a significant influence on efficiency rankings were identified. The second comparison was an inter-ownership evaluation of utility efficiency within each utility category type using different selections of input and output variables. The results of this evaluation were then used to identify significant variables for each utility category.
DEA provides a numerical non-arbitrary score of efficiency that can be employed to improve utility operations. Efficiency rankings can be used to guide utilities to improve operational efficiencies by providing operational targets and to identify best practices at highly efficient utilities. The DEA approach can also be used to identify treatment utilities that are efficient under their particular environmental conditions but which might not be considered efficient using traditional metrics, e.g. expenditure and treatment volume. The DEA procedure requires numerical measurement data for all appropriate input and output variables. Each utility is evaluated with respect to peer utilities using unique sets of measurement variables. The DEA approach defines an efficiency frontier consisting of all fully efficient utilities, and an efficiency score is calculated for all non-efficient utilities based on their relative distance from the efficiency frontier.
The efficiency score is a non-arbitrary value based on the relative amount of inputs and outputs respectively used and produced by each utility. The DEA procedure can be either input or output oriented. An input oriented DEA model assigns the most efficient DMUs an efficiency score of one, and assigns the less efficient DMUs an efficiency score between one and zero representing the fraction of their original input they could use to still produce as much output as their peer DMUs if they were as efficient as the most efficient DMUs. An output oriented DEA model assigns the most efficient DMUs an efficiency score of one and assigns all of the other DMUs efficiency scores greater than one representing the fraction increase in output they could achieve with the same input if they were as efficient as the most efficient DMUs. Mathematically, the DEA analysis is performed by optimizing a series of linear programming equations by varying the relative weights of the measurement variables. Additional background and methodology can be found in Cooper et al. (2000).
This efficiency analysis was performed using an input-oriented non-discretionary (non-controllable) output approach. This approach was dictated by the fundamental environment of utility service requirements. Water utilities are constrained to fulfill customer requirements. They do not have the option, for instance, to reduce the number of connections within their distribution system. Since it is not reasonable to quantify water utility efficiency based on improving output for a given input, the input-oriented approach was selected.
In some cases it was not clear whether a variable is an input or an output parameter. In these cases, it was considered an output variable if an increase in its value required more efficient management ability in order to maintain all other variables constant. For instance, if two systems were the same in all aspects except total number of connections, the system having greater connections must be more efficient.
The data used in this analysis was obtained from the US EPA Community Water System Survey Data (EPA, 2002). Due to blanks entries and the presence of illogical data a significant amount of review was performed to obtain a suitable data set. Water treatment systems were removed from the set of DMUs when relevant data was missing or nonsensical. For example, systems which had either zero water delivery reported or which left this field blank were removed from the analysis.
In all cases, DMUs with data which were not obviously correctable were deleted from the analysis, even if other components of the specific DMU contained data which might have been useful. The result of removing such DMUs is that the DEA analysis might produce a conservative estimate of the efficiency frontier. All water systems with non-water revenues were removed from the analysis. Almost uniformly, systems which had non-water revenuepresent did not report an average residential bill, indicating that their production of water was an ancillary activity. Water quality data was not available and thus was not used as an efficiency ranking.
The final set of 714 utilities was comprised of 549 public, 96 private not-for-profit, and 62 private for-profit utilities, with 7 utilities either not reporting ownership information or were reported as ancillary operations without clear ownership structure. The 714 utilities were comprised of 389 utilities that used a ground water source and 325 utilities which used a surface water source.
The DEA analysis was performed using DEA-Solver Pro, an Excel add-in (Saitech, Inc., 2004) on a Dell Inspiron 5000.
The water utilities were analyzed for efficiency differences based on ownership type and water source. The three ownership types, public, private not-for-profit, and private for-profit, and the two water sources, ground and surface, had their efficiencies calculated with DEA using several variable sets. The influence of each variable set on the efficiency ranking was then determined by comparing the relative efficiency of the utilities within each utility type to the relative efficiency of the utilities within the other utility types. Comparisons between utility types were checked using the Wilcoxon-Mann-Whitney (WMW) ranked-sum comparison of the efficiency measurements. The WMW comparison was used because the underlying distribution of DEA efficiency measurements is unknown and thus a non-parametric statistic was required.
Because we used the WMW rank-sum test to compare the efficiencies of the two populations, the negative t-statistic means that we can claim the first population was generally more efficient than the second population, while a positive t-statistic means that we can claim the first population was generally less efficient than the second population. The significance level of this claim is calculated from the t-statistic using the inverse of the standard normal distribution.
The utilities were analyzed with DEA using thirty eight different sets of output variables, as shown along the right side of Figure 1-Public Versus Private not-for-profit. The three other comparisons also had figures generated for analysis but were not included due to space limitations. The output variables were selected from four main categories: age and length of distribution system and various combinations of connections and treatment volume. The age and length categories were the average age of the distribution system, and the reported length of the distribution system, respectively. For connections, the categories were total number of connections (shown as " total" on the figures), a partial separation into residential and non-residential connections (shown as "res/non-res" on the figures), and then a complete separation into residential, industrial/commercial, agriculture, and other connections (shown as "RCAO" on the figures). For volume, the categories were total flow (shown as total on the figures), a partial separation into residential and non-residential flow (shown as "res/non-res" on the figures), and then a complete separation into residential, industrial/commercial, agriculture, unaccounted for water loss, and other flow demands (shown as "RCAOU" on the figures). Each DEA efficiency analysis was performed using the entire combined set of 714 utilities of all ownership categories and used the described selection of variables to calculate an efficiency score for each utility. For each efficiency analysis, the input variables were annual expenses and average 5-year capital investments.
After the DEA efficiency analysis was performed for the combined set of ownership categories, the WMW comparison t-statistic was calculated between the utilities of each of the three ownership sub-categories: private not-for-profit versus private for-profit; private for-profit versus public; and public versus private not-for-profit, and the two water source categories, ground water versus surface water. In order to perform each WMW comparison, the efficiency data for the combined set of utilities was separated into the relevant sub-categories, and the efficiency data for the utilities in one of the sub-categories was then compared against the efficiency data for the utilities in another one of the subcategories. A t-statistic for the comparison was then calculated.
Figure 1 shows the results of efficiency comparison for the public versus private not-for-profit ownership categories. Figures for the other comparisons: private not-for-profit versus private for-profit, private for-profit versus public, and ground water source versus surface water source are not shown. The WMW t-statistic for each set of variables was plotted from largest to smallest, with a description of the variables used for each efficiency analysis shown next to each t-statistic plot. The description of the four categories of variables used in the DEA analysis is shown as a series of columns along the right side of the plot.
The ranked efficiencies of public utilities versus the private not-for-profit utilities generally showed a moderate yet statistically significant advantage of the public utilities over the private not-for-profit utilities for the majority of cases, as shown in Fig. 1. Twelve of the variable sets used for efficiency analysis indicated that private not-for-profit utilities were more efficient, while twenty-six of the variable sets used for efficiency analysis indicated that the public utilities were more efficient.
Only two of the variable sets used for efficiency analysis that showed greater efficiency from the private not-for-profit utilities had more than 90 percent significance. Similar to the previous comparison of private for-profit utilities versus public utilities, the variable sets which showed the least public utility efficiency were either distribution network length and average pipeline age, or length and age combined with total water volume. Both showed a statistically significant (greater than 90 percent) efficiency advantage of the private not-for-profit utilities over the public utilities. It should be noted that the variable set of distribution network length and average pipeline age used to compare the public utilities against the private not-for-profit utilities resulted in the most significant t-statistic of any of the variable sets used for any of the utility comparisons, with essentially 100 percent significance.
In addition, the top eight variable sets showing the greatest efficiency of the private not-for-profit utilities all used distribution network length and average pipeline age as measurement variables. In contrast, none of the top eight variable sets which indicated the greatest efficiency of the public utilities used average network length as a measurement variable and only three of the eight used average distribution network age as a measurement variable.
Eleven of the variable sets used for efficiency analysis which showed greater efficiency from the public utilities had more than 90 percent significance. All but one of these variable sets measured some combination of both network connections and water volume delivery. More than half of these cases used the most discrete measurement possible of both network connections and water volume delivery in the form of either residential, industrial/commercial, agriculture, andother connections, or residential, industrial/commercial, agriculture, unaccounted for water loss, and other flow demands. By comparison, none of the variable sets which showed greater efficiency from the private not-for-profit utilities used either connection data or water volume data at this level of detail. Thus, it appears that public water utilities are more efficient than private not-for-profit utilities at serving a variety of customer types, and are not as efficient at serving a single customer type.
The ranked efficiencies of private not-for-profit utilities versus the private for-profit utilities showed a moderate yet statistically significant advantage of the private not-for-profit utilities over the private for-profit utilities for almost all cases. Only six sets of output variables showed greater efficiency of the private for-profit utilities, with the largest difference in efficiency being when using residential and non-residential connections and residential and nonresidential treatment volume as output variables. The comparison when using the most pro-private for-profit utility variable set had a t-statistic of 0.39, indicating only a 30.6 percent chance of true difference between utilities measured using these variables, which is not a statistically significant difference. None of the variable sets which indicated that private for-profit utilities were more efficient used distribution system length as an output variable. This implies that the systems with the greatest distribution system length were the private not-for-profit systems and that they would use less input, in the form of capital investment and yearly expenses, to manage a distribution system of any particular length.
By comparison, 32 variable sets showed that the private not-for-profit utilities are more efficient than the private for-profit water utilities. The largest t-statistic showing greater private not-for-profit efficiency was -4.40, and resulted from using distribution network length and average pipeline age as output variables. This result is fairly uninteresting since water utilities are typically valued by the quantity of water they produce, measured by flow volume, and the number of customers they serve, measured by number of connections. However, the next highest t-statistic, -2.12, resulted from using distribution network length, average pipeline age, and total treatment volume as output variables, and was statistically significant with a 96.6 percent probability of difference. Three more comparisons also demonstrated that the private not-for-profit utilities are more efficient than the private for-profit water utilities with greater than 90 percent significance.
The ranked efficiencies of private for-profit utilities versus public utilities showed a strong, statistically significant advantage of the public utilities over the private for-profit utilities for all but two cases. The only variable sets used for efficiency analysis which resulted in private for-profit utilities being more efficient than public utilities were when using distribution network length and average pipeline age, either alone or combined with total water volume, with 99.6 and 67.6 percent significance respectively. Every variable set which used some measure of connections within the distribution system to measure efficiency resulted in the public utilities being evaluated as more efficient than the private for-profit utilities. There were 25 variable sets used to measure efficiency which indicated greater efficiency of public utilities over private for-profit utilities with greater then 90 percent significance. None of the top ten variable sets which showed the greatest public utility efficiency used distribution system length as a measurement variable. However, 13 of the data sets which used distribution system length still showed greater public utility efficiency with significance greater than 90 percent. It appears that using distribution system length as a measurement variable tends to moderately decrease the efficiency of private for-profit water utilities compared to public water utilities, but the effect isn't enough to completely overcome the efficiency effects of the rest of the variables. The use of completely separated connection or flow variables tended to push the efficiencies towards the public utilities.
The ranked efficiencies of ground water utilities versus surface water utilities generally showed a slight, yet statistically significant advantage of the ground water utilities over the surface water utilities for the majority of cases, as shown in Fig. 4. Overall, there were twelve variable sets that demonstrated an efficiency advantage to surface water utilities while there were twenty-six variables sets that showed efficiency advantage to ground water utilities. However, eight of the variable sets that showed a surface water utility advantage and nine of the variable sets that showed a ground water utility advantage had less than 50 percent significance, indicating that there was a greater than 50-50 chance that these data sets were identical.
Only one variable set used for efficiency measurement that demonstrated an efficiency advantage to surface water utilities had greater than 90 percent significance, while nine variables sets that showed efficiency advantage to ground water utilities had greater than 90 percent significance.
The only statistically significant variable set which indicated surface water utility efficiency advantage was when total water delivery was used as the sole measurement variable. Adding additional measurement variables which would account for either distribution system length, or numbers or types of customers, all caused a reduction in surface water utility efficiency and an increase in ground water utility efficiency. This implies that surface water utilities are most efficient when they have a few, high-volume, customers such as irrigation or large industrial demands.
However, another data trend indicates potentially contrasting results, and that is when the water delivery category was fully broken down into the component variables of residential, commercial/industrial, agricultural, other, and unaccounted for water deliveries. DEA efficiency measurements which used these variables tended to show a moderate trend towards ground water utilities compared to surface water utilities, with seven of the measurements indicating the superior efficiency of surface water utilities, and only three indicting the efficiency of ground water utilities. This result does not indicate a strong trend, since none of the results were at high levels of significance, but does indicate a mild trend towards the efficiency of surface water utilities over ground water utilities when delivering a lot of water to a wide variety of customers.
Since tracking and maintaining information variables entails a cost, it is reasonable to discuss variable selection criteria. The essential question is: What variables reasonably add new information? As part of this study, we briefly investigated the additional information provided as a result of additional measurement variables by using an informational surrogate which measured the informational spread in efficiency ranking obtained when using additional variables. Informational spread γ was defined as
(1)
where a and b are the utility rankings using two sets of measurement variables A and B. Spread approximates the difference in informational content measured by two variable sets because it measures the absolute difference in efficiency ranking due to the change in measurement variables. Large changes in efficiency ranking between variable sets imply a large informational difference between the variable sets and would result in a large calculated spread. Correspondingly, small or random changes in efficiency ranking between variable sets imply a small informational difference between the variable sets and would result in a small calculated spread.
For this investigation, the information spread between different measurement variables was determined for a series of variable sets that were kept constant for all but one measurement category. Each variable set used for efficiency measurement consisted of distribution network length, average pipeline age, and variables from both the connection and volume categories. All the variables measuring length, age, and one of the two remaining categories were kept constant, and the base efficiency scores for the utilities were determined by excluding the remaining measurement category. The remaining variable category then was increased to a single value representing the total value for that category, then to a partial separation into the residential and non-residential values for that category, and then to a complete separation for that category. The informational spread was calculated between the basic case, which did not include the varying category, and between the more advanced cases, which did include the varying category. The informational spread was plotted against the number of new variables added for each case, as shown in Fig. 2. The legend shows the number of variables in the basic case, while the x-axis shows the number of additional variables added for each analysis. There is clearly a trend towards marginal returns as the variable categories are broken down into more discrete measurement. The only outlier is the line representing the increase in volume measurement when length, age, and RCAO connections were kept constant. This plot reveals the decreasing marginal return on information gained from using a more discrete measurement of any particular data category.
The work reported here reveals a distinct efficiency advantage of public utilities over private for-profit utilities. Every analysis that used some measure of number of connections within a distribution system resulted in public utilities being evaluated as more efficient than private for-profit utilities. In comparing public utilities to private not-for-profit utilities, a much more moderate yet statistically significant efficiency advantage was evident for the former in the majority of cases studied. None of the variable set cases that showed greater efficiency for private not-for-profit utilities used either connection data or water volume data in full detail. The results also indicate that while public water utilities are more efficient than private not-for-profit utilities for serving a variety of customer types, the latter are more efficient for serving a single customer type. Private not-for-profit utilities were found to have a statistically significant efficiency advantage over private for-profit utilities for almost all selections of management variables, particularly for managing larger distribution networks.
Comparisons of ground water source utilities versus surface water source utilities generally showed a slight, yet statistically significant, efficiency advantage for the former in the majority of cases studied. Utilities employing surface-water sources are most efficient when they serve a few, high-volume demand consumers, such as irrigation or large industrial systems, while ground-water source utilities tend to be more efficient when delivering large volumes of water to a wide variety of different types of consumers..
Finally, informational spread behavior as a function of measurement variables employed indicates decreasing marginal returns on information gained from using more discrete measurements of any particular data category.
The authors wish to acknowledge Lawrence M. Seiford, Chairman of the Department of Industrial and Operations Engineering, Warren Sutton, doctoral candidate in the Department of Industrial and Operations Engineering, and Jill Ostrowski, undergraduate in the Department of Civil and Environmental Engineering, for their critical appraisal and evaluation of the manuscript.

